{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import duckdb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to Duckdb and selecting onyl 3 columns with query also limiting the number of rows for 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../duck_db/isrecon_AIS11.duckdb'\n",
    "\n",
    "with duckdb.connect(database=db_path, read_only=True) as conn:\n",
    "    query = 'SELECT article_id, title, abstract FROM papers LIMIT 500'\n",
    "    df = conn.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Examining interdependence between product user...</td>\n",
       "      <td>Firm-sponsored online user communities have be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Computer support for strategic organizational ...</td>\n",
       "      <td>While information systems continue to be promo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Essence: facilitating software innovation</td>\n",
       "      <td>This paper suggests ways to facilitate creativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The dark side of data ecosystems: A longitudin...</td>\n",
       "      <td>Data are often vividly depicted as strategic a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Symbolic Action Research in Information System...</td>\n",
       "      <td>An essay is presented as an introduction to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0           1  Examining interdependence between product user...   \n",
       "1           2  Computer support for strategic organizational ...   \n",
       "2           3          Essence: facilitating software innovation   \n",
       "3           4  The dark side of data ecosystems: A longitudin...   \n",
       "4           5  Symbolic Action Research in Information System...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Firm-sponsored online user communities have be...  \n",
       "1  While information systems continue to be promo...  \n",
       "2  This paper suggests ways to facilitate creativ...  \n",
       "3  Data are often vividly depicted as strategic a...  \n",
       "4  An essay is presented as an introduction to th...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate columns together into a list. We do this beacuse in the vector database we cannot store the vectores in column, meaning it does not have tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = (df['title'] + ' ' + df['abstract']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Examining interdependence between product users and employees in online user communities: The role of employee-generated content Firm-sponsored online user communities have become product innovation and support hubs of strategic importance to firms. Product users and host firm employees comprise the participants of firm-sponsored online user communities. The online user community provides a forum wherein the product users and firm employees discuss questions, problems or issues resulting from the use of host firms’ products. Extant research on online user communities has largely focused on either product users or employees and has examined the various dynamics that ensue from each entity’s community participation. This paper seeks to investigate the interdependence between the two entities in the communities and, in particular, how product users’ reading of employee-generated content influences subsequent knowledge contribution by product users as well as employees. Analyzing data from an online user community over a two-year period, our study shows that employees whose content is read by product users generate additional content and product users who read employee content themselves contribute more knowledge to the community. Thus, the reading of content is not entirely a passive, individual action that only affects the reader. On the contrary, reading sparks additional knowledge contribution by the reader and having readers sparks additional knowledge contribution by the original source of the content, thereby creating a sustainable online user community.', 'Computer support for strategic organizational decision-making While information systems continue to be promoted within organizations as tools to support strategic decision-making, there is growing concern over the ability of such systems to model the social and political complexity of the situations to which they are being applied. This paper examines the nature of organizational decision-making and the use of computer-based systems to support this activity. The debate queries the extent to which such artifacts should be allowed to become enmeshed and embedded within the strategic decision-making activities of organizations which operate within increasingly complex environments.', 'Essence: facilitating software innovation This paper suggests ways to facilitate creativity and innovation in software development. The paper applies four perspectives – Product, Project, Process, and People – to identify an outlook for software innovation. The paper then describes a new facility – Software Innovation Research Lab (SIRL) – and a new method concept for software innovation – Essence – based on views, modes, and team roles. Finally, the paper reports from an early experiment using SIRL and Essence and identifies further research.', 'The dark side of data ecosystems: A longitudinal study of the DAMD project Data are often vividly depicted as strategic assets that organisations can (re)use to create value for myriad purposes. However, the same qualities that make data so appreciated – that is, their volume, their value for a plurality of stakeholders and their indefinite reuse capacity – also have a dark side: data reuse can lead to deviant data use that undermines the legitimacy of data analytics initiatives. To investigate this dynamic, we build on the notion of data ecosystems and provide empirical evidence from a longitudinal, 15-year case study of the emergence, expansion and eventual collapse of a large-scale data analytics project – called DAMD – in the Danish health care sector. We demonstrate that data reuse in the evolving data ecosystem elicited a data reuse dark side that was so dominant that it eventually resulted in the project’s demise. We conceptualise the reuse dark side’s three major mechanisms as function creep, stakeholder creep and data creep. Based on these insights, we develop an empirically grounded Data Analytics Ecosystem Model that extends the current understanding of data ecosystems and provides a view of these ecosystems as having both a bright and a dark side.', 'Symbolic Action Research in Information Systems: Introduction to the Special Issue1 An essay is presented as an introduction to the special issue that focuses on symbolic action research in information systems and highlights papers in the issue that offer expanded research of symbolic action research within Information Systems (IS) discourse. It discusses addressing the tendency to view communicative processes in information systems as psychological processes and suggests the need for better design theories.', 'The Making of Data Commodities: Data Analytics as an Embedded Process This paper studies the process by which data are generated, managed, and assembled into tradable objects we call data commodities. We link the making of such objects to the open and editable nature of digital data and to the emerging big data industry in which they are diffused items of exchange, repurposing, and aggregation. We empirically investigate the making of data commodities in the context of an innovative telecommunications operator, analyzing its efforts to produce advertising audiences by repurposing data from the network infrastructure. The analysis unpacks the processes by which data are repurposed and aggregated into novel data-based objects that acquire organizational and industry relevance through carefully maintained metrics and practices of data management and interpretation. Building from our findings, we develop a process theory that explains the transformations data undergo on their way to becoming commodities and shows how these transformations are related to organizational practices and to the editable, portable, and recontextualizable attributes of data. The theory complements the standard picture of data encountered in data science and analytics, and renews and extends the promise of a constructivist Information Systems (IS) research into the age of datafication. The results provide practitioners, regulators included, vital insights concerning data management practices that produce commodities from data.', 'Everything counts in large amounts: a critical realist case study on data-based production Contemporary digital ecosystems produce vast amounts of data every day. The data are often no more than microscopic log entries generated by the elements of an information infrastructure or system. Although such records may represent a variety of things outside the system, their powers go beyond the capacity to carry semantic content. In this article, we harness critical realism to explain how such data come to matter in specific business operations. We analyse the production of an advertising audience from data tokens extracted from a telecommunications network. The research is based on an intensive case study of a mobile network operator that tries to turn its subscribers into an advertising audience. We identify three mechanisms that shape data-based production and three properties that characterize the underlying pool of data. The findings advance the understanding of many organizational settings that are centred on data processing.', 'Building nation-wide information infrastructures in healthcare through modular implementation strategies Initiatives that seek to realize the vision of nation-wide information infrastructures (II) in healthcare have often failed to achieve their goals. In this paper, we focus on approaches used to plan, conduct, and manage the realization of such visions. Our empirical material describes two Danish initiatives, where a national project failed to deliver interoperable Electronic Patient Record (EPR) systems while a small, local solution grew and now offers a nation-wide solution for sharing patient record information. We apply II theory, specifically the five design principles proposed by Hanseth and Lyytinen, to contrast the organization and implementation strategies of the two projects. Our findings highlight how implementation strategies differ with respect to how stakeholders are mobilized. We argue that the realization of nation-wide IIs for healthcare not only requires a gradual transition of the installed base, which current II theory advocates. Here we articulate and exemplify a modular implementation strategy as an approach that also addresses the challenges related to mobilization and organization of multiple stakeholders.', \"Collective mindfulness in post-implementation IS adaptation processes The organizational consequences of implementing information systems (IS) in organizations have primarily been studied during the implementation or early post-implementation phase. We argue for the need to study the continuous organizational adaptation of evolving IS because of the challenges such processes pose for users, as well as the organizational capabilities they demand. We report from a qualitative study in a hospital setting in which a scanning project was initiated two years after the initial implementation of an Electronic Health Record system. The project was initially conceived to be minor, but led to thorough redesign of work processes and routines. We give a detailed account of the challenges encountered and the actions taken as part of the users' sensemaking in this project. By describing how the making, giving, demanding, specification, and breaking of sense were carried out, we identify the way in which the organizational capability we call “collective mindfulness” was achieved. Being aware of how to practically achieve collective mindfulness, managers may be able to better facilitate mindful handling of post-implementation IS adaptation processes.\", 'Infrastructuring Work: Building a State-Wide Hospital Information Infrastructure in India Information and communication technologies that strengthen knowledge-based governance in low and middle-income countries (LMIC) will affect work processes and organizations on a massive scale. This paper draws attention to demands on public sector organizations in resource-constrained contexts that face different challenges than in high-income societies. This paper from the Indian public healthcare sector reports on design, development, implementation, and scaling of a free and open-source software-based hospital information system for district hospitals. The paper focuses on the implications for work, competencies, and organization, building on and extending the concepts of “automate” and “informate.” The paper focuses on the emerging and recursive interplay between information infrastructure and work within the context of organizational realities of a district hospital in an LMIC context, captured by the concepts of “infrastructuring of work” and “work of infrastructuring.”', \"Scanning World Wide Web documents with the vector space model The vector space model used in Information Retrieval is combined with discriminant analysis to provide an automated WWW environment scanning system to detect signals of interest to an organization. The vector space model converts text-based information to numerical vectors that are then used in discriminant analysis. We illustrate the methodology using news articles pertaining to a predefined randomly selected set of stocks to test whether they provide predictive signals on whether the stock's return will increase or decrease relative to the market in the target period following the report or whether the stock's trading volume will increase or decrease.\", \"A new dynamic integrated framework for surgical patients' prioritization considering risks and uncertainties This study reviews current patients' prioritization systems and presents an innovative integrated three-step decisional framework in an attempt to overcome their limitations. In its first step, the proposed framework encompasses fuzzy logic, analytic hierarchy process (AHP) to formalize stakeholders' goals and objectives. In the second step, the assessments made on each patient's condition are integrated by data envelopment analysis (DEA) and compared among them by a min–max regret approach (MRA) to obtain a primary prioritization of patients. The third step uses the delay ratio, the risk criteria score, and a profile matrix to introduce dynamic aspects related to the evolution of patients' condition and changes in the patient's list to the prioritization process. This three-step framework not only considers the surgery team members' opinions but also considers the patient's opinions in the decision-making process. The new framework has been implemented in the Orthopedic Surgery Ward, Shohada University Hospital, Iran, showing very promising results and advantages.\", 'Metafraud: A Meta-Learning Framework for Detecting Financial Fraud Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies. In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.', 'Stylometric Identification in Electronic Markets: Scalability and Robustness Online reputation systems are intended to facilitate the propagation of word of mouth as a credibility scoring mechanism for improved trust in electronic marketplaces. However, they experience two problems attributable to anonymity abuse—easy identity changes and reputation manipulation. In this study, we propose the use of stylometric analysis to help identify online traders based on the writing style traces inherent in their posted feedback comments. We incorporated a rich stylistic feature set and developed the Writeprint technique for detection of anonymous trader identities. The technique and extended feature set were evaluated on a test bed encompassing thousands of feedback comments posted by 200 eBay traders. Experiments conducted to assess the scalability (number of traders) and robustness (against intentional obfuscation) of the proposed approach found it to significantly outperform benchmark stylometric techniques. The results indicate that the proposed method may help militate against easy identity changes and reputation manipulation in electronic markets.', \"Cybergate: A Design Framework and System for Text Analysis of Computer-Mediated Communication Content analysis of computer-mediated communication (CMC) is important for evaluating the effectiveness of electronic communication in various organizational settings. CMC text analysis relies on systems capable of providing suitable navigation and knowledge discovery functionalities. However, existing CMC systems focus on structural features, with little support for features derived from message text. This deficiency is attributable to the informational richness and representational complexities associated with CMC text. In order to address this shortcoming, we propose a design framework for CMC text analysis systems. Grounded in systemic functional linguistic theory, the proposed framework advocates the development of systems capable of representing the rich array of information types inherent in CMC text. It also provides guidelines regarding the choice of features, feature selection, and visualization techniques that CMC text analysis systems should employ. The CyberGate system was developed as an instantiation of the design framework. CyberGate incorporates a rich feature set and complementary feature selection and visualization methods, including the writeprints and ink blots techniques. An application example was used to illustrate the system's ability to discern important patterns in CMC text. Furthermore, results from numerous experiments conducted in comparison with benchmark methods confirmed the viability of CyberGate's features and techniques. The results revealed that the CyberGate system and its underlying design framework can dramatically improve CMC text analysis capabilities over those provided by existing systems.\", 'Data Science for Social Good  Data science has been described as the fourth paradigm of scientific discovery. The latest wave of data science research, pertaining to machine learning and artificial intelligence (AI), is growing exponentially and garnering millions of annual citations. However, this growth has been accompanied by a diminishing emphasis on social good challenges-our analysis reveals that the proportion of data science research focusing on social good is less than it has ever been. At the same time, the proliferation of machine learning and generative AI has sparked debates about the sociotechnical prospects and challenges associated with data science for human flourishing, organizations, and society. Against this backdrop, we present a framework for \"data science for social good\" (DSSG) research that considers the interplay between relevant data science research genres, social good challenges, and different levels of sociotechnical abstraction. We perform an analysis of the literature to empirically demonstrate the paucity of work on DSSG in information systems (and other related disciplines) and highlight current impediments. We then use our proposed framework to introduce the articles appearing in the JAIS special issue on data science for social good. We hope that this editorial and the special issue will spur future DSSG research and help reverse the alarming trend across data science research over the past 30-plus years in which social good challenges are attracting proportionately less attention with each passing day. ', 'The Phishing Funnel Model: A Design Artifact to Predict User Susceptibility to Phishing Websites Phishing is a significant security concern for organizations, threatening employees and members of the public. Phishing threats against employees can lead to severe security incidents, whereas those against the public can undermine trust, satisfaction, and brand equity. At the root of the problem is the inability of Internet users to identify phishing attacks even when using anti-phishing tools. We propose the phishing funnel model (PFM), a design artifact for predicting user susceptibility to phishing websites. PFM incorporates user, threat, and tool-related factors to predict actions during four key stages of the phishing process: visit, browse, consider legitimate, and intention to transact. We used a support vector ordinal regression with a custom kernel encompassing a cumulative-link mixed model for representing users’ decisions across funnel stages. We evaluated the efficacy of PFM in a 12-month longitudinal field experiment in two organizations involving 1,278 employees and 49,373 phishing interactions. PFM significantly outperformed competing models/methods by 8%–52% in area under the curve, correctly predicting visits to high-severity threats 96% of the time—a result 10% higher than the nearest competitor. A follow-up three-month field study revealed that employees using PFM were significantly less likely to interact with phishing threats relative to comparison models and baseline warnings. Furthermore, a cost-benefit analysis showed that interventions guided by PFM resulted in phishing-related cost reductions of nearly $1,900 per employee more than comparison prediction methods. These results indicate strong external validity for PFM. Our findings have important implications for practice by demonstrating (1) the effectiveness of predicting user susceptibility to phishing as a real-time protection strategy, (2) the value of modeling each stage of the phishing process together, rather than focusing on a single user action, and (3) the considerable impact of anti-phishing tool and threat-related factors on susceptibility to phishing.', 'Don’t Mention It? Analyzing User-Generated Content Signals for Early Adverse Event Warnings With greater impetus on broad postmarket surveillance, the Voice of the Customer (VoC) has emerged as an important source of information for understanding consumer experiences and identifying potential issues. In organizations, risk management groups are increasingly interested in working with their information technology teams to develop robust VoC listening platforms. Two key challenges have impeded success. First, prior work has leveraged diverse sets of channels, adverse event types, and modeling methods, resulting in diverging conclusions regarding the viability and efﬁcacy of various user-generated channels and accompanying modeling methods. Second, many existing detection methods rely on “mention models” that have low detection rates, have high false positives, and lack timeliness. Following the information systems design science approach, in this research note we propose a framework for examining key design elements for VoC listening platforms. As part of our framework, we also develop a novel heuristic-based method for detecting adverse events. We evaluate our framework and method on two large test beds each encompassing millions of tweets, forums postings, and search query logs pertaining to hundreds of adverse events related to the pharmaceutical and automotive industries. The results shed light on the interplay between user-generated channels and event types, as well as the potential for more robust event modeling methods that go beyond basic mention models. Our analysis framework reveals that user-generated content channels can facilitate timelier detection of adverse events: on average, two to three years or earlier than commonly used databases. The inclusion of negative sentiment polarity in the models can further reduce false-positive rates. Additionally, we ﬁnd social media channels provide higher detection rates but lower precision than do search-based signals. The search and web forum channels are timelier than Twitter. The proposed heuristic-based method attains markedly better results than do existing methods—with earlier detection rates of 50%–80% and far fewer false positives across an array of VoC channels and event types. The heuristic method is also well suited for signal fusion across channels. Our note makes several contributions to research. The results also have important implications for various practitioner groups, including regulatory agencies and risk management teams at product manufacturing ﬁrms.', \"Big Data Research in Information Systems: Toward an Inclusive Research Agenda Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data's characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data's disruptive effects.\", 'Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools’ warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the “what” but rather on operationalizing the “why” into salient detection cues.', 'Detecting Fake Websites: The Contribution of Statistical Learning Theory Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for e-commerce and online security.', 'Text Analytics to Support Sense-Making in Social Media: A Language-Action Perspective Social media and online communities provide organizations with new opportunities to support their businessrelated functions. Despite their various benefits, social media technologies present two important challenges for sense-making. First, online discourse is plagued by incoherent, intertwined conversations that are often difficult to comprehend. Moreover, organizations are increasingly interested in understanding social media participants’ actions and intentions; however, existing text analytics tools mostly focus on the semantic dimension of language. The language-action perspective (LAP) emphasizes pragmatics; not what people say but, rather, what they do with language. Adopting the design science paradigm, we propose a LAP-based text analytics framework to support sense-making in online discourse. The proposed framework is specifically intended to address the two aforementioned challenges associated with sense-making in online discourse: the need for greater coherence and better understanding of actions. We rigorously evaluate a system that is developed based on the framework in a series of experiments using a test bed encompassing social media data from multiple channels and industries. The results demonstrate the utility of each individual component of the system, and its underlying framework, in comparison with existing benchmark methods. Furthermore, the results of a user experiment involving hundreds of practitioners, and a four-month field experiment in a large organization, underscore the enhanced sense-making capabilities afforded by text analytics grounded in LAP principles. The results have important implications for online sense-making and social media analytics.', 'Everywhere and nowhere: nearshore software development in the context of globalisation† Offshore software development has been identified as one of the most striking manifestations of contemporary globalisation and as evidence of placelessness, the idea that information and communication technologies have rendered location irrelevant. Research in the International Business and Information Systems fields, in contrast, has suggested that all locations are not equal and has identified a number of characteristics that may influence the attractiveness of a location for multinational investment and offshoring, respectively. These literatures, however, focus almost exclusively on quantitative, economic characteristics that are seen as fixed and applying uniformly throughout a whole country. They therefore offer little guidance on the suitability of particular locations as offshoring destinations, especially in countries without a track record in offshore software development. Drawing on two cases of nearshore software development centres set up by offshore service providers in the Caribbean, this paper illustrates that, while the initial decision to establish the ventures reflected a logic of placelessness, characteristics of these particular locations affected their subsequent success. Through the findings, we therefore develop a typology of espoused, unanticipated and remediable locational characteristics, which illustrates that locational attractiveness may vary significantly within countries and that offshore service providers and government agencies can modify locational characteristics to their advantage.', 'From boundary spanning to creolization: A study of Chinese software and services outsourcing vendors In achieving success in global sourcing arrangements, the role of a cultural liaison, boundary spanner or transnational intermediary is frequently highlighted as being critical. This paper critiques, builds upon and synthesizes relevant streams of ideas in relation to boundary-spanning and cross-cultural management across a number of disciplines, and constructs a multi-layered creolization framework, encompassing processes at the individual, intra- and inter-organizational and inter-national levels which, we argue, are entangled and interrelated. Viewed as a vital and innovative phenomenon, creolization embodies the interactive, contentious and creative processes of network expansion, mutual sensemaking, cultural hybridity and identity multiplicity. Qualitative empirical data from the software and services outsourcing industry in Northwest China is used to demonstrate the complexity of cross-cultural practices in offshore collaborations and illustrate creolization processes. Potentials for theoretical development are outlined and implications for cross-cultural practices are discussed.', \"Scad-elastic net and the estimation of individual tourism expenditure determinants This paper introduces the use of scad-elastic net in the assessment of the determinants of individual tourist spending. This technique approaches two main estimation-related issues of primary importance. So far studies of tourism literature have made a wide use of classic regressions, whose results might be affected by multicollinearity. In addition, because of the absence of robust economic theory on tourism behavior, regressor selection is often left to researcher's choice when not driven by non-optimal automatic criteria. Scad-elastic net is an OLS model that accounts for both these problems by including two types of parameters constraints, namely the smoothly clipped absolute deviation (scad) and the ℓ2-norm. We analyze an official dataset of incoming tourists to Uruguay. Socio-demographic, psychographic and trip-related variables are used as explanatory of per capita per day tourist expenditure. Significant impact on tourism expenditure of some accommodation facilities such as expensive one and second dwellings, personal experiences rather than the number of past visits, doing certain activities, place of stay, and seasonality are the main conclusions that are drawn from the analysis.\", 'Using semiotics to analyze representational complexity in social media Data from social media offer us multimedia data brimming with multiple layers of meanings. Social media enable rapid-fire digital communications. These communications are incredibly complex in content, form and meaning. This representational complexity is a stumbling block in data analysis that stands in the way of deeper explanations. These unstructured data, rich in social meanings, are as complex as the phenomena they represent. While it is possible to formulate an entire research methodology around semiotics, it is not always necessary. We can adapt semiotic analysis within existing methodologies. This paper offers and illustrates an analytical technique to address representational complexity that can be used in conjunction with other methodologies such as case study, ethnography, etc. This analytical technique espouses a critical realist philosophy to develop much needed, deeper explanations from qualitative data.', 'Examining the case of French hesitancy toward IDaaS solutions: Technical and social contextual factors of the organizational IDaaS privacy calculus Identity-as-a-service (IDaaS) is a cloud security service to which companies can outsource the identity and access management (IAM) functions that administer their employee’s access to organizational resources. Engaging with the information systems (IS) privacy literature, our qualitative analysis develops a framework for an organiza\\xad tional privacy calculus that informs French organizational consumers’ decisions to pursue IDaaS solutions. We collect data from employees of a multinational IDaaS provider operating in Europe but headquartered in the US. Our case study reveals the organizational privacy calculus associated with transferring control of a primary security control to a multinational cloud service provider.', 'The Impact of Goals on Software Project Management: An Experimental Investigation  Over the last three decades, a significant stream of research in organizational behavior has established the importance of goals in regulating human behavior. The precise degree of association between goals and action, however, remains an empirical question since people may, for example, make errors and~or lack the ability to attain their goals. This may be particularly true in dynamically complex task environments, such as the management of software development.To date, goal setting research in the software engineering field has emphasized the development of Robert Zmud was the accepting, senior editor for this paper. tools to identify, structure, and measure software development goals. In contrast, there has been little microempirical analysis of how goals affect managerial decision behavior. The current study attempts to address this research problem. It investigated the impact of different project goals on software project planning and resource allocation decisions and, in turn, on project performance. The research question was explored through a role-playing project simulation game in which subjects played the role of software project managers. Two multigoal structures were tested, one for cost~schedule and the other quality/schedule. The cost~schedule group opted for smaller cost adjustments and was more willing to extend the project completion time. The quality/schedule group, on the other hand, acquired a larger staff level in the later stages of the project and allocated a higher percentage of the larger staff level to quality assurance. A cost/schedule goal led to lower cost, while a quality/schedule goal led to higher quality. These findings suggest that given specific software project goals, managers do make planning and resource allocation choices in such a way that will meet those goals. The implications of the results for project management practice and research are discussed. Introduction and Motivation mManagers widely rely on goal setting as a motivational technique to regulate task performance, ', \"The Economics of Software Quality Assurance: A Simulation-Based Case Study  Software quafity assurance (QA) is a critical function in the successful development and maintenance of software systems. Because the QA activity adds significantly to the cost of developing software, the cost-effectiveness of QA has been a pressing concern to software quality managers. As of yet, though, this concern has not been adequately addressed in the fiterature.The objective of this article is to investigate the tradeoffs between the economic benefits and costs of QA. A comprehensive system dynamics model of the software development process was developed that serves as an experimentation vehicle for QA poficy. One such experiment, involving a NASA software project, is discussed in detail. In this experiment, the level of QA expenditure was found to have a significant impact on the project's total cost. The model was also used to identify, the optimal QA expenditure level and its distribution throughout the project's lifecycle. \", 'How AI-Based Systems Can Induce Reflections: The Case of AI-Augmented Diagnostic Work  This paper addresses a thus-far neglected dimension in human-artificial intelligence (AI) augmentation: machine-induced reflections. By establishing a grounded theoretical-informed model of machine-induced reflection, we contribute to the ongoing discussion in information systems (IS) regarding AI and research on reflection theories. In our multistage study, physicians used a machine learning-based (ML) clinical decision support system (CDSS) to see if and how this interaction can stimulate reflective practice in the context of an X-ray diagnosis task. By analyzing verbal protocols, performance metrics, and survey data, we developed an integrative theoretical foundation to explain how ML-based systems can help stimulate reflective practice. Individuals engage in more critical or shallower modes depending on whether they perceive a conflict or agreement with these CDSS systems, which in turn leads to different levels of reflection depth. By uncovering the process of machine-induced reflections, we offer IS research a different perspective on how such AI-based systems can help individuals become more reflective, and consequently more effective, professionals. This perspective stands in stark contrast to the traditional, efficiency-focused view of MLbased decision support systems and also enriches theories on human-AI augmentation. ', \"The Impact of Computer Alienation on Information Technology Investment Decisions: An Exploratory Cross-National Analysis  Organizations in both developed and developing countries use information technology to support their operational, tactical, and strategic  processes (cf., Bogod, 1979;Cooper and Zmud, 1990). Any strategic competitive advantage of information technology, however, is contingent on acquisition and assimilation of information technology products and applications into organizational processes. Using a value expectancy approach, this study proposes an expanded model to examine the variables that correlate with information technology investment decisions. The theory of alienation from social psychology is used as a basis to systematically define and measure decision makers' attitudes and internal beliefs toward information technology in an investment context. Detailed discussion of the development of a computer alienation measurement scale is presented. The scale was used to collect data from 97 decision makers in the United States, a developed country, and Saudi Arabia, a developing country. Results provide empirical evidence on the appropriateness of applying the computer alienation construct to computer purchase decisions. Computer-alienated decision makers were found to be more inclined to resist information technology adoption by refraining from buying computers. This resistance was evident in both the U.S. and the Saudi samples. The study findings also indicate that decision-maker computer knowledge, computer experience, and education level are closely associated with alienated beliefs and attitudes toward information technology. Alienated decision makers reported paying less attention to information technology information sources. Assuming technologies can provide advantages, these findings point to the need for change agents to minimize afienating befiefs and attitudes. \", 'Policies for a single-vendor multi-buyer system with finite production rate We deal with a multi-echelon inventory system in which one vendor supplies an item to multiple buyers. The vendor produces the item at a finite rate and customer demand occurs at each buyer at a constant rate. There is a holding cost per unit stored per unit time at the vendor and at each buyer. Each time a production is carried out the vendor incurs a setup cost. Moreover, placing an order at a buyer entails a fixed ordering cost. Shortages are not allowed. The goal is to determine the order quantities at the buyers and the production and shipment schedule at the vendor in order to minimize the average total cost per unit time. We formulate the problem in terms of integer-ratio policies and we develop a heuristic procedure. We also show how the problem should be addressed in case of independence among the vendor and the buyers. Both solution procedures are illustrated with a numerical example. Finally, we present the results of a numerical study which illustrates the performance of the heuristic for computing integer-ratio policies. Additionally, we compare the integer-ratio policies with the decentralized policies, and a sensitivity analysis of parameters is also reported.', \"Inventing Together: The Role of Actor Goals and Platform Affordances in Open Innovation  With the ubiquity of the internet and social media platforms, open innovation (OI) opportunities now extend to individuals with creative ideas and interests in innovation. Understanding why individuals are willing to engage in open innovation and how their diverse goals affect their participation is important for assessing the viability of various OI models and to inform platform design. In this paper, we develop a theoretical model that examines the impact of three categories of human goalsextrinsic, intrinsic, and internalized extrinsic--on actors' continuous intentions to participate in three general categories of open innovation behaviors: ideation, collaboration, and socialization. The model also considers how perceived platform participation affordances mediate the influence of goals on these innovation behaviors. We validate this goals-affordances-behavior model via a field survey of participants on a social product development (SPD) platform. By theorizing and empirically examining how goals influence participation in the SPD context, our study advances knowledge about open innovation behaviors, provides a foundation for future research across various OI models, and highlights practical insights for OI platform design.Likoebe Maruping was the accepting senior editor. This research article was submitted on March 19, 2018 and underwent three revisions.Marisa has a million ideas but only a few minutes to spare. She had an idea for a brand-new product her kids would love, so-naturally-she shared it on the Quirky invention platform. Talented renderers, sketchers, and toy enthusiasts in our community helped strengthen her idea submission. In turn, she shared some of her Influence (i.e., a cut of the product revenue) with the people that helped out the most.-Quirky.com \", 'Business Models in the Sharing Economy: Manufacturing Durable Goods in the Presence of Peer-to-Peer Rental Markets Business models that provide access to assets rather than transfer ownership of goods have become an important industry trend, representing a challenge for incumbent firms. This paper analyzes the interaction of a peer-to-peer (P2P) rental market and a manufacturer of durable goods, and highlights the important role of consumer heterogeneity in usage rates in determining which business model would be preferred by the manufacturer. The introduction of a P2P rental market creates an equalizing effect, which leads to purchases from low-usage consumers. P2P rentals act as a discrimination device, allowing the manufacturer to segment consumers and extract a larger fraction of surplus, which might hurt consumers. The manufacturer is better off with P2P rentals when the heterogeneity in usage rates is intermediate, whereas the consumers are better off with P2P rentals when the heterogeneity is sufficiently high. This paper examines different business models such as the manufacturer with only sales, with rentals in addition to sales (the “dual” firm), with its own P2P rentals platform alongside sales (the “P2P-sponsoring” firm), and with a mixed structure in which the manufacturer competes against P2P rentals by introducing its own direct rentals (the “dual-plus-P2P” firm). Consumer heterogeneity in usage rates plays a fundamental role in business model outcomes. When usage rates and heterogeneity in usage rates are sufficiently large, the manufacturer is better off offering sales and facilitating a P2P rental market. In contrast, if heterogeneity in usage rates is too low, the manufacturer prefers to offer only sales. If heterogeneity is too high but usage rates are below a threshold, the manufacturer prefers to operate as a dual firm that offers both sales and rentals directly to consumers. If P2P rentals are unavoidable, introducing its own rentals to compete against P2P rentals might not be the best strategy for the manufacturer under certain conditions. Overall, contrary to what could be expected, the manufacturer has an incentive to facilitate P2P rentals in a large variety of cases.', 'Effective use of information technologies by seniors: the case of wearable device use Healthcare is an area that has benefitted from the developments in wearable device technology. Seniors, who usually suffer from multiple comorbidities, are among the target users of these devices, and research has shown potential health benefits for seniors when they use these devices effectively. However, the adoption rate of wearable devices is low, especially among seniors, preventing the full utilisation of their data in healthcare. In this study, we interviewed forty-four seniors across North America and collected data from their wearable devices to develop a theoretical affordance network-based model to explain seniors’ effective use of wearable devices. Our model indicates that despite the apparent simplicity of wearable devices, they have multiple affordances that help seniors achieve several goals, including activity monitoring, activity planning, and activity improvement. Furthermore, we identified factors that enable seniors to actualise the affordances of wearable devices and achieve their goals. The results of this study suggest a strong relationship between seniors’ mental and physical capabilities and their willingness to use and benefit from wearable devices. We join other researchers in their call for a contextual study on consumer technology use.', 'Enriching our theoretical repertoire: the role of evolutionary psychology in technology acceptance Information systems (IS) research has drawn heavily on social and cognitive psychology to explain technology adoption. Indeed, the many variations of the technology acceptance model all share these same theoretical foundations. Focusing exclusively on the socio-cognitive lens can lead to overlooking enhanced explanations of technology acceptance, such that new theoretical perspectives may be warranted. In this qualitative grounded theory study, we discovered how the lens of evolutionary psychology, as embodied in the Four-Drive model, was helpful in understanding technology acceptance across three organizational sites. We contend that evolutionary psychology is an important addition to the theoretical repertoire of IS researchers, and propose including ‘evolved psychological mechanisms’ within traditional models of technology acceptance.', 'From cacophony to harmony: A case study about the IS implementation process as an opportunity for organizational transformation at Sentara Healthcare The cacophony of criticisms emanating from an organization facing an information technology-enabled transformation can be deafening and deleterious. This is especially true in healthcare in the US, where information systems investments are typically huge and often perceived by change resistant stakeholders as disruptive or even potentially life threatening. We describe how the IS implementation process itself contributed to organizational transformation in terms of changes in coordination, culture, and learning at a successful organization, Sentara Healthcare, which transformed the discordant cacophony of the change process into a harmonious implementation.', 'Supporting decision support: Where information on DSS is located DSS professionals may differ in their opinion and practice as to where they locate the most useful information relevant to their work. Online and other electronic form databases are increasingly becoming the key resource for literature searches. This study empirically compared 31 online databases identified as promising for DSS relevant information according to their coverage of DSS. Rankings for recent years and temporally unconstrained conditions were obtained and discussed. INSPEC was the highest ranked database overall and for recent information. INSPEC was also the highest rated database for coverage of major DSS journals. However, there are many other databases that also provide coverage of DSS materials. It is hoped that DSS professionals will use these results to improve the effectiveness of their information search process.', 'Concept comparison engines: A new frontier of search In a traditional search engine interaction scenario, a user begins with a certain concept and finds documents that are similar to their concept. However, the user may wish to compare alternatives and a search capability should compare concepts and present the best alternatives. This task can be difficult without proper decision aids. We propose a concept comparison engine as a decision support tool that may be used to compare attributes of different alternatives and aid in making an informed selection. We describe an architecture and an interaction scenario and implement a prototype. We propose a number of evaluation metrics for measuring the viability of different terms for the purpose of comparing concepts. In scripted experiments, orderings for candidate terms from the prototype are compared to gold standard ranking lists from structured external sources. Our results indicate that a Rankor analysis may be promising as a measure of the differentiating power of candidate terms a user might choose to support concept comparison.', \"What's buzzing in the blizzard of buzz? Automotive component isolation in social media postings In the blizzard of social media postings, isolating what is important to a corporation is a huge challenge. In the consumer-related manufacturing industry, for instance, manufacturers and distributors are faced with an unrelenting, accumulating snow of millions of discussion forum postings. In this paper, we describe and evaluate text mining tools for categorizing this user-generated content and distilling valuable intelligence frozen in the mound of postings. Using the automotive industry as an example, we implement and tune the parameters of a text-mining model for component diagnostics from social media. Our model can automatically and accurately isolate the vehicle component that is the subject of a user discussion. The procedure described also rapidly identifies the most distinctive terms for each component category, which provides further marketing and competitive intelligence to manufacturers, distributors, service centers, and suppliers.\", 'Vehicle defect discovery from social media A pressing need of vehicle quality management professionals is decision support for the vehicle defect discovery and classification process. In this paper, we employ text mining on a popular social medium used by vehicle enthusiasts: online discussion forums. We find that sentiment analysis, a conventional technique for consumer complaint detection, is insufficient for finding, categorizing, and prioritizing vehicle defects discussed in online forums, and we describe and evaluate a new process and decision support system for automotive defect identification and prioritization. Our findings provide managerial insights into how social media analytics can improve automotive quality management.', 'A decision support system for patient scheduling in travel vaccine administration The administration of travel vaccines presents a number of operations management challenges. The interplay between shared consumption of multi-dose vaccine packages, rapid spoilage upon opening, the high cost of wastage, and the unique vaccination needs of the patients makes for a very interesting and complex scheduling problem that could benefit from computerized decision support. We compare the performance of a novel binary integer programming model and a genetic algorithm solution technique with conventional scheduling approaches. Computational results show that significant cost savings can be achieved with the DSS while simultaneously considering scheduling preferences of patients and mitigating scheduling inconvenience.', '‘Lots done, more to do’: the current state of agile systems development research The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.', \"The role of users and customers in digital innovation: Insights from B2B manufacturing firms Diffusion of digital technologies into the manufacturing industry has created new opportunities for innovation that firms must address to remain competitive. We investigate the role of customer and user knowledge in the digital innovation processes of three global B2B manufacturing companies. We find that the B2B manufacturing industry's characteristics influence how users and customers may be leveraged. Customers making the purchasing decisions are considered for knowledge about short-term changes in market needs, while users working directly with the products provide long-term guidance for digital innovation. We identify practices for acquiring, distributing, and using customer and user knowledge for digital innovation.\", \"Digital entrepreneurship and indigenous value systems: An Ubuntu perspective This paper investigates the embeddedness of digital entrepreneurship in the entrepreneurs' indigenous value system by examining the influence of ‘Ubuntu’ on digital entrepreneurship activities in the South African context. We do so through an interpretive field study of two innovation clusters in South Africa. Our findings reveal Ubuntu as the basis of a community orientation to digital entrepreneurship that offers an alternative to the prevalent heroic view in which digital entrepreneurship narratives are centred around the individual entrepreneur(s). They also highlight the tensions faced by digital entrepreneurs as they attempt to uphold the Ubuntu values of humility, reciprocity, and benevolence while operating in a competitive and fast-paced environment. In addition, our study indicates that the way entrepreneurs draw on their indigenous value system is dynamic, giving rise to what we call digital Ubuntu, reflecting a reworking of Ubuntu values into their increasingly digital reality. The concept of digital Ubuntu brings to light how indigenous values can become entangled with the capabilities of digital technologies and highlights the need for indigenous perspectives to advance our understanding of the diversity of digital phenomena, such as digital entrepreneurship, across cultural contexts.\", 'How do technologists do “ICT for development”? A contextualised perspective on ICT4D in South Africa We take a layered approach to contextualise Information Communication Technology for Development (ICT4D) to understand digital technologists’ motivations to implement technologies to address socio-economic issues based on their capabilities and kinship affiliations. We adopt an interpretive approach to conducting an inductive qualitative study of digital technologists based in South Africa. We propose three mechanisms (emotional connectedness, user-centred technologies, and symbiotic relations) through which digital technologists undertake ICT4D to exercise their agency and enhance the socio-economic well-being of disadvantaged members of society. Taking the kinship perspective and capability approach as underlying motivations for undertaking ICT4D projects allows us to contribute to the ICT4D literature.', 'The role of formal controls in facilitating information system diffusion Information system (IS) studies highlight that IS usage, a pre-requisite for IS diffusion, may be difficult to attain when usage is voluntary because users can resist using the system. User resistance may be overcome through the application of organizational controls. Control theory explains how users’ actions and practices are shaped in line with organizational guidelines and procedures. This paper reports on a qualitative case study and shows how formal control mechanisms (behavior and outcome controls) can have a positive and conclusive impact on IS diffusion. The paper makes three contributions to knowledge. First, it explains how the application of outcome control mechanisms can lead to IS diffusion despite user resistance. Second, it suggests that IS diffusion paths are iterative rather than smooth and linear. Finally, the paper demonstrates that in some contexts, despite a lack of reward expectancy, sanction expectancy can be an effective force during the IS diffusion process.', 'Issues in computer and non-computer supported GDSSs Considerable attention in recent years, especially in the US, has been paid to computer based Group Decision Support Systems (GDSS). These systems expect to capitalise on the benefits provided by computers and networks. These computer based GDSSs are heralded as being relatively new, however systems for supporting group decision making developed in the UK which use only manual methods and techniques have been available for much longer. This paper aims to examine these two types of group decision support system, and suggest a third type — that of the partially computer based system. The paper considers aspects of GDSS design such as location, flexibility of design, levels of participation in data capture, presentational difficulties, managing complexity of data, client control, and management of group dynamics. By so doing the paper aims to demonstrate that all three types of system can benefit from consideration of the other types, that each type has both positive and negative features, and that some combination of all of the types could provide groups with the best form of support.', \"Impact of meta-analytic decisions on the conclusions drawn on the business value of information technology Meta-analysis is a quantitative methodology that allows for summarizing the results of primary research studies in a field to provide new insights in terms of the phenomenon observed or the outcomes reported. This paper attempts to answer the fundamental question, “Do methodological decisions in a meta-analytic study affect the conclusions drawn from that study?” Specifically, this paper examines the effects of meta-analytic decisions when applied to the business value of information technology (BVIT) research stream. A closer examination of the variation in the methodological decisions informs us that, with each different decision alternative, we are examining slightly different phenomenon. The findings reveal that study outcomes do change, depending on the meta-analytic decisions that are tested. In other words, methodological decisions matter. Based on the data from 99 primary studies and 531 effect sizes, we tested seven hypotheses, in the BVIT research stream, using a comprehensive set of different methodological conditions. We find support for two findings that are consistent across all the different conditions. First, investing in information technology (IT) is positively associated with the firm's performance. We also find that large firms get more benefits from IT than small firms. These and the overall findings suggest that researchers should be cognizant of their methodological decisions, as they may be observing the phenomenon under different boundary conditions with different methodologies.\", 'Being an ‘it’ in IT: gendered identities in IT work This paper reflects on aspects of gender and IT work. The core hypothesis is that, if technical skill and masculinity are fundamentally related, then women working in IT jobs who are, in effect, challenging masculine skills by gaining them themselves, must develop a number of strategies to cope with the challenge that they feel is being made to their own gender identities and those of the men with whom they work. One strategy is for women to distance themselves from IT work; a second strategy is for women to distance themselves from their identities as women. Our results are drawn from a set of semi-structured interviews. We adopt the approach of critical research that seeks to expose asymmetric power relations in the organization and to let silenced voices be heard. This is related to the literature on silence in organizations. Within the critical approach, we chose a feminist methodology that looks towards identifying practices that are problematic for women and that acknowledges our biases and interests as researchers. Additionally, we draw upon the theoretical constructs of the gender and technology literature to theorize the relationship between gender and technical skill and how this impacts conceptions of gender identity.', \"Computer ethics in a different voice This paper argues that the potential of writing on computer ethics to contribute to a deeper understanding of inequalities surrounding the use of information and communications technologies is threatened by forms of technological determinism and liberalism. Such views are prevalent in professional and more popular literature, and even in policy documents, albeit expressed tacitly. Adopting this standpoint substantially reduces explanatory power in relation to certain computer ethics topics, especially equality and participation, particularly in relation to gender. Research on gender and information and communications technologies has analyzed inequalities between men and women both inside and outside the workplace, drawing heavily from feminist theory. The paper argues that feminist ethics, coupled with aspects of feminist legal and political theory, may offer a fruitful, novel direction for analyzing computer ethics problems, and certainly those that contain substantial differences, and therefore inequalities, in men's and women's experiences on-line. Furthermore, feminist ethics can offer a more collectivist approach toward computer ethics problems. Emerging themes in existing research on gender and computer ethics are discussed before exploring some of the outcomes of applying feminist theory to a problem of privacy in the extreme form of Internet-based harassment known as “cyberstalking”, where traditional liberal and determinist views have proved problematic.\", \"Exploring the gender question in critical information systems This paper addresses ways in which theorizing gender may be important in forming an understanding of the topic of emancipation, which is central to the new critical information systems (IS) based on the thinking of Habermas. After briefly discussing some problems with current research on gender and IS the paper argues that appropriate feminist theory may be useful in augmenting our understanding of foundational issues such as emancipation. The development of feminist philosophy and epistemology is briefly introduced. Habermas' 'ideal speech situation' is problematized in relation to feminist writing on male and female communication juxtaposed with recent research in computer-mediated communications. The paper continues by exploring the concept of emancipation through feminist epistemology and it closes with a preliminary consideration of how these concerns may be applied to critical IS.\", 'IS and its agenda The Journal of Information Technology is of interest to academics, scholars, advanced students and reflective practitioners in management science, information systems and computer science disciplines. The journal will also inform those seeking an update on current experience and future prospects in the areas of contemporary information and communications technology.', 'A framework for the classification of DSS usage across organizations The research project reported in this paper constitutes an attempt to build upon existing research in the DSS area, namely the well-known framework for DSS developed by Gorry and Scott Morton. The authors put forward a revised framework which researchers in the field can use to classify DSS applications meaningfully and make comparisons across large samples of systems and organizations. The findings of a three year action research study were used to develop a framework for the classification of DSS usage which was then tested empirically. The organizations studied were classified based on the extent to which they used DSS for different decision situations using two specific measurements: DSS spread and DSS complexity. The results obtained suggest that the framework which was developed in this research is a useful vehicle for categorizing the degree of maturity of organizations regarding their usage and development of actual DSS applications.', \"Information flows amongst executives: their implications for systems development Senior managers have tended to resist the incursion into their personal domain of computer systems meant for their use. Their main criticism is that technical solutions are being imposed on them without an adequate analysis of the problems at hand. This suggests that the way in which executives obtain and exchange information may not be adequately understood. With the help of a framework designed to identify top executives' networks of information flows, the study reported in this paper analysed the information practices of 16 executives from four organizations. The findings of the research indicated that executives use a combination of communication flows and information flows in a proportion which varies depending upon the context of their different activities. It also revealed that executives initiated information and communication flows of a different nature depending upon the role they play and the level of those with whom they deal within the organization. The results of the study suggest that very specific approaches are needed when identifying executives' needs in terms of developing systems aimed at supporting top managers' strategic activities.\", 'Lessons from enterprise resource planning implementations in Ireland – towards smaller and shorter ERP projects The enterprise resource planning (ERP) software market has been growing at a very fast pace over the last few years and has been predicted to keep growing rapidly in the long term. This has led to an abundance of media reports on the subject of ERP and to managers wondering whether their companies should implement ERP systems. In order to separate the reality of the ERP phenomenon from the hype that surrounds it, we studied 14 ERP implementation projects in Irish organizations and focused on the key relationships between organizations which attempt to implement ERP systems and their implementing partners. We found that the ERP implementations that are going on in Ireland at the moment are different to the projects that have been reported elsewhere in two key respects. Firstly, the organizations interested in ERP software are, on average, far smaller than the case studies reported in the literature and the majority of the cases we reviewed were small and medium enterprises (SMEs). Secondly, the durations of implementation were far shorter than reported elsewhere. These results are not surprising if one considers the smaller average size of Irish organizations, but they indicate that the ERP movement is truly ready for an extension towards the SME market. They also indicate that the duration of the implementation of ERP software may be related to the size and complexity of the client organization and that SMEs can expect to have an easier time implementing ERPs than the current literature suggests. We also found that software implementers play a key role, not only in technical terms, but also in managerial and political terms, because they can help their clients in correcting their expectations and perceptions of ERP systems and ERP implementations.', \"Input control and its signalling effects for complementors' intention to join digital platforms Existing information systems (IS) research on platform control has largely focused on examining how input control (i.e., the mechanisms used to control platform access) affects complementors' intentions and behaviours after their decision to join a digital platform. Yet, our understanding of how input control is perceived before this decision and how such perceptions influence prospective complementors' intention to join a platform is still nascent. In this regard, our study views input control as a salient signal that shapes prospective complementors' expected benefits and costs (i.e., their performance and effort expectancy), and ultimately their decision to join a digital platform. Drawing on signalling theory and the antecedent-benefit-cost (ABC) framework, we conducted a randomized online experiment in the context of donation-based crowdfunding. The experiment results offer empirical support for this view by showing that input control has distinct and complex signalling effects for prospective complementors. In particular, our findings reveal curvilinear and competing signalling effects, with perceived input control increasing both performance expectancy (at a decreasing rate) and effort expectancy (at an increasing rate). Also, we find that performance expectancy linearly increases prospective complementors' intention to join a platform, whereas effort expectancy linearly decreases their intention to do so. These findings imply that the overall relationship between perceived input control and intention to join follows an inverted U-shape curve, which means that neither a low nor a high, but a moderate degree of perceived input control maximizes prospective complementors' intention to join. In sum, the results of our study provide novel and important insights into the signalling role that perceived input control plays in shaping prospective complementors' decision to join a digital platform.\", 'Gamified monetary reward designs: Offering certain versus chance-based rewards To motivate visitors to engage with websites, e-tailers widely employ monetary rewards (e.g., vouchers, discounts) in their website designs. With advances in user interface technologies, many e-tailers have started to offer gamified monetary reward designs (MRDs), which require visitors to earn the monetary reward by playing a game, rather than simply claiming the reward. However, little is known about whether and why gamified MRDs engage visitors compared to their non-gamified counterpart. Even less is known about the effectiveness of gamified MRDs when providing certain or chance-based rewards, in that visitors do or do not know what reward they will gain for successfully performing in the game. Drawing on cognitive evaluation theory, we investigate gamified MRDs with certain or chance-based rewards and contrast them to non-gamified MRDs with certain rewards in user registration systems. Our results from a multi-method approach encompassing the complementary features of a randomised field experiment (N = 651) and a randomised online experiment (N = 330) demonstrate differential effects of the three investigated MRDs on user registration. Visitors encountering either type of gamified MRD are more likely to register than those encountering a non-gamified MRD. Moreover, gamified MRDs with chance-based rewards have the highest likelihood of user registrations. We also show that MRDs have distinct indirect effects on user registration via anticipated experiences of competence and sensation. Overall, the paper offers theoretical insights and practical guidance on how and why gamified MRDs are effective for e-tailers.', 'Human vs. Automated Sales Agents: How and Why Customer Responses Shift Across Sales Stages Customers in sales processes increasingly encounter automated sales agents (ASAs) that complement or replace human sales agents (HSAs). Yet, little is known about whether, how, and why customers respond to ASAs in contrast to HSAs across successive decision stages of the same sales process. Even less is known about customer responses to HSA-ASA combinations, where both agents assume distinct roles and focus on complementary tasks that are traditionally performed by only one single sales agent. Against this backdrop, this paper explores the influence of increasingly common sales representative (rep) types (i.e., ASA, HSA, and HSA-ASA) on customer decisions across sales stages. Drawing on information processing theory and the literature on hedonic-utilitarian decision making, we investigate customer responses to text-based ASAs from vendor companies in two common early stages of email sales processes (i.e., sales initiation stages) when customers successively decide whether to indicate their initial interest in an offer and then, whether to provide their contact information. Specifically, we conducted two complementary multi-decision experiments, namely (1) a randomized field experiment in a high-stakes sales initiation setting (n = 325) and (2) a subsequent randomized online experiment to complement the real-world insights (n = 408). Our core findings reveal reversing effect patterns of sales rep types across stages: although customers are more likely to indicate their initial interest to HSAs (versus ASAs) because of HSAs’ higher levels of social presence, they are less likely to provide contact information to HSAs because of HSAs’ lower levels of performance expectancy and effort expectancy. We also show that HSA-ASA combinations can be reasonable options for single ASAs, yet contextual features of the sales setting may affect differential customer responses to HSA-ASA combinations (versus ASAs) in each sales stage. Taken together, we uncover shifting effect patterns in customer responses to sales rep types across successive sales stages and shed light on the consecutive underlying mechanisms that explain these shifts. These findings have significant implications for vendor companies seeking to allocate HSAs and/or ASAs effectively across various decision stages in sales processes and beyond. History: Wonseok Oh, Senior Editor and Khim Yong Goh; Associate Editor. Funding: This work was supported by the Center for Responsible Digitality (ZEVEDI) and the German Research Foundation (DFG) [Grant 471168026]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1171.', 'No man is an island: Social and human capital in IT capacity building in the Maldives In many developing countries, lack of IT skills and human capital impede the potential of IT investments in organizations in developing countries [Lee, J. (2001). Education for technology readiness: Prospects for developing countries. Journal of Human Development, 2(1), 115–151]. This paper draws upon theories of human and social capital, and knowledge, to explain enablers/obstacles for knowledge creation and transfer for IT capacity building in a tourism organization in a developing country – the Maldives. IT capacity building is intimately linked to knowledge and skills at the level of human resource development. Using the Nahapiet and Ghoshal (1998) [Nahapiet, J., & Ghoshal, S. (1998). Social capital, intellectual capital, and the organizational advantage. Academy of Management Review, 23, 242–267] framework for the role of social capital in knowledge creation and transfer, we examine the major issues of IT capacity building for the case organization. We conclude that the role of cognitive capital is the most important for the tourism sector of the Maldives, and may play a vital role in accumulating structural and relational capital, together with appropriate government policies on ICT.', 'Containing COVID-19 through physical distancing: the impact of real-time crowding information With the rise of COVID-19, decision support systems (DSS) increasingly display crowding information (CI) (e.g. how crowded a medical practice is) to encourage physical distancing when users select locations. Despite important implications for containing COVID-19, little is known about the causal effect of CI on user selection behaviour and how the immediacy of CI (e.g. “updated 2 minutes ago“) as well as users’ health anxiety further influence the effect of CI. Drawing on literature on digital choice environments and construal level theory, we conducted a multi-national online experiment to investigate the effect of CI on selecting differently crowded medical practices. Our results demonstrate that present (vs. absent) CI in DSS increases the likelihood of users selecting less crowded medical practices, while the effect is strongest when employed with real-time (vs. historical average) CI and, surprisingly, when users’ health anxiety is low (vs. high). Overall, our study adds to the growing body of research on IS in the age of pandemics and provides actionable insights for DSS providers and policymakers to endow users with information to identify and select less crowded locations, thus containing COVID-19 through improved physical distancing without paternalistically restricting users’ freedom of choice.', 'A Form-Based Approach to Natural Language Query Processing We describe a methodology for processing data retrieval and update queries using a form-based natural language interface. For the purpose of illustration, we use computer integrated manufacturing (CIM) as the application domain. The interface consists of a set of fourth-generation interface tools (SQL forms), a set of form definitions, a lexicon, and a parser. The forms are developed from the functional and data models of the system. A form definition consists of a form name, a form object, a set of form fields, and a set of fragment grammars. A form object is a single or composite entity that uniquely identifies a form. Form fields consist of database fields whose values can be entered by users (user-defined), and others whose values can be derived by the system (system-defined). Fragment grammars are templates that identify the information requested by user queries. The lexicon consists of all words recognized by the system, their grammatical categories, synonyms, and associations (if any) with database objects and forms. The parser scans a natural language query to identify a form in a bottom-up fashion. The information requested by the user query is determined in a top-down manner by matching the fragment grammars associated with a form against the user query. Extragrammatical inputs with limited deviations from the grammar rules are supported. Elliptical queries are supported by deriving the missing information from those specified in previous queries and forms. Combining a natural language processor with SQL forms allows update queries and prevents violation of database integrity constraints, duplication of records, and invalid data entry.', 'Heterogeneous Demand Effects of Recommendation Strategies in a Mobile Application: Evidence from Econometric Models and Machine-Learning Instruments In this paper, we examine the effectiveness of various recommendation strategies in the mobile channel and their impact on consumers’ utility and demand levels for individual products. We find significant differences in effectiveness among various recommendation strategies. Interestingly, recommendation strategies that directly embed social proofs for the recommended alternatives outperform other recommendations. In addition, recommendation strategies combining social proofs with higher levels of induced awareness due to the prescribed temporal diversity have an even stronger effect on the mobile channel. We also examine the heterogeneity of the demand effect across items, users, and contextual settings, further verifying empirically the aforementioned information and persuasion mechanisms and generating rich insights. We also facilitate the estimation of causal effects in the presence of endogeneity using machine-learning methods. Specifically, we develop novel econometric instruments that capture product differentiation (isolation) based on deeplearning models of user-generated reviews. Our empirical findings extend the current knowledge regarding the heterogeneous impact of recommender systems, reconcile contradictory prior results in the related literature, and have significant business implications.', 'Demand Effects of the Internet-of-Things Sales Channel: Evidence from Automating the Purchase Process The internet of things (IoT) is rapidly becoming one of the most popular emerging technologies in business and society. One of the major verticals that has recently begun to effectively use IoT technologies is the retail industry. Given the unprecedented opportunities IoT generates for brands and retailers, it is important to glean timely insights regarding the business value of IoT and understand whether the introduction of an IoT technology as an alternative purchase channel for consumers affects the sales of physical products. In this paper, using empirical data from a multinational online retailer that adopted an IoT technology that largely automates consumers’ purchases and a quasi-experimental framework, we study the effect of the introduction of IoT on product sales. Our analyses reveal a statistically and economically significant increase in sales as a result of adopting an IoT technology and demonstrate the business value of the IoT channel for retailers and brands. In addition, we conduct analyses of the IoT phenomenon to also delve into the effect heterogeneity and empirically validate the underlying mechanism by examining the impact of IoT for products in different price ranges, levels of substitutability, and product categories (e.g., search versus experience goods and hedonic versus utilitarian), drawing on mental accounting and automaticity theory. For instance, our analyses reveal that less expensive and more differentiated products as well as experience and utilitarian goods can accrue higher benefits leveraging more effectively novel IoT technologies. We validate the robustness of our findings using an extensive set of robustness checks and falsification tests. This is the first paper to study the impact of an IoT technology on product sales, drawing important theoretical and managerial implications and seeding new future research directions for devices and technologies largely automating the purchase process.', 'Integrating Decision Technologies: Implications for Management Curriculum  The essence of management is decision making. Decision making requires the availability and proper use of data. Three evolving technologies relate to the support of decision making: information processing; decision science methods; and organization of decision makers and decision processes. More and more, because of technological developments and increased understanding of complex decision situations, these technologies must be seen as an integrated whole in order to support efficient and effective decision making. Within organizations, different groups are often charged with each of the three technologies that should support decision making. As a result, we often experience problems where the data is unavailable, is not compatible with the analyses desired, or is not relevant to the decision-making processes that managers wish to use. A substantive integration of these support groups and their expertise would help.Acing as a change agent, business and management schools can develop a conceptualiza; tion of the three technology areas that integrates them with respect to terminology. They can also develop a unified set of general constructs that carry throughout each of the areas. Further, a new sedes of core courses can be developed that present the integrated technology subjects in a logical sequence.Management schools have been concerned th teaching concepts and skills related to data acquisition, manipulation and presentation to their students for some Eme. However, in organizing the delivery of data-handling concepts and ski/Is, these schools have relied on courses from vadous discipline bases such as decision sciences (DSci), management information systems (M/S), managerial decision making (MDM). These courses in management school curricula are generally redundant, use similar terminology differently, and are not mutually supportive.This a~cle focuses on the design and delivery of an integrated sequence of core courses that addresses the three technology areas. Drawing on experience at our own schools, we address genera/course design principles, resolution of issues on content requirements and delivery, implementation issues, and current and future problems related to the effective integration of topics that have traditionally been addressed in DSci, MIS, and MDM courses. ', 'The journal list and its use: motivation, perceptions, and reality The Bauer College of Business at the University of Houston uses lists of journals as part of its process to evaluate and reward the research productivity of its faculty. The creation of the journal lists was proposed for three reasons: to target faculty research as to which journal outlets the College deemed acceptable, to encourage cross-disciplinary research, and to decrease the internal politicizing of the journals themselves. This article discusses the history and use of the lists as well as surveying faculty perceptions. The use of the journal list is imperfect. Faculty seem unclear as to the use of the lists and there does not seem to have been a substantial increase in cross-discipline research. While the tenor of the discussions about the merits of a journal has changed, interdepartmental politicizing of journals continues. Consistency of annual reviews, however, has increased substantially.', 'Perceived Usefulness, Ease of Use, and Usage of Information Technology: A Replication  This paper presents the findings of two studies that replicate previous work by Fred Davis on the subject of perceived usefulness, ease of use, and usage of information technology. The two studies focus on evaluating the psychometric properties of the ease of use and usefulness scales, while examining the relationship between ease of use, usefulness, and system usage. Study 1 provides a strong assessment of the convergent validity of the two scales by examining heterogeneous user groups dealing with heterogeneous implementations of messaging technology. In addition, because one might expect users to share similar perspectives about voice and electronic mail, the study also represents a strong test of discriminant validity. In this study a total of 118 respondents from 10 different organizations were surveyed for their attitudes toward two messaging technologies: voice and electronic mail. Study 2 complements the approach taken in Study I by focusing on the ability to demonstrate discriminant validity. Three popular software applications (WordPerfect, and Harvard  Graphics)  were examined based on the expectation that they would all be rated highly on both scales. In this study a total of 73 users rated the three packages in terms of ease of use and usefulness.The results of the studies demonstrate reliable and valid scales for measurement of perceived ease of use and usefulness. In addition, the paper tests: the relationships between ease of use,.. usefulness, and usage using structural equation modelling. The results of this model are consistent with previous research for Study 1, suggesting that usefulness is an important determinant of system use. For Study 2 the results are somewhat mixed, but indicate the importahce of both ease of use and usefulness. Differences.in conditions of usage are explored to explain these findings. ', 'The many faces of information technology interruptions: a taxonomy and preliminary investigation of their performance effects Despite the growing importance of information technology (IT) interruptions for individual work, very little is known about their nature and consequences. This paper develops a taxonomy that classifies interruptions based on the relevance and structure of their content, and propositions that relate different interruption types to individual performance. A qualitative approach combining the use of log diaries of professional workers and semi-structured interviews with product development workers provide a preliminary validation of the taxonomy and propositions and allow for the discovery of a continuum of interruption events that fall in-between the extreme types in the taxonomy. The results show that some IT interruptions have positive effects on individual performance, whilst others have negative effects, or both. The taxonomy developed in the paper allows for a better understanding of the nature of different types of IT interruption and their consequences on individual work. By showing that different types of interruptions have different effects, the paper helps to explain and shed light on the inconsistent results of past research.', 'E-Mail Interruptions and Individual Performance: Is There a Silver Lining? The article presents a study on the impact of interruptions of work e-mail and other communication technologies on the performance of individuals. Topics include the use of action regulation theory to determine the direct and indirect impact of incongruent and congruent e-mail interruptions, and the positive impact of technology capabilities such as rehearsing and reprocessing on mindfulness of individuals.', 'Theorizing the Multilevel Effects of Interruptions and the Role of Communication Technology Our understanding of how interrupting the work of an individual affects group outcomes and the role of communication technologies (CT) in shaping these effects is limited. Drawing upon coordination theory and the literatures on computer-mediated communication and interruptions, this paper develops a multilevel theory of work interruptions. It suggests that interruptions that target individuals can also affect other group members through various ripple effects and a crosslevel direct effect. We also discuss how the usage of five CT capabilities during interruption episodes can moderate the impact of interruptions at the individual and group levels. Our theoretical model draws attention to the importance of examining the individual-to-group processes to better understand the impact of interruptions in group environments. Additionally, by accounting for the role of the use of CT capabilities during interruption episodes, our work contributes to both the interruptions literature, which dedicates scant attention to the interrupting media, and to IS research on media use and media effects.', 'Information Technology and Government Corruption in Developing Countries: Evidence from Ghana Customs The literature on information technology (IT) and government corruption in developing countries indicates contradictory evidence about the realization of anti-corruption effects. So far, there is no theoretical explanation of why the anti-corruption potential of IT demonstrated in some countries is not realized in many other countries. Drawing evidence from a case study of information systems interventions at Ghana customs over 35 years, we investigate how and why IT’s anti-corruption potential may be curtailed in the context of developing countries’ governments and societies. We focus on IT-mediated petty corruption practices of street-level officers, which we consider to be socially embedded and institutionally conditioned phenomena. We find that conditions of possibility for the IT-mediated petty corruption practices are created during the implementation of information systems. The configuration of IT and organizational processes of a government agency are constrained by the broader government administration system and influenced by the vested interests of government officers, politicians, and businesses. Subsequently, the co-optation of IT for petty corruption practices is enabled by networks of relationships and institutions of patronage that extend across government, business, and society. We thus explain the often limited effects of IT on petty corruption as the inability of localized information systems implementations to change modes of government administration that are embedded in the enduring neopatrimonial institutions and politics of many developing countries.', \"Information technology and public administration modernization in a developing country: Pursuing paperless clearance at Ghana customs Despite significant information technology (IT) implementations in public administrations of developing countries to change their dysfunctional traditional practices towards modern forms, the outcomes are typically disappointing relative to the potential of IT for organizational change. With a case study of the customs clearance process when importing goods through Ghana's main port, the Tema Harbour, we explore why IT struggles to modernize traditional practices of public administration in a developing country context. We focus on explaining the persistence of paper use at Ghana customs despite more than a decade of digitalization and automation to establish paperless processes that eliminate malpractices of traditional clearance. We view modernization as a process of long-term institutional change and therefore draw from the literature on IT and institutional change to focus our investigation on the dynamics of incongruent institutional logics of IT and public administration. We show that in the administration context of a developing country like Ghana, the endurance of patrimonial logics in the state and broader society, coupled with high levels of administrative discretion and ambivalent or weak compliance pressure limit the realization of IT for modernization and allows hybrid practices to emerge.\", 'Orchestrating a digital platform ecosystem to address societal challenges: A robust action perspective Orchestration of digital platform ecosystems has been well examined in the context of markets and the private sector where an orchestrator is a resourceful firm exploiting commercial opportunities in an industry. However, little is known about how it occurs when a government organization orchestrates to address societal challenges. We build upon the construct of ?robust action??identified with chess masters who play to advance a broad strategy while simultaneously maintaining flexibility?to explain how orchestration by a government organization might overcome initiation, stabilization, expansion, and meta-governance challenges through digital platform-enabled participative architecture, multivocal inscriptions, and distributed experimentation. Evidence for our theoretical framework is drawn from empirical studies of the Aadhaar, a platform ecosystem orchestrated to address multiple societal challenges stemming from identity and its management.', \"Effects of media formats on emotions and impulse buying intent One way of generating revenue from broadband media content rests upon the assumption that multi-media content may trigger a greater intent to buy products and services impulsively. An experiment was performed in order to explore the effects of media formats on the emotions and impulse buying intentions for music compact discs (CDs). Three distinct media formats of World Wide Web pages were set up: (1) the text of the lyrics, (2) still images from the song's music video and (3) the music video itself. Each had a varying degree of visual/verbal intensity while simultaneously playing the soundtrack in all three conditions. The results of this study indicate that displaying the text of the lyrics had a greater effect on the impulse buying intent than showing still images of the music video. In addition, different media formats caused emotional responses that can explain the participant's impulse buying intent to buy the CD. Unexpectedly, the still images and video did not necessarily generate more buying intention than combinations of the text and music. Therefore, it is recommended that electronic commerce and marketing managers explore innovative ways of integrating visual and verbal media formats for eliciting an effective consumer response.\", 'Towards an evidence-based decision making healthcare system management: Modelling patient pathways to improve clinical outcomes The concept of patient flow modelling has attracted managers, commissioners and clinicians to better understand the operational and clinical functions of the healthcare system. In this context, the current study has two objectives: First, to introduce a random effects continuation-ratio logit model, suitable for detecting stage wise transitions, to patient pathways modelling. Second, we aim at advancing our knowledge with regard to the application of modelling techniques to patient pathways. We study individual clinical pathways of chronic obstructive pulmonary disease (COPD) patients, a source of concern for major stakeholders. Data on COPD patients were extracted from the national English Hospital Episodes Statistics dataset. Individual patient pathways from initial admission through to more than four readmissions are captured. We notice that as patients are frequently readmitted, males are more likely to be in the higher risk group than females. Furthermore, the number of previous readmissions has a direct impact on the propensity of experiencing a further readmission. This model is very useful in detecting the most critical threshold at which multiple readmissions are more probable. Clinicians should note that a first readmission signifies a problem in the process of care and if care is not taken this may be the beginning of many subsequent readmissions. Our method could easily be implemented as a decision support tool to determine disease specific probabilities of multiple readmissions. Therefore, this could be a valuable tool for clinicians, health care managers, and policy makers for informed decision making in the management of diseases, which ultimately contributes to improved measures for hospital performance management.', 'Efficient clustering of databases induced by local patterns Many large organizations have multiple large databases as they transact from multiple branches. Most of the previous pieces of work are based on a single database. Thus, it is necessary to study data mining on multiple databases. In this paper, we propose two measures of similarity between a pair of databases. Also, we propose an algorithm for clustering a set of databases. Efficiency of the clustering process has been improved using the following strategies: reducing execution time of clustering algorithm, using more appropriate similarity measure, and storing frequent itemsets space efficiently.', 'The Effects of Tree-View Based Presentation Adaptation on Mobile Web Browsing Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.', 'Reducing Medicare Spending Through Electronic Health Information Exchange: The Role of Incentives and Exchange Maturity Health information exchanges (HIEs) are entities that have emerged in healthcare delivery markets across the United States. By providing an interorganizational information system (IOIS) and governance over use of this system and the information exchanged through it, HIEs enable more routine and efficient electronic sharing of patient information between disparate and fragmented healthcare providers. This should result in improved quality and efficiency of care. However, significant questions persist about the extent to which HIEs produce these benefits in practice, particularly in terms of reducing healthcare spending. We use transaction cost economics (TCE) to theorize that HIEs establish a quasi-hierarchy that decreases frictions associated with information sharing in ways that reduce healthcare spending, and that the magnitude of reductions is greater when (1) insurer and provider incentives align, and (2) HIE capabilities mature. We can test these conjectures because HIEs, unlike other efforts that provide IOIS, are typically confined to regional markets and develop heterogeneously between these markets, introducing variation in insurer-provider incentive alignment and HIE maturity. Leveraging a unique national panel data set, we evaluate whether HIEs reduce spending for the largest insurer in the United States, i.e., Medicare, and whether incentives and HIE maturity modify the magnitude of reductions. We find significant spending reductions in healthcare markets that have established operational HIEs, with an average reduction of $139 per Medicare beneficiary per year (1.4% decrease) or a $3.12 billion annual reduction in spending if HIEs were nationally implemented in 2015. We also find that these reductions occur disproportionately in healthcare markets where providers have financial incentives to use an HIE to reduce spending and when HIEs are more mature. Our results inform an important open empirical question in the healthcare domain related to the value of HIEs, while also joining perspectives from TCE with the IOIS literature to understand the factors that may be relevant to IOIS value creation more generally.', 'Does Analytics Help Resolve Equivocality in the Healthcare Context? Contrasting the Effects of Analyzability and Differentiation  Organizations are increasingly using data analytics to help make decisions and drive positive outcomes. But organizational scholarship has warned us that the sort of information processing associated with analytic capabilities, while effective for uncertainty reduction, may be less effective in equivocal contexts. Equivocality is evident when tasks are not easily analyzable (task analyzability) or when organizational departments are highly differentiated (differentiation). We hypothesize that analytics will be less effective in driving positive outcomes when equivocality is high because of low task analyzability. However, when an organization is more differentiated, resulting in high equivocality, we anticipate that analytics will be more effective in driving positive outcomes. To test this theory, we studied how clinical healthcare analytics influenced experiential quality (akin to patient satisfaction) in over 3,000 hospitals across nine years. Our results show that analytics capabilities, on average, do improve outcomes in terms of patient experiential quality, suggesting that analytics can reduce uncertainty, but we also found evidence for the moderating role of equivocality. Specifically, as task analyzability decreases (i.e., increasing equivocality), clinical healthcare analytics becomes less effective in improving experiential quality. However, when equivocality is high because of differentiation, there is a positive relationship between clinical healthcare analytics and experiential quality but only in larger hospitals. From a managerial perspective, this study has implications for boundary conditions of data analytics in organizations. ', 'Beyond the Privacy Paradox: Objective Versus Relative Risk in Privacy Decision Making The article presents a study on both the objective and relative risks involved with privacy decision making. Topics include the impact of changes in the objective risk of disclosure and the impact of changes in the relative perceptions of risk of disclosure on hypothetical and actual consumer privacy choices, a decrease in objective risk going from hypothetical to actual choice settings, and an increase in relative risk going from hypothetical to actual choice settings.', \"Using group support systems for strategic planning with the United States Air Force Strategic planning is a critical part of establishing an organization's direction. Although strategic planning is utilized throughout the United States Air Force today in various forms, group sessions can become time-consuming without structured planning and a focus on group communication. Computer-supported strategic planning, making effective use of technology, is one way to improve the strategic planning process. This research implements a group support system (GSS) as a communication tool to facilitate the strategic planning process. The researchers investigate effects of a facilitator's using technology to structure verbal and electronic communication, with the goal of increasing quality output and improving group member satisfaction. This project was completed at Mountain Home Air Force Base with the support of the 366th Wing. As predicted, a GSS facilitator's structuring verbal and electronic communication improved the quality of the strategic plan, reduced time to complete a strategic plan, and increased satisfaction with the strategic planning process. The results did not indicate increased commitment to implement the strategic plans developed by a group using GSS facilitation.\", \"A complexity perspective on collaborative decision making in organizations: The ecology of group-performance Networks of communication are essential when managing corporate work and performing information exchange; the systems must allow them to be dynamic and well-structured. They help provide high organizational performance and innovative capacity in today's knowledge intense corporations, and this means that organizations must manage the networks strategically. Despite the fact that practitioners are aware of the huge influence of informal communication on decision making, little is known about the underlying principles of efficient employee network collaboration, which is dynamic in nature, especially for complex environments resulting from steady innovation and high competitive pressure. We addressed this issue from a complexity perspective, using an agent based simulation to visualize the key elements of efficient, information-based, collaborative decision making. Our findings suggested that information and communication technologies (ICT) may not be able to leverage corporate performance of the increasingly complex adaptive organizations. There seem to be elementary natural constraints on the cognitive capacities of people dealing with and managing information. Rather than a better technical approach, a more ecologic one is therefore advocated as the best way to improve decision making.\", 'Participatory sociotechnical design of organizations and information systems – an adaptation of ETHICS methodology This paper examines a practical adaptation of the ETHICS methodology used in redesigning an information technology (IT) support service in an academic setting. The purpose of the project was to design appropriate organizational structures and functions and an accompanying information system (IS), to increase the effectiveness of the existing service. A participative sociotechnical approach was adopted for the entire design process which was carried out by the practitioners themselves. The staff’s views were elicited during informal participatory group sessions as well as in one-to-one informal discussions. While ETHICS was the overall guiding methodology for the design, QUICKethics was used as a complementary means of analysing the requirements of the new IS. This paper describes the methodology used and the design process; it reflects on the adaptation and its match with the ETHICS methodology, exploring the claimed association with the viable systems methodology and concludes with suggestions for further research.', \"Reducing Recommender System Biases: An Investigation of Rating Display Designs Prior research has shown that online recommendations have a significant influence on consumers' preference ratings and economic behavior. Specifically, biases induced by observing personalized system recommendations can lead to distortions in users' self-reported preference ratings after consumption of an item, thus contaminating the users' subsequent inputs to the recommender system. This, in turn, provides the system with an inaccurate view of user preferences and opens up possibilities of rating manipulation. As recommender systems continue to become increasingly popular in today's online environments, preventing or reducing such system-induced biases constitutes a highly important and practical research problem. In this paper, we address this problem via the analysis of different rating display designs for the purpose of proactively preventing biases before they occur (i.e., at rating collection time). We use randomized laboratory experimentation to test how the presentation format of personalized recommendations affects the biases generated in post-consumption preference ratings. We demonstrate that graphical rating display designs of recommender systems are more advantageous than numerical designs in reducing the biases, although none are able to remove biases completely. We also show that scale compatibility is a contributing mechanism operating to create these biases, although not the only one. Together, the results have practical implications for the design and implementation of recommender systems as well as theoretical implications for the study of recommendation biases.\", \"Do Recommender Systems Manipulate Consumer Preferences? A Study of Anchoring Effects Recommender systems are becoming a salient part of many e-commerce websites. Much research has focused on advancing recommendation technologies to improve accuracy of predictions, although behavioral aspects of using recommender systems are often overlooked. In our studies, we explore how consumer preferences at the time of consumption are impacted by predictions generated by recommender systems. We conducted three controlled laboratory experiments to explore the effects of system recommendations on preferences. Studies 1 and 2 investigated user preferences for television programs across a variety of conditions, which were surveyed immediately following program viewing. Study 3 investigated the granularity of the observed effects within individual participants. Results provide strong evidence that the rating presented by a recommender system serves as an anchor for the consumer's constructed preference. Viewers' preference ratings are malleable and can be significantly influenced by the recommendation received. The effect is sensitive to the perceived reliability of a recommender system and, thus, not a purely numerical or priming-based effect. Finally, the effect of anchoring is continuous and linear, operating over a range of perturbations of the system. These general findings have a number of important implications (e.g., on recommender systems performance metrics and design, preference bias, potential strategic behavior, and trust), which are discussed.\", 'Effects of Online Recommendations on Consumers’ Willingness to Pay Recommender systems are an integral part of the online retail environment. Prior research has focused largely on computational approaches to improving recommendation accuracy, and only recently researchers have started to study their behavioral implications and potential side effects. We used three controlled experiments, in the context of purchasing digital songs, to explore the willingness-to-pay judgments of individual consumers after being shown personalized recommendations. In Study 1, we found strong evidence that randomly assigned song recommendations affected participants’ willingness to pay, even when controlling for participants’ preferences and demographics. In Study 2, participants viewed actual system-generated recommendations that were intentionally perturbed (introducing recommendation error), and we observed similar effects. In Study 3, we showed that the influence of personalized recommendations on willingness-to-pay judgments was obtained even when preference uncertainty was reduced through immediate and mandatory song sampling prior to pricing. The results demonstrate the existence of important economic side effects of personalized recommender systems and inform our understanding of how system recommendations can influence our everyday preference judgments. The findings have significant implications for the design and application of recommender systems as well as for online retail practices.The online appendix is available at https://doi-org.tudelft.idm.oclc.org/10.1287/isre.2017.0703.', 'Effects of Personalized Recommendations Versus Aggregate Ratings on Post-Consumption Preference Responses Online retailers use product ratings to signal quality and help consumers identify products for purchase. These ratings commonly take the form of either non-personalized, aggregate product ratings (i.e., the average rating a product received from a number of consumers such as “the average rating is 4.5/5 based on 100 reviews”), or personalized predicted preference ratings for a product (i.e., recommender-system-generated predictions for a consumer’s rating of a product such as “we think you’d rate this product 4.5/5”). Ratings in either format can provide decision aid to the consumer, but the two formats convey different types of product quality information and operate with different psychological mechanisms. Prior research has indicated that each recommendation type can significantly affect consumer’s post-experience preference ratings, constituting a judgmental bias, but has not compared the effects of these two common product-rating formats. Using a laboratory experiment, we show that aggregate ratings and personalized recommendations create similar biases on post-experience preference ratings when shown separately. Shown together, there is no cumulative increase in the effect. Instead, personalized recommendations tend to dominate. Our findings can help retailers determine how to use these different types of product ratings to most effectively serve their customers. Additionally, these results help to educate the consumer on how product-rating displays influence their stated preferences.', 'Bundling Effects on Variety Seeking for Digital Information Goods Prior research with consumable goods has consistently found that consumers have a preference for greater variety when selecting items simultaneously as a bundle, rather than as a sequential series of individual decisions. However, digital information goods have a number of important differences from consumable goods that may impact variety-seeking behavior. In three experiments, we address two general research questions. First, as a precursor to studying digital goods, we disentangle the role of bundle cohesion (i.e., item relatedness) from the role of timing (simultaneous vs. sequential choice) as factors in variety seeking with consumable goods. Next, based on differences between digital and consumable goods, we theorize differences in the behavioral effects of bundle cohesion and timing on variety preferences for digital goods. The results show a reduction of influences upon variety-seeking behavior with digital goods, providing important implications for the sellers of such goods in contrast to what has been suggested for consumable goods. Therefore, a key takeaway is that, for digital goods such as music, the use of consumer-driven bundling variations does not suggest an advantage in terms of their ability to affect consumers’ variety-seeking behavior.', 'Making Sense of Technology Trends in the Information Technology Landscape: A Design Science Approach A major problem for firms making information technology investment decisions is predicting and understanding the effects of future technological developments on the value of present technologies. Failure to adequately address this problem can result in wasted organization resources in acquiring, developing, managing, and training employees in the use of technologies that are short-lived and fail to produce adequate return on investment. The sheer number of available technologies and the complex set of relationships among them make IT landscape analysis extremely challenging. Most IT-consuming firms rely on third parties and suppliers for strategic recommendations on IT investments, which can lead to biased and generic advice. We address this problem by defining a new set of constructs and methodologies upon which we develop an IT ecosystem model. The objective of these artifacts is to provide a formal problem representation structure for the analysis of information technology development trends and to reduce the complexity of the IT landscape for practitioners making IT investment decisions. We adopt a process theory perspective and use a combination of visual mapping and quantification strategies to develop our artifacts and a state diagram-based technique to represent evolutionary transitions over time. We illustrate our approach using two exemplars: digital music technologies and wireless networking technologies. We evaluate the utility of our approach by conducting in-depth interviews with IT industry experts and demonstrate the contribution of our approach relative to existing techniques for technology forecasting.', 'Modeling Supply-Side Dynamics of IT Components, Products, and Infrastructure: An Empirical Analysis Using Vector Autoregression Prior IS research on technological change has focused primarily on organizational information systems and technology innovation; however, there is a growing need to understand the dynamics of supply-side forces in the introduction of new technologies. In this paper we investigate how the interdependencies among information technology components, products, and infrastructure affect the release of new technologies. Going beyond the ad hoc heuristic approaches applied in previous studies, we empirically validate the existence of several patterns of supply-side technology relationships in the context of wireless networking. We use vector autoregression (VAR) to model the comovements of new component, product, and infrastructure introductions and provide evidence of strong Granger-causal interdependencies. We also demonstrate that substantial improvements in forecasting can be gained by incorporating these cross-level effects into models of technological change. This paper provides some of the first research that empirically demonstrates these cross-level effects and also provides an exposition of VAR methodology for both analysis and forecasting in IS research.', 'Impact of Information Feedback in Continuous Combinatorial Auctions: An Experimental Study of Economic Performance Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions-in which bidders can bid on combinations of goods-have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidder scan submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism', 'Designing Real-Time Feedback for Bidders in Homogeneous-Item Continuous Combinatorial Auctions Although combinatorial auctions are important mechanisms for many specialized applications, their adoption in general-purpose marketplaces is still fairly limited, partly due to the inherent difficulty in evaluating the efficacy of bids without the availability of comprehensive bidder support. In this paper, we present both theoretical results and computational designs to support real-time feedback to bidders in continuous combinatorial auctions, where bidders are free to join and leave the auction at any time. In particular, we focus on the broad class of single-item multi-unit (SIMU) combinatorial auctions, where multiple identical units of one homogenous item are being auctioned. We also consider two common ways to express bidding preferences: OR bids and XOR bids. For SIMU auctions with each of the two bid types, we present comprehensive analyses of auction dynamics, which can determine winning bids that satisfy allocative fairness, and compute critical evaluative metrics needed to provide bidder support, including bid winning and deadness levels. We also design the data structures and algorithms needed to provide bidder support in real time for SIMU auctions of practically relevant sizes. The computational tools proposed in this paper can facilitate the efficient and more transparent implementation of SIMU combinatorial auctions in business- and consumer-oriented markets.', \"Effect of Information Feedback on the Outcomes and Dynamics of Multisourcing Multiattribute Procurement Auctions Electronic auctions are increasingly being used to facilitate the procurement of goods and services in organizations. Multiattribute auctions, which allow bids on multiple dimensions of the product and not just price, are information technology-enabled sourcing mechanisms that can increase the efficiency of procurement for configurable goods and services compared to price-only auctions. Given the strategic nature of procurement auctions, the amount of information concerning the buyer's preferences that is disclosed to the suppliers has implications on the profits of the buyer and the suppliers and, consequently, on the long-term relationship between them. This study explores novel feedback schemes for multisourcing multiattribute auctions that require limited exchange of strategic information between the buyer and the suppliers. To study the impact of feedback on the outcomes and dynamics of the auctions, we conduct laboratory experiments wherein we analyze bidder behavior and economic outcomes under three different treatment conditions with different types of information feedback. Our results indicate that, in contrast to winner-take-all multiattribute auctions, multisourcing multiattribute auctions, with potentially multiple winners, allow bidders (i.e., suppliers) to extract more profit when greater transparency in terms of provisional allocations and prices is provided. We develop several insights for mechanism designers toward developing sustainable procurement auctions that efficiently allocate multiple units of an asset with multiple negotiable attributes among multiple suppliers.\", 'Bidder Support in Multi-item Multi-unit Continuous Combinatorial Auctions: A Unifying Theoretical Framework Despite known advantages of combinatorial auctions, wide adoption of this allocation mechanism, especially in consumer-oriented marketplaces, is limited partially by the lack of effective bidder support information that can assist bidders to make bidding decisions. In this paper, we study the bidder support problem for general multi-item multi-unit (MIMU) combinatorial auctions, where multiple heterogeneous items are being auctioned and multiple homogeneous units are available for each item. Specifically, we consider continuous MIMU auctions, which impose minimal restrictions on bidding activities, thereby reducing the complexity of participation. Two prevalent bidding languages: OR bidding and XOR bidding, are discussed separately. For MIMU auctions with XOR bids, we derive theoretical results to calculate important bidder support metrics. We further demonstrate that bidder support results for MIMU auctions with OR bids can be derived directly from those with XOR bids, by viewing OR bids as XOR bids with each bid submitted by a unique bidder. Consequently, we establish MIMU auctions with XOR bids as the most general case, and unify the theoretical insights on bidder support problem for different bidding languages as well as different special cases of general MIMU auctions, namely single-item multi-unit (SIMU) auctions and multi-item single-unit (MISU) auctions. The derived theoretical results lead to algorithmic procedures that are capable of providing bidder support information efficiently in practice, and that outperform the commonly used integer programming approach. Theoretical insights of the general MIMU auctions also extend to auctions with additional bidding constraints, including batch-based combinatorial auctions, hierarchical combinatorial auctions, and combinatorial reverse auctions. History: This paper has been accepted for the Information Systems Research Special Section on Market Design and Analytics. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2021.1068.', 'Designing Intelligent Software Agents for Auctions with Limited Information Feedback This paper presents analytical, computational, and empirical analyses of strategies for intelligent bid formulations in online auctions. We present results related to a weighted-average ascending price auction mechanism that is designed to provide opaque feedback information to bidders and presents a challenge in formulating appropriate bids. Using limited information provided by the mechanism, we design strategies for software agents to make bids intelligently. In particular, we derive analytical results for the important characteristics of the auction, which allow estimation of the key parameters; we then use these theoretical results to design several bidding strategies. We demonstrate the validity of designed strategies using a discrete event simulation model that resembles the mechanisms used in treasury bills auctions, business-to-consumer (B2C) auctions, and auctions for environmental emission allowances. In addition, using the data generated by the simulation model, we show that intelligent strategies can provide a high probability of winning an auction without significant loss in surplus.', \"Toward Comprehensive Real-Time Bidder Support in Iterative Combinatorial Auctions Many auctions involve selling several distinct items simultaneously, where bidders can bid on the whole or any part of the lot. Such auctions are referred to as combinatorial auctions. Examples of such auctions include truck delivery routes,industrial procurement, and FCC spectrum. Determining winners in such auctions is an NP-hard problem, and significant research is being conducted in this area. However, multiple- round (iterative) combinatorial auctions present significant challenges in bid formulations as well. Because the combinatorial dynamics in iterative auctions can make a given bid part of a winning and nonwinning set of bids without any changes in the bid, bidders are usually not able to evaluate whether they should revise their bid at a given point in time or not. Therefore, in this paper we address various computational problems that are relevant from the bidder's perspective. In particular, we introduce two bid evaluation metrics that can be used by bidders to determine whether any given bid can be a part of the winning allocation and explore their theoretical properties. Based on these metrics, we also develop efficient data structures and algorithms that provide comprehensive information about the current state of the auction at any time, which can help bidders in evaluating their bids and bidding strategies. Our approach uses exponential memory storage but provides fast incremental update for new bids, thereby facilitating bidder support for real-time iterative combinatorial auctions.\", 'REQUEST: A Query Language for Customizing Recommendations Initially popularized by Amazon.com, recommendation technologies have become widespread over the past several years. However, the types of recommendations available to the users in these recommender systems are typically determined by the vendor and therefore are not flexible. In this paper, we address this problem by presenting the recommendation query language REQUEST that allows users to customize recommendations by formulating them in the ways satisfying personalized needs of the users. REQUEST is based on the multidimensional model of recommender systems that supports additional contextual dimensions besides traditional User and Item dimensions and also OLAP-type aggregation and filtering capabilities. This paper also presents the recommendation algebra RA, shows how REQUEST recommendations can be mapped into this algebra, and analyzes the expressive power of the query language and the algebra. This paper also shows how users can customize their recommendations using REQUEST queries through a series of examples.', 'Centralization as a design consideration for the management of call centers A call center and its associated information technology (IT) provide an opportunity to redesign and improve service-delivery operations. Managers at all levels should understand the role of organizational design as call centers are established or expanded, in particular the relative centralization (distribution of authority) associated with delivering services to customers. This article argues that centralization moderates and influences the organization’s efforts to improve customer service through the implementation of the call center and its IT. If managers fail to capitalize on the particular way that centralization moderates between IT and competitive strategy, the organization may not enjoy an important benefit of the call center, which is competitive advantage through increased efficiency and improved customer service. Based on survey responses from 68 call-center managers, the authors found that both centralization and decentralization are associated with call-center service operations. While the call center provides managers with the ability to influence decision-making (centralization), there are also opportunities for agents in the call center to exercise authority in managing the organization’s communications with customers (decentralization). Implications for organizational practice are considered.', 'Development and validation of a rule-based time series complexity scoring technique to support design of adaptive forecasting DSS Evidence from forecasting research gives reason to believe that understanding time series complexity can enable design of adaptive forecasting decision support systems (FDSSs) to positively support forecasting behaviors and accuracy of outcomes. Yet, such FDSS design capabilities have not been formally explored because there exists no systematic approach to identifying series complexity. This study describes the development and validation of a rule-based complexity scoring technique (CST) that generates a complexity score for time series using 12 rules that rely on 14 features of series. The rule-based schema was developed on 74 series and validated on 52 holdback series using well-accepted forecasting methods as benchmarks. A supporting experimental validation was conducted with 14 participants who generated 336 structured judgmental forecasts for sets of series classified as simple or complex by the CST. Benchmark comparisons validated the CST by confirming, as hypothesized, that forecasting accuracy was lower for series scored by the technique as complex when compared to the accuracy of those scored as simple. The study concludes with a comprehensive framework for design of FDSS that can integrate the CST to adaptively support forecasters under varied conditions of series complexity. The framework is founded on the concepts of restrictiveness and guidance and offers specific recommendations on how these elements can be built in FDSS to support complexity.', 'Architectures in context: on the evolution of business, application software, and ICT platform architectures This paper distinguishes between the business domain, the application software domain, and the Information and Communication Technology (ICT) platform domain. It analyses historical developments in each of these three domains and shows that they experienced parallel development. The parallelism can be explained by mutual influence and alignment. Innovation in one domain may enable or drive developments in another. In order to be able to analyse alignment patterns, the notions of business architecture, application software architecture, and ICT platform architecture are introduced and defined. Interdependent historical developments sometimes demonstrate a radical change. Each can be described as a shift in “dominant design”, and we identify six such changes in the history of the modern enterprise. Professionals and scientific researchers working in Information and Management can benefit from these insights by assuming that radical changes in dominant designs will affect their field in the future according to the same pattern.', \"The platform shapes the message: How website design affects abstraction and valence of online consumer reviews Online consumer reviews provide relevant information about products and services for consumers. In today's networked age, the online consumer review platform market is hyper-competitive. These platforms can easily change different design characteristics to get more reviewers and to nudge reviewers to deliver higher quality reviews. This study explored the relation between online consumer review platforms' design characteristics and the reviewers' construal level. A psycholinguistic coding scheme was used to assess which social and physical design characteristics impact the language abstraction in accompanying online consumer reviews. To this end, we content analyzed reviews of services and products posted on eight different online consumer review platforms (N=400). This resulted in a number of key design characteristics (e.g., reviewer identification, reviewer status, order of instructions and length instructions) that led to a decrease in language abstraction used in online consumer reviews. Moreover, results showed that language abstraction mediated the relationship between the four design characteristics and valence. The findings and their broader theoretical, methodological and practical implications are discussed. Online consumer review platforms could capitalize on our findings in adaptive design choices.\", 'Do Organic Results Help or Hurt Sponsored Search Performance? We study the impact of changes in the competitors’ listings in organic search results on the performance of sponsored search advertisements. Using data from an online retailer’s keyword advertising campaign, we measure the impact of organic competition on both click-through rate and conversion rate of sponsored search advertisements. We find that an increase in organic competition leads to a decrease in the click performance of sponsored advertisements. However, organic competition helps the conversion performance of sponsored ads and leads to higher revenue. We also find that organic competition has a higher negative effect on click performance than does sponsored competition. Our results inform advertisers on how the presence of organic results influences the performance of their sponsored advertisements. Specifically, we show that organic competition acts as a substitute for clicks, but has a complementary effect on the conversion performance.', 'Cosearch Attention and Stock Return Predictability in Supply Chains The ability to make predictions based on online searches in various contexts is gaining substantial interest in both research and practice. This study investigates a novel application of correlated online searches in predicting stock performance across supply chain partners. If two firms are economically dependent through a supply chain relationship and if information related to both firms diffuses in the market slowly or rapidly, then our ability to predict stock returns increases or decreases, respectively. We use online cosearches of stock as a proxy for the extent of information diffusion across supply chain-related firms. We identify publicly traded supply chain partners using Bloomberg data and construct cosearch networks of supply chain partners based on the weekly coviewing pattern of these firms on Yahoo! Finance. Our analyses show that the cosearch intensity across supply chain partners helps determine cross-return predictability. When investors of a focal stock pay less attention to its supply chain partners, we can use lagged partner returns to predict the future return of the focal stock. When investors’ coattention to focal and partner stocks is high, the predictability is low. Our simulated trading strategy using returns of supply chain partners with low coattention generates a significant and positive return above the market returns and performs better than the previously established trading strategy using returns of all supply chain partners. The online appendix is available at https://doi-org.ezproxy2.utwente.nl/10.1287/isre.2016.0656.', 'The Impact of Competing Ads on Click Performance in Sponsored Search Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.', \"Managing paradoxical tensions in the development of a telemedicine system The global pandemic has escalated the demand for telemedicine systems across the world, particularly for vulnerable populations such as the elderly in nursing homes. However, challenges in implementation and high failure rates continue to affect the sustainability and capability of telemedicine systems. This study therefore addresses the question of how to sustain and develop telemedicine systems, and offers a conceptual model developed from longitudinal study data and paradox theory. We found that in the inter-organizational context of telemedicine systems, par\\xad adoxical tensions arise from conflict between demands and interests of the telemedicine system versus those of its members. We also identified when the specific tensions of belonging, learning, organizing, and performing are likely to occur. These tensions are addressed through responses, initiated by the hub, that address both system and member level demands and interests through creating collaborative governance, uplifting member capabilities, and targeted resourcing. We further demonstrate the temporal dynamics of how the hub's responses create inter-organizational norms and structures that in turn influence responses to tensions in subsequent phases. We examined variations in members' reactions to the responses, and found that they were influenced by member-specific resource factors, suggesting that while the hub does sustain the development of the telemedicine system through addressing common member demands, there are limits with regard to aspects that are more member-specific. Finally, we show how technology can be both enabler-trigger and enabler-response due to its inherent attributes of malleability and reconfigurability.\", \"Social Interactions and the “Digital Divide”: Explaining Variations in Internet Use Given the increasingly important role of the Internet in education, healthcare, and other essential services, it is important that we develop an understanding of the “digital divide.” Despite the widespread diffusion of the Web and related technologies, pockets remain where the Internet is used sparingly, if at all. There are large geographic variations, as well as variations across ethnic and racial lines. Prior research suggests that individual, household, and regional differences are responsible for this disparity. We argue for an alternative explanation: Individual choice is subject to social influence (“peer effects”) that emanates from geographic proximity; this influence is the cause of the excess variation. We test this assertion with empirical analysis of a data set compiled from a number of sources. We find, first, that widespread Internet use among people who live in proximity has a direct effect on an individual's propensity to go online. Using data on residential segregation, we test the proposition that the Internet usage patterns of people who live in more ethnically isolated regions will more closely resemble usage patterns of their ethnic group. Finally, we examine the moderating impact of housing density and directly measured social interactions on the relationship between Internet use and peer effects. Results are consistent across analyses and provide strong evidence of peer effects, suggesting that individual Internet use is influenced by local patterns of usage. Implications for public policy and the diffusion of the Internet are discussed.\", 'Editorial—Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems (IS) community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.', 'The Digital Transformation of Healthcare: Current Status and the Road Ahead As the United States expends extraordinary efforts toward the digitization of its health-care system, and as policy makers across the globe look to information technology (IT) as a means of making health-care systems safer, more affordable, and more accessible, a rare and remarkable opportunity has emerged for the information systems research community to leverage its in-depth knowledge to both advance theory and influence practice and policy. Although health IT (HIT) has tremendous potential to improve quality and reduce costs in healthcare, significant challenges need to be overcome to fully realize this potential. In this commentary, we survey the landscape of existing studies on HIT to provide an overview of the current status of HIT research. We then identify three major areas that warrant further research: (1) HIT design, implementation, and meaningful use; (2) measurement and quantification of HIT payoff and impact; and (3) extending the traditional realm of HIT. We discuss specific research questions in each domain and suggest appropriate methods to approach them. We encourage information systems scholars to become active participants in the global discourse on health-care transformation through IT.', 'Editorial Overview—The Interplay Between Digital and Social Networks Social networks constructed on digital platforms are becoming increasingly pervasive in all aspects of individual and organizational life. This special issue of Information Systems Research includes 10 papers that focus on the interplay between digital and social networks. The interplay draws attention to the fact that digital interaction among individuals and organizations is almost always embedded in, influenced by, and in turn influences a social network. The papers in this special issue collectively shed light on the technical, behavioral, and economic challenges and implications of such networks and contribute to our understanding of how the power of such networks can be harnessed.', 'Time Flies When You’re Having Fun:  Cognitive Absorption and Beliefs About Information Technology Usage  Extant explanations of why users behave in particular ways toward information technologies have tended to focus predominantly on instrumental beliefs as drivers of individual usage intentions. Prior work in individual psychology, however, suggests that holistic experiences with technology as captured in constructs such as enjoyment and flow are potentially important explanatory variables in technology acceptance theories. In this paper, we describe a multidimensional construct labeled cognitive absorption and defined as a state of deep involvement with software. Cognitive absorption, theorized as 1 Cynthia Beath was the accepting senior editor for this paper.being exhibited through the five dimensions of temporal dissociation, focused immersion, heightened enjoyment, control, and curiosity, is posited to be a proximal antecedent of two important beliefs about technology use: perceived usefulness and perceived ease of use. In addition, we propose that the individual traits of playfulness and personal innovativeness are important determinants of cognitive absorption. Based on the conceptual definition of this construct, operational measures for each dimension are developed. Using the World Wide Web as the target technology, scale validation indicates that the operational measures have acceptable psychometric properties and confirmatory factor analysis supports the proposed multi-dimensional structure. Structural equation analysis provides evidence for the theorized nomological net of cognitive absorption. Theoretical and practical implications are offered. ', 'Infusing learning into the information systems organization Contemporary information systems (IS) organizations need mechanisms to cope with both the complexity that is created by rapid technological change and escalating business demands, and the resulting uncertainty engendered in organizational operations and policies. The IS function will not be in a position to manage such changes effectively unless it undergoes a fundamental restructuring towards becoming a ‘learning’ organization. This paper presents a framework that identifies the core drivers than an IS organization can proactively influence as it attempts to become learning-oriented. The framework identifies three essential components: (1) the learning context that defines various dimensions for measuring organizational and individual performance; (2) procedures and management initiatives that will facilitate individual learning to improve such performance; and (3) norms and culture that are established by the leadership to encourage learning. Using a case study methodology, the actions taken by one specific IS organization in its attempts to infuse learning capabilities among its members are examined. The conceptual framework for examining what it takes to be a learning IS organization and the detailed documented experiences of one specific organization may provide valuable insights to other IS organizations in their efforts to become more adaptive and responsive to change.', \"The Information Systems Identity Crisis: Focusing on High-Visibility and High-Impact Research This paper presents an alternative view of the Information Systems identity crisis described recently by Benbasat and Zmud (2003). We agree with many of their observations, but we are concerned with their prescription for IS research. We critique their discussion of errors of inclusion and exclusion in IS research and highlight the potential misinterpretations that are possible from a literal reading of their comments. Our conclusion is that following Benbasat and Zmud's nomological net will result in a micro focus for IS research. The results of such a focus are potentially dangerous for the field. They could result in the elimination of IS from many academic programs. We present an alternative set of heuristics that can be used to assess what lies within the domain of IS scholarship. We argue that the IS community has a powerful story to tell about the transformational impact of information technology. We believe that a significant portion of our research should be macro studies of the impact of IT. It is important for academic colleagues, deans, and managers to understand the transformational power of the technology. As IS researchers with deep knowledge of the underlying artifact, we are best positioned to do such research.\", nan, 'A Conceptual and Operational Definition of Personal Innovativeness in the Domain of Information Technology The acceptance of new information technologies by their intended users persists as an important issue for researchers and practitioners of information systems. Several models have been developed in the literature to facilitate understanding of the process by which new information technologies are adopted. This paper proposes a new construct that further illuminates the relationships explicit in the technology acceptance models and describes an operational measure for this construct that possesses desirable psychometric properties. The construct, personal innovativeness in the domain of information technology, is hypothesized to exhibit moderating effects on the antecedents as well as the consequences of individual perceptions about a new information technology. The construct was developed and validated in the context of the innovation represented by the World-Wide Web. Implications for theory and practice are discussed.', 'The antecedents and consequents of user perceptions in information technology adoption A common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables. Although a fairly significant body of research that empirically tests these models is now in existence, some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered. This paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application. Two research objectives that have both theoretical and practical relevance motivated and guided the study. One, the study challenges an assumption which is implicit in technology acceptance models: that of the non-existence of moderating influences on the relationship between perceptions and adoption decisions. Specifically, the study examines the effects of an important moderating influence – personal innovativeness – on this relationship. Two, the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development. Theoretical and practical implications that follow from the results are discussed.', 'MIS planning: A methodology for systems prioritization The literature proposes a number of approaches that allow for the consideration of corporate goals and objectives in prioritizing information systems using both financial and non-financial criteria. Research in cognitive psychology suggests that an individual confronted with a simultaneous consideration of both qualitative and quantitative factors tends to assign greater salience to concrete factors than to more abstract criteria. This paper proposes a multi-dimensional methodology that allows for the prioritization of systems proposals based on attributes that are mostly qualitative as the first step in the resource allocation phase of MIS planning. This initial prioritization is used in conjunction with other quantitative factors to arrive at a final system portfolio. The methodology is illustrated with the aid of a case study conducted at a non-profit organization.', 'Research Report: The Evolving Relationship Between General and Specific Computer Self-Efficacy—An Empirical Assessment The concept of computer self-efficacy (CSE) recently has been proposed as important to the study of individual behavior toward information technology. This paper extends current understanding about the concept of self-efficacy in the context of computer software. We describe how two broad types of computer self-efficacy beliefs, general self-efficacy and task-specific self-efficacy, are constructed across different computing tasks by suggesting that initial general CSE beliefs will strongly predict subsequent specific CSE beliefs. The theorized causal relationships illustrate the malleability and development of CSE beliefs over time, within a training environment where individuals are progressively provided with greater opportunity for hands-on experience and practice with different software. Consistent with the findings of prior research, judgments of self-efficacy then serve as key antecedents of the perceived cognitive effort (ease of use) associated with technology usage. Further, we theorize that self-efficacy judgments in the task domain of computing are strongly influenced by the extent to which individuals believe that they are personally innovative with respect to information technology. Panel data were collected using a longitudinal research design within a training context where 186 subjects were taught two software packages in a sequential manner over a 14-week period. The emergent patterns of the hypothesized relationships are examined using structural equation modeling techniques. Results largely support the relationships posited.', 'Cognitive Fit in Requirements Modeling: A Study of Object and Process Methodologies Requirements modeling constitutes one of the most important phases of the systems development life cycle. Despite the proliferation of methodologies and models for requirements analysis, empirical work examining their relative efficacy is limited. This paper presents an empirical examination of object-oriented and process-oriented methodologies as applied to object-oriented and process-oriented tasks. The conceptual basis of the research model is derived from the theory of cognitive fit, which posits that superior problem-solving performance will result when the problem-solving task and the problem-solving tool emphasize the same type of information. Two groups of subjects participated in an experiment that required them to construct solutions to two requirements-modeling tasks, one process-oriented and the other object-oriented. One group employed the object-oriented tool while the other used the process-oriented tool. As predicted by the theory of cognitive fit, superior performance was observed when the process-oriented tool was applied to the process-oriented task. For the object-oriented task, however, the performance effects of cognitive fit require further investigation since there was no difference in subject performance across the two tools.', 'Knowledge-based model validation support for end-user computing environments Encouraging individuals to use corporate data and build computer-based decision models locally, while simultaneously ensuring that the modelling activity is consistent with corporate policies and guidelines poses a challenge to many organizations. Although it is desirable to encourage user autonomy in decision making, it is equally imperative to assure appropriate quality of the decisions made. In this paper interdependencies within the organizational decision making activity are used to identify some generic categories of support required to maintain consistency and quality in end-user model construction. Five distinct cases of model building activity in an end-user computing environment are described and support for ensuring consistency in user constructed models for two of these cases is discussed. An object-oriented knowledge-based system that provides such support has been developed; the architecture of this system is described. System implementation and interaction is illustrated with the aid of a financial budgeting application.', 'Editorial—Evolvable Systems: Through the Looking Glass of IS We explore how “Red Queen” competition is increasing the competitive premium on “evolvable” information systems (IS). Ephemeral market advantage coupled with relentless innovation spawning trends such as the Internet-of-Things and additive manufacturing are amplifying the importance of evolvable systems across all industries. We discuss uncharted theoretical and empirical territory for IS research on evolvable systems. The elusiveness of some of these phenomena to other disciplines offers a unique opportunity for IS scholars.', \"Assessing a Firm's Web Presence: A Heuristic Evaluation Procedure for the Measurement of Usability Web site usability is a critical metric for assessing the quality of a firm's Web presence. A measure of usability must not only provide a global rating for a specific Web site, ideally it should also illuminate specific strengths and weaknesses associated with site design. In this paper, we describe a heuristic evaluation procedure for examining the usability guidelines developed by Microsoft. We present the categories and subcategories comprising these guidelines, and discuss the development of an instrument that operationalizes the measurement of usability. The proposed instrument was tested in a heuristic evaluation study where 1,475 users rated multiple Web sites from four different industry sectors: airlines, online bookstores, automobile manufacturers, and car rental agencies. To enhance the external validity of the study, users were asked to assume the role of a consumer or an investor when assessing usability. Empirical results suggest that the evaluation procedure, the instrument, as well as the usability metric exhibit good properties. Implications of the findings for researchers, for Web site designers, and for heuristic evaluation methods in usability testing are offered.\", \"Editorial Notes: Reflections on Year One: A 2011 Retrospective The author reflects on the information systems and technology research study papers and notes that were published in the journal. She highlights the dual notions of technological innovation and impact in the field's progress. She also talks about cognitive neuroscience applied to information studies, the improvement in the traditional metrics used to review the quality of the journal, and the influence of the research studies published in the journal and their economic and social values.\", 'Editorial Notes An introduction is presented in which the editor discusses various reports within the issue on topics including the experimental study of online privacy information revelation and consumer transaction behavior, the specification of precise requirements for information technology (IT) artifact design and development and the iterative auction designs with linear and nonlinear pricing schemes.', nan, nan, \"Editorial Notes In this end of the year editorial commentary ISR's accomplishments for 2013 are summarized. I recognize the contribution of members of the editorial board who are retiring and welcome new editorial team members. Winners of the annual ISR awards for 2012, including best paper, AE of the year, and reviewer of the year are acknowledged and congratulated. An overview of changes to the journal's mission for 2014 is presented. The commentary closes with a discussion of papers in this issue. The research commentaries, articles, and notes span a broad gamut of research domains and theoretical traditions, ranging from digital networks, to information technology value, to online communities, to the design of software for games. The papers also exhibit considerable methodological diversity, including econometric analyses, qualitative approaches, and analytical modeling.\", nan, nan, nan, 'Editorial—On the Intellectual Structure and Evolution of ISR Scholars within fields frequently introspect about the evolution of their research disciplines, asking if the discipline is developing a cumulative research tradition, influencing scholarly work in other fields, and expanding its boundaries. In this spirit, I examine the intellectual structure and evolution of one disciplinary journal, ISR, over the most recent four-year period (2012–2015). I use a census of citation data and classify journals citing ISR and cited by ISR into the disciplines they represent. Analyzing 12,833 citations to ISR from 100 journals, and 7,731 citations from ISR to papers published in 67 journals I find that, consistent with prior studies, ISR reflects the multidisciplinarity of the information systems (IS) discipline, its boundaries are expanding to include new disciplines, and the journal draws extensively on other IS journals. I raise some questions for further introspection about the impact and visibility of IS research.', \"The More, the Merrier? How the Number of Partners in a Standard-Setting Initiative Affects Shareholder's Risk and Return1 The article presents research on collaboration between business enterprises to set standards for information technology examining if such cooperation reduces the financial risks faced by stockholders of the individual companies involved. It was found that an increase in the number of companies involved in cooperation on standards decreased the market risk on stockholder rate of return as measured by beta, but increased the idiosyncratic risk to the individual firms' returns. This indicates companies elected to participate in a large standardization project obtain a reduction in abnormal returns on stocks.\", 'Intellectual Property Bundle (IPB) theory: Managing transaction costs in technology development through network governance Technology is a bundle of inventions, which are increasingly protected by intellectual property rights. Typically, these rights are owned by multiple different entities, operating in different industries and countries. Moreover, once an invention protected by intellectual property right is incorporated in a product, it becomes very difficult to substitute it with an alternative technology, especially when the product has been widely adopted. Thus, technology creators must coordinate the disparate interests of various intellectual property owners in order to create useful technology. In this paper we introduce a new theory as an extension of transaction cost economics to explain the relative merits of different governance forms vis-à-vis the creation of technology that is a bundle of inventions. From this theoretical extension, we derive a number of testable hypotheses.', 'Putting Money Where the Mouths Are: The Relation Between Venture Financing and Electronic Word-of-Mouth External financing is critical to ventures that do not have a revenue source but need to recruit employees, develop products, pay suppliers, and market their products/services. There is an increasing belief among entrepreneurs that electronic word-of-mouth (eWOM), specifically blog coverage, can aid in achieving venture capital financing. Conflicting findings reported by past studies examining eWOM make it unclear what to make of such beliefs of entrepreneurs. Even if there were generally agreed-upon results, a stream of literature indicates that because of the differences in traits between the prior investigated contexts and venture capital financing, the findings from the prior studies cannot be generalized to venture capital financing. Extant studies also fall short in examining the role of time and the status of entities generating eWOM in determining the influence of eWOM on decision making. To address this dearth of literature in a context that attracts billions of dollars every year, we investigate the effect of eWOM on venture capital financing. This study entails the challenging task of gathering data from hundreds of ventures along with other sources including VentureXpert, surveys, Google Blogsearch, Lexis-Nexis, and Archive.org. The key findings of our econometric analysis are that the impact of negative eWOM is greater than is the impact of positive eWOM and that the effect of eWOM on financing decreases with the progress through the financing stages. We also find that the eWOM of popular bloggers helps ventures in getting higher funding amounts and valuations. The empirical model used in this work accounts for inherent selection biases of entrepreneurs and venture capitalists, and we conduct numerous robustness checks for potential issues of endogeneity, selection bias, nonlinearities, and popularity cutoff for blogs. The findings have important implications for entrepreneurs and suggest ways by which entrepreneurs can take advantage of eWOM.', \"Blog, Blogger, and the Firm: Can Negative Employee Posts Lead to Positive Outcomes? Consumer-generated media, particularly blogs, can help companies increase the visibility of their products without spending millions of dollars in advertising. Although a number of companies realize the potential of blogs and encourage their employees to blog, a good chunk of them are skeptical about losing control over this new media. Companies fear that employees may write negative things about them and that this may bring significant reputation loss. Overall, companies show mixed response toward negative posts on employee blogs—some companies show complete aversion; others allow some negative posts. Such mixed reactions toward negative posts motivated us to probe for any positive aspects of negative posts. In particular, we investigate the relationship between negative posts and readership of an employee blog. In contrast to the popular perception, our results reveal a potential positive aspect of negative posts. Our analysis suggests that negative posts act as catalyst and can exponentially increase the readership of employee blogs, suggesting that companies should permit employees to make negative posts. Because employees typically write few negative posts and largely write positive posts, the increase in readership of employee blogs generally should be enough to offset the negative effect of few negative posts. Therefore, not restraining negative posts to increase readership should be a good strategy. This raises a logical question: what should a firm's policy be regarding employee blogging? For exposition, we suggest an analytical framework using our empirical model.\", 'Early to Adopt and Early to Discontinue: The Impact of Self-Perceived and Actual IT Knowledge on Technology Use Behaviors of End Users For organizations to achieve the benefits of new information technology (IT) systems, their users must adopt and then actually use these new systems. Recent models help to articulate the potentially different explanations for why some users will adopt and then continue using new technologies, but these models have not explicitly incorporated IT knowledge. This is particularly important in contexts where the user base may be non-IT professionals—i.e., the users may vary substantially in their basic IT knowledge. We draw on psychology to argue that in situations where there is a wide variance in actual IT knowledge, there will often exist a U-shaped relationship between actual and self-perceived IT knowledge such that the least knowledgeable believe themselves to be highly knowledgeable. We then draw on individual-level adoption theories to argue that users with high self-perceived IT knowledge will be more likely to adopt new technologies and do so faster. We also draw on individual-level continuance theories to argue that users with low actual IT knowledge will be more likely to discontinue using new technologies and do so faster. We test our expectations using a proprietary data set of 225 sales professionals in a large Indian pharmaceutical company that is testing a new customer relationship management system. We find strong support for our hypotheses.', 'Differential Impact of Content in Online Communication on Heterogeneous Candidates: A Field Study in Technical Recruitment Recruitment is a critical activity for companies, and the research community has also shown significant interest in examining technology recruitment. Companies often communicate how they value their employees along with job requirements to potential candidates in a bid to attract them. However, there is an overall lack of understanding of how candidates react to such information and how their motivation toward the job changes with such online communication. Although there is substantial work that examines the decision-making process of managers who do technical hiring, to the best of our knowledge, there is a paucity of work that investigates the decision-making process of technical candidates. The broad research question studied is how including certain content in online communication about a technical job opportunity may (de)motivate heterogeneous candidates differently in applying for the job. We theorize the underlying process of how including certain content in online job communication may influence candidates’ propensity to apply and their minimum acceptable salary increase if they were to join the company. Additionally, we capture mediating variables that influence the effect of different online content on candidates’ propensity to apply and on candidates’ minimum acceptable salary increase. By testing actual job application behavior in a field study, we confirmed the content that can attract high performers while discouraging low performers from applying.History: Yong Tan, Senior Editor; Yan Huang, Associate Editor.Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2022.1120.', 'Superlatives and Scope of Improvement in Online Recommendations: Breath of Life or a Kiss of Death?  Online professional networks are important tools used by recruiters to find qualified candidates for job openings. Within these networks, professional recommendations are used to supplement profiles and add credibility. These recommendations tend to be overly positive, full of superlatives, and lacking in critical statements (referred to as scope of improvement). We draw on the theory of online trust to argue that having scope of improvement and superlatives may affect various dimensions of trust and to show how online trust, in turn, can affect the usefulness of a recommendation and the likelihood of receiving an interview. We contribute to the body of work on online trust both theoretically and empirically. From a theory perspective, we explain why including scope of improvement and superlatives in recommendations on online professional networks may help certain candidates in getting an interview but hurt others. From an empirical perspective, we provide a unique empirical setting that allows us to observe not only the effect of scope of improvement and superlatives, but also validate the theoretically argued underlying process. Furthermore, through discussion with recruiters, we identify then test contextual factors that differentiate recommendations on online professional networks from traditional recommendations. In this study, we use a scenario-based, quasiexperimental survey to test the effects of superlatives and scope of improvement on the usefulness and effectiveness of recommendations. Further, we test the mediating role of trust and how the experience levels of the recommendee affect the sign and strength of these relationships. Our findings indicate that including scope of improvement increases the effectiveness and usefulness of recommendations for candidates at low-and middle levels of experience. For the most experienced candidates, including scope of improvement has a negative effect on effectiveness. Superlatives negatively affect the perceived competence of the recommender and thus should be avoided. This negative effect is reduced when combined with scope of improvement. ', 'Differential Influence of Blogs Across Different Stages of Decision Making: The Case of Venture Capitalists In this paper, we study the differential influence of online user-generated content (UGC), specifically blogs, across the multiple stages of decision making of venture capitalists: screening stage, choice stage, and contract stage. We conjecture that, first, blogs are influential at the screening stage; second, after the screening stage, blogs are noninfluential since decision makers evaluate entities closely at later stages; third, blogs increase the interest from multiple decision makers which in turn increases the cost of the deal for a decision maker. This empirical investigation provides support for the hypotheses, which we tested for funding decisions by venture capitalists in information technology ventures. In particular, this study indicates that blogs can help managers in getting their products/services selected at the screening stage, but, beyond that, blogs do not help directly. However, since more decision makers screen products/services that receive blog coverage, the competition among decision makers helps managers in negotiating better contract terms. We advance the boundary of existing studies on the influence of UGC from single stage process to multiple stages.', 'Learning to Be Creative: A Mutually Exciting Spatiotemporal Point Process Model for Idea Generation in Open Innovation This study investigates the creative idea generation process in an open innovation platform. The idea generation process is simultaneously influenced by multiple activities: knowledge acquisition from participants’ interactions with each other’s ideas, deliberate practice through persistent participation, and learning through failures. Due to the dynamic interplay across these activities, it is challenging to identify each activity’s influence on creative ideation outcomes using reduced-form regression analysis. To overcome these challenges, we employ a comprehensive empirical framework, the mutually exciting spatiotemporal point process model with unobserved heterogeneity, which endogenizes the occurrences of these activities in continuous time and allows for user-dependent effects. By utilizing the activity stream data of 13,028 participants from 2010 to 2016 in an open innovation platform, we uncovered synergistic effects of these activities on creative outcomes. We find that knowledge acquired through interaction with others (i.e., stimulus ideas) plays a vital role in the creative ideation process, but their effect is more nuanced than what we have known so far. In contrast to the prior belief that distant analogies, stimulus ideas outside of a problem domain, spur creativity, we find that distant analogies lead to failures. Yet, we further find that such failures are indispensable to the creative ideation process because failures motivate idea generators (1) to acquire more knowledge by increasing their future interactions with other participants’ ideas (learning from others), and (2) to persist in generating ideas that lead to improvements in their ability to apply the acquired knowledge and to identify innovation tasks that are relevant to their stock of acquired knowledge (learning by doing). Our results indicate that failures are a stronger driver of the learning activities than successes. Based on our findings, we offer insights on how to cultivate creativity in an open innovation setting.', 'Desktop video conferencing in the organisation While the potential advantages of video conferencing are appealing, the technology has not been implemented in more than a handful of organisations. This is likely to change in the future as video conferencing moves to the desktop. This paper examines the issues surrounding the area and, in particular, attempts to determine the impact of video conferencing on organisations. A framework is provided for understanding desktop video conferencing in the organisational context, and the relative benefits of and problems in the use of desktop video conferencing are discussed. Finally, a number of suggestions are made on how organisations may reconcile the implications of utilising desktop video conferencing technology.', '“How does tech make you feel?” a review and examination of negative affective responses to technology use The study of individual, affect-related consequences from technology adoption and use is gaining traction in the information systems discipline. Efforts to explore affective reactions to technology have considered positive, affective constructs (e.g., enjoyment, playfulness, and flow), with a more recent focus on the dark side of technology use and constructs such as technostress, technophobia, and computer anxiety. While some research has examined these negative affective responses to technology, construct definitions and relationships are not well-defined or theoretically grounded. In this research, an integrative literature review is conducted on computer anxiety, technophobia and technostress, and the known antecedents, dimensions, and outcomes of each concept are organised into nomological networks. These nomological networks are then combined to identify inconsistencies and omissions in the literature. The Affective Response Model, a recently advanced, theoretically grounded taxonomy of affective responses to technology, is applied to differentiate the three constructs and to introduce technology-induced state anxiety (TISA), a new temporal (state-like) negative response to a specific instance of technology. Two empirical studies are conducted using existing and newly developed scales, and demonstrate that computer anxiety, technophobia, technostress and TISA are conceptually and empirically distinct and provide insight into how these constructs are related. Future research opportunities on affective responses to technology are described based on the integrated nomological network and empirical findings.', 'Matching intermediaries for information goods in the presence of direct search: an examination of switching costs and obsolescence of information This paper investigates patterns of revenues earned by an intermediary that matches buyers and sellers in the presence of direct search markets. We develop a theoretical structure and a computer simulation model of such a marketplace where vendors are horizontally differentiated, and an intermediary matches clients to the optimal vendor for a fee. The model is applicable to information services such as application service providers (ASPs). The contribution of this paper is the identification of scenarios under which intermediaries that match clients and vendors are likely to be profitable considering switching costs and obsolescence of information.', 'Market reactions to E-business outsourcing announcements: An event study Stock markets have reacted favorably to firms who announced their implementation of E-business projects for commercial exploitation. Those that outsourced E-business projects in order to achieve swift execution also achieved abnormal positive returns. Contrary to expectations, outsourcing E-business projects with high task complexity also led to positive results. We analyzed the process and found that these three factors explained more than 20% of the variance in abnormal returns. The results were obtained from an event study of 96 E-business-related announcements, including those made by firms in the S&P500 index during 1999–2002. This paper contains information that should therefore help firms identify E-business projects for outsourcing.', 'Catch Me If You Can: Effectiveness and Consequences of Online Copyright Enforcement We evaluate the unexpected shutdown of kino.to, a major platform for unlicensed video streaming in the German market. Using highly disaggregated clickstream data in a difference-in-differences setting, we compare the web behavior of 20,000 consumers in Germany and three control countries. We find that this intervention was not very effective in reducing unlicensed consumption or encouraging licensed consumption, mainly because users quickly switch to alternative unlicensed sites. We highlight that the shutdown additionally had important unintended externalities. Individuals who never visited kino.to and who additionally clicked on news articles that covered the shutdown increased their visits to piracy websites substantially. We show that this effect largely comes from articles that explicitly mention alternative websites or suggest that users do not have to fear legal consequences from unlicensed streaming. Finally, we document that the unlicensed video streaming market is much more fragmented after the shutdown, potentially affecting future interventions, at least in the short run. We argue that our results can be helpful to understand why online piracy rates are still high, despite a plethora of enforcement efforts.', 'Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form1 Researchers in a number of disciplines, including Information Systems, have argued that much of past research may have incorrectly specified the relationship between latent variables and indicators as reflective when an understanding of a construct and its measures indicates that a formative specification would have been warranted. Coupled with the posited severe biasing effects of construct misspecification on structural parameters, these two assertions would lead to concluding that an important portion of our literature is largely invalid. While we do not delve into the issue of when one specification should be employed over another, our work here contends that construct misspecification, but with a particular exception, does not lead to severely biased estimates. We argue, and show through extensive simulations, that a lack of attention to the metric in which relationships are expressed is responsible for the current belief in the negative effects of misspecification.', 'A Rejoinder to Rigdon et al. (2014) We appreciate the interest shown by Rigdon et al. [Rigdon EE, Becker J-M, Rai A, Ringle CM, Diamantopoulos A, Karahanna E, Straub DW, Dijkstra TK (2014) Conflating antecedents and formative indicators: A comment on Aguirre-Urreta and Marakas. Inform. Systems Res. 25(4):780–784.] in our recent work and for the time and effort spent in carefully considering it and offering their comments and concerns. In what follows, and within the limitations of a short rejoinder, we offer our response to their comments, highlighting points of agreement and noting where more research is necessary.', 'Partial Least Squares and Models with Formatively Specified Endogenous Constructs: A Cautionary Note Information systems researchers have recently begun to propose models that include formatively specified constructs, and largely rely on partial least squares (PLS) to estimate the parameters of interest in those models. In this research, we focus on those cases where the formatively specified constructs are endogenous to other constructs in the research model in addition to their own manifest indicators, which are quite common in published research in the discipline, and analyze whether PLS is a valid statistical technique for estimating those models. Although there is evidence that covariance-based approaches can accurately estimate them, this is the first research that examines whether PLS can indeed do so. Through a theoretical analysis based on the inner workings of the PLS algorithm, which is later validated and extended through a series of Monte Carlo simulations, we conclude that is not the case. Specifically, estimates obtained from PLS are capturing something other than the relationship of interest when the formatively specified constructs are endogenous to others in the model. We show how our results apply more generally to a class of models, and discuss implications for future research practice.', 'Statistical Inference with Plsc Using Bootstrap Confidence Intervals Partial least squares (PLS) is one of the most popular statistical techniques in use in the Information Systems field. When applied to data originating from a common factor model, as is often the case in the discipline, PLS will produce biased estimates. A recent development, consistent PLS (PLSc), has been introduced to correct for this bias. In addition, the common practice in PLS of comparing the ratio of an estimate to its standard error to a t distribution for the purposes of statistical inference has also been challenged. We contribute to the practice of research in the IS discipline by providing evidence of the value of employing bootstrap confidence intervals in conjunction with PLSc, which is a more appropriate alternative than PLS for many of the research scenarios that are of interest to the field. Such evidence is direly needed before a complete approach to the estimation of SEM that relies on both PLSc and bootstrap CIs can be widely adopted. We also provide recommendations for researchers on the use of confidence intervals with PLSc.', 'A two-stage machine learning framework to predict heart transplantation survival probabilities over time with a monotonic probability constraint The overarching goal of this paper is to develop a modeling framework that can be used to obtain personalized, data-driven and monotonically constrained probability curves. This research is motivated by the important problem of improving the predictions for organ transplantation outcomes, which can inform updates made to organ allocation protocols, post-transplantation care pathways, and clinical resource utilization. In pursuit of our overarching goal and motivating problem, we propose a novel two-stage machine learning-based framework for obtaining monotonic probabilities over time. The first stage uses the standard approach of using independent machine learning models to predict transplantation outcomes for each time-period of interest. In the second stage, we calibrate the survival probabilities over time using isotonic regression. To show the utility of our framework, we applied it on a national registry of U.S. heart transplants from 1987 to 2016. The first stage produces an area under the receiver operating curve (AUC) between 0.60 and 0.71 for years 1–10. While the 1year prediction AUC result is comparable to the reported results in the literature, our 10-year AUC of 0.70 is higher than the current state-of-the-art results. More importantly, we show that the application of isotonic regression to calibrate the survival probabilities for each patient over the 10-year period guarantees mono\\xad tonicity, while capitalizing on the data-driven and individualized nature of machine learning models. To pro\\xad mote future research, our code and analysis are publicly available on GitHub. Furthermore, we created a web app titled “H-TOP: Heart Transplantation Outcome Predictor” to encourage practical applications.', \"The Effects of Time Pressure and Completeness of Information on Decision Making The Israeli Air Force (IAF) has developed a simulation system to train its top commanders in how to use defensive resources in the face of an aerial attack by enemy combat aircraft. During the simulation session, the commander in charge allocates airborne and standby resources and dispatches or diverts aircraft to intercept intruders. Seventy-four simulation sessions were conducted in order to examine the effects of time pressure and completeness of information on the performance of twenty-nine top IAF commanders. Variables examined were: (1) display of complete versus incomplete information, (2) time-constrained decision making versus unlimited decision time, and (3) the difference in performance between top strategic commanders and mid-level field commanders. The authors' results show that complete information usually improved performance. However, field commanders (as opposed to top strategic commanders) did not improve their performance when presented with complete information under pressure of time. Time pressure usually, but not always, impaired performance. Top commanders tended to make fewer changes in previous decisions than did field commanders.\", 'Factors Affecting Policy for Distributing Computing Resources  This article discusses the findings of an empirical study whose major purpose was to analyze the relationship between various organizational attributes and the deployment of hardware resources. The study used a sample of 303 organizations. The article begins by reviewing some fundamentals of information systems (IS) distribution policy. After providing the basic research hypothesis and discussing the data collection process, it then describes the sample and the distribution of the data and presents the statistical analysis. The last section discusses the findings and provides some concluding remarks. AbstractThis article discusses the findings of an empirical study conducted on 303 organizations. The major purpose of the study was to analyze the relationship between various organizational attributes and the deployment of hardware resources. The salient finding was that the most influential variable is the distribution of decisionmaking processes in the organization. The more decision making is distributed, the more hardware is distributed. No significant relationships were detected between hardware distribution and any of the following variables: organizational structure, economic sectorial association, and the size of the organization. ', 'A Flexible Approach to Information System Development  The Information Systems Development Life Cycle (ISDLC) is usually treated as a rigid sequence of ac. tivities. This article asserts that differences in the nature of development projects should affect the planning of the ISDLC. Two classes of factors affecting the ISDLC are identified: factors relating to the environment and factors relating to the development effort (e.g., inhouse development vs. canned software package). Each step along the ISDLC is decomposed into several dimensions relating to the activities that should be performed, the degree of control that should be exerted, to human resources, to other resources, and to the time factor. The relationship between the six dimensions and the two classes of factors are explained. Finally, a practical approach to ISDLC planning is suggested based on a structured procedure and a number of working forms. It assists in preliminary planning of the development process as well as in periodic reviews and revisions whenever the project reaches a certain milestone. ', 'Instant Quality Control of Large Batch Processing Jobs  The most common way to identify success or failure of a job running in a batch-processing mode is by examining a completion code sent by the job to the host operating system. Yet, for a variety of reasons the completion code may inaccurately indicate a successful termination of the job. This article describes a different approach to monitoring the quality of batch processing jobs while in operation. A pattern of behavior is suggested for a program. The pattern reflects ratios of consumption of various hardware resources. The ratios are determined by collecting historical performance variables of the job and analyzing the data by means of statistical methods. Once a pattern is set, the performance variables of every individual run of the program are compared with the precalculated pattern of behavior and if the deviation is beyond certain limits an alarm is triggered.The proposed quality control technique has been tested on real applications, as well as on some artificial programs. The findings suggest that the technique is reliable in that # successfully distinguishes between proper and malfunctioning runs of a program. ', \"Environmental scanning and information systems in relation to success in introducing new products CEOs in 46 firms were interviewed in regard to the pattern of the environmental scanning they performed. The results were analyzed to determine the degree of use of information systems by CEOs in their strategic decision making and to seek a link with the firm's success in introducing new products. The study indicates significant differences in the level of environmental scanning and in the use of information systems between firms that were more successful in introducing new products into the market and firms that were less successful. The differences are in the pattern and the frequency of conducting environmental scanning, in the number of computerized applications, and in the number of advanced marketing information systems.\", 'A Systematic Approach Toward Assessing the Value of an Information System  A bstractA multiattribute utility approach is adopted to assess the value of an information system. Various economic analyses of the value of information are reviewed and the conceptual problems regarding the definition of this value and some measurement difficulties are discussed. A list of possible utility attributes is proposed for a reporting system value assessment, and for each attribute a measure and a utility function is suggested. Some techniques which constitute a joint utility function are presented, accompanied by two examples. A real case of minicomputer selection is given in order to illustrate the structured approach. ', 'A resource-based perspective of value generation through enterprise architecture management Our study contributes to the nascent discourse on enterprise architecture management (EAM) benefits by pro\\xad posing a theory-led, empirically validated model to explain how EAM benefits unfold. Drawing on the resourcebased theory, and based on empirical insights from 8 case studies, we find that EAM does not create benefits per se, but only creates value if an organization develops four second-order EAM capabilities – EA modeling, EA planning, EA implementation, and EA governance. The discovery that EAM resources only unfold their potential when forming EAM capabilities casts doubt on the established practice of initiating EAM as a modeling and documentation endeavor.', 'Composite quality of service and decision making perspectives in wireless networks Supporting quality of service (QoS) in wireless networks has been a very rich and interesting area of research. Many significant advances have been made in supporting QoS in single wireless networks. However, the support for the QoS across multiple heterogeneous wireless networks will be required in the future wireless networks. In connections spanning multiple wireless networks, the end-to-end QoS will depend on several factors such as mobility and connection patterns of users, and the QoS policies in each of the wireless networks. The end-to-end QoS is also affected by multiple decisions that must be made by several different network entities for resource allocation. The paper has two objectives: one is to demonstrate the decision making process for resource allocation in multiple heterogeneous wireless networks and the second is to present a novel concept of composite QoS in such wireless environment. More specifically, we present an architecture for multiple heterogeneous wireless networks, decision making process for resource request and allocation, a simulation model to study composite QoS, and several interesting results. We also present potential implications of composite QoS on users and network service providers. We also show how the QoS ideas presented in this paper can be used by wireless carriers for improved QoS support and management. The paper can form the basis for a significant further research in DSS for emerging 3G/4G wireless networks supporting QoS for a range of sophisticated and resource intensive mobile applications.', 'Vulnerability disclosure mechanisms: A synthesis and framework for market-based and non-market-based disclosures Vulnerability disclosure has been a controversial topic among scholars and practitioners. Most scholars agree on adopting the responsible disclosure practices for vulnerability disclosures, which give firms a protected period to address the vulnerability before public disclosure is made. However, the firms may not fully utilize the protected period resulting in financial and reputational losses. The recent popularity in market-based disclosure methods such as bug bounty programs has provided new methods to control ethical hackers and effectively manage the disclosure timelines. Through a systematic literature review, we investigate and identify various vulnerability disclosure mechanisms and elaborate the disclosure process of each mechanism. We synthesize and compare the antecedents and consequences of the vulnerability disclosure under market- and non-market-based disclosure mechanisms by proposing two research frameworks. Our analysis suggests that incentivizing hackers in market mechanisms change hackers’ motivations, leading to behavioral changes and eventually giving firms more control over the disclosure process. Additionally, our research frameworks provide a basis for further theorizing in this area. We also identify several open research questions addressing issues and challenges in the marketbased disclosures. The research has important implications for firms, hackers, policymakers, and researchers in this area.', 'Knowledge-based scenario management — Process and support Scenario planning is a widely accepted management process for decision support activities. Though conventional decision support systems provide a strong database, modeling and visualization capabilities for the decision maker, they do not explicitly support scenario management. We propose an integrated life cycle approach for knowledge-based scenario-driven decision support incorporating three interrelated frameworks at different abstraction levels to support this process. The macro-level knowledge-based framework guides the Meso-level Scenario-driven framework, and these two in turn guide and inform the micro-level process-oriented framework. We develop a domain independent, component-based, and layered architecture to support the scenario management process and framework. The framework and architecture are realized through a concrete prototype.', 'Sustainability modelling and reporting: From roadmap to implementation Sustainable business management aspires towards balancing and integrating social, economic and environmental dimensions. Existing roadmaps, frameworks and systems do not comprehensively support sustainable business transformation nor do they allow decision makers to explore interrelationships and influences between the sustainability dimensions. This leads to silo-based decision making where vision and strategies are not mapped to execution, and sustainability modelling and reporting processes are uncoordinated. This research proposes and implements a generic sustainable business transformation roadmap, which is supported by a framework and architecture for integrated sustainability modelling and reporting. The implementation leverages system dynamics, workflow modelling and adaptable system concepts.', \"A design and implementation model for life cycle cost management system Design-to-cost is a management philosophy that emphasizes the selection and design of a system based on minimizing life-cycle cost. In some instances, systems alternatives are evaluated using such analysis, but actual implementation of design-to-cost philosophy throughout the entire system life is an exception rather than the rule. Management's lack of planning makes it difficult to implement this important philosophy. This paper analyzes and identifies the issues and provides a framework for design and implementation of a life-cycle cost management system.\", 'IS diffusion: A dynamic control and stakeholder perspective Our research uses a novel perspective of control balancing and stakeholder orientation to explore an information system (IS) phenomenon called enterprise IS diffusion. Employing a case-based, grounded theory approach, we analyze four large-scale IS implementation projects in a Canadian government organization. Our findings offer a synergistic approach to IS diffusion by integrating a dynamic control configuration perspective with stakeholder engagement, stakeholder sensitivity, and the impact of shifting stakeholder orientations on IS diffusion stages.We draw from control, control balancing, and stakeholder theories. Moreover, we refer to the IS implementation and IS diffusion literature to develop our argument. In doing so, this research develops a new and vital con\\xad struct—stakeholder orientation—for IS implementation projects. We also identify the existence of four distinct stakeholder orientations, a combination of stakeholder engagement with stakeholder sensitivity in large ISD projects: (a) strategic, (b) responsibility, (c) best interest, and (d) economic cost.Through analysis and synthesis, we establish vital relationships among shifting control configurations, stakeholder engagement, stakeholder sensitivity, and IS diffusion stages, where maintaining an optimal stakeholder orientation, facilitated by appropriate control configuration, leads to a successful IS diffusion outcome. Furthermore, we propose an extension to the existing control balancing theory by identifying missing links and trigger factors.', 'Utilizing knowledge context in virtual collaborative work The understanding of knowledge can be impaired if it is isolated from the proper context. Despite the importance of contextual information, there has been limited support for utilizing context in current knowledge management and collaborative systems. This paper presents a knowledge context model, called KC-V, which facilitates the use of contextual information in virtual collaborative work. Four benefits of using KC-V are suggested: evolutionary accumulation of knowledge aligned with collaborative activities, supporting the virtual team lifecycle, improved understanding by rich navigation paths, and searching for knowledge with similar context. A web-based collaboration system called VWSS is developed using KC-V.', \"Attention Adjustment, Renewal, and Equilibrium Seeking in Online Search: An Eye-Tracking Approach Using eye-tracking field experiments, we examine the dynamics underlying consumers' attention-allocation behaviors in online search, with focus on attention adjustment, attention renewal, and equilibrium seeking. In particular, we probed into how consumers' e-commerce search behaviors vary when they are exposed to an advertisement during a search and when they are not. The findings from the two separate experiments suggest that consumers' attention span decreases exponentially, instead of linearly, as they maneuver from the top to the bottom of a search result webpage. The total number of available options significantly influences the speed and pattern of attention decay. However, attention decay does not simply move in the direction of depletion but can be refreshed and renewed upon encountering attention-diverting ad stimuli. Although ad stimuli are often considered distracting and worthless, they can produce positive effects when positioned in the middle of a search results listing, where a consumer's attention resources are rejuvenated by ads. Finally, because of consumers' propensity to seek equilibrium, attention decay occurs more rapidly after, rather than before, attention renewal. We extend the literature on the mere categorization effect by investigating how ad stimuli structurally separate search choices into mental categories and diminish on-going attention decay patterns.\", 'Assessing the contribution of knowledge to business performance: the KP3 methodology Knowledge is inherently difficult to measure. However, without valid and reliable measurement, it is very difficult to develop a comprehensive theory of knowledge and provide a practical guide for knowledge management. In this paper, we do not measure knowledge directly, but assess how much knowledge contributes to business performance. The KP3 methodology developed in this paper assesses the contribution of knowledge to business performance by employing product and process as intermediaries between the two. The understanding of the contribution is essential because it makes it possible to assess the productivities of knowledge entities, evaluate and compensate knowledge workers, and to allocate and develop human capital.', \"Decision support for real-time telemarketing operations through Bayesian network learning Many knowledge discovery systems have been developed in diverse areas, but few systems address the use of knowledge in decision problems explicitly. This paper presents a decision support system for real-time telemarketing operations using the information extracted from the Bayesian network learning model. A prototype decision support system was developed for AT&T customer-contact employees to provide a recommendation regarding the promotion of a telephone discount plan. The system integrated a Bayesian network learning model (knowledge discovery process) and decision-making technique (influence diagram) to provide real-time decision support. A Bayesian network learning model was used to predict a probability of the customer's response from the previous promotion/response history. The influence diagram framework was used to integrate the predicted probability with the cost and benefit related to the possible actions. It was demonstrated that decision support by the Bayesian network learning model itself can be misleading. However, by linking the Bayesian network learning model with rigorous decision-making techniques such as influence diagrams, the decision support system developed in this paper was shown to provide an intelligent decision advice.\", 'Managing risk in a new telecommunications service development process through a scenario planning approach Managing risk in a new product and service development process is one of the major challenges for many business managers. A scenario planning approach was incorporated into a new telecommunications service development process in order to understand the uncertainties shaping the future economic, business and technological environments. Understanding the major drivers for uncertainties helped in gaining insight and thereby generated new strategies for reducing risks and taking advantage of opportunities from uncertainty. In order to demonstrate the process and value of the approach, it was applied to a new telecommunications service concept, the Phoneweb service, which allows Internet access through telephones rather than a computer interface.', 'The impact of Web quality and playfulness on user acceptance of online retailing We investigated the effect of playfulness on user acceptance of online retailing and tested the relationship between Web quality factors and user acceptance behavior. A survey of 942 users of Web-based online retailing was conducted to test our model. The results showed that playfulness plays an important role in enhancing user attitude and behavioral intention to use a site. We also found that Web quality, categorized into system, information, and service quality, had a significant impact on the perceived ease of use, playfulness, and usefulness, and consequently, that it encouraged website use in the context of online retailing. Our study thus provided a balanced and integrative framework for determining Web quality. It enhanced our knowledge of the effect of playfulness, which should help Web practitioners and researchers better understand user behavior in Web-based online retailing.', \"Tailoring Database Training for End Users Lack of familiarity with database design methods could prevent many end users from effectively implementing their database management system packages. An inexpensive solution would be for end users to learn required database design skills from software tutors tailored to their needs. This research describes two tutors developed to teach these skills to end users. The tutors were based on a modified Entity-Relationship database design method. They improved an end user's natural learning process by incorporating design principles and facilitators. Empirical comparison of the tutors tested the teaching effectiveness of the facilitators. The results lead to recommendations for closing the gap between skills required and skills learned by end users in database design. Development of tutors that teach specific database design skills irrespective of the software package used in implementation has important implications for practitioners and researchers.\", 'When Algorithmic Predictions Use Human-Generated Data: A Bias-Aware Classification Algorithm for Breast Cancer Diagnosis When algorithms use data generated by human beings, they inherit the errors stemming from human biases, which likely diminishes their performance. We examine the design and value of a bias-aware linear classification algorithm that accounts for bias in input data, using breast cancer diagnosis as our specific setting. In this context, a referring physician makes a follow-up recommendation to a patient based on two inputs: the patient’s clinical-risk information and the radiologist’s mammogram assessment. Critically, the radiologist’s assessment could be biased by the clinical-risk information, which in turn can negatively affect the referring physician’s performance. Thus, a bias-aware algorithm has the potential to be of significant value if integrated into a clinical decision support system used by the referring physician. We develop and show that a bias-aware algorithm can eliminate the adverse impact of bias if the error in the mammogram assessment due to radiologist’s bias has no variance. On the other hand, in the presence of error variance, the adverse impact of bias can be mitigated, but not eliminated, by the bias-aware algorithm. The bias-aware algorithm assigns less (more) weight to the clinical-risk information (radiologist’s mammogram assessment) when the mean error increases (decreases), but the reverse happens when the error variance increases. Using point estimates obtained from mammography practice and the medical literature, we show that the bias-aware algorithm can significantly improve the expected patient life years or the accuracy of decisions based on mammography.The online appendix is available at https://doi.org/10.1287/isre.2018.0789.', \"It Road Warriors: Balancing Work--Family Conflict, Job Autonomy, and Work Overload to Mitigate Turnover Intentions This study examines the antecedents of turnover intention among information technology road warriors. Road warriors are IT professionals who spend most of their workweek away from home at a client site. Building on Moore's (2000) work  on turnover intention, this article develops and tests a model that is context-specific to the road warrior situation. The model highlights the effects of work--family conflict and job autonomy, factors especially applicable to the road warrior's circumstances. Data were gathered from a company in the computer and software services industry. This study provides empirical evidence for the effects of work--family conflict,  perceived work overload, fairness of rewards, and job autonomy on organizational commitment and work exhaustion for road warriors. The results suggest that work--family conflict is a key source of stress among IT road warriors because they have to juggle family and job duties as they work at distant client sites during the week. These findings  suggest that the context of the IT worker matters to turnover intention, and that models that are adaptive to the work context will more effectively predict and explain turnover  intention.\", \"Moving Beyond Intentions and Toward the Theory of Trying: Effects of Work Environment and Gender on Post-Adoption Information Technology Use Grounded in the theory of trying, this study examines the influence of the work environment and gender on trying to innovate with information technology. The study extends the innovation diffusion literature by offering a theory-driven explanation for examining trying to innovate with IT and a parsimonious measure for this construct. Drawing on the theory of reasoned action, we argue that work environment impediments render intentions inadequate for examining post-adoption IT use. Instead of examining intentions, we introduce the goal-based construct of trying to innovate with IT as an appropriate dependent variable for examining post-adoption IT use. Statistical analysis supports the reliability and validity of a parsimonious measure of trying to innovate with IT. The study focuses on two research questions. First, do perceptions of the work environment such as overload and autonomy influence individuals' trying to innovate with IT? Second, does gender influence the relationship between perceptions of the environment and trying to innovate with IT? The model articulates how perceptions of the environment moderated by gender may influence trying to innovate with IT. Results provide evidence that overload and autonomy are antecedents to trying to innovate with information technology. Further, findings confirm that autonomy interacts with overload to determine trying to innovate with IT and that these relationships vary by gender. Implications for research and practice are offered.\", 'Women in the information technology profession: a literature review, synthesis and research agenda Gender differences in IT careers appear to be affecting the competitiveness of companies globally. It is posited that given the current labor shortage in the IT industry, it has become more important than ever to reduce sources of leakage in the IT career paths of women. A model of barriers faced by women in the field of information technology is presented. Three distinct career stages of career choices, persistence and advancement are analyzed. At each stage, the effects of social and structural factors which may act as barriers are identified and discussed. Social factors include social expectations, work–family conflict and informal networks, while the structural factors are occupational culture, lack of role models and mentors, demographic composition and institutional structures. A proposed research agenda is offered. It is suggested that these social and structural factors as well as their interactions will result in turnover of women in IT.', 'Responsible innovation with digital platforms: Cases in India and Canada Marginalized communities globally encounter grand challenges such as lack of access to education, healthcare, and sustained livelihoods. Several initiatives to address these complex, global problems have resulted in fragmented solutions. Recognizing this, there have been several calls for the study of responsible innovation (RI) to address grand challenges. Digital platforms such as AirBnB, Uber and so forth have now become commonplace and are known to generate economic value but also face criticism for being exploitative and exclusive. Only a handful of studies show how similar platforms can innovate responsibly to serve marginalized communities by generating simultaneous economic and social value. To address this gap, our study examines the cases of two platforms that orchestrated ecosystems consisting of individuals from marginalized communities, government agencies, and other entities to provide physical, digital and societal solutions based on principles of RI. We contribute to the RI and IS literatures to show how RI solutions can be fostered through digital platforms to address grand challenges. The article provides empirical evidence of all four dimensions of the RI framework—anticipation, reflexivity, inclusion, and responsiveness - and their operationalization through digital platforms. This research lays the foundation for future studies at the intersection of RI and digital platforms literature. The study also provides practice insights on developing digital platform solutions for marginalized communities to address grand challenges and is useful to policymakers to formulate appropriate interventions. It pushes the theoretical and practice boundaries of our understanding of RI and digital platforms.', 'Content Sampling, Household Informedness, and the Consumption of Digital Information Goods Technology and media are delivering content that is transforming society. Providers must compete for consumer attention to sell their digital information goods effectively. This is challenging, since there is a high level of uncertainty associated with the consumption of such goods. Service providers often use free programming to share product information. We examine the effectiveness of content sampling strategy used for on-demand series dramas, a unique class of entertainment goods. The data were extracted from a large set of household video-on-demand (VoD) viewing records and combined with external data sources. We extended a propensity score matching (PSM) approach to handle censored data, which permitted us to explore the main causal relationships. Relevant theories in the marketing and information systems disciplines informed our research on consumer involvement and informedness for decision making under uncertainty, the consumption of information goods, and seller strategies for digital content. The results show that content sampling stimulates higher demand for series dramas, but in a more nuanced way than was expected. Samples of the series reveal quality information to consumers and allow them to assess preference fit directly. As a result, they become more informed about their purchase decisions. Also, households seem to be willing to pay more to be better informed, and informed households tend to purchase more. This suggests that content providers should invest in strategies that help consumers to understand the preference fit of information goods.', 'A Korean group decision support system A Group Decision Support System (GDSS) can be used to improve communication in many languages. Heretofore, the vast majority of research with these systems has been conducted on American groups using English. Here, we demonstrate how groups of Korean students used a GDSS developed at the University of Mississippi to exchange comments in Korean and English anonymously and simultaneously. The study found no significant differences between the English and Korean systems in terms of self-assessed ratings of evaluation apprehension, production blocking, and process satisfaction. Participants rated both systems favourably, supporting our hypothesis that Korean groups can benefit from the use of a GDSS.', \"Electronic brainstorming in small and large groups An experiment was conducted with small groups averaging about eight people and large groups averaging about 48 people in size. We compared the group members' perceived production blocking, evaluation apprehension, and satisfaction with a meeting of each group size using electronic and verbal brainstorming. While there was no significant difference in perceived satisfaction, evaluation apprehension, or production blocking in either group size using electronic brainstorming, there was a significant difference between small verbal groups and large verbal groups. There was also a significant difference in the dependent measures between large verbal groups and large electronic brainstorming groups. Results suggest that the benefits of electronic brainstorming are more pronounced with larger groups than with smaller groups.\", \"A group decision support system for multilingual groups Communication in multilingual groups is very difficult. Even if all participants in the group know a common language, it may be a first language for some of the group and a second language for others. Communication in such situations is not equal for all group members. A multilingual Group Decision Support System (GDSS) allows all members to communicate in their native languages, eliminating the group's linguistic problems. This paper describes a prototype multilingual GDSS that provides a high degree of translation accuracy while providing other benefits of Group Decision Support Systems, such as anonymity, parallel communication, and automated recording of the discussion.\", 'A group decision support system for multicultural and multilingual communication A Group Decision Support System (GDSS) can be used to lower or break barriers to group communication that are caused by differences in language and culture among meeting participants. This paper describes typical examples of communication barriers among group members with different cultural and lingual backgrounds and how a GDSS can help with these problems. The paper also describes a prototype GDSS developed at the University of Mississippi that translates among English, German, and Spanish.', 'An abductive model of group support systems Few researchers have attempted to model group support system meeting behavior mathematically. Using Abductive Information Modeling (AIM), we show that group size and idea generation type are primary predictors of group process satisfaction. While similar to artificial neural networks, abduction frequently provides simpler models and yields weights for links among the model variables. Results show that the interrelationships among the model variables are non-linear.', 'A comparison of two electronic idea generation techniques Much research has compared verbal with electronic brainstorming, but very few studies have investigated the effects of different electronic techniques. Most studies of electronic brainstorming have been based upon the individual poolwriting technique. However, gallery writing may be superior to it in some situations. Here, we report on an experiment involving 88 subjects in nine groups of approximately ten people each. The subjects used both electronic techniques to discuss different problems. Results show that the subjects were more satisfied with gallery writing and preferred it. Although more raw comments were generated using poolwriting, the number of quality comments and the number of unique, quality comments were not significantly different.', 'A comparison of synchronous and virtual legislative session groups faced with an idea generation task An experiment was conducted with groups of about eight people in face-to-face and geographically-distributed electronic meeting environments. While similar studies have focused on the behavior of group members working together in a single room or working individually in different rooms (a nominal group), this research looks at a hybrid environment in which part of a group is working in one room and another part is working simultaneously in a different room that is linked via a local area network, both parts forming a virtual group. Experimental results showed that such groups generated significantly more unique, quality comments than did face-to-face groups, and that participants were significantly more satisfied with that type of meeting. These and other results indicate that groups may be able to meet effectively when distributed geographically.', 'Flaming among first-time group support system users Numerous benefits, including increases in efficiency, effectiveness, and participant satisfaction, have been noted in the literature when electronic meetings are used in place of traditional, oral meetings. However, several costs, or process losses, have also been observed, including an increase in ‘flaming’ characterized by insults or even obscenities. This paper describes how flaming may be correlated with numerous task and group member characteristics. Results of a case study show that a large number of flames are unrelated to the topic and that a small minority of those writing comments are responsible for the majority of flames. No variables were found to be significant predictors, but its incidence was exclusively among males.', \"Requirements-driven data engineering In the early 1990s, the effectiveness and efficiency of the information systems (IS) supporting the US Department of Defense's non-combat operations was questioned. As had many organizations, the support had evolved into multiple, redundant, unintegrated, undocumented, stove-piped IS. These systems require unnecessarily large non-combat IS expenses, supporting war fighting efforts. Lack of integration hindered the Department from effectively providing mission support information. DOD's efforts to re-engineer the non-combat IS is one of the first attempts to apply requirements-driven data engineering to a large systems environment. Its application to DOD's non-combat IS data environment has provided tangible results: (1) from the top down, an enterprise model (EM) now specifies Department-wide requirements capable of guiding future integration and development efforts; (2) from the bottom up, non-combat IS are being significantly reduced, simplifying the overall problem; and (3) data quality engineering methods, guided by the EM, are being developed and applied to the remaining IS. This success has achieved a prerequisite necessary to increase the effectiveness and efficiency of the systems.\", 'Two decades of research on business intelligence system adoption, utilization and success – A systematic literature review In the recent era of technological advances and hyper-competition, business intelligence (BI) systems have attracted significant attention from executives and decision makers due to their ability to provide complex and competitive information inputs for the decision process. Following the world of practice, research into the adoption, utilization and success of BI systems has grown substantially over the past two decades. The literature suggests that organizations have largely failed to capture the benefits of BI systems to their full extent and are seeking ways to leverage value from the implemented systems. However, prior studies do not have any comprehensive study that discusses the issues and challenges related to adoption, utilization and success of BI systems. In this study, using a systematic literature review, we present comprehensive knowledge about what has been found in the domain of BI system adoption, utilization and success. A total of 111 peer-reviewed studies, covering three categories – adoption, utilization and success – published between 2000 and 2019, were selected. The findings present the research methods, underpinning theories and key factors employed to study BI system adoption, utilization and success. In addition, the review identified the key issues related to BI adoption, utilization and success and highlighted the areas that have attracted more or less attention. This study also suggests future directions for researchers and practitioners in terms of unexplored themes that may help organizations to obtain value from BI systems.', 'Network sampling and classification: An investigation of network model representations Methods for generating a random sample of networks with desired properties are important tools for the analysis of social, biological, and information networks. Algorithm-based approaches to sampling networks have received a great deal of attention in recent literature. Most of these algorithms are based on simple intuitions that associate the full features of connectivity patterns with specific values of only one or two network metrics. Substantive conclusions are crucially dependent on this association holding true. However, the extent to which this simple intuition holds true is not yet known. In this paper, we examine the association between the connectivity patterns that a network sampling algorithm aims to generate and the connectivity patterns of the generated networks, measured by an existing set of popular network metrics. We find that different network sampling algorithms can yield networks with similar connectivity patterns. We also find that the alternative algorithms for the same connectivity pattern can yield networks with different connectivity patterns. We argue that conclusions based on simulated network studies must focus on the full features of the connectivity patterns of a network instead of on the limited set of networkmetrics for a specific network type. This fact has important implications for network data analysis: for instance, implications related to the way significance is currently assessed.', \"An entropy approach to disclosure risk assessment: Lessons from real applications and simulated domains We live in an increasingly mobile world, which leads to the duplication of information across domains. Though organizations attempt to obscure the identities of their constituents when sharing information for worthwhile purposes, such as basic research, the uncoordinated nature of such environment can lead to privacy vulnerabilities. For instance, disparate healthcare providers can collect information on the same patient. Federal policy requires that such providers share “de-identified” sensitive data, such as biomedical (e.g., clinical and genomic) records. But at the same time, such providers can share identified information, devoid of sensitive biomedical data, for administrative functions. On a provider-by-provider basis, the biomedical and identified records appear unrelated, however, links can be established when multiple providers' databases are studied jointly. The problem, known as trail disclosure, is a generalized phenomenon and occurs because an individual's location access pattern can be matched across the shared databases. Due to technical and legal constraints, it is often difficult to coordinate between providers and thus it is critical to assess the disclosure risk in distributed environments, so that we can develop techniques to mitigate such risks. Research on privacy protection has so far focused on developing technologies to suppress or encrypt identifiers associated with sensitive information. There is a growing body of work on the formal assessment of the disclosure risk of database entries in publicly shared databases, but less attention has been paid to the distributed setting. In this research, we review the trail disclosure problem in several domains with known vulnerabilities and show that disclosure risk is influenced by the distribution of how people visit service providers. Based on empirical evidence, we propose an entropy metric for assessing such risk in shared databases prior to their release. This metric assesses risk by leveraging the statistical characteristics of a visit distribution, as opposed to person-level data. It is computationally efficient and superior to existing risk assessment methods, which rely on ad hoc assessment that are often computationally expensive and unreliable. We evaluate our approach on a range of location access patterns in simulated environments. Our results demonstrate that the approach is effective at estimating trail disclosure risks and the amount of self-information contained in a distributed system is one of the main driving factors.\", \"Enterprise architecture operationalization and institutional pluralism: The case of the Norwegian Hospital sector Enterprise architecture (EA) is a systematic way of designing, planning, and implementing process and technology changes to address the complexity of information system (IS) landscapes. EA is operationalized when architecture visions move towards realization through concrete projects. We report a case study on the dynamics of operationalizing EA in the Norwegian hospital sector by exploring different EA project trajectories. Our empirical context is an institutionally pluralistic setting where multiple logics coexist. We show that the distinct logic of EA is added to the institutional context and we find that tensions among existing medical, technical, and managerial logics and EA principles and assumptions emerge. We contribute to the under-researched topic of EA operationalization by suggesting a model that demonstrates how the meeting of multiple institutional logics can lead to varying degrees of differentiation or even disassociation from EA visions during decision-taking in projects. Furthermore, we advance extant research on IS projects' implementation in institutionally pluralistic settings by providing an empirical account of actors' interactions and project leadership arrangements that contribute to the persistence of coexisting logics in a dynamic equilibrium.\", \"The re-regulation of working communities and relationships in the context of flexwork: A spacing identity approach Existing studies on flexwork stress its individualizing inclination by showing how it gives au\\xad tonomy to employees, boosts individual productivity, or supports personal well-being at the expense of group cohesiveness, social ties and other characteristics of the “collective” in orga\\xad nizations. Obviously, flexwork both continues and contributes to an individualization process of working activities and relationships. But, how exactly does flexwork re-regulate working re\\xad lationships and communities? Is the “collective” irremediably damaged and doomed to disap\\xad pear? Building on a case study conducted in an insurance company having implemented flexwork, we observe invisibilized employees working from diverse premises (e.g., home, office, etc.) initiating alternative ways of staying united and close. This article shows the re-regulation of these working relationships and communities' through a collective identity process involving de/ re-spacing identity; i.e., the spatial and material aspects of flexible work in relation to identity.\", \"An adaptive learning to rank algorithm: Learning automata approach The recent years have witnessed the birth and explosive growth of the web. It is obvious that the exponential growth of the web has made it into a huge interconnected source of information wherein finding a document without a searching tool is unimaginable. Today's search engines try to provide the most relevant suggestions to the user queries. To do this, different strategies are used to enhance the precision of the information retrieval process. In this paper, a learning method is proposed to rank the web documents in a search engine. The proposed method takes advantage of the user feedback to enhance the precision of the search results. To do so, it uses a learning automata-based approach to train the search engine. In this method, the user feedback is defined as its interest to review an item. Within the search results, the document that is visited by the user is more likely relevant to the user query. Therefore, its choice probability must be increased by the learning automaton. By this, the rank of the most relevant documents increases as that of the others decreases. To investigate the efficiency of the proposed method, extensive simulation experiment is conducted on well-known data collections. The obtained results show the superiority of the proposed approach over the existing methods in terms of mean average precision, precision at position n, and normalized discount cumulative gain.\", \"Knowledge networks in new product development projects: A transactive memory perspective Even though an individual's knowledge network is known to contribute to the effectiveness and efficiency of his or her work in groups, the way that network building occurs has not been carefully investigated. In our study, activities of new product development teams were analyzed to determine the antecedents and consequences on the transactive memory systems, the moderating affect of task complexity was also considered. We examined 69 new product development projects and found that team stability, team member familiarity, and interpersonal trust had a positive impact on the transactive memory system and also had a positive influence on team learning, speed-to-market, and new product success. Further, we found that the impact of the transactive memory system on team learning, speed-to-market, and new product success was higher when there was a higher task complexity. Theoretical and managerial implications of the study findings are discussed.\", \"New product development team intelligence: Antecedents and consequences Our study investigated the effect of team knowledge on new product development (NPD). By investigating 207 NPD projects, we found that the declarative and procedural knowledge of the team and their use of IT had a positive influence on the team's knowledge base; and that the higher the functional diversity of the project team, the greater their overall knowledge. We also found that team knowledge positively impacted new product creativity and success in the market place.\", 'Antecedents and consequences of team potency in software development projects Developing new software quickly, successfully, and at low cost is critical in organizations. Ways of assessing the effectiveness of development teams has highlighted measures of factors, such as teamwork, group cohesiveness, and team integration, but the use of group potency theory (the collective belief of a group that it can be effective) is rare. In our study, we investigated antecedents of and consequences to group potency in software development project teams. By examining 53 software development project teams collected from small and medium-sized software firms in Turkey, we found, that team potency positively affected speed-to-market, development cost, and market success of the product. We also found that trust among project team members, past experiences of the members, and team empowerment had a positive impact on the team potency during the project. Managerial and theoretical implications are discussed.', \"Antecedents and consequences of collective empathy in software development project teams The term empathy has attracted many researchers from a variety of disciplines; however, a team's collective empathy, which is composed of cognitive, affective, and behavioral dimensions, has rarely been addressed in the literature. In this study, we empirically investigated the relationship between the collective empathy of a team and the effectiveness of its project process. Additionally, we tested the role of team intimacy-related factors, such as interpersonal trust, within-team communication, and team member familiarity, in collective empathy, as well as the moderating role of group norms on the collective empathy-process effectiveness link. By studying 122 software development projects, we found that cognitive-based trust, formal within-team communication, and team member familiarity influence the collective empathy of project teams. We also found that collective empathy affects team learning and product speed-to-market and results in lower project development costs. Furthermore, we determined that the existence of group norms moderates the relationships among collective empathy, speed-to-market, and lower development costs. The managerial and theoretical implications of the study have also been provided.\", 'From Placebo to Panacea: Studying the Diffusion of IT Management Techniques with Ambiguous Efficiencies: The Case of Capability Maturity Model In light of the inherent shortcomings of single-perspective approaches in IT diffusion research, in this paper, we develop a multi-perspective framework for studying the diffusion of IT management techniques. The framework is then applied to explain the diffusion of capability maturity model (CMM). This research contributes to information systems theory by (a) illustrating how several different theoretical perspectives (i.e., forced-selection, efficient choice, fashion, and fad) can be used to explain an IT management innovation diffusion; (b) identifying the specific limitations of each perspective; and (c) demonstrating how these perspectives can be reconciled and yield a holistic understanding of the diffusion trajectory. Building on 20+ years of CMM research, the propositions of this paper shed more light on the underlying dynamics driving the adoption decision among software vendors, and will inform IS scholars and practitioners about the types of actions that can foster the dissemination of emerging IT management techniques.', 'The ongoing quest for the IT artifact: Looking back, moving forward More than 10 years ago, Orlikowski and Iacono (2001) examined the conceptualization of Information Technology (IT) in Information Systems Research (ISR) articles published in the 1990s. Their main conclusion was that the majority of these articles did not properly conceptualize the IT artifact. They recommended that IS researchers start to theorize about the IT artifact and employ rich conceptualizations of IT. The Orlikowski and Iacono paper provides a strong anchor point from which to analyze the evolution of the IS discipline. In order to obtain an up-to-date image of contemporary IS research, and to assess how the IS field has evolved since the 1990s, we carried out a similar analysis on a more recent and broader set of articles, that is, the full set (N=644) of papers published between 2006 and 2009 by six top North American (ISR, MISQ, JAIS) and European (JIT, ISJ, EJIS) journals. The statistics in our results reveal no drastic advance in terms of deeper engagement with the IT artifact; more than 39% of the articles in our set are virtually mute about the artifact, and less than 16% employ an ensemble view of IT. Moreover, we note differences among the North American and European journals. Implications of the findings for two perspectives central to the IS research legitimacy debate are discussed.', 'Vicious and virtuous cycles in ERP implementation: a case study of interrelations between critical success factors ERP implementations are complex undertakings. Recent research has provided us with plausible critical success factors (CSFs) for such implementations. This article describes how one list of CSFs (Somers & Nelson, 2001) was used to analyse and explain project performance in one ERP implementation in the aviation industry. In this particular case, poor project performance led to a serious project crisis but this situation was turned around into a success. The list of CSFs employed was found to be helpful and appropriate in explaining both the initial failure and the eventual success of the implementation. CSFs in this case appeared to be highly correlated, ie changes in any one of them would influence most of the others as well. The reversal in performance after the project crisis was caused by substantial changes in attitudes with most of the stakeholders involved, such as top management, project management, project champion and software vendor.', 'Reversing a relationship spiral: From vicious to virtuous cycles in IT outsourcing IT outsourcing (ITO) remains a popular business practice, but many buyers and suppliers of IT services are caught in a vicious relationship spiral of low trust, bad collaboration and mediocre performance. This paper describes a novel process understanding of how vicious cycles work and suggests a new method for how they can be reversed into virtuous cycles. Based on the action research and complementary system dynamics simulation, this paper demonstrates how an ineffective ITO relationship between a European Harbour Authority and its main IT supplier ITCo was formed and, later, transformed. The method, involving collaborative redesign of service workflows, applied in this action research triggered the reversal of an otherwise downward relationship spiral. Both the empirical facts from the action research data and the system dynamics simulation data are provided as evidence. We conclude the paper with conceptual and methodological contributions as well as scope for future research.', 'Strategies for global information systems development Developing global information systems is a formidable task. Multinational companies operate in regions that are thousands of miles, many time zones, and many cultures away from headquarters. Organizing the activities and aligning the tasks and mindsets of people that are so far apart and to change the way that business is conducted through the use of IS is a major challenge. This study discusses alternative global IS development strategies and the factors that impact their selection. Four systems from a large transportation company are presented as real life examples to demonstrate the viability of these strategies and the accompanying factors.', \"Knowledge contributions in design science research: Paths of knowledge types Design science research addresses important, complex real-world problems. Although well-accepted as part of research in information systems, initiating or progressing a design science research project still requires effort to describe how knowledge creation emerges and its underlying dynamics. Given the existing body of knowledge on design science research, it should be possible to learn from that knowledge to progress future work. This paper analyzes design science research projects to identify and make explicit their knowledge contributions while recognizing the plurality of a project's knowledge contributions with respect to a project's knowledge scope and knowledge goals. The construct of a path of knowledge types is introduced that represents how knowledge con\\xad tributions are dynamically created throughout a project. These paths form the basis for the derivation of seven design science research strategies, which lead to guidelines for initiating or progressing a project. This effort is compared to other research that analyzes the growing body of work in design science with respect to knowledge contributions and project classifications.\", 'Experimental evaluation of user performance on two-dimensional and three-dimensional perspective displays in discrete-event simulation Several experiments were carried out to compare the impacts of using a two dimensional (2D) plan view or a three dimensional (3D) perspective view in discrete event simulation visual displays. The experiments measured the performance of participants in spotting errors, describing the model, and suggesting improvements to the system. The participants using the 3D perspective display performed much better in spotting errors, taking on average about one third of the time of participants observing the 2D display. They also did much better in describing the model. There was no significant difference in suggesting improvements although this may have been because this task was easy. Most participants preferred the 3D perspective view when asked to compare the displays. The experiments indicate that the detailed design of the visual display may have a considerable effect on some of the tasks in a simulation project and hence on whether the overall project is successful.', 'Trust and commitment within a virtual brand community: The mediating role of brand relationship quality This study seeks to clarify the antecedents and consequences of trust and commitment within the brand fan page context on Facebook, examining a sample of 210 respondents using structural equation modeling. The results highlight the positive effect of economic and hedonic benefits on trust and commitment within the brand fan page. Mediation analysis reveals that trust and commitment developed within the brand fan page will be transformed into positive “word of mouth” for the respective brand if fans have a strong relationship quality with the brand. Further, we found that young and female fans with a high level of engagement, having a strong relationship with the brand, spread positive WOM. Our findings broaden ways for developing relational governance in a firm-initiated virtual brand community by providing new levers and guidance for marketers to build strong customer relationships.', 'A Comprehensive Review and Synthesis of Open Source Research The open source movement has grown steadily and matured in recent years, and this growth has been mirrored by a rise in open source related research. The objective of this paper is to pause and reflect on the state of the field. We start by conducting a comprehensive literature review of open source research, and organize the resulting 618 peer-reviewed articles into a taxonomy. Elements of this taxonomy are defined and described. We then draw on a number of existing categorization schemes to develop a framework to situate open source research within a wider nomological network. Building on concepts from systems theory, we propose a holistic framework of open source research. This framework incorporates current research, as represented by the taxonomy, identifies gaps and areas of overlap, and charts a path for future work.', 'Development and validation of an instrument to measure user perceived service quality of mHealth The role of service quality in fostering the growth of mHealth services has gained much attention in the academic and practitioner communities. However, empirical research in this area has been beset by inadequate conceptualization and the lack of a validated scale. This study addresses these limitations by theoretically conceptualizing and empirically validating a multidimensional service quality scale in the mHealth context. The findings show that mHealth service quality is a hierarchical, multidimensional, and reflective construct, which consists of three primary dimensions and eight subdimensions. The results also confirm that the mHealth service quality scale is more effective at predicting satisfaction and continuance in a nomological network.', \"Drug prescription behavior and decision support systems Adverse drug events plague the outcomes of health care services. In this research, we propose a clinical learning model that incorporates the use of a decision support system (DSS) in drug prescriptions to improve physicians' decisions about the initial drug selection and administration. The model allows for both the analytical investigation of the effects of different DSS features on clinical learning and the estimation of the physician learning behavior given a panel data set. The analytical results suggest that using a DSS to improve physicians' prescription decisions would positively influence their clinical learning. Conversely, without improvements in successful drug selection, the use of a DSS would negatively affect clinical learning. The empirical results provide further evidence on the factors that drive physicians' responses to information sources and the extent to which they rely on clinical experience in prescribing drugs.\", 'A Strategic Analysis of Multi-Channel Expert Services Using stylized models, we investigate when and how expert service providers should offer their services online, and whether they should charge separate prices for face-to-face and online services or provide the online service as a free supplement. Interestingly, consumer surplus can rise when a monopolist charges different prices for face-to-face and online services, and it may drop when the monopolist starts offering the online service as a free add-on to its face-to-face service. We find that a market-wide adoption of the online channel by competing experts in a duopoly setting intensifies price competition and thereby reduces overall profits. Furthermore, the rate of adoption is highest when the online service is moderately effective, whereas one of the experts refuses to offer the service when it is highly effective. These results provide theoretical support for the viability of online expert services as well as practical guidance on pricing strategies.', 'Brand Crisis and Customer Relationship Management on Social Media: Evidence from a Natural Experiment from the Airline Industry In this study, we investigate the effect of a brand crisis on the customer relationship management (CRM) efforts of brands on social media. Despite the opportunities social media offers to brands to connect and engage with customers, it is still unclear how a brand crisis can change a brand’s social CRM efforts. Social media platforms can amplify the negative consequences of a brand crisis. However, the unique and public nature of social media platforms can offer new means for brands to handle a brand crisis and publicly manage their customer relationships. Compared with traditional CRM, social media enables brands to communicate with their customers publicly, which can strengthen their relationships with customers and maintain higher levels of customer engagement. Leveraging a natural experiment setting, we investigate the impact of the United Airlines crisis on three dimensions of social CRM efforts: informativeness, timeliness, and attentiveness. Contrary to traditional CRM efforts and recommendations, we find that the brand crisis increases informativeness efforts but reduces timeliness and attentiveness efforts.History: Yong Tan, Senior Editor; Beibei Li, Associate Editor.Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1159.', \"Why people keep coming back to Facebook: Explaining and predicting continuance participation from an extended theory of planned behaviour perspective This study examines the continuance participation intentions and behaviour on Facebook, as a representative of Social Networking Sites (SNSs), from a social and behavioural perspective. The study extends the Theory of Planned Behaviour (TPB) through the inclusion of perceived value construct and utilizes the extended theory to explain users' continuance participation intentions and behaviour on Facebook. Despite the recent massive uptake of Facebook, our review of the related-literature revealed that very few studies tackled such technologies from the context of post-adoption as in this research. Using data from surveys of undergraduate and postgraduate students in Jordan (n=403), the extended theory was tested using statistical analysis methods. The results show that attitude, subjective norm, perceived behavioural control, and perceived value have significant effect on the continuance participation intention of post-adopters. Further, the results show that continuance participation intention and perceived value have significant effect on continuance participation behaviour. However, the results show that perceived behavioural control has no significant effect on continuance participation behaviour of post-adopters. When comparing the extended theory developed in this study with the standard TPB, it was found that the inclusion of the perceived value construct in the extended theory is fruitful; as such an extension explained an additional 11.6% of the variance in continuance participation intention and 4.5% of the variance in continuance participation behaviour over the standard TPB constructs. Consistent with the research on value-driven post-adoption behaviour, these findings suggest that continuance intentions and behaviour of users of Facebook are likely to be greater when they perceive the behaviour to be associated with significant added-value (i.e. benefits outperform sacrifices).\", \"Developing a unified framework of the business model concept Recent rapid advances in Information and Communication Technologies (ICTs) have highlighted the rising importance of the Business Model (BM) concept in the field of Information Systems (IS). Despite agreement on its importance to an organization's success, the concept is still fuzzy and vague, and there is little consensus regarding its compositional facets. Identifying the fundamental concepts, modeling principles, practical functions, and reach of the BM relevant to IS and other business concepts is by no means complete. This paper, following a comprehensive review of the literature, principally employs the content analysis method and utilizes a deductive reasoning approach to provide a hierarchical taxonomy of the BM concepts from which to develop a more comprehensive framework. This framework comprises four fundamental aspects. First, it identifies four primary BM dimensions along with their constituent elements forming a complete ontological structure of the concept. Second, it cohesively organizes the BM modeling principles, that is, guidelines and features. Third, it explains the reach of the concept showing its interactions and intersections with strategy, business processes, and IS so as to place the BM within the world of digital business. Finally, the framework explores three major functions of BMs within digital organizations to shed light on the practical significance of the concept. Hence, this paper links the BM facets in a novel manner offering an intact definition. In doing so, this paper provides a unified conceptual framework for the BM concept that we argue is comprehensive and appropriate to the complex nature of businesses today. This leads to fruitful implications for theory and practice and also enables us to suggest a research agenda using our conceptual framework.\", 'Information technology (IT) in Saudi Arabia: Culture and the acceptance and use of IT The unified theory of acceptance and use of technology (UTAUT), a model of the user acceptance of IT, synthesizes elements from several prevailing user acceptance models. It has been credited with explaining a larger proportion of the variance of ‘intention to use’ and ‘usage behavior’ than do preceding models. However, it has not been validated in non-Western cultures. Using a survey sample collected from 722 knowledge workers using desktop computer applications on a voluntary basis in Saudi Arabia, we examined the relative power of a modified version of UTAUT in determining ‘intention to use’ and ‘usage behavior’. We found that the model explained 39.1% of intention to use variance, and 42.1% of usage variance. In addition, drawing on the theory of cultural dimensions, we hypothesized and tested the similarities and differences between the North American and Saudi validations of UTAUT in terms of cultural differences that affected the organizational acceptance of IT in the two societies.', \"A research case study: Difficulties and recommendations when using a textual data mining tool Although many interesting results have been reported by researchers using numeric data mining methods, there are still questions that need answering before textual data mining tools will be considered generally useful due to the effort needed to learn and use them. In 2011, we generated a dataset from the legal statements (mainly privacy policy and terms of use) on the websites of 475 of the US Fortune 500 Companies and used it as input to see what we could detect about the organizational relationships between the companies by using a textual data mining tool. We hoped to find that the tool would cluster similar corporations into the same industrial sector, as validated by the company's self-reported North American Industry Classification System code (NAICS). Unfortunately, this proved only marginally successful, leading us to ask why and to pose our research question: What problems occur when a data-mining tool is used to analyze large textual datasets that are unstructured, complex, duplicative, and contain many homonyms and synonyms? In analyzing our large dataset we learned a great deal about the problem and fortunately, after significant effort, determined how to “massage” the raw dataset to improve the process and learn how the tool can be better used in research situations. We also found that NAICS, as self-reported by companies, are of dubious value to a researcher—a matter briefly discussed.\", 'A semantic enhanced hybrid recommendation approach: A case study of e-Government tourism service recommendation system Recommender systems are effectively used as a personalized information filtering technology to automatically predict and identify a set of interesting items on behalf of users according to their personal needs and preferences. Collaborative Filtering (CF) approach is commonly used in the context of recommender systems; however, obtaining better prediction accuracy and overcoming the main limitations of the standard CF recommendation algorithms, such as sparsity and cold-start item problems, remain a significant challenge. Recent developments in personalization and recommendation techniques support the use of semantic enhanced hybrid recommender systems, which incorporate ontology-based semantic similarity measure with other recommendation approaches to improve the quality of recommendations. Consequently, this paper presents the effectiveness of utilizing semantic knowledge of items to enhance the recommendation quality. It proposes a new Inferential Ontology-based Semantic Similarity (IOBSS) measure to evaluate semantic similarity between items in a specific domain of interest by taking into account their explicit hierarchical relationships, shared attributes and implicit relationships. The paper further proposes a hybrid semantic enhanced recommendation approach by combining the new IOBSS measure and the standard item-based CF approach. A set of experiments with promising results validates the effectiveness of the proposed hybrid approach, using a case study of the Australian e-Government tourism services.', \"The influence of attitudes on personal computer utilization among knowledge workers: the case of Saudi Arabia Since the introduction of personal computers (PCs) in the early 1980s, Saudi Arabia has made major investments in PCs to match its rapidly growing economy. As a result, the PC business has become one of the fastest growing sectors in the Kingdom of Saudi Arabia. Our paper reports on the results of a study which investigates the relationships between end-users' attitudes and PC utilization among knowledge workers in the context of Saudi Arabia. To gain a better understanding of the factors that influence the use of PCs, we adopted Triandis' theory which suggests that behavior is determined by attitudes, social norms, habits and expected consequences of behavior. Our study is based on previous efforts to test the theory's validity in Saudi Arabia. Our results suggest that PC utilization is determined by individual attitudes, personal characteristics, such as PC experience, facilitating conditions, such as PC access and social factors. We also observed that respondents to our questionnaire differ in the level of importance they attribute to the factors hypothesized as influencing PC utilization compared to Canadian respondents in a previous study.\", 'ERP software implementation: an integrative framework ERP implementation is a socio-technical challenge that requires a fundamentally different outlook from technologically-driven innovation, and will depend on a balanced perspective where the organisation as a total system is considered. ERP implementation is considered to rely on behavioural processes and actions. It is a process that involves macro-implementation at the strategic level, and micro-implementation at the operational level. This therefore means that implementation in the context of ERP systems is not possible through an ON/OFF approach whereby deployment of the new systems will necessarily yield the desired and expected results. Understanding the implementation process through a balanced perspective will therefore prevent any unpleasant surprises, and will ensure and guide the change process to be embedded in a painless fashion. The balanced perspective means that socio-technical considerations must be borne in mind; the strategic, tactical and operational steps clearly defined; and the expected benefits evaluated and tracked through creating seamless and solid integration. This paper proposes an integrative framework for ERP implementation based on an extensive review of the factors and the essential elements that contribute to success in the context of ERP implementation.', \"The Role of Design Characteristics in Shaping Perceptions of Similarity: The Case of Online Shopping Assistants This research proposes that technological artifacts are perceived as social actors, and that users can attribute personality and behavioral traits to them. These formed perceptions interact with the user's own characteristics to construct an evaluation of the similarity between the user and the technological artifact. Such perceptions of similarity are important because individuals tend to more positively evaluate others, in this case technological artifacts, to whom they are more similar. Using an automated shopping assistant as one type of technological artifact, we investigate two types of perceived similarity between the customer and the artifact: perceived personality similarity and perceived behavioral similarity. We then investigate how design characteristics drive a customer's perceptions of these similarities and, importantly, the bases for those design characteristics. Decisional guidance and speech act theory provide the basis for personality manifestation, while normative versus heuristic-based decision rules provide the basis for behavioral manifestation. We apply these design bases in an experiment. The results demonstrate that IT design characteristics can be used to manifest desired personalities and behaviors in a technological artifact. Moreover, these manifestations of personality and behavior interact with the customer's own personality and behaviors to create matching perceptions of personality and behavioral similarity between the customer and the artifact. This study emphasizes the need to consider technological artifacts as social actors and describes the specific ways in which technology design can manifest social attributes. In doing so, we show that it is possible to match the social attributes of a technological artifact with those of the user.\", \"The Adoption of Online Shopping Assistants: Perceived Similarity as an Antecedent to Evaluative Beliefs In recent work, researchers have supplemented traditional IS adoption models with new constructs that capture users' relational, social, and emotional beliefs. These beliefs have given rise to questions regarding their antecedents and the nature of the user-artifact relationship. This paper sheds light on these questions by asserting that users perceive and respond to information technology (IT) artifacts as social partners and form perceptions about their social characteristics. Subsequently, users' perceptions of the similarity of these characteristics to their own affect evaluations of these artifacts. Within the context of online shopping and using an automated shopping assistant, our paper draws upon social psychology and human-computer interaction research in developing hypotheses regarding the effects of perceived personality similarity (PPS) and perceived decision process similarity (PDPS) on a number of beliefs (enjoyment, social presence, trust, ease of use, and usefulness). The results indicate that PDPS acts as an antecedent to these beliefs, while the effects of PPS are largely mediated by PDPS. Furthermore, the results reveal that the effects of perceived similarity, in general, exceed those of the effects of the individual assessments of the user's and the assistant's personalities and decision processes. These results have important implications for IS design. They highlight the importance of designing artifacts that can be matched to users' characteristics. They also underscore the importance of considering similarity perceptions rather than solely focusing on perceptions of the IT artifact's characteristics; a common approach in IS adoption research.\", \"Designing Online Virtual Advisors to Encourage Customer Self-disclosure: A Theoretical Model and an Empirical Test Virtual advisors (VA) are tools that assist users in making decisions. Using VAs necessitates the disclosure of personal information, especially when they are employed in personalized contexts such as healthcare, where disclosure is vital to providing valid and accurate advice. Yet, extant research has largely overlooked the factors that encourage or inhibit users' from disclosing to VAs. In contrast, this study investigates the determinants of users' intentions to self-disclose, and examines how VAs can be designed to enhance these intentions. The results of a study in the context of skin care advice reveal that the intention to disclose to a VA is not only the product of a rational process, but that perceptions of the VA and the relationship with it are important. The results further show that a parsimonious set of design elements can be used to endow a VA with desired characteristics that enhance the willingness to disclose. The study contributes to our understanding of the factors influencing users' intentions to provide personal information to a VA, which extend beyond the expected benefits and costs. The study further demonstrates that social exchange theory can be applied in contexts in which humans are interacting with automated VAs.\", \"The Adoption and Use of IT Artifacts: A New Interaction-Centric Model for the Study of User-Artifact Relationships The question of why a user adopts an information technology (IT) artifact has received ample research attention in the past few decades. Although recent adoption research has focused on investigating some of the relational and experiential aspects associated with adopting and using IT artifacts, the theories utilized have been static in nature. Furthermore, many have been based on traditional models like TAM and TPB, which focus on the utilitarian benefits that users accrue from their interactions with IT artifacts. Independently, recent research has paid much-needed attention to factors surrounding the use of IT artifacts. In this paper, we offer an overview of a theoretical model that connects these two interrelated processes. Starting with a survey of concepts related to social interactions, we present an argument in support of viewing IT artifacts as social actors, whose characteristics are manifested within the context of interactions. The proposed interaction-centric model highlights how the characteristics of an IT artifact, together with the user's internal system and other structuring factors, affect users' choices in terms of how to utilize the artifact. The nature of that utilization, subsequently, affects the beliefs users form about the artifact and the outcomes from using it. Furthermore, the model proposes that users will also form beliefs about their bond or relationship with the IT artifact. These beliefs do not refer to observations made in a single interaction, but rather concern users' mental representations of past interactions and outcomes. To facilitate the study of the relationship that develops from user-artifact interactions over time, the model describes how past interactions affect future ones. Specifically, it proposes that deciding how to utilize an IT artifact in subsequent interaction, consistent with theories of relationship development, is influenced by already held beliefs about the artifact and the relationship with it.\", 'An Empirical Investigation of the Antecedents and Consequences of Privacy Uncertainty in the Context of Mobile Apps When using digital goods that extensively collect user information, privacy uncertainty, which is consumers’ difficulty in assessing the privacy of the data they entrust to others, is a major concern. We extend the existing literature on uncertainty in online marketplaces by incorporating privacy uncertainty, and we distinguish among three subdimensions of privacy uncertainty—namely collection, use, and protection. We subsequently theorize and empirically test the antecedents and consequences of this new construct in the context of mobile apps. Consistent with economic theory, we argue that because consumers possess less information than the app seller as a result of the presence of hidden characteristics and hidden action, privacy uncertainty is evoked. Using the factorial survey method, we test our theoretical model in the context of buying a mobile app. The results show that privacy uncertainty significantly influences potential users’ intention to use an app above and beyond their uncertainty about the seller and the product. Privacy uncertainty also affects the perceived risk associated with using an app and the price consumers are willing to pay for it. In addition, we find that privacy uncertainty is driven by the uncertainty about privacy practices in regard to the collection, use, and protection of data collected at the time of downloading the app—and more important, the data collected while the app is being used. The results of another factorial survey study suggest that privacy uncertainty is distinct from seller and product uncertainties and has unique drivers. The results of a survey of current users of existing mobile apps indicate that the effects of privacy uncertainty extend to the postadoption stage, where it remains a strong influencer of continued use intentions and perceived risk.', 'BRM: A methodology for improving the practical relevance of belief-based information technology usage theories There has been extensive research in Information Systems (IS) regarding the individual-level beliefs that are germane to technology usage. Theories like the technology acceptance model, user satisfaction, service quality, the diffusion of innovations theory, and others identify a wide variety of influential beliefs, such as those that target properties of the system itself (e.g., its features), the use of it (e.g., its ease of use), or other contextual aspects (e.g., the pressure from peers to use it). These theories have been invaluable for understanding one of the most fundamental phenomena of interest to IS researchers and practitioners: individuals’ adoption and use of information technology (IT). However, as valuable as these theories are, there has been growing criticism about the fact that several of their constituting belief constructs (e.g., usefulness or quality) do not lend themselves to prescriptions for actionable interventions, in particular those geared toward IT design. This is especially prob\\xad lematic when it comes to applying or extending them to the contexts of emerging technologies, where best design practices are still to be discovered and tested. We address this concern by developing a Broadness Reduction Methodology (BRM) that relies on Fishbein and Ajzen’s (1975) foundational work on the nature and formation of beliefs to help researchers develop belief-based theoretical models that are more relevant to IT practitioners. In order to illustrate the application of our methodology and demonstrate its validity, we apply it to the context of ebusiness. In doing so, we decompose the broad construct of supporting-service functionality, which has been shown to be a strong predictor of satisfaction and continued usage, into more specific constructs that are more amenable to actionable design recommendations.', \"Taxing the development structure of open source communities: An information processing view Committers in Free/Libre and Open Source Software (FLOSS) projects shoulder responsibility for evaluating contributions and coordinating the broader community development effort. Given committers' central role in development processes, we examine whether how they are organized influences FLOSS community performance. Specifically, drawing on the lens of Organizational Information Processing Theory (OIPT), we develop a model that explains how committal a structure's ability to manage information impacts FLOSS community performance. Based on archival data drawn from 237 active FLOSS communities, we found that the performance of centralized and decentralized FLOSS communities varied with three conditions tied to information flows: task routineness, uncertainty and task interdependence. Our empirical results support the idea that FLOSS communities performing development tasks that are generally routine, highly interdependent, and generate little contributor uncertainty will perform better under a centralized committal structure. On the other hand, decentralized committal structures thrive under the conditions of task non-routineness, low task interdependence, and high contributor uncertainty. We conclude with a discussion of results, limitations, and directions for future research.\", 'Developing and validating an instrument for measuring user-perceived web quality Many of the instruments to measure information and system quality were developed in the context of mainframe and PC-based technologies of yesteryears. With the proliferation of the Internet and World Wide Web applications, users are increasingly interfacing and interacting with web-based applications. It is, therefore, important to develop new instruments and scales, which are directly targeted to these new interfaces and applications. In this article, we report on the development of an instrument that captures key characteristics of web site quality from the user’s perspective. The 25-item instrument measures four dimensions of web quality: specific content, content quality, appearance and technical adequacy. While improvements are possible, the instrument exhibits excellent psychometric properties. The instrument would be useful to organizations and web designers as it provides an aggregate measure of web quality, and to researchers in related web research.', 'An Integrated Performance Model of Information Systems Projects This study makes an initial attempt to validate an integrated, theoretically driven performance model of information systems (IS) projects. IS project performance is defined in terms of task, psychological, and organizational outcomes. We draw upon different theoretical perspectives including IS, organizational teams, and project management to link six categories of variables to IS project performance: technology characteristics, project characteristics, task characteristics, people characteristics, organizational characteristics, and work processes. Data collected via a field survey of IS project leaders in 84 manufacturing organizations were used to test the proposed model. Support is found for three conclusions: (1) IS project performance is a multidimensional construct, (2) certain preconditions falling into the above categories have to exist to achieve a high performing IS project, and (3) there is a possible cross-relationship among the variables studied by IS research, organizational teams research, and project management research. We discuss the implications of this study for future research and managerial practice.', 'An empirical examination of the role of social integration in system development projects Abstract. In spite of the apparent importance of social integration for work collectives within organizations, information systems researchers have so far paid little, if any, attention to evaluating its role in system development projects. The present study tries to contribute to the literature by proposing and testing a model that examines some of the antecedents and consequences of social integration in system development projects. Data collected from system development project leaders working in 84 US organizations were used to test the model. The findings suggest that higher social integration and, consequently, higher system development project performance is best attained when management provides basic support for the work of the project. The results also reveal that the nature of the relationship between social integration and project performance may be contingent upon some other factors. The implications of the findings of this research are discussed.', 'The development of two tools for measuring the easiness and usefulness of transactional Web sites The worldwide diffusion of Electronic Commerce shifts the exchange relationship between buyers and sellers from the face-to-face model to the face-to-screen model; and it is, therefore, important for organisations to consider the Web attributes that attract users. Yet, conceptualising, empirically testing, and refining tools for measuring the easiness and usefulness of transactional Web sites are lacking. The present study tries to fill this gap in the literature by describing the development of two tools for measuring perceived easiness and usefulness in a Web context. The findings of exploratory and confirmatory factor analyses reveal that the two instruments demonstrate sound measurement properties and would be useful to organisations interested in setting up an electronic business, and to scholars interested in Web research.', 'An assessment of the use of Transaction Cost Theory in information technology outsourcing Transaction Cost Theory (TCT) has been widely used in information technology outsourcing (ITO) research to explain and predict outsourcing decisions and outsourcing-related outcomes. This research, however, has led to mixed and unexpected results in terms of the effects of transaction attributes on outsourcing decisions and outcomes. This study assesses the empirical literature employing TCT-based ITO models in terms of its faithfulness to the precepts of TCT, and argues that one possible explanation for the mixed results is that the extant models do not capture all the essential elements of TCT. First, there are core TCT constructs that the extant models do not take into account; second, the linkages among constructs that the IT outsourcing models have hypothesized are not always in line with TCT precepts; and third, the normative nature of the theory is not always captured by the extant models. This paper, therefore, aims to provide one possible answer to the question: “Why have the appropriations made of TCT to study IT outsourcing produced mixed results?”', 'Platforms as service ecosystems: Lessons from social media The growing business expansion of social media platforms is changing their identity and transforming the practices of networking, data and content sharing with which social media have been commonly associated. We empirically investigate these shifts in the context of TripAdvisor and its evolution since its very establishment. We trace the mutations of the platform along three stages we identify as search engine, social media platform and end-to-end service ecosystem. Our findings reveal the underlying patterns of data types, technological functionalities and actor configurations that punctuate the business expansion of TripAdvisor and lead to the formation of its service ecosystem. We contribute to the understanding of the current trajectory in which social media find themselves as well as to the literature on platforms and ecosystems. We point out the importance of services that develop as commercially viable and constantly updatable data bundles out of diverse and dynamic data types. Such services are essential to the making of the complementarities that are claimed to underlie ecosystem formation.', 'Customer relationships and the small software firm: A framework for understanding challenges faced in marketing This paper identifies the major marketing challenges small software firms face during their growth and internationalization processes. It starts with an analysis of small software company activities along a continuum from ‘project business’ to ‘product business.’ This is followed by a brief analysis of the two major schools of thought in marketing, in which there is a paradigm shift from the traditional notion of marketing-mix management towards ‘relationship marketing’ is discernible. Finally, the discussion is summarized in a framework for identifying the major marketing challenges facing software company managers at the beginning of the next millenium.', \"Decision support capabilities of enterprise content management systems: An empirical investigation Enterprise content management (ECM) systems help organizations cope with the increasing complexity and volume of data and information. Despite the growing popularity of ECM, published literature indicates that organizations primarily use ECM for operational benefits, while the strategic decision making capabilities are rarely considered. Thus, the most significant rewards of ECM implementation may be largely forgone. This study investigates the potential of ECM technology for decision support. A research model is proposed and validated via an empirical investigation. The results show that ECM positively influences problem identification and definition, decision making speed and analysis, decision quality, and decision makers' satisfaction.\", 'Temporal Motivations of Volunteers to Participate in Cultural Crowdsourcing Work Crowdsourcing (CS) by cultural and heritage institutions engage volunteers in online projects without monetary compensation. Uncertainty concerning online volunteer motivation has led to a growing body of academic research. This study contributes to that debate, by extending focus to CS volunteer work in nonprofit cultural institutions where no monetary benefit is offered to volunteers. This study examines motivations of high performing volunteers in a newspaper digitisation CS project, initiated by the National Library of Australia. Volunteers are motivated by personal, collective, and external factors, and these motivations change over time. Volunteers initially show intrinsic motivations, though both intrinsic and extrinsic motivations play a critical role in their continued participation. Volunteer contributions range from data shaping (e.g., correcting digitised optical character recognition data) to knowledge shaping (e.g., shaping historical data through tagging and commenting, but also through development of norms and social roles). The locus of motivation (intrinsic or extrinsic) also changes with different kinds of contributions. The distinction between data and knowledge shaping contributions, and the locus and focus of motivation behind these activities, has implications for the design of CS systems. Design for improved usability through cognitive and physical system affordances and development of social mechanisms for ongoing participation is discussed.', \"The role of system-use practices for sustaining motivation in crowdsourcing: A technology-in-practice perspective The success of crowdsourcing (CS) systems depends on sustained participation, which is an ongoing challenge for the majority of CS providers. Unfortunately, participants are frequently demotivated by technical difficulties and the incorrect use of CS systems, which can result in CS failure. Although the literature generally assumes that sustained participation in CS is determined by a shift between intrinsic and extrinsic motivation, the role of system-use practices in facilitating such a shift remains unknown. We explore how CS system-use practices influence participants' sustained motivation, evolving from initiation to progression to sustention. Using the notion of technology-in-practice as a lens, we develop and examine a process model using an in-depth case study of a large-scale ongoing CS project, the Australian Newspaper Digitisation Program. The findings suggest that CS participants' motivation is shaped by an evolving combination of three basic components (i.e., contextual condition, outcome and action intensity) and mediated by two types of system-use practice (i.e., passive and active). Passive-use practices facilitate sustaining motivation from initiation to progression, whereas active-use practices have a key role in sustention. Our study contributes to the emerging literature on the substantial role of system-use practices in sustaining motivation, resulting in sustained participation. The findings also offer actionable insights into improving the viability of CS systems in retaining and motivating continuous and increased contributions from participants.\", 'Too Tired and in Too Good of a Mood to Worry About Privacy: Explaining the Privacy Paradox Through the Lens of Effort Level in Information Processing The confluence of digital transactions, growing cybersecurity threats, and the internet of the future (e.g., web 3.0 and the metaverse) have made information privacy increasingly important to consumers and companies that rely on consumers willingly sharing their personal information. Although information privacy has been of interest to researchers for decades and much has been learned, one thing that perplexes scholars is the privacy paradox, which we define as a mismatch between stated privacy concerns and actual disclosure behaviors. In this paper, we shed light on this phenomenon and show that low-effort information processing triggered by cognitive depletion (Experiment 1), positive mood (Experiment 2), or both (Experiment 3) significantly attenuates the association between stated privacy concerns and disclosure behaviors. These findings do not indicate that individuals do not care about privacy because we find consistent evidence in the three experiments for a significant negative association between stated privacy concerns and disclosure behaviors when individuals have sufficient cognitive capacity (Experiment 1), experience a negative (or neutral) mood (Experiment 2), or have sufficient cognitive capacity coupled with a negative mood state (Experiment 3). Our findings reveal that the paradox is neither an absolute phenomenon nor a myth, but its existence is conditional on contextual factors, including psychological factors related to information processing. We discuss our contribution to privacy theory and provide implications for consumers, companies, and policymakers. History: Alessandro Acquisti, Senior Editor; Idris Adjerid, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1182.', 'Revisiting DSS Implementation Research: A Meta-Analysis of the Literature and Suggestions for Researchers  Information systems are becoming increasingly critical to the daily operations and success of many firms. This, combined with the rising investments in design and development of these system.s, make implementation a high priority research topic. Although information systems implementation has been a topic of interest to researchers over the past two decades, the extent to which the existing body of research reflects substantial and cumulative development is not entirely clear.The objective of this study is to conduct a rigorous and quantitative review of the empirical DSS implementation fiterature as a basis for providing guidelines for implementation management and conduct of future research. Metaanalysis of 144 findings from 33 studies indicates that user-situational variables (involvement, training and experience) are more important than psychological factors to DSS implementation success and that user-situational variables can improve the implementation success by as much as 30 percent. Furthermore, the meta-analytic findings regarding the methodological characteristics of studies provide useful insights for the design of future research studies .of implementation. The findings also allow us to put into perspective the incremental contribution of additional substantive and empirical studies in this area. Additionally, several specific domains (e.g., construct validation research on user involvement and casual modeling) might profit most from future research efforts. Research Framework and Variables,~. large body of DSS implementation studies have investigated the relationship between userrelated factors and implementation success. The framework in Figure 1 encompasses different perspectives and common themes from the previous work in the DSS implementation genre. The core of the framework consists of four sets of user-related factors believed to influence DSS implementation success: cognitive style, personality, demographics, and user-situational variables. The relationships between these factors and DSS implementation are believed to be irffluenced by a number of contextual variables consisting of decision-making tasks, (e.g., task type or task complexity), organizational factors (e.g., top management support), and external factors (e.g., competitive considerations). Although the potential moderating effects of the contextual variables are recognized at a conceptual level, the empirical studies of user factors and DSS implementation are rarely inclusive of all the variable sets displayed in Figure 1. Therefore, our meta-analytic study was organized around the core relationships between user factors and DSS implementation success. ', \"An Empirical Examination of the Influence of Organizational Culture on Knowledge Management Practices Knowledge management to facilitate the creation, storage, transfer, and application of knowledge in organizations has received wide attention in practice and research in the past several years. Often cited as a significant challenge in knowledge management practices is the issue of organizational culture. Although many studies raise the issue of organizational culture's influence on knowledge management success, few investigate the way in which this influence manifests itself. This paper aims to explore how organizational culture influences knowledge management practices. Using a case study method, we examine the cultural values and knowledge management approaches within a large global information services company and one of its knowledge communities. The findings highlight the influence of culture on the use of knowledge management technologies and the outcomes of such use.\", 'Research Commentary: Technology-Mediated Learning—A Call for Greater Depth and Breadth of Research Universities and corporate training facilities have been investing in information technologies to improve education and training at an increasing rate during the past decade. Many new companies are emerging to provide tools and services to enable the effective design of IT-based learning solutions. Although research on technology-mediated learning has increased in recent years, it still lags behind developments in practice. This essay suggests potential research avenues in the area of technology-mediated learning. It seeks to motivate greater depth of research into the question of how technology enhances learning. This question requires an explicit consideration of relationships among technology capabilities, instructional strategy, psychological processes, and contextual factors involved in learning. The essay also recommends attention to a greater breadth of research questions, including issues of how technology-mediated learning affects program design and what structures and processes universities can employ to facilitate innovation.', 'Review:  Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues  Knowledge is a broad and abstract notion that has defined epistemological debate in western philosophy since the classical Greek era. In the past Alavi & Leidner/Knowledge Management ', \"A Comparative Study of Distributed Learning Environments on Learning Outcomes Advances in information and communication technologies have fueled rapid growth in the popularity of technology-supported distributed learning (DL). Many educational institutions, both academic and corporate, have undertaken initiatives that leverage the myriad of available DL technologies. Despite their rapid growth in popularity, however, alternative technologies for DL are seldom systematically evaluated for learning efficacy. Considering the increasing range of information and communication technologies available for the development of DL environments, we believe it is paramount for studies to compare the relative learning outcomes of various technologies. In this research, we employed a quasi-experimental field study approach to investigate the relative learning effectiveness of two collaborative DL environments in the context of an executive development program. We also adopted a framework of hierarchical characteristics of group support system (GSS) technologies, outlined by DeSanctis and Gallupe (1987), as the basis for characterizing the two DL environments. One DL environment employed a simple e-mail and listserv capability while the other used a sophisticated GSS (herein referred to as Beta system).Interestingly, the learning outcome of the e-mail environment was higher than the learning outcome of the more sophisticated GSS environment. The post-hoc analysis of the electronic messages indicated that the students in groups using the e-mail system exchanged a higher percentage of messages related to the learning task. The Beta system users exchanged a higher level of technology sense-making messages. No significant difference was observed in the students' satisfaction with the learning process under the two DL environments.\", 'Computer-Mediate Collaborative Learning: An Empirical Evaluation  National commissions and scholarly reports on the status of contemporary higher education have frequently been critical of the college, experience; the emphasis on transmitting fixed bodies of information and a failure to develop problem solving and critical thinking skills have been cited as serious weaknesses in higher education systems. Colleges and universities have additional reasons to redevelop central pedagogies for students. Individuals need to learn at higher rates of effectiveness and efficiency than ever before because of rapidly growing bodies of relevant information and the escalation of knowledge and skill requirements for most jobs.Recent developments in computer hardware, software, and communication technologies create exciting new opportunities for the educational use of these technologies. The objective of this study is to go I~eyond the traditional classroom instructional modes (e.g., lectures and class discussions) to develop and evaluate computer-supported pedagogical approaches. More specifically, this study investigates whether the use of a group decision support system (GDSS) in a collaborative learning process enhances student learning and evaluation of classroom experiences.The findings of a study involving 127 MBA students indicate that GDSS-supported collaborative learning leads to higher levels of perceived skill development, self.reported learning, and evaluation of classroom experience in comparison with non-GDSS supported collaborative learning. Furthermore, the final test grades of the group of students who were exposed to GDSS-supported collaborative learning were significantly higher than those of the other group of students who participated in the experimenL ', \"Gist: A Model for Design and Management of Content and Interactivity of Customer-Centric Web Sites Customer-centric Web-based systems, such as e-commerce Web sites, or sites that support customer relationship management (CRM) activities, are themselves information systems, but their design and maintenance need to follow vastly different approaches from the traditional systems lifecycle approach. Based on marketing frame-  works that are applicable to the online world, and following design science principles, we develop a model to guide the design and the continuous management of such sites. The model makes extensive use of current technologies for tracking the customers and their behaviors, and combines elements of data mining and statistical analyses. A case study based on a financial services Web site is used to provide a preliminary validation and design evaluation of our approach. The case study showed considerable measured improvement in the effectiveness of the company's Web site. In addition, it also highlighted an important benefit of the our approach: the identification of previously unknown or unexpected segments of visitors. This finding can lead to promising new  business opportunities.\", 'Marketplace and technology standards for B2B e-commerce: progress, challenges, and the state of the art We have examined standards required for successful e-commerce (EC) architectures and evaluated the strengths and limitations of current systems that have been developed to support EC. We find that there is an unfilled need for systems that can reliably locate buyers and sellers in electronic marketplaces and also facilitate automated transactions. The notion of a ubiquitous network where loosely coupled buyers and sellers can reliably find each other in real time, evaluate products, negotiate prices, and conduct transactions is not adequately supported by current systems. These findings were based on an analysis of mainline EC architectures: EDI, company Websites, B2B hubs, e-Procurement systems, and Web Services. Limitations of each architecture were identified. Particular attention was given to the strengths and weaknesses of the Web Services architecture, since it may overcome some limitations of the other approaches.', 'eHR software, multinational corporations and emerging China: Exploring the role of information through a postcolonial lens This paper seeks to offer an alternative account of Human Resources Information software (eHR) informed by a critical/postcolonial view on information systems. In so doing, it aims to explore the possibilities for managing people that information brings when Human Resources Management practices are transferred from “developed” to “developing” countries. The paper relies on several qualitative in-depth interviews with renowned Chinese Human Resources experts in Shanghai, and the examination of diverse eHR software-related documentation and functionalities. Critical discourse analysis was used to examine these sources. The findings show that eHR information systems bring new governance possibilities that support and expand the discipline of Human Resources Management. The use of eHR software in people management gives a new momentum and increased dominance to key Western-originated practices, such as HR-based performance management. Information brings new ordering options that facilitate the transferability, mobility and standardization of HR values, discourse and practices and, ultimately, the construction of a global “generified employee”. The paper offers a first critical analysis of eHR software, showing the need to understand the relevancy of the informating power of these systems for a postcolonial critique of ICT. It offers a view of the “micro-processes” that facilitate organizational transfer from the multinational corporation headquarters to the subsidiaries and across countries. In so doing, it challenges mainstream deterministic assumptions and apolitical approaches to this technology.', 'Clarifying the effects of Internet monitoring on job attitudes: The mediating role of employee trust The Internet is a fast growing mechanism for providing workplace monitoring. We examined how its implementation affects employees’ trust in the organization. We hypothesized that giving employees advance notice of monitoring and providing them a justification for it would enhance their trust. We investigated how employees’ perceptions of organizational support prior to monitoring moderated these relationships by conducting a longitudinal field experiment. We found that advance notice and perceived organizational support exerted significant main and interactive effects on post-implementation trust. In turn, trust significantly affected employees’ job satisfaction, organizational commitment, and turnover intentions.', 'Information systems control alignment: Complementary and conflicting systems development controls This study presents a new concept called information systems control alignment, which examines the degree that the underlying characteristics of four main information systems (IS) control dimensions are mutually complementary. Using three case studies, our research uncovers two high-functioning control patterns – one with traditional characteristics and one with agile characteristics – that demonstrate positive alignment among the control environment, control mechanisms, socio-emotional behaviors, and execution of controls. By better understanding the circumstances that contribute to control conflicts, organizations can be increasingly mindful of cultivating a complementary relationship among the control dimensions when designing, implementing, monitoring and adjusting controls within IS processes.', 'Utopia in the solution of the Bucket Order Problem This paper deals with group decision making and, in particular, with rank aggregation, which is the problem of aggregating individual preferences (rankings) in order to obtain a consensus ranking. Although this consensus ranking is usually a permutation of all the ranked items, in this paper we tackle the situation in which some items can be tied, that is, the consensus shows that there is no preference among them. This problem has arisen recently and is known as the Optimal Bucket Order Problem (OBOP). In this paper we propose two improvements to the standard greedy algorithm usually considered to approach the bucket order problem: the Bucket Pivot Algorithm (BPA). The first improvement is based on the introduction of the Utopian Matrix, a matrix associated to a pair order matrix that represents the precedences in a collection of rankings. This idealization constitutes a superoptimal solution to the OBOP, which can be used as an extreme (sometimes feasible) best value. The second improvement is based on the use of several items as pivots to generate the bucket order, in contrast to BPA that only uses a single pivot. The set of items playing the role of decision-maker is dynamically created. We analyze separately the contribution of each improvement and also their joint effect. The statistical analysis of the experiments carried out shows that the combined use of both techniques is the best choice, showing a significant improvement in accuracy (17%) with respect to the original BPA and providing an important reduction in the variance of the output. Moreover, we provide decision rules to help the decision maker to select the right algorithm according to the problem instance.', \"Advances in intelligent information technology: re-branding or progress towards conscious machines? Is artificial intelligence (AI) just something that is done in laboratories disconnected from the development of the pragmatic computing, which constitutes current information technology or does it contribute to progress in computing and information technology? It has even been suggested that advances in AI are merely a re-branding exercise for promises that are rarely kept. This paper is a personal view of the forces that have driven the development of AI in the past and what might be a serious paradigm shift in the future. The latter points to what appears to be the most abstruse corner of the subject: the modelling of the human brain and the possibility of designing systems with the brain's ability to create conscious thought. There have been accusations that AI is always ahead on promise and behind on delivery. This is an inaccurate view. In broad terms, the argument presented here suggests that as AI developed, progress was achieved by overcoming unforeseen difficulties in the pursuit of very ambitious targets, not just a re-branding of promises. This process not only advanced AI but also fed into the mainstream of computing that underpins the information technology of the present time. While the outcome of the paradigm shift towards conscious machines, which is examined at the end of this paper is still unclear, it is possible to speculate how information technology might be affected in the future.\", 'Partners of humans: a realistic assessment of the role of robots in the foreseeable future As robots are generally thought to perform human-like tasks, they depend on the successes of information technology in the area of artificial intelligence to succeed in such pursuits. But robots, through their anthropomorphic character and their weighty presence in science fiction, attract the attention of the press and the media in a way that, at times, blurs the distinction between the actual state of the art and exaggerated claims. This makes it hard to assess the true functional positioning of robots, how this is likely to move forward and whether the outcome of progress could be detrimental to human society. The aim of this paper is to review the actual level of competence that is being achieved in robotics research laboratories and a plausible impact that this is likely to have on human control over life and jobs. The key thesis here is that cognition in machines and even an artificial form of consciousness lead to operations in a set of tasks (the ‘algorithmic’ category) which is different from that available to truly cognitive and conscious human beings (the ‘life-need’ category): that is, in the paper it is argued that a major category error (Ryle in The concept of mind, University of Chicago Press, Chicago, 1949) looms in predictions of serious threats to humanity. As far as a threat to jobs goes, it is argued that early attention to education and re-skilling of humans in the workplace can lead to an effective symbiosis between people and robots.', nan, \"The Soft Edge: A Natural History and Future of the Information Revolution  Organizational memory and organizational learning are two approaches towards improving organizational processes. Organi zational memory is intended to augment the knowledge of groups by providing some record of the organization's know-how. Organizational learning on the other hand aims at turning organizations into systems able to think, reason, and learn as opposed to learning on the level of the individual. The business processes of modern organizations are highly supported by information technology, such as workflow systems, collaboration supporting tools, or special purpose applications. Recently, the intersecting area between the two fields, organizational memory and learning on the one hand, and information technology on the other hand, has gained a lot of attention in information systems research. Robert Neilson's book represents an interesting contribution to this area.To be more precise, the book presents the results of a case study investigating the effect of collaborative technology, namely Lotus Notes, on organizational learning. Two main issues are examined: \", nan, nan, nan, nan, 'Designing Collaborative Systems. A Practical Guide to Ethnography The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.', 'Knowledge Management Systems The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.', 'Visualizing Argumentation: Software Tools for Collaborative and Educational Sense-making The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.', 'The Business of Systems Integration The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.', 'A user-friendly marketing decision support system for the product line design using evolutionary algorithms A marketing decision support system (MDSS) is presented. It has a user-friendly and easy to learn menu driven interface. Its purpose is to assist a marketing manager in designing a line of substitute products. Optimal product line design is a very important marketing decision. The MDSS uses three different optimization criteria. It examines different scenarios using the “What if analysis”. Also, it finds optimal solutions only for small sized problems using the complete enumeration method and near optimal solutions for real sized problems using evolutionary algorithms. The user is not forced to be familiar with the underlying models.', 'Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks The goal of this study is to show an alternative method to corporate failure prediction. In the last decades Artificial Neural Networks have been widely used for this task. These models have the advantage of being able to detect non-linear relationships and show a good performance in presence of noisy information, as it usually happens, in corporate failure prediction problems. AdaBoost is a novel ensemble learning algorithm that constructs its base classifiers in sequence using different versions of the training data set. In this paper, we compare the prediction accuracy of both techniques on a set of European firms, considering the usual predicting variables such as financial ratios, as well as qualitative variables, such as firm size, activity and legal structure. We show that our approach decreases the generalization error by about thirty percent with respect to the error produced with a neural network.', 'Customer relationship management in call centers: The uneasy process of re(form)ing the subject through the ‘people-by-numbers’ approach Real-time technology has the capability of symbolising both customers and call center representatives (and the moment of interaction), purely by/as numbers, or forms. The pinnacle of this data processing is customer relationship management (CRM), where the digitised data is assembled so as to reproduce a mimetic model of the customer. This could be seen as a metamyth (Adams & Ingersoll, 1990) that, in its concealed appearance within corporate databases, seems to cuts loose from any critical inquiry. In this paper, we offer an embryonic form of such a critique through the analysis of a number of original call center case studies. It seeks to analyze the nature of abstraction at the heart of IT-based CRM practices, and the contradictions that such abstraction can foster.', \"An empirical study on the susceptibility to social engineering in social networking sites: The case of Facebook Research suggests that social engineering attacks pose a significant security risk, with social networking sites (SNSs) being the most common source of these attacks. Recent studies showed that social engineers could succeed even among those organizations that identify themselves as being aware of social engineering techniques. Although organizations recognize the serious risks of social engineering, there is little understanding and control of such threats. This may be partly due to the complexity of human behaviors in failing to recognize attackers in SNSs. Due to the vital role that impersonation plays in influencing users to fall victim to social engineering deception, this paper aims to investigate the impact of source characteristics on users' susceptibility to social engineering victimization on Facebook. In doing so, we identify source credibility dimensions in terms of social engineering on Facebook, Facebook-based source characteristics that influence users to judge an attacker as per these dimensions, and mediation effects that these dimensions play between Facebook-based source characteristics and susceptibility to social engineering victimization. © 2017 The OR Society.\", 'Adverse Selection in B2B Secondary Market Online Auctions for IT Equipment: An Empirical Analysis Online business-to-business auctions for used IT products have emerged as a viable market for finding a second life for these products, rather than having them end up in landfills as e-waste. As part of the growing “secondary market” landscape, these online B2B auctions are significantly affected by adverse selection since uncertainty about product quality from their first life remains in place. We study how these adverse selection costs may be identified and reduced in online B2B auctions for mobile phones using a proprietary data set for pallets of iPhone devices. We focus on the differences between carrier-locked and unlocked iPhones, and the degree to which the jailbreaking of devices may lead to adverse selection costs. We first show that uncertainty with respect to the possibility of jailbreaking-to-unlock induces significant adverse selection costs in this market. We identify a clear method that these adverse selection costs may be reduced through policies implemented in the primary market. We find that adverse selection costs exist with respect to jailbreaking-to-unlock, by comparing prices obtained for locked and unlocked devices, as well as pallets where this information is not disclosed. However, when some of the uncertainty surrounding jailbreaking-to-unlock is removed by virtue of an exogenous policy change implemented by Verizon in the primary market, i.e., to sell all iPhones as factory unlocked, adverse selection costs are significantly reduced. Our work has significant implications for enhancing the efficiency of secondary markets for IT products, by virtue of highlighting the connections between primary and secondary markets. Managerial and theoretical implications that emerge from this work are discussed in the paper.', 'Impressionable or Immune? Examining the Influence of Marquee Sellers in B2B Secondary Market Platforms for IT Products Although digital platforms have become mainstream, there still remain some unanswered questions pertaining to managing platform ecosystems. One such unexplored question pertains to the effect of marquee sellers on the platform—marquee sellers arguably attract other sellers and buyers to the platform, thereby enhancing the platform’s value. In this paper, we study how adding a marquee seller to a business-to-business (B2B) secondary-market platform for IT products affects other sellers, in terms of the prices they obtain for comparable products. Using proprietary data on B2B secondary-market auctions obtained from a platform provider, we show that the entry of a marquee seller has a positive effect on the prices obtained by other sellers on the platform, reflecting a reference-price effect. We further show that this positive effect on final prices is moderated by the extent to which bidders are active on multiple seller sites on the same platform and the extent to which bidders participate in the marquee seller site. We explain these effects using theory in multihoming and involvement, in terms of how reference prices are set. Our work extends the platforms literature by considering the specific influence of a marquee seller on other sellers, thereby informing platform owners on the implications of the advice to add more marquee sellers to platforms. We also contribute to the literature on secondary markets for durable, used IT products, which are instrumental in reducing electronic waste and mitigating the environmental damage done by electronic products in landfills.History: Raghu Santanam, Senior Editor; Pallab Sanyal, Associate Editor.Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1137.', 'Linking dimensions of social media use to job performance: The role of social capital Organizations are increasingly adopting new technologies, such as social media, that afford employees a repertoire of uses not simply focused on work, but also on socialization and entertainment. Knowledge regarding the impact of such diverse technologies on job performance, however, is currently limited. This study adopts a technology use lens to study the effect of three categories of social media use – social, hedonic, and cognitive – on job performance, as mediated by three dimensions of social capital. The research was conducted via a large-scale survey within a multinational Information Technology company. Social and cognitive uses of technology were empirically shown to have a positive, albeit indirect, effect on employees’ routine and innovative job performance. Hedonic use of the technology, while having a direct negative impact on routine performance was shown to positively contribute to the development of social ties, leading to a mitigating positive influence on innovative performance. This interesting positive side of hedonic use, along with all findings from our study, are discussed and used to offer insights to future research and practice.', 'Intelligent Web proxy caching approaches based on machine learning techniques In this paper, machine learning techniques are used to enhance the performances of conventional Web proxy caching policies such as Least-Recently-Used (LRU), Greedy-Dual-Size (GDS) and Greedy-Dual-Size-Frequency (GDSF). A support vector machine (SVM) and a decision tree (C4.5) are intelligently incorporated with conventional Web proxy caching techniques to form intelligent caching approaches known as SVM–LRU, SVM–GDSF and C4.5–GDS. The proposed intelligent approaches are evaluated by trace-driven simulation and compared with the most relevant Web proxy caching polices. Experimental results have revealed that the proposed SVM–LRU, SVM–GDSF and C4.5–GDS significantly improve the performances of LRU, GDSF and GDS respectively.', 'Post-Story: Influence of Introducing Story Feature on Social Media Posts Driven by the need to enhance user traffic on social media (SM) platforms for increasing their advertising revenues, SM platforms are experimenting with new content creation features. However, it is unclear if such initiatives are also beneficial for SM profile owners such as influencers, who are the prime content creators on the SM platforms who use SM posts to build their influence within their network of followers. Our study investigates the effect of introducing one such new SM feature: the \"story\" on the creation and consumption of SM posts. Leveraging social penetration theory, we hypothesize the influence of introducing story feature on (1) the frequency of SM post creation by profile owners and (2) the extent of follower engagement with SM posts. Employing a quasi-experimental design, we find that the introduction of the story feature reduces the frequency of SM post creation, but the enhanced self-disclosure through the story feature increases follower engagement with the SM posts. However, these effects are moderated by the situating culture of the SM communities: while low-power-distance cultures value profile owners\\' self-disclosure, high-power-distance cultures exhibit a mixed influence. Advancing literature on social penetration theory and SM user engagement, our study demonstrates that new self-disclosive SM content creation features do not necessarily benefit all the concerned stakeholders and that the effectiveness of such features might vary from one community to another. Hence, the intended impact of introducing new SM features needs to be carefully evaluated by SM platforms in a holistic manner.', \"A collaborative filtering approach for recommending OLAP sessions While OLAP has a key role in supporting effective exploration of multidimensional cubes, the huge number of aggregations and selections that can be operated on data may make the user experience disorientating. To address this issue, in the paper we propose a recommendation approach stemming from collaborative filtering. We claim that the whole sequence of queries belonging to an OLAP session is valuable because it gives the user a compound and synergic view of data; for this reason, our goal is not to recommend single OLAP queries but OLAP sessions. Like other collaborative approaches, ours features three phases: (i) search the log for sessions that bear some similarity with the one currently being issued by the user; (ii) extract the most relevant subsessions; and (iii) adapt the top-ranked subsession to the current user's session. However, it is the first that treats sessions as first-class citizens, using new techniques for comparing sessions, finding meaningful recommendation candidates, and adapting them to the current session. After describing our approach, we discuss the results of a large set of effectiveness and efficiency tests based on different measures of recommendation quality.\", \"Identity matching and information acquisition: Estimation of optimal threshold parameters With the growing volume of collected and stored data from customer interactions that have recently shifted towards online channels, an important challenge faced by today's businesses is appropriately dealing with data quality problems. A key step in the data cleaning process is the matching and merging of customer records to assess the identity of individuals. The practical importance of this research is exemplified by a large client firm that deals with private label credit cards. They needed to know whether there existed histories of new customers within the company, in order to decide on the appropriate parameters of possible card offerings. The company incurs substantial costs if they incorrectly “match” an incoming application with an existing customer (Type I error), and also if they falsely assume that there is no match (Type II error). While there is a good deal of generic identity matching software available, that will provide a “strength” score for each potential match, the question of how to use the scores for new applications is of great interest and is addressed in this work. The academic significance lies in the analysis of the score thresholds that are typically used in decision making. That is, upper and lower thresholds are set, where matches are accepted above the former, rejected below the latter, and more information is gathered between the two. We show, for the first time, that the optimal thresholds can be considered to be parameters of a matching distribution, and a number of estimators of these parameters are developed and analyzed. Then extensive computations show the effects of various factors on the convergence rates of the estimates.\", \"Is more always better? Investigating the task-technology fit theory in an online user context We used Task-Technology Fit (TTF) theory to examine the drivers and consequences of successful task completion by a user in an online context. The theory suggests that the fit between characteristics of the task and those of the website predicts user performance and behavioral intentions. Our hypotheses were tested using the input of two large scale studies performed in twelve industries and involving 13,135 participants. Results, which were replicated in a proximate culture, lend support to the predictions of Task-Technology Fit theory. The site information quality and ease of use were the only technology factors that significantly drove the users to a successful completion of their information tasks, rather than the site's graphical attractiveness, interactivity, security and privacy factors. The findings further suggested that focusing on the enhancement of site characteristics that have low fit with the task is not effective as it resulted in slowing the successful completion of the online task.\", nan, 'Information Architecture: In Search of Efficient Flexibility  This article addresses how information systems architecture can be used to support organizations in the 1990s--organizations that face the dual challenge of \"speed and flexibility\" and \"low cost and efficiency. \"At the heart of this challenge is the basic notion that information systems have been anything but flexible in the past and .that, for many firms, information systems are more disablers of flexibility than enablers. The article discusses two architectural solutions to this problem: \"the high road and the low road, \"and the benefits and pitfalls of each. We conclude that neither solution will succeed on its own and that firms need to combine elements of both to meet the challenges of the 1990s. This article is based on some of the things we have learned through research, case writing, and consulting while working with a variety of organizations over the past three years. These experiences have illustrated the importance of and the struggle with IS architecture for today\\'s global competitors. The content is intended to help guide, provoke, stimulate, and entertain others who believe that the integration of information technology with organizational strategy and structure is of paramount concern to senior managers. ', 'How Should Technology-Mediated Organizational Change Be Explained? A Comparison of the Contributions of Critical Realism and Activity Theory In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within \"activity systems\" and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the \"inscription\" of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.', 'Trust, power and interorganizational information systems: the case of the electronic trading community TransLease Abstract. This paper focuses on Cap Gemini’s electronic commerce system, TransLease. TransLease is an interorganizational information system (IOS), which facilitates electronic commerce between motor vehicle leasing and repair companies. During our investigation, the system was used by approximately 1000 repair agents working for seven of the UK’s leading vehicle leasing and contract hire companies. This system was originally developed by AT&T and acquired by Cap Gemini in July 1998. At the time of acquisition, the system was seen as being of high strategic value, although it was also seen as underperforming. This paper reports the results of an action research project, which formed one element of the process by which Cap Gemini investigated the former problem. In the paper, TransLease is described as a complex electronic community, dependent upon the existence of symbiotic relationships. As such, the problems that the system users and developers experienced can be attributed to factors that impeded the mutual benefit accruing from participation in the system. The efficacy of the terms of exchange and the degree to which participants mutually benefit through electronic interaction is determined by the complex interplay of a number of relational and organizational factors. The research therefore illustrates the importance of the ‘soft’ organizational issues in IOS management and development, and suggests a conceptual model of the factors relevant in this case. At the time of this study, TransLease was still in the early stages of its life cycle, having only been available in the marketplace for approximately 18 months. During this time, through recognizing the complex problems and issues detailed in this paper, Cap Gemini accordingly redressed the way in which the system was managed and maintained. TransLease is now seen as having matured into a highly successful example of an IOS – a view reflected by its position as market...', 'Information sharing and interoperability: the case of major incident management Public sector inter-organisational information sharing and interoperability is an area of increasing concern and intense investment for practice and an area of increasing scholarship. This paper focuses on one particular set of public sector organisations (emergency services) and illuminates the key technological and organisational issues they face concerning information sharing and interoperability. The particular contexts in which these are studied are ones where decisions are non-trivial and made in high-velocity environments. In these conditions the problems and significance of inter-organisational information sharing and interoperability are accentuated. We analyse data gathered from two studies: the first focused on ‘first responders’ (police, fire and ambulance services) in the United Kingdom. The second, a follow on study, with emergency service managers and interoperability project managers in the United Kingdom and the European Union. Using activity theory as a conceptual framework we describe the informational problems critical emergency responders face in their initial response to, and management of, an incident. We argue that rather than focusing on interoperability as a primarily technological issue it should be managed as an organisational and informational issue. Second, we argue that rather than designing for anomalous situations we should design systems, which will function during both anomalous and routine situations. Third, we argue for focus on harmonisation of policies, procedures and working practices.', 'Culture, power and politics in ICT outsourcing in higher education institutions UK public sector policy changes have driven Higher Education Institutions (HEI) towards a competitive and often turbulent market-focused environment. To respond to these dramatic institutional changes, many institutions began to strategically re-focus their management efforts on adapting and surviving in this environment. As part of their efforts, HEIs identified their Information and Communication Technology (ICT) function as an essential function to survive in this environment. This implied that HEIs had to address the institutional role of ICT by defining detailed strategies that aligned the ICT function to the HEIs educational goals. On the other hand, HEIs had to make sure their ICT could support their technology and service requirements, for which they considered pursing a more radical approach, that of outsourcing their ICT to a third party supplier. This research paper reports on embryonic attempts by three British Universities to outsource their ICT, highlighting in particular the ‘cultural, power and political’ issues that arose when public sector institutions follow the example of private sector organisations—by outsourcing to a third party service supplier.', \"Information Systems Research Behaviors: What Are the Normative Standards? Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers' careers influence their judgments of these behaviors. Membership in the Association for Information Systems (AIS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers' judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the AIS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.\", 'Academic Data Collection in Electronic Environments: Defining Acceptable Use of Internet Resources Academic researchers access commercial web sites to collect research data. This research practice is likely to increase. Is this appropriate? Is this legal? Such commercial web sites are maintained to achieve business objectives; research access uses site resources for other purposes. Web site administrators may, therefore, deem academic data collection inappropriate. Is there a process to make research access more open and acceptable to web site owners and administrators? These are significant issues. This article clarifies the problems and suggests possible approaches to handle the issues with sensitivity and openness. Research access to commercial web sites may be manual (using a standard web browser) or automated (using automated data collection agents). These approaches have different effects on web sites. Researchers using manual access tend to make a limited number of page requests because manual access is costly to perform. Researchers using automated access methods can request large numbers of pages at a low cost. Therefore, web site administrators tend to view manual access and automated access very differently. Because of the number of accesses and the nonbusiness purpose, automated research requests for data are sometimes blocked by site administration using a variety of means (both technological and legal). This paper details the pertinent legal issues including trespass, copyright violation, and breech of contract. It also explains the nature of express and implied consent by site administration for research access. Based on the issues presented, guidelines for researchers are proposed to reduce objections to research activities, to facilitate communication with web site administration, and to achieve express or implied consent. These include notification to web site administration of intended automated research activity, description of the research project posted as a web page, and clear identification of automated requests for web pages. In order to encourage good research practices with respect to automated data collection, suggestions are made with respect to disclosing methods used in research papers and for self regulation by academic associations', \"The Effects of State-Based and Event-Based Data Representation on User Performance in Query Formulation Tasks Ad hoc query formulation is an important task in effectively utilizing organizational data resources. To facilitate this task, managers and casual end-users are commonly presented with database views expressly constructed for their use. Differences in the way in which things, states, and events are represented in such views can affect a user's ability to understand the database, potentially leading to different levels of performance (i.e., accuracy, confidence, and prediction of the accuracy of their queries). An experiment was conducted over the Internet involving 342 subjects from 6 universities in North America and Europe to investigate these effects. When presented with an event-based view, subjects expressing low or very low comfort levels in reading entity-relationship diagrams expressed confidence that better predicted query accuracy although there were no significant differences in actual query accuracy or level of confidence expressed.\", 'A Research Note on Representing Part-Whole Relations in Conceptual Modeling Empirical research is an important methodology for the study of conceptual modeling practices. The recently published article \"Representing Part-Whole Relations in Conceptual Modeling: An Empirical Evaluation\" (Shanks et al. 2008) uses the lens of ontology to study a relatively sophisticated aspect of conceptual modeling practice, the representation of aggregation and composition. It contends that some analysts argue that a composite should be represented as a relationship while others argue that a composite should be represented as an entity. We find no evidence of such a dispute in the data modeling literature. We observe that composites are objects. By definition, all object-types should be represented as entities. Therefore, using the relationship construct to represent composites should not be seen as a viable alternative. Additionally, we found significant conceptual and methodological issues within the study that call its conclusions into question. As a way to offer insight into the requisite methodological procedures for research in this area, we conducted two experiments that both explicate and address the issues raised. Our results call into question the utility of using ontology as a foundation for conceptual modeling practice. Furthermore, they suggest a contrary but at least equally plausible explanation for the results reported by Shanks et al. In conducting this work we hope to encourage dialogue that will be beneficial for future endeavors aimed at identifying, developing, and evaluating appropriate foundations for the discipline of conceptual modeling.', 'Is Query Reuse Potentially Harmful? Anchoring and Adjustment in Adapting Existing Database Queries Reusing database queries by adapting them to satisfy new information requests is an attractive strategy for extracting information from databases without involving database specialists. However, the reuse of information systems artifacts has been shown to be susceptible to the phenomenon of anchoring and adjustment. Anchoring often leads to a systematic adjustment bias in which people fail to make sufficient changes to an anchor in response to the needs of a new task. In a study involving 157 novice query writers from six universities, we examined the effect of this phenomenon on the reuse of Structured Query Language (SQL) queries under varying levels of domain familiarity and for different types of anchors. Participants developed SQL queries to respond to four information requests in a familiar domain and four information requests in an unfamiliar domain. For two information requests in each domain, participants were also provided with sample queries (anchors) that answered similar information requests. We found evidence that the opportunity to reuse sample queries resulted in an adjustment bias leading to poorer quality query results and greater overconfidence in the correctness of results. The results also indicate that the strength of the adjustment bias depends on a combination of domain familiarity and type of anchor. This study demonstrates that anchoring and adjustment during query reuse can lead to queries that are less accurate than those written from scratch. We also extend the concept of anchoring and adjustment by distinguishing between surface-structure and deep-structure anchors and by considering the impact of domain familiarity on the adjustment bias.', \"How well do shopbots represent online markets? A study of shopbots’ vendor coverage strategy Consumers often use shopbots to search for information when making purchase decisions in Internet markets. Although they have varying sensitivity to shopbot bias, consumers generally prefer accurate market representation. However, in choosing the accuracy of market representation, shopbots must balance the desires of consumers with the costs of providing their services and with the desires of the vendors, who are often the largest source of their revenue. In this paper, we study how accurately shopbots represent a market and analyze the strategies shopbots adopt to achieve market representativeness. We theoretically identify two important drivers in shopbot vendor coverage strategy – how many vendors it covers (shopbot size) and which vendors it covers (shopbot affiliation) – and analytically show how the drivers affect shopbot market representativeness. We report the results of a large-scale study in which we collected 2.2 million vendor price listings from eight shopbots and develop metrics for measuring shopbot size, shopbot affiliation, and shopbot market representativeness. We found that (1) shopbots do not represent markets equally well; (2) size drives a shopbot's market representativeness positively whereas affiliation drives a shopbot's market representativeness negatively; (3) shopbots follow differnet vendor representative strategies to pursue market representativeness.\", 'IT and the video game industry: tensions and mutual shaping This paper examines the influence of information technology (IT) on a distinct but closely related industry, the video game industry. We conceptualize the effects of IT as a process of translating three related dimensions of a technological frame – technology performance, industry practices, and use vision – from one industry to another. Through historical examples, we argue that the impact of IT on the video game industry is shaped and limited by this translation process, particularly when tensions between the two industries lead to the development of new complementary or replacement technologies, practices, or visions. Although heavily dependent on IT, the video game industry has had to ignore, postpone, or substantially modify important IT software tools, processors, storage media, graphics, and networking technologies because of these industry contradictions.', 'Factors impacting the perceived organizational support of IT employees Organizations today face shortages of IT personnel. We investigated workplace factors in one state government in hope of identifying factors that influence perceived organizational support (POS) within an IT work environment. A combination of job characteristics (challenging job and perceived workload), job stressors (work exhaustion, role conflict, and role ambiguity), and the organization’s discretionary actions (pay-for-performance and mentoring opportunities) were measured and hierarchical regression was used to determine the relationships. Four control variables were also included (age, gender, organizational tenure, and professional versus administrator status). Role ambiguity, role conflict, work exhaustion, career mentoring, and pay-for-performance together explained 62% of the variance in the IT employees’ POS. Career mentoring and role ambiguity explained most of the variance.', \"A co-evolutionary complex systems perspective on information systems The co-evolution of information systems (IS) and the processes that underpin the construction and development of IT systems are explained from a complex systems perspective. Evolution operates at the microscopic level; in organizations, this is the individual or agent. Each agent has an idiosyncratic view of the organization, using to some extent personal constructs in dealing with the reality of organizational life. These objects or constructs can be described and measured by most agents; they are well defined. Many of these objects are represented in electronic, IT systems. Each agent also has their own view as to how they know what they know, that is, their epistemology, which we argue is their IS, and is wider than the IT systems they use. The IS of each agent co-evolves, by interaction with other agents, based on the agent's view of reality. The interaction of all agents constitutes the organization. Even more importantly, different values and interests motivate each agent. This is their axiology and it is what motivates them to learn and to develop their IS. An agent-based axiological framework is essential to understanding the evolution of organizations. It is the interaction of agents that builds consensus as to the shared reality of the organization, and this affects each agent's ability and motivation to evolve IS further. In addition, we propose that it is time that IT systems included modelling capabilities, based on multi-agent representations of the organization and its context, to explore and support strategic thinking and decision making.\", \"User Managers' Systems Needs  A bstractBased on responses from over five hundred user managers, this article investigates managers' demand for new application systems. To begin, the current situation is assessed from two aspects. First, how many systems by type do user managers now have and how appropriate are the systems. Second, for important managerial tasks, what support (by systems type) users have and how appropriate are those systems. Then the two components of user managers' demand for new systems, the number of systems and the types of systems, are examined. The results reveal an overwhelming level of managerial demand for new systems and major shifts in demand mix by systems type. The implications of this current and future demand for IS management are presented. \", 'A research note regarding the development of the consensus on appropriation scale Measurement is perhaps the most difficult aspect of behavioral research. In a recent edition of ISR, a scale for consensus on appropriation was developed. Consensus on appropriation is one of [...]', 'Tun-OCM: A model-driven approach to support database tuning decision making Database tuning is a task executed by Database Administrators (DBAs) based on their practical experience and on tuning systems, which support DBA actions towards improving the performance of a database system. It is notoriously a complex task that requires precise domain knowledge about possible database configurations. Ideally, a DBA should keep track of several Database Management Systems (DBMS) parameters, configure data structures, and must be aware about possible interferences among several database (DB) configurations. We claim that an automatic tuning system is a decision support system and DB tuning may also be seen as a configuration management task. Therefore, we may characterize it by means of a formal domain conceptuali\\xad zation, benefiting from existing control practices and computational support in the configuration management domain. This work presents Tun-OCM, a conceptual model represented as a well-founded ontology, that en\\xad compasses a novel characterization of the database tuning domain as a configuration management conceptu\\xad alization to support decision making. We develop and represent Tun-OCM using the CM-OPL methodology and its underlying language. The benefits of Tun-OCM are discussed by instantiating it in a real scenario.', 'Multivariate data quality assessment based on rotated factor scores and confidence ellipsoids This study explores the nature of the correlation in data to estimate the data quality to be used in decision-making processes. The main contribution of this research is the introduction of a new multivariate method based on rotated factor scores by varimax strategy for the repeatability and reproducibility study to effectively identify possible data of poor quality leading to measurement errors. In addition, a new confidence ellipsoid-based decision support method is developed. The efficiency of the proposed method was demonstrated using the metallographic measurements of the geometric characteristics of the resistance spot welding process. To prove the efficiency of the proposed method, it was compared with other consolidated techniques such as the analysis of variance, weighted principal components method, and factor analysis without rotation. Thus, we verified that the proposed method performed better interpretation of the latent information, minimizing the dimensionality of the data, and separating the quality attributes analyzed by clusters. One response group was classified as acceptable, and the other as marginal. These results were verified by the confidence ellipsoids, in which the proposed method obeyed the Bonferroni bilateral limits, outlining the factors which demonstrated superior discriminatory power with non-overlapping ellipsoids avoiding the confounding and favoring the better data quality analysis for multicriteria decision-making. When compared with the other approaches, the proposed method demonstrated more reliable and robust results without such deficiencies as inversion of the groupings, neglection of the variance-covariance structure, and the variability attributed to the data within the measurement system.', 'Situated with Infrastructures: Interactivity and Entanglement in Sensor Data Interpretation This paper elaborates on situatedness as an empirical phenomenon in computer-mediated settings. It is based on studies of petroleum engineers and how they work with digital sensor data. We show how their work practices are born out of a history of constitutive entanglement with specific types of sensors, the data they produce, and the information systems that process them. This entanglement arises from interaction between humans, technology, and the oil reservoir and is a fundamental aspect of the situations in which interpretative work occurs. We empirically show how different sensors in the petroleum production systems produce data in interaction with their surroundings, and that these data are creatively \"stretched\" to represent subsurface phenomena. When groups of engineers collaborate remotely with colleagues to make sense of problematic data, entanglement with specific II\\'s is an important aspect of situatedness. The situationally particular in these settings is not as much a matter of locations as of histories of interaction with specific technologies. The notion of situatedness has been pivotal in stressing the importance of the particular circumstances in which work is performed. It has throughout its history been a counterweight to rationalistic accounts of work and the focus on design of standardized work processes. Here we show that patterns of interaction with specific information infrastructures make up a crucial part of situated work and that these may have non-local dimensions.', 'Team Size, Dispersion, and Social Loafing in Technology-Supported Teams: A Perspective on the Theory of Moral Disengagement The article reports the results of a study which investigated social loafing in a team setting. The study involved 32 teams of students assigned to brainstorming tasks using group systems software. Cognitive mechanisms derived from moral disengagement theory were tested as possible drivers of social loafing behavior. These included attribution of blame, diffusion of responsibility, and dehumanization. All of these correlated with the effect of team size on social loafing, but only dehumanization was found to mediate the effect of dispersion.', 'Risk management in ERP project introduction: Review of the literature In recent years ERP systems have received much attention. However, ERP projects have often been found to be complex and risky to implement in business enterprises. The organizational relevance and risk of ERP projects make it important for organizations to focus on ways to make ERP implementation successful. We collected and analyzed a number of key articles discussing and analyzing ERP implementation. The different approaches taken in the literature were compared from a risk management point of view to highlight the key risk factors and their impact on project success. Literature was further classified in order to address and analyze each risk factor and its relevance during the stages of the ERP project life cycle.', 'A comparative axiomatic characterization of the Banzhaf–Owen coalitional value A compact axiomatic characterization of the modified Banzhaf value for games with a coalition structure (Banzhaf–Owen value, for short) is provided. The axiomatic system used here can be compared with parallel axiomatizations of other coalitional values such as the Owen value or the Alonso–Fiestras value, thus giving arguments to defend the use of one of them that will depend on the context where they are to be applied.', 'A new power index based on minimal winning coalitions without any surplus In this paper we propose a new power index useful for the evaluation of each member in a committee, or democratic institution, and the degree of influence over the voting decision making system. The proposed solution is based on the observation that democratic organizations not only tend to form coalitions which can by themselves guarantee the control of the organization, but that they also do it in an extremely efficient way that avoids the inclusion of powerful members if they can be replaced by weaker ones. The mathematical foundation of the new measure is based on two different axiomatizations. A comparison with other well-known measures is also done.', 'Understanding web site redesigns in small- and medium-sized enterprises (SMEs): a U.K.-based study on the applicability of e-commerce Stage Models Despite the efforts of governments and the various support programmes, achievement of advanced stages of e-commerce by small- and medium-sized enterprises (SMEs) is still very low. There have been some attempts to study the dynamic nature of websites, but there is still little research evidence to explain why and how SMEs evolve their web presence. This paper aims to develop a comprehensive classification of drivers for web site redesign based on interviews with various members of staff from SMEs in the U.K. that have recently redesigned their web sites. A sequential mixed-methodological analysis, involving the use of qualitative and quantitative data analysis, was used to develop the classification. This enabled the development of a framework that classified seven main categories of drivers for web site redesign. The drivers identified were: changing business requirements, evolving internet strategies, addressing user needs, maintenance, changing technology, pressure from peers/competitors, and the influence of developers. However, only the first four were found to be significant in the study. The categorisation and the findings suggest a number of key determinants not explicitly addressed by other work. In addition, the findings provide little support for the staged approach to e-commerce progression as few companies reported the implementation of sophisticated internet technology features as a main reason for their web site redesigns. The contributions of this paper are firstly, to provide an instrument to the academic and practitioner communities interested in the topic of web site evolution. Secondly, the categorisation of drivers for redesign and the individual reasons found in this study are expected to provide assistance to SME managers to justify, plan and strategise internet investments realistically and effectively.', 'Flaming in electronic communication Communication through computer networks, electronic salons, and virtual communities has its price. Often relatively anonymous and socially detached, electronic communication allows people to write things online that they would seldom consider saying face-to-face, sometimes generating flames. In a study of the motives to flame based upon Uses and Gratifications Theory (UGT), 160 subjects generated comments anonymously in parallel with a group support system (GSS) idea generation program. Results showed that high levels of assertiveness and sensation seeking predicted flaming, and males tended to participate more in the activity than did females.', \"Sequential Pricing of Multiple Products: Leveraging Revealed Preferences of Retail Customers Online and with Auto-ID Technologies Technological advances enable sellers to price discriminate based on a customer's revealed purchasing intentions. E-tailers can track items in online shopping carts and radio frequency identification tags enable retailers to do the same in brick-and-mortar stores. To leverage this information, it is important to understand how this new visibility impacts pricing and market outcomes. We propose a model in which a seller sets prices for goods A and B, allowing for the possibility of sequentially revising the price for good B if the buyer reveals a preference for good A by making an initial purchase decision. We derive comparative statics results for the prices of products that have superadditive or subadditive values, and also for the associated profits. We also run simulations for a range of distributions of buyer values, to compare sequential pricing with mixed bundling. The results indicate that information technology-enabled sequential pricing can increase profits relative to mixed bundling or pure components pricing for substitute goods due to a reduction of intraseller competition. We also consider the case of goods with positively or negatively correlated values and find that when sellers can condition the second good's price on the buyer's decision to purchase the first good, sequential pricing increases profits when customer's values for the goods are highly positively correlated.\", 'Market share analysis and prognosis using qualitative reasoning Today, extensive quantitative modelling of the performance of marketing activities is possible due to the availability of rich data sets on product sales and related marketing actions. However, there are a number of product categories for which only inaccurate sales figures or data about marketing efforts exist (e.g., because a significant part of the sales is realized in stores without scanners or which do not make the data available to firm outsiders). In such cases, reliable quantitative findings about the impact of marketing-mix variables on sales may not be achievable but qualitative reasoning may, at least, indicate likely implications of past or planned marketing activities in terms of directions of change. Even in markets with good data qualitative reasoning may be a worthwhile first modelling approach when market conditions change so significantly that a previously used model is deemed unsatisfactory and not enough data exist yet for a specification of a revised quantitative model. We show specifically how qualitative reasoning can be used to diagnose and predict market share changes. The diagnostic part represents a combination of qualitative reasoning and extensions of a previously published rule-based expert system for that task while the prognostic part is solely based on qualitative process theory and order of magnitude reasoning.', 'Understanding the global diffusion of B2B E-commerce (B2B EC): An integrated model Using institutional theory, this study offers an integrated framework that describes the diffusion of business-to-business e-commerce within a country. The model specifies the role of national institutional frameworks, international institutional pressures, and market complexity in business-to-business e-commerce diffusion. We test this model using archival, cross-sectional data from 146 countries for the period from 2013 to 2016. The study also compares the roles of these factors in business-to-business e-commerce diffusion across developed and developing countries. The results suggest that national institutional frameworks, international institutional pressures, and market complexity contribute positively to business-to-business e-commerce diffusion and that the influence of these variables varies according to the degree of a country’s development. Theoretical, research, and managerial implications of the study are also discussed.', 'Virtual team effectiveness: The role of knowledge sharing and trust Organizations utilize virtual teams to gather experts who collaborate online to accomplish organizational tasks. The virtual nature of these teams creates challenges to effective collaboration and team outcomes. This research addresses the social effects of knowledge sharing on virtual teams. We propose a conceptual model which hypothesizes a relationship between knowledge sharing, trust, collaboration, and team effectiveness in virtual team settings. The findings suggest that knowledge sharing positively influences trust and collaboration among virtual team members. The findings also suggest that while trust positively influences virtual team collaboration, it does not have a significant direct effect on team effectiveness.', nan, 'Software Licenses in Context: The Challenge of Heterogeneously-Licensed Systems The prevailing approach to free/open source software and licenses has been that each system is developed, distributed, and used under the terms of a single license. But it is increasingly common for information systems and other software to be composed with components from a variety of sources, and with a diversity of licenses. This may result in possible license conflicts and organizational liability for failure to fulfill license obligations. Research and practice to date have not kept up with this sea-change in software licensing arising from free/open source software development. System consumers and users consequently rely on ad hoc heuristics (or costly legal advice) to determine which license rights and obligations are in effect, often with less than optimal results; consulting services are offered to identify unknowing unauthorized use of licensed software in information systems; and researchers have shown how the choice of a (single) specific license for a product affects project success and system adoption. Legal scholars have examined how pairs of software licenses conflict but only in simple contexts. We present an approach for understanding and modeling software licenses, as well as for analyzing conflicts among groups of licenses in realistic system contexts, and for guiding the acquisition, integration, or development of systems with free/open source components in such an environment. This work is based on an empirical analysis of representative software licenses and of heterogeneously-licensed systems. Our approach provides guidance for achieving a \"best-of-breed\" component strategy while obtaining desired license rights in exchange for acceptable obligations.', 'Incorrect data in the widely used Inside Airbnb dataset Several recently published papers in Decision Support Systems discussed issues related to data quality in In\\xad formation Systems research. In this short research note, I build on the work introduced in these papers and document two data quality issues discovered in a large open dataset commonly used in research. Inside Airbnb (IA) collects data from places and reviews as posted by users of Airbnb.com. Visitors can effortlessly download data collected by IA for several locations around the globe. While the dataset is widely used in academic research, no thorough investigation of the dataset and its validity has been conducted. This note examines the dataset and explains an issue of incorrect data added to the dataset. Findings suggest that this issue can be attributed to systemic errors in the data collection process. The results suggest that the use of unverified open datasets can be problematic, although the discoveries presented in this work may not be significant enough to challenge all published research that used the IA dataset. Additionally, findings indicate that the incorrect data happens because of a new feature implemented by Airbnb. Thus, unless changes are made, it is likely that the consequences of this issue will only become more severe. Finally, this note explores why reproducibility is a problem when two different releases of the dataset are compared.', 'Development Patterns for Decision Support Systems  Examples from a case-oriented survey of fifty-six decision support systems are cited to illustrate each of four implementation patterns that were observed in the sample. These four system development patterns are defined in terms of high or low degrees of (I) initiation by the user and (2) participation the development process. A surprising finding is that systems initiated by users and implemented with their active participation account for less than one fourth of all cases. A qualitative analysis of the cases explains this finding by interpreting the four patterns in terms of six frequently observed system development situations, each of which has its own purpose and dynamics. Categorizing these situations under the headings \"built for, \" \"sold to, \"\\'  or \"forced upon, \"\\' key questions and concerns for various stakeholders in system development efforts are cited. A concluding quandary for management involves the choice of a project portfolio that is neither too risky nor too safe. ', 'A work system view of DSS in its fourth decade The initially revolutionary DSS agenda is now ancient history. This paper argues that “decision support” provides a richer basis than “DSS” in both practice and research. Using a loan-processing example involving two banks, it shows how work system concepts might be applied to understand decision support in real world settings, and how decision support can come from many sources other than technical artifacts such as DSS. Shifting the focus from “DSS as artifact” to “decision support within a work system” reduces the chances of being misled by techno-hype, vendor sales pitches, and incomplete understanding of determinants of success in organizations.', 'Possibilities for cross-fertilization between interpretive approaches and other methods for analyzing information systems This paper explores possibilities for cross-fertilization between interpretive approaches and other approaches for performing the initial analysis of an information system as part of an effort to redesign and improve it. The paper presents a hypothetical situation concerning the analysis of a loan approval system in a large bank. It assumes that ethnographers observed three systems analysis projects that applied different approaches in three identical banks. It uses hypothetical accounts of the three analysis efforts to propose likely differences in the process and in the results. These differences illustrate possible opportunities for cross-fertilization that might make each approach more powerful and reliable. The paper concludes that the most likely direction for cross-fertilization is from interpretive approaches to the other approaches. An earlier version of this paper was presented at the First International Workshop on Interpretive Approaches to Information Systems and Computing Research, SIG-IAM, Brunel University, July 25–27, 2002, to motivate discussion about the applications, strengths, and limitations of interpretive approaches and to help in the further development of systems analysis methods.', 'Defining information systems as work systems: implications for the IS field The lack of an agreed upon definition of information system (IS) is one of many obstacles troubling the academic IS discipline. After listing a number of definitions of IS, this paper defines IS as a special case of work system as defined in Alter (1999a). This definition has many desirable characteristics: it is easy to understand; differentiates IS from information technology (IT); covers totally manual, partially automated, and totally automated ISs; links to a life cycle model that generates many insights about development and implementation problems; provides a simple guideline that helps in interpreting common IS/IT jargon; and has other useful implications related to IS concepts, IS terminology, and the analysis and design of ISs. The paper presents the proposed IS definition and evaluates the definition in terms of simplicity, clarity, scope, systematic power, explanatory power, validity, reliability, and fruitfulness. An app1Appendix summarizes previously published concepts and two frameworks that flow from the proposed definition and are useful for appreciating many points in the evaluation section.', \"Work System Theory: Overview of Core Concepts, Extensions, and Challenges for the Future This paper presents a current, accessible, and overarching view of work system theory. WST is the core of an integrated body of theory that emerged from a long-term research project to develop a systems analysis and design method for business professionals called the work system method (WSM). After discussing WST's basic premises and its two central frameworks, this paper summarizes the relationship between WST and WSM. It shows how experience with early versions of WSM led to three extensions of WST that addressed limitations-in-use in one of the central frameworks in WST. After comparisons with related theories, this paper closes with an evaluation of progress to date, new directions for research related to WST, and implications for the IS discipline. The two appendices summarize the long term research from which WST emerged and use a positioning map to show how WST is related to other topics in the IS discipline.\", 'The concept of ‘IT artifact’ has outlived its usefulness and should be retired now Vastly inconsistent definitions of the term “the IT artifact” in leading journals and conferences demonstrate why it no longer means anything in particular and should be retired from the active IS lexicon. Examples from the literature show why artifact-cousins, such as the IS artifact, sociotechnical artifact, social artifact, and ensemble artifact should be used with great care, if not retired as well. Any void created by these retirements could be filled through the following approaches: (i) relabeling with simple terms that are immediately understandable; (ii) adopting guidelines for making sense of the whole X-artifact family; and (iii) sidestepping the IT artifact and focusing directly on IT-enabled work systems in organizations.', 'Work System Theory as a Platform: Response to a Research Perspective Article by Niederman and March In this paper, I respond to \"Moving the Work System Theory Forward\" (Niederman & March, 2014), a JAIS research perspective paper about another paper on work system theory (Alter, 2013e). The research perspective paper recognizes value in the work system approach, suggests that WST is not a proper theory, and suggests areas for related theory development. After summarizing the main ideas in WST, I explain disagreements between Niederman and March (2014) and Alter (2013e)-(hereafter called N&M and the WST paper) about what WST is and what WST should become. I note that N&M interprets basic ideas in WST differently than the WST paper defines them. I note that N&M\\'s critique of WST is anchored in issues about the nature of theory, especially a preference for Gregor\\'s type 4 theory. I explain that WST is a special case of general system theory and, as such, should not and cannot take the form of a theory that expresses relationships between independent variables, moderating variables, and dependent variables. I also explain why the WST paper called WST a theory when it might have been called something else, and also why the WST paper does not treat the development of the work system method (WSM) as a design science research project. Lastly, I respond directly to N&M\\'s title, \"Moving the Work System Theory Forward\" by explaining that WST is becoming a platform for applications and extensions in IS and other disciplines, which I illustrate with examples under five categories.', \"Nothing is more practical than a good conceptual artifact… which may be a theory, framework, model, metaphor, paradigm or perhaps some other abstraction This research commentary proposes a way to make progress in the IS discipline's inconclusive discussion about the nature and role of theory. In some ways, the creation and testing of theory seems to be the primary goal of IS research. Despite that, there are persistent questions whether theory has become a fetish in the IS discipline and whether the routinized production and testing of mid-range theories is little more than an uninspired script that reduces the value and interest of IS research. This paper reframes the discussion around the idea of ‘conceptual artifact’ that has been discussed widely in educational psychology for over a decade. Conceptual artifacts are abstract knowledge objects that can be produced, tested and improved. This paper recognizes the value of both abstract knowledge (conceptual artifacts) and non-abstract knowledge. It explains that theorizing produces, evaluates or improves useful conceptual artifacts that may or may not be theories. It validates four premises related to conceptual artifacts by showing that theorizing related to work system theory created or used many different types of conceptual artifacts. It identifies nine criteria for evaluating conceptual artifacts and shows that some of them differ from typical criteria for evaluating Gregor Type IV theories. As a whole, it argues that that privileging theory over other types of conceptual artifacts may not be beneficial in pursuing the research questions that the IS discipline needs to study.\", 'Help that is not recognized: Harmful neglect of decision support systems Decision support systems (DSSs) aim to enhance the performance of decision makers, but to do so DSSs have to be adopted and used. Technology acceptance research shows that user evaluations (i.e., beliefs, perceptions, and attitudes) are key drivers of adoption and use. This article first presents evidence from the literature suggesting that the link between user evaluations of DSSs and actual performance may be weak, or sometimes even negative. The authors then present two empirical studies in which they found a serious disconnect between user evaluations and actual performance. If user evaluations do not accurately reflect performance, then this may lead to harmful neglect of performance-enhancing DSSs. The article concludes with a discussion of interventions that may alleviate this problem.', 'The Effects of IT-Enabled Cognitive Stimulation Tools on Creative Problem Solving: A Dual Pathway to Creativity We investigate the effectiveness of three types of IT-enabled cognitive stimulation tools for enhancing creative problem solving: mind mappers, process guides, and stimuli providers. Based on the dual pathway to creativity models, the authors examine the extent to which these tools are capable of stimulating individuals to explore their knowledge base more deeply (i.e., the persistence pathway) and more broadly (i.e., the flexibility pathway) and, hence, help to produce more novel ideas. In a laboratory study with business students, they find that, as compared to unaided individuals, IT-enabled stimuli providers enhance individual creativity more than process guides and mind mappers. As for the underlying creative process, stimuli providers push individuals to explore their knowledge base more deeply and more broadly, leading to more novel but, unexpectedly, also more useful ideas. The reported findings may facilitate the development of creativity support systems and their assignment to individuals and tasks.', 'Supporting Creative Problem Solving with a Case-Based Reasoning System Attention for the division of work between computers and humans is growing due to ever-increasing computer capabilities. Over the past decades, creativity support systems (CSSs) have gained ground as a means to enhance individual, group, and organizational creativity. Whereas prior research has focused primarily on the main effects of CSSs, we explore the interaction effects with the creative ability of the individual. In this paper, we investigate the use of the case-based reasoning (CBR) technology, which is based on the principle of analogical reasoning, to aid individuals in solving business problems creatively. The expectations as to why the CBR technology should enhance individual creativity, and under what conditions (i.e., the type and number of cases that are made available), are derived from creative cognition theory, and are tested empirically. In a series of studies, a CBR system loaded with a diverse set of cases was found to enhance the performance of individuals with lower creative ability, but it did not help the most creative individuals. Although the literature suggests that cases from remote problem domains should lead to more novel solutions, loading the CBR system only with cases closely related to the problem domain proved more effective than remote cases only. Finally, loading the CBR system with a larger set of diverse cases was found to positively influence the creativity of the solutions. These findings have the following implications for CSSs and creative cognition theory: (1) when considering the effectiveness of CSSs it is important to take into account the creative ability of the individual (i.e., \"one size does not fit all\"), (2) making a sufficiently large and diverse set of cases available is better for stimulating creativity, and (3) providing cases that are too remote may be counterproductive. On a practical note, organizations seeking to redesign their division of labor between individuals and machines can easily follow the CBR approach presented here using their own set of cases.', 'Using structural technology acceptance models to segment intended users of a new technology: Propositions and an empirical illustration This article aims to offer an alternative method to analyse technology acceptance models, namely a segment-wise analysis. The empirical illustration of this method involves data that were collected during a company-wide implementation of a Sales Force Automation technology in Europe. The data comprise a variety of commonly used technology-related, context-related, and person-related variables. The segmentation procedure, which involved a finite mixture partial least squares estimation, provides more insight into the different ways in which people come to accept new technologies. Unlike other segmentation studies published in IS journals, the resulting segments are based on similarities and differences in the structure of the underlying theoretical models rather than (a collection of) individual variables. Further research or a re-analysis of existing data should help establish robust “technology acceptance model”-based segments as well as comprehensive profiles of the individuals in each segment.', 'Productivity and Performance Effects of Business Process Reengineering: A Firm-Level Analysis We empirically investigate whether business process reengineering (BPR), which requires substantial investment in information technology to integrate separate tasks into complete cross-functional processes, is associated with enhanced firm productivity and performance. We analyze firm-level panel data covering the period 1987–2008 using fixed effects and first differencing, standard methods that account for unobservable firm-level effects. We find that return on assets drops significantly during the project initiation year. According to fixed effects results, the performance and productivity measures improve in a decreasing manner after project initiation, suggesting that BPR indeed positively affects firm performance on average. We also find that enterprise-wide BPR projects are associated with more negative returns during project initiation than functionally focused projects. However, there is no clear evidence regarding their superiority over functionally focused BPR projects in terms of performance improvements after project initiation, perhaps because grand projects are risky and sometimes lead to grand failures.', \"Cost and benefit analysis of authentication systems This study investigates the key elements an online service or product provider needs to consider when adopting another single-factor or two-factor authentication system. We also uncover the conditions that make the new one-factor or two-factor authentication system more preferable. By using the probability of system failure, this study generalizes all possible combination of authentication systems into four different cases. This generalization allows us to compare different systems and to determine the key factors managers need to consider when adopting a new authentication system. The key factors are (1) additional implementation costs, (2) customer switching which is determined by the market share and customers' preferences, and (3) expected losses when the new system fails. This study also suggests that if the provider chooses an expensive new system, the provider needs to have a larger market share to justify the spending. Also, regulators can encourage the adoption of a more secure authentication system by changing the penalty a firm faces when the system fails. Finally, it could also be preferable to have both one-factor and two-factor authentication systems depending on the customers' characteristics.\", \"A Multigeneration Diffusion Model for IT-Intensive Game Consoles The video game industry has attracted more and more attention not only from technology giants such as Microsoft but also from software developers and private investors. Information technology dictates how game console producers compete in the marketplace. Intensive IT competition in each console generation has shifted the market balance. Competitors jockey to position themselves as the first-mover within a generation or to wait and enter the market with cheaper and more advanced technologies. To capture the characteristics of IT-intensive products, we propose a multigeneration diffusion model that captures both cannibalization and competition effects. We apply the model to analyze game console diffusion with real shipment data for three game consoles from two companies: Sony and Microsoft. We analyze two scenarios: one with only Sony's products, and one with both companies' products. We find that the cannibalization between Sony's products is minimal, and Microsoft maintains a strong competitive edge that has challenged Sony's market position. The results also explain how Sony has maintained its position as the market leader over the last two generations. This research sheds light on the nature of an IT-intensive game console competition between companies and generations.\", 'The pursuit of trust in ad hoc virtual teams: how much electronic portrayal is too much? This study develops and tests the concept of electronic portrayal in synchronous computer-mediated communication of ad hoc virtual teams. Electronic portrayal is the extent to which a communication system portrays the true identity of its users. A theoretical model is developed based upon which it is hypothesized that increased information available due to electronic portrayal will impact trust in ad hoc virtual teams. An experiment is conducted to test the model by manipulating the graphical identification of users of a system as well as the rehearsability of the system. Rehearsability is the extent to which users can reread and edit their messages before submitting them to the synchronous communication system. The results show that the combination of both factors – identification and rehearsability – impacts trust among team members. Specifically, partial electronic portrayal (only one form of true-to-life representation) has the most positive impact on trust. This effect is moderated by communication-related variables such as self-disclosure, impressions and virtual co-presence. The implication of these results is that too much true identity information negatively impacts trust. This research provides theoretical and practical contributions for understanding the importance of identification and rehearsability in synchronous group communication.', 'The stability of electric energy markets Power system markets represented by dynamic equations provide insights into the market behavior which are not available from static models. In particular: (1) markets that are required to balance supply and demand precisely at all times may be unstable if one supplier exhibits economies of scale and will be unstable if two suppliers exhibit this behavior. The instability is characterized by one or more positive eigenvalues. (2) Markets where some energy imbalance is allowed to accumulate can exhibit an instability, depending on the exact values of time constants and delays in the system. (3) Congestion can be helpful from the perspective of stability: a market can become unstable in the eigenvalue sense if congestion is removed. (4) A power system (with stable electromechanical dynamic behavior when considered by itself) and market (by itself stable) can, when analyzed jointly, exhibit unstable behavior. Some of the instabilities alluded here are nothing more than fluctuations in demands and prices. However, fluctuations are likely to require larger security margins, thus greater costs to operate the system.', 'Solving power flow problems with a Matlab implementation of the Power System Applications Data Dictionary This paper implements a power flow application and variations using the IEEE Power System Application Data Dictionary (PSADD) within a Matlab environment. It describes a number of useful data and implementation techniques for a variety of applications. The techniques include the use of very compact and efficient code for the computation of Power Transfer Distribution Factors (PTDF). PTDF are an important element of present and proposed congestion management strategies for power systems, particularly when these systems must operate in a deregulated environment.', 'Controlling power systems with price signals This paper revisits the possibility of controlling the power system entirely by means of price signals. It expands on notions introduced in an earlier paper and addresses several unresolved issues: problems with linear cost structures, response delays, varying costs, market power and stability problems caused by market/system interactions. The results suggest that control by price can, in fact, be made to work with some caveats.', 'Confessions of an information worker: a critical analysis of information requirements discourse This paper seeks to demonstrate the benefit of critical discourse analysis as a research approach for examining information systems development. Research has shown that eliciting user requirements is a critical activity of information systems development. However, the requirements phase is not only a key activity, it is highly problematic. Requirements determination is considered a process fraught with conflicting, inconsistent and competing viewpoints in which users and analysts do not share a “consensual domain,” thus barring them from reaching agreements about requirements. Therefore, analytic tools that recognize and examine requirements analysis as a polyphonic interaction hold much promise for improving requirements elicitation and analysis. Critical discourse analysis offers the tools to examine systematically the fundamental substance of requirements elicitation — interactional talk. This analysis employs sociolinguistic methods for specifying the linguistic features of different types of discourse units and the way they are tied together to create meaning, but also concerns itself with critically examining social context. In this paper requirements elicitation takes on the form of a “confessional” act where the individual verbalizes thoughts, intentions and consciousness. Findings show that during this ritual, discourse is revealed as a dialectic between two different domains of meaning, that of analyst and client. Analysts, in their official roles, propose a “frame” which conflicts with that proposed by clients during interviews. Changes in frames and deft face-saving work during interviews function to discursively produce and challenge client identities. The paper explores the tension between the frames proposed by the analyst and client during interviews which explains some of the frustrations and “gaps” which characterize this type of encounter. Issues of power inequality, identity formation, and symbolic control are presented as explanations of why competing frames are proposed and sustained while resisted by clients.', 'Examining technology, structure and identity during an Enterprise System implementation Abstract.\\u2002 This paper presents a longitudinal study of an Enterprise System (ES) implementation by critically examining the discursive context in which an ES implementation unfolds. The findings show that users strongly supported the ES in the earlier stage of implementation when the technology was an imaginary phenomenon. However, in later stages, when the technology is in use, user support was not consistent. In this phase, the ES produces loss of control and an inability to function as an arbiter of fairness (in allocating resources associated with the system), thereby directly challenging existing professional identities and roles. These outcomes, in turn, generate acts of resistance on the part of workers. Users reach inside the technology and reshape it by devising creative workarounds that produce a sense of reskilling to counter the deskilling produced by the loss of control and power. The analysis also shows that an ES is a complex social phenomenon that is intricately linked to and complicit in shaping organizational structure and identity. In particular, this study shows how technology, structure and identity are in a mutually constitutive relationship.', 'Algorithmic Processes of Social Alertness and Social Transmission:  How Bots Disseminate Information on Twitter Despite increased empirical attention, theory on bots and how they act to disseminate information on social media remains poorly understood. Our study leverages the conduit brokerage perspective and the findings of a multiple case study to develop a novel framework of algorithmic conduit brokerage for understanding information dissemination by bots and the design choices that may influence their actions. Algorithmic conduit brokerage encompasses two intertwined processes. The first process, algorithmic social alertness, relies on bot activity to curate and reconfigure information. Algorithmic social alertness is significant because it involves action triggers that dictate the kinds of information being searched, discovered, and retrieved by bots. The second process, algorithmic social transmission, relies on bot activity to embellish and distribute the information curated. Algorithmic social transmission is important because it can broaden the reach of information disseminated by bots through increased discoverability and directed targeting. The two algorithmic conduit brokerage processes we offer are unique to bots and distinct from the original conceptualization of conduit brokerage, which is rooted in human activity. First, since bots lack the human ability of sensemaking and are instead fueled by automation and action triggers rather than by emotions, algorithmic conduit brokerage is more invariant and reliable than human conduit brokerage. Second, automation increases the speed and scale of information curation and transfer, making algorithmic conduit brokerage not only more consistent but also faster and more extensive. Third, algorithmic conduit brokerage includes a set of new concepts (e.g., action triggers and rapid scaling) that are specific to bots and therefore not applicable to human conduit brokerage.', 'Trading team composition for the intraday multistock market Automated traders operate market shares without human intervention. We propose a Trading Team based on atomic traders with opportunity detectors and simple effectors. The detectors signalize trading opportunities. For each trading signal, the effectors follow deterministic rules on when and what to trade in the market. The detectors are based on Partial Least Squares. We perform some trading experiments with twelve BM&FBovespa stocks. The empirical findings indicate that the proposed trading strategy reaches a 77.26% annualized profit, outperforming by 380.07% the chosen baseline strategy with a 16.07% profit. We also investigate Multistock Resolution Strategy (MSR) performance subject to brokerage commissions and income tax. Whenever the initial investment is at least US$ 50,000, the MSR strategy provides a profit of at least 38.63%.', 'Where You Live Matters: Local Bank Competition, Online Marketplace Lending, and Disparity in Borrower Benefits In the past decade, the proliferation of online marketplace lending has been disrupting the consumer credit market, especially personal loans used for debt consolidation. These lenders, for example, Lending Club, transcend the geographic boundaries within which local banks operate and offer homogeneous access and terms to borrowers. However, the ultimate benefits borrowers derive from marketplace lending can differ significantly because local alternatives may be used to replace marketplace loans when they are available and favorable. Correspondingly, if local bank competition drives the substitution of an existing marketplace loan with a traditional bank loan, the promise of equal benefits to all borrowers from marketplace lending is unlikely to fully materialize. This competitive dynamic also has implications for policy making, particularly in judging the ramifications of bank mergers and acquisitions. We utilize data from Lending Club, a major peer-to-peer (P2P) lending platform, to study whether local market structure drives heterogeneous incentives of traditional banks to convert borrowers of marketplace lending to conventional bank loans. The results indicate that a borrower who resides in a more competitive market is more likely to pay off a P2P loan early by making a large, one-time payment compared with a borrower from a less competitive market. Thus, borrowers from different markets do not benefit equally from online marketplace lending disrupting the consumer credit market. Our study has implications for online marketplace lending, other FinTech-based markets, and the consumer credit market in general.', 'Shared Prosperity (or Lack Thereof) in the Sharing Economy This paper examines the potential economic spillover effects of a home-sharing platform—Airbnb—on the growth of a complimentary local service—restaurants. By circumventing traditional land-use regulations and providing access to underutilized inventory, Airbnb attracts visitors to locales that are not traditional tourist destinations. Although visitors generally bring significant spending power, it is unclear whether visitors use Airbnb primarily for lodging and thus do not contribute to the adjacent economy. To evaluate this, we focus on the impact of Airbnb on the restaurant employment growth across locales in New York City (NYC). Specifically, we focus on areas in NYC that did not attract a significant tourist volume prior to the emergence of a home-sharing service. Our results indicate a salient and economically significant positive spillover effect on restaurant job growth in an average NYC locality. A one-percentage-point increase in the intensity of Airbnb activity (Airbnb reviews per household) leads to approximately 1.7% restaurant employment growth. Since home-sharing visitors are lodging in areas that are not accustomed to tourists, we also investigate the demographic and market-structure-related heterogeneity of our results. Notably, restaurants in areas with a relatively high number of White residents disproportionately benefit from the economic spillover of Airbnb activity, whereas the impact in majority Black areas is not statistically significant. We validate the underlying mechanism behind the results by evaluating the impact of Airbnb on Yelp visitor reviews, revealing that areas with increased Airbnb activity experience a surge in their share of NYC visitor reviews. This result is further validated by evaluating the impact of a unique Airbnb neighborhood-level exogenous policy recently implemented in New Orleans.', 'Empirical studies of geographically distributed agile development communication challenges: A systematic review There is increasing interest in studying and applying geographically distributed agile development (GDAD). Much has been published on GDAD communication. There is a need to systematically review and synthesize the literature on GDAD communication challenges. Using the SLR approach and applying customized search criteria derived from the research questions, 21 relevant empirical studies were identified and reviewed in this paper. The data from these papers were extracted to identify communication challenges and the techniques used to overcome these challenges. The findings of this research serve as a resource for GDAD practitioners and researchers when setting future research priorities and directions.', 'Ontology-supported case-based reasoning approach for intelligent m-Government emergency response services There is a critical need to develop a mobile-based emergency response system (MERS) to help reduce risks in emergency situations. Existing systems only provide short message service (SMS) notifications, and the decision support is weak, especially in man-made disaster situations. This paper presents a MERS ontology-supported case-based reasoning (OS-CBR) method, with implementation, to support emergency decision makers to effectively respond to emergencies. The advantages of the OS-CBR approach is that it builds a case retrieving process, which provides a more convenient system for decision support based on knowledge from, and solutions provided for past disaster events. The OS-CBR approach includes a set of algorithms that have been successfully implemented in four components: data acquisition; ontology; knowledge base; and reasoning; as a sub-system of the MERS framework. A set of experiments and case studies validated the OS-CBR approach and application, and demonstrate its efficiency.', 'The world and business computing in 2051 This paper projects the future of information technology within the context of the business and social environment in the mid 21st century. Some time in the next two decades we will enter our fourth industrial era, fueled by evolution in power, genetics, space exploration and information technologies. Unlike other industrial revolutions, this will be attended by widespread consolidations in industries as diverse as utility, retail, pharmaceuticals as well as the government. The trade wing of the UN, the United Nations Trade Organization (UNTO) is established to deal with global issues. The UNTO passes a number of resolutions including # 1/10 which deals with a uniform system of personal identification for all individuals. Due to free trade facilitated by electronic bureaucracies and global transportation grids, GNP growth rates triple. However, the consolidations will lead to the disappearance of clerical, administrative and management employees, leaving only the top management and highly skilled professionals. These changes will result in companies outsourcing all their routine processing to transaction and data centres. Employees will work from well equipped IT centres. The internet is in its third incarnation and is supported by a comprehensive web intelligence to answer all possible questions. Other technological changes include Terahertz computers, Personal identification Devices (PID), AI chips and hierarchical databases with relational interfaces. The cornucopia resulting from progress will lead to a more enlightened society. However, the stress of change will result in an increased degree of philosophical inquiry.', 'Business-IT alignment as a coevolution process: An empirical study In this paper, we provide a detailed insight into the complex coevolution dynamics that shape the alignment process by analyzing how different mechanisms and factors are mutually related in complex networks of feedback loops. We combine insights from the literature on alignment as a (coevolution) process with literature on alignment as a state to identify the different components of the organization’s socio-technical system that influence alignment, the relationships between these components, and the role that different factors play. In our empirical analysis (based multiple case studies) we then focus on the actual interplay between relevant factors. Using a causal loop diagramming approach - based on system dynamics – we analyze how these factors mutually influence each other through various feedback loops and thus shape the alignment process. We extend previous literature on the alignment process by identifying the way that the complex interplay between different factors shapes this process. By identifying the feedback loops between relevant factors, we also provide more insight into the complex bottom-up and top-down dynamics that shape the process, and that provide explanations for why this process is charac\\xad terized by transitions between alignment and misalignment. For practice, our paper provides a deeper understanding of the alignment process, which is a precondition for improving alignment practices in organisations.', 'Capabilities and metrics in DevOps: A design science study Customer demands, competition, regulatory environments, and sophisticated external threats have all increased the importance of DevOps in IT organizations. However, DevOps adoption is still uneven, emphasizing the need to provide management with relevant IS data and insights. Regrettably, there is a measurement inefficiency between these capabilities.', \"A socio-cognitive interpretation of the potential effects of downsizing on software quality performance Organizational downsizing research indicates that downsizing does not always realize its strategic intent and may, in fact, have a detrimental impact on organizational performance. In this paper, we extend the notion that downsizing negatively impacts performance and argue that organizational downsizing can potentially be detrimental to software quality performance. Using social cognitive theory (SCT), we primarily interpret the negative impacts of downsizing on software quality performance by arguing that downsizing results in a realignment of social networks (environmental factors), thereby affecting the self-efficacy and outcome expectations of a software professional (personal factors), which, in turn, affect software quality performance (outcome of behaviour undertaken). We synthesize relevant literature from the software quality, SCT and downsizing research streams and develop a conceptual model. Two major impacts of downsizing are hypothesized in the conceptual model. First, downsizing destroys formal and informal social networks in organizations, which, in turn, negatively impacts software developers' self-efficacy and outcome expectations through their antecedents, with consequent negative impacts on software development process efficiency and software product quality, the two major components of software quality performance. Second, downsizing negatively affects antecedents of software development process efficiency, namely top management leadership, management infrastructure sophistication, process management efficacy and stakeholder participation with consequent negative impacts on software quality performance. This theoretically grounded discourse can help demonstrate how organizational downsizing can potentially impact software quality performance through key intervening constructs. We also discuss how downsizing and other intervening constructs can be managed to mitigate the negative impacts of downsizing on software quality performance.\", \"Conviviality of Internet social networks: An exploratory study of Internet campaigns in Iran In this study, we focus on the relationship between Internet social networks and societal change by examining case studies of the impact of Internet-based campaigns in Iran. Ivan Illich's theory of ‘Conviviality of Tools’ enables an analysis of the conviviality of the Internet. Subsequently, this conceptual lens is used to examine empirical data from two Internet-based campaigns. The paper contributes theoretical and practical implications regarding conviviality of Internet social networks and the accomplishment of conviviality in society. Our findings show that Internet conviviality cannot be treated as an independent variable with deterministic outcomes on society, but as a technology that is shaped by ongoing economic and political forces. The Iranian Internet social networks are not universally accessible, frequently induce fragmented, nonsensical, and enraged discussion and its potential as a tool of liberation is tempered by the Iranian government adaption of systems of surveillance and censorship. We argue that the findings of this study have some general implications of value to researchers studying computerisation movements and Internet social networks in other countries.\", 'Searching for information in a time-pressured setting: experiences with a Text-based and an Image-based decision support system Searching for the right information and making quick, accurate decisions within time-pressured settings is often non-trivial. We contrast the relative efficacies of written English (Text) and a more concise, compact communication mode (Image) for information search and decision making by using a financial incentive scheme to apply implicit time pressure on subjects. We found that, while Image users earned as much as Text users, they achieved this earnings parity by following speedier but less accurate strategies. We conclude with thoughts on possible refinements to our work that could steer subjects in the ideal direction of fast, accurate, lucrative decisions with languages like Image.', 'A Cost–Benefit framework for online management of a metacomputing system Managing a large collection of networked machines, with a series of incoming jobs, requires that the jobs be assigned to machines wisely. A new approach to this problem is presented, inspired by economic principles: the Cost–Benefit framework. This framework simplifies complex assignment and admission control decisions, and performs well in practice. We demonstrate this framework in the context of an Internet-wide market for computational services and verify its utility for a classic network of workstations.', 'Customer-oriented catalog segmentation: Effective solution approaches We consider in this paper the customer-oriented catalog segmentation problem that consists of designing K catalogs, each of size r products that maximize the number of covered customers. A customer is covered if he/she has interest in at least the specified minimum number of products in one of the catalogs. The problem addresses the crucial issue of the design of the actual contents of the catalogs that serves as a back-end to catalog production for the purpose of more focused design of catalogs as a targeted marketing tool. We developed two algorithms to solve the problem. Results of an extensive computational study using real and synthetic data sets show that one of the proposed algorithms outperforms the state-of-the-art algorithm found in the literature in terms of customer coverage, resulting potentially in significant increase in organization profit. In the spirit of the guidance role that a Decision Support System (DSS) should play by recommending alternative, satisfactory solutions to the decision maker, the prototype of a DSS integrating all three algorithms is presented to provide the decision maker with an easy-to-use, yet powerful tool to examine various catalog design options and their implications on the contents of the catalogs and the clusters of covered customers.', 'Dare to share: Protecting sensitive knowledge with data sanitization Data sanitization is a process that is used to promote sharing of transactional databases among organizations while alleviating concerns of individual organizations by preserving confidentiality of their sensitive knowledge in the form of sensitive association rules. It hides the frequent itemsets corresponding to the sensitive association rules that contain sensitive knowledge by modifying the sensitive transactions that contain those itemsets. This process is guided by the need to minimize the impact on the data utility of the sanitized database by allowing mining as much as possible of the non-sensitive knowledge in the form non-sensitive association rules from the sanitized database. We propose three heuristic approaches for the sanitization problem. Results from extensive tests conducted on publicly available real datasets indicate that the approaches are effective and outperform a previous approach in terms of data utility at the expense of computational speed. The proposed approaches sanitize also the databases with great data accuracy, thus resulting in little distortion of the released databases. We recommend that the database owner sanitize the database using the third proposed hybrid approach.', 'Optimal input/output reduction in production processes While conventional Data Envelopment Analysis (DEA) models set targets for each operational unit, this paper considers the problem of input/output reduction in a centralized decision making environment. The purpose of this paper is to develop an approach to input/output reduction problem that typically occurs in organizations with a centralized decision-making environment. This paper shows that DEA can make an important contribution to this problem and discusses how DEA-based model can be used to determine an optimal input/output reduction plan. An application in banking sector with limitation in IT investment shows the usefulness of the proposed method.', 'Technological Entitlement: It’s My Technology and I’ll (Ab)Use It How I Want To Entitlement has been identified as a potentially valuable employee characteristic in the prediction of computer abuse but has not been studied systematically in the IS domain. We introduce the construct of technological entitlement as the persistent sense of being more deserving of technological resources, uses, and privileges compared to other employees. Adapting a model of general entitlement to the work technology context, we theorize that technological entitlement predicts computer abuse and that this relationship is amplified by perceptions of technology restriction. After developing and validating a scale to measure technological entitlement, we conduct three studies with working adult samples to test our hypotheses. In Study 1 (n = 187), using a behavioral design, we find that technological entitlement predicts computer abuse behavior (beyond general entitlement) and that this relationship is stronger when employees perceive organizational restrictions on technology usage. We replicate these findings in Study 2 (n = 339) with an experiment. In Study 3 (n = 156), we manipulate the context of restrictiveness within our experimental vignette to establish the generalizability of our moderator. We discuss how technological entitlement helps explain existing inconsistencies in the effectiveness of deterrence measures as well as other theoretical and practical implications of our work.', 'Effects of structural and trait competitiveness stimulated by points and leaderboards on user engagement and performance growth: A natural experiment with gamification in an informal learning environment Rooted in theories of competitiveness and social comparison, we model the effects of users’ structural and trait competitiveness on their engagement and performance growth in an informal learning environment. We hypothesise that game elements of points and leaderboards stimulate users’ structural competitiveness, which affects users’ engagement and has an inverted-U effect on performance growth. We further hypothesise that these effects are stronger among individuals with higher trait competitiveness. We tested our hypotheses using data from a natural experiment conducted over 300 days on 88,310 unique users who made 215,920 game interactions within the Cyber Detectives exhibit at the Tech Interactive museum in California. Our results are based on two objective measures of trait-competitiveness as both behaviour and outcome (percentile ranking on total time spent and number of badges earned, respectively), multiple objective measures of user engagement (time spent per attempt, number of reattempts, and daily user attempts), and an objective measure of performance growth (points). Results provide overall support to our hypotheses. We contribute to the gamification literature by providing strong causal evidence of points and leaderboards triggering structural and trait competitiveness, which interact to affect both engagement and performance growth in informal learning contexts.', 'An extension of the technology acceptance model in an ERP implementation environment This paper presents an extension to the technology acceptance model (TAM) and empirically examines it in an enterprise resource planning (ERP) implementation environment. The study evaluated the impact of one belief construct (shared beliefs in the benefits of a technology) and two widely recognized technology implementation success factors (training and communication) on the perceived usefulness and perceived ease of use during technology implementation. Shared beliefs refer to the beliefs that organizational participants share with their peers and superiors on the benefits of the ERP system. Using data gathered from the implementation of an ERP system, we showed that both training and project communication influence the shared beliefs that users form about the benefits of the technology and that the shared beliefs influence the perceived usefulness and ease of use of the technology. Thus, we provided empirical and theoretical support for the use of managerial interventions, such as training and communication, to influence the acceptance of technology, since perceived usefulness and ease of use contribute to behavioral intention to use the technology.', \"Discuss practical importance of results based on interval estimates and p-value functions, not only on point estimates and null p-values  It has long been argued that we need to consider much more than an observed point estimate and a p-value to understand statistical results. One of the most persistent misconceptions about p-values is that they are necessarily calculated assuming a null hypothesis of no effect is true. Instead, p-values can and should be calculated for multiple hypothesized values for the effect size. For example, a p-value function allows us to visualize results continuously by examining how the p-value varies as we move across possible effect sizes. For more focused discussions, a 95% confidence interval shows the subset of possible effect sizes that have p-values larger than 0.05 as calculated from the same data and the same background statistical assumptions. In this sense a confidence interval can be taken as showing the effect sizes that are most compatible with the data, given the assumptions, and thus may be better termed a compatibility interval. The question that should then be asked is whether any or all of the effect sizes within the interval are substantial enough to be of practical importance. et al. (2022)  advise looking at the practical importance of an effect estimate, keeping in mind its uncertainty, rather than only describing results as 'statistically significant' or 'nonsignificant'. We applaud this and many of their other recommendations, for example that single p-values should be given with sensible precision and not be degraded to stars, letters, or binary inequalities ('p < 0.05') and that we should avoid using the phrase 'statistically significant' entirely. All of this is consistent with over 70 years of calls by many statistical writers to emphasize interval estimates over statistical tests (e.g. Altman et al. Sen \", 'Exploring the impact of socio-technical core-periphery structures in open source software development In this paper we apply the social network concept of core-periphery structure to the socio-technical structure of a software development team. We propose a socio-technical pattern that can be used to locate emerging coordination problems in Open Source projects. With the help of our tool and method called TESNA, we demonstrate a method to monitor the socio-technical core-periphery movement in Open Source projects. We then study the impact of different core-periphery movements on Open Source projects. We conclude that a steady core-periphery shift towards the core is beneficial to the project, whereas shifts away from the core are clearly not good. Furthermore, oscillatory shifts towards and away from the core can be considered as an indication of the instability of the project. Such an analysis can provide developers with a good insight into the health of an Open Source project. Researchers can gain from the pattern theory, and from the method we use to study the core-periphery movements.', 'OSPM: A design methodology for open strategic planning This study employs a design science perspective to propose a methodology for open strategic planning (OSP). Habermas’ discourse theory and Bryson’s strategy change cycle are used as informing kernel theories. A methodology is proposed to satisfy the requirements retrieved from the kernel theories. The proposed methodology contains modules for a planning system and a planning process. Design principles are explained through a blueprint of the system and process. The proposed methodology is applied and evaluated in two cases. Contributions to the literature involve extending the literature on OSP to an applicable methodology with guidelines on how to implement open strategy.', \"Winner's curse and parallel sales channels—Online auctions linked within e-tail websites A sample of 416 online auctions was examined to determine the extent of overpayment (winner's curse) where online auctions and e-tail websites were linked together to form a parallel sales channel. The results indicated that 8.7% of the highest winning online auction bidders exceeded e-tail posted reference prices of identical retail merchandise found at the same website. Significantly, such bids exceeded the reference prices by a mean percentage dollar amount of 14.1% thus suggesting the existence of a winner's curse. The results also indicated that (1) there was a significant negative association between reference price and mean percentage dollar amount overbid; (2) there was a significant negative association between auction lot size and mean percentage dollar amount overbid; and (3) there was no significant association between overtime auctions and mean percentage dollar amount overbid. While manipulation of reference price and auction lot size might minimize winner's curse, erratic or irrational behavior (by online auction and/or e-tail websites) may lead to disinformation.\", 'The Effects of Operational and Financial Performance Failure on BI&A-Enabled Search Behaviors: A Theory of Performance-Driven Search Business intelligence and analytics (BI&A) systems enable firms to analyze data and search for insights that could potentially lead to improved organizational performance. While there is evidence that BI&A systems can improve performance through search, our theoretical understanding of how and under what conditions firms leverage BI&A systems to conduct search is rather limited. In particular, while problemistic search theory posits that performance failures motivate search, how firms respond to different types of performance failures remains unclear. We draw on and extend problemistic search theory by theorizing that BI&A-enabled search is influenced by complex interactions between failures in financial and operational performance and performance-related aspirations. We refer to this notion as the Theory of Performance-driven Search (TPS) and test it using longitudinal data gathered for a four-year period from seven U.S. hospitals. We find evidence that firms employ BI&A systems to search in a narrow set of circumstances. We find that performance failures are an important antecedent of BI&A-enabled search. In particular, failures in financial performance, failures in operational performance, and their joint failures are important conditions that trigger BI&A-enabled search. We find that historical and social aspiration levels of financial and operational performance influence BI&A-enabled search during failures in operational performance. We also find that only in organizations experiencing a sustained failure in financial performance do operational performance failures trigger BI&A-enabled search and that the latency of search response is dependent on the speed of failure in financial performance. Through our findings, we make two important contributions: we extend the business value of IT literature by identifying the contextual conditions that trigger BI&A use for search and we extend problemistic search theory by theorizing for the differential effects of operational and financial performance failures.', nan, \"Matching client/server processing architectures with information processing requirements: A contingency study The 1990s are witnessing the rapid growth of client/server (C/S) computing, but for an organization to benefit from a C/S model, it should ensure that the processing architecture matches its information needs. Researchers have suggested that organizations moving to this model should identify their information requirements, and then determine the appropriate architectures to support them. This study utilizes information processing theory to examine the match between an organization's information processing requirements and its C/S architectures. The independent variables in this study are task characteristics, and the processing architectures. The dependent variable is effectiveness. The data for this study was obtained from C/S managers and users in a variety of industries, through a combination of archival data, telephone interviews, and a mailed survey. It was analyzed using hierarchical regression. The results indicate that an appropriate match between task characteristics and C/S processing architectures is an important determinant of system effectiveness.\", \"Profiling Web Usage in the Workplace: A Behavior-Based Artificial Intelligence Approach Employees' nonwork-related Web surfing behavior results in millions of dollars of expenditure for organizations. This paper proposes the use of a behavior-based artificial intelligence system to profile employee Web usage behavior. Two artificial neural networks (ANN) incorporating genetic algorithm techniques were developed for this purpose. The system was validated with two different data sets. The classification performance of the neural network models was compared to that of a statistical method. The results indicate that one of the ANN models, namely the simple recurrent network, was a superior classifier for this behavior-based problem. In addition, the uncertainty inherent in such classification decisions was examined with a loss matrix, and the holdout samples were reclassified using a loss matrix. The output of this intelligent system can be highly beneficial to managers in designing effective Web management policies.\", 'Cognitive Conflict and Consensus Generation in Virtual Teams During Knowledge Capture: Comparative Effectiveness of Techniques Effective knowledge management has been increasingly cited as critical for businesses to compete successfully. Knowledge acquisition/capture, the first step in knowledge management, continues to be a bottleneck and is exacerbated when experts are geographically distributed. Furthermore, knowledge from multiple experts is likely to generate inconsistent knowledge for a given problem domain. There is thus a compelling need to generate consensus by resolving inconsistencies and conflicts that may occur among experts during the process of knowledge acquisition. This process is more challenging when dealing with virtual teams of experts. This study addresses task-based or cognitive conflicts among experts. A key objective of this study is to examine the effectiveness of two cognitive techniques-the repertory grid (or RepGrid) and Delphi-in generating consensus among experts during the knowledge capture process. A field experiment with geographically distributed real-world network experts involving multiple rounds of interaction over an extended period of time was conducted. Findings from this research indicate that, in the short run, Delphi works better than the RepGrid in reducing conflict and generating consensus. However, the RepGrid technique appears to perform better in the long run. We find similar results for satisfaction with the process and outcome. Our findings also indicate that experts using the RepGrid technique elicited more knowledge as well as higher-quality knowledge than experts using the Delphi technique. To sum up, our study indicates that RepGrid is superior to Delphi, and therefore managers should seriously consider the use of RepGrid in capturing knowledge from multiple and distributed experts when dealing with complex real-world issues.', 'A Tangled Web: Should Online Review Portals Display Fraudulent Reviews? The growing interest in online product reviews for legitimate promotion has been accompanied by an increase in fraudulent reviews. However, beyond algorithms for initial fraud detection, little is known about what review portals should do with fraudulent reviews after detecting them. In this paper, we address this question by studying how consumers respond to potentially fraudulent reviews and how review portals can leverage this knowledge to design better fraud management policies. To do this, we combine theoretical development from the trust literature with randomized experiments and statistical analysis using large-scale data from Yelp. We find that consumers tend to increase their trust in the information provided by review portals when the portal displays fraudulent reviews along with non-fraudulent reviews, as opposed to the common practice of censoring suspected fraudulent reviews. The impact of fraudulent reviews on consumers’ decision-making process increases with the uncertainty in the initial evaluation of product quality. We also find that consumers do not effectively process the content of fraudulent reviews (negative or positive). This result furthers the case for a decision heuristic that will incorporate the motivational differences between positive fraudulent reviews and negative fraudulent reviews to effectively aid consumers’ decision making.', 'Self-Organizing in Blockchain Infrastructures: Generativity Through Shifting Objectives and Forking  Given the ubiquity of digital technologies, and increased use of autonomous algorithms, it is likely that many of today\\'s social and organizational processes will one day include autonomous elements. The Bitcoin blockchain is likely the first case of an increasingly generative and autonomous way of organizing, and the specific properties of blockchain infrastructuresdistribution of control, openness to manipulation, and generativity of the underlying source codemake it an ideal case to study patterns of self-organizing. This paper investigates the phenomenon of self-organizing through a study of forking in the Bitcoin blockchain infrastructure between 2010 and 2016. It adds to the emerging body of research on digital infrastructures, and particularly blockchain infrastructures, by conceptualizing forking as a pattern of self-organizing in blockchain infrastructures that specifically involves the underlying infrastructure, the scale of code changes, individual objectives, and collective adoption, whether specific or general. Thus, this paper demonstrates how forking in blockchain infrastructures mediates between divergent organizing objectives and existing capabilities, on the one hand, and generates self-organizing on the other hand. In this paper, we further contextualize our findings in extant work on digital infrastructures, offer a guide for designers of blockchain infrastructures, and propose the concept of \"generative mirroring\" as a pattern through which blockchain infrastructures and organizing adaptively coevolve. Abstract Self-Organizing in Blockchain Infrastructures ', 'Bayesian logic We combine probabilistic logic and Bayesian networks to obtain the advantages of each in what we call Bayesian logic. Like probabilistic logic, it is a theoretically grounded way of representing and reasoning with uncertainty that uses only as much probabilistic information as one has, since it permits one to specify probabilities as intervals rather than precise values. Like Bayesian networks, it can capture conditional independence relations, which are probably our richest source of probabilistic knowledge. The inference problem in Bayesian logic can be solved as a nonlinear program (which becomes a linear program in ordinary probabilistic logic). We show that Benders decomposition, applied to the nonlinear program, allows one to use the same column generation methods in Bayesian logic that are now being used to solve inference problems in probabilistic logic. We also show that if the independence conditions are properly represented, the number of nonlinear constraints grows only linearly with the number of nodes in a large class of networks (rather than exponentially, as in the general case).', 'A linear programming framework for logics of uncertainty Several logics for reasoning under uncertainty distribute “probability mass” over sets in some sense. These include probabilistic logic, Dempster-Shafer theory, other logics based on belief functions, and second-order probabilistic logic. We show that these logics are instances of a certain type of linear programming model, typically with exponentially many variables. We also show how a single linear programming package can implement these logics computationally if one “plugs in” a different column generation subroutine for each logic, although the practicality of this approach has been demonstrated so far only for probabilistic logic.', \"Information technology and transitions in the public service: a comparison of Scandinavia and the United States  New information technologies have the potential to transform the ways governments are organized, the activities they perform, the manner in which such activities are performed and even the nature of the work itself. Governments in the US and Scandinavia have followed fundamentally different approaches to the introduction of computing and to dealing with its effects. In the US, automation has been individualistic -each individual unit of government has introduced the technology according to its own needs. For the most part, the implemented systems were small scale, have followed functional lines, have merely automated existing operations, were implemented incrementally and have evolved slowly over time. In contrast, automation in Scandinavia has been communal -systems have been designed, developed and implemented by communal data processing agencies serving an entire level of government, national or local. The systems introduced were relatively large scale, have crossed functional lines, have involved the reorganization of work, have integrated both data and work processes, and were implemented more or less simultaneously for all units or agencies of government. These differences in approach to automation have influenced each country's view of the role of government in anticipating and dealing with the effects of changes in computer technology on the public workforce. \", 'Activity-based design In many types of activities, communicative and material activities are so intertwined that the one cannot be understood without taking the other into account. This is true of maritime and hospital work that are used as examples in the paper. The spatial context of the activity is also important: what you can do depends upon where you are. Finally, human and automatic machinery alternate in filling certain roles in the activity: sometime the officer maintains the course, sometimes the autopilot. Such activities require us to rethink the traditional oppositions between communication and instrumental actions, between human and non-human participants, and between an activity and its spatio-temporal context. The advent of pervasive technologies, where active or passive systems become embedded in our working and living spaces, from where they offer their services to us, puts the need to reconsider these basic oppositions high on the research agenda. This paper presents a consistent framework called habitats for understanding communicative and material activities and their interplay, for understanding how activities can be associated to physical surroundings, and for understanding how humans and automatic machinery can replace one another in an activity. It also gives an example of how to use the framework for design.', 'The impact of IT on decision structure and firm performance: evidence from the textile and apparel industry Clarifying the relationships between information technology (IT), organizational performance, and decision structure remains an important area of inquiry in IS research. Through an empirical analysis and complementary case examples, our study examines these associations among firms operating in the US-based apparel and textile industry from 1992 to 1997. Based on data gathered from 50 public firms located across the USA, the study finds that IT used to enhance internal communication supports a decentralized decision structure, which in turn is associated with higher financial performance. Hence, IT exhibits an indirect performance effect. However, use of IT to enhance communication is also found to have a direct performance effect in large organizations. This paper proposes that use of communication enhancing IT can support organizational learning processes by facilitating flexible exchange of skills and knowledge across functional areas. Case examples are used to illustrate how these learning effects can materialize.', \"Information technology, strategic decision making approaches and organizational performance in different industrial settings The present study considers potential performance effects associated with the communication enhancing capacity of information technology. Enhancement of an organization's communication capabilities may influence performance through improved strategic decision making, better coordination of strategic actions and by facilitating learning from strategic initiatives. Accordingly, the paper investigates the effects of internal communication through use of computer networks, Intranet, and external communication via the Internet in association with autonomous and participatory strategic decision making approaches and strategic planning. These relationships are tested in two different industrial settings characterized by low and high levels of dynamism and complexity to assess possible environmental contingencies. In less dynamic and complex industries, the results show a positive association between Intranet use and innovation, while Internet use has a positive association to profitability and to innovation in organizations adhering to a participatory decision approach. In more dynamic and complex industries, Intranet use combined with an autonomous decision approach is associated with high profitability and sales growth, while Internet use combined with participatory decision making is associated with higher innovation. Hence, the study finds evidence that innovation relates to use of the Internet and participation across industries, and that economic efficiency relates to use of Intranet and autonomy in dynamic and complex industries.\", \"Strategic Investments for Platform Launch and Ecosystem Growth: A Dynamic Analysis Multi-sided platforms must make decisions on both pricing and engineering investment and must continually adjust them as the platform scales over its lifecycle. Engineering investments can be allocated to features that improve a platform's standalone value, social features to take advantage of same-side network effects, or integration tools and boundary resources to facilitate third-party content creation. Guidance in the academic or practitioner literature is not granular. Moreover, relevant normative economic models that consider externalities are rarely dynamic. Hence, there is a gap in knowledge about how to best balance tradeoffs between different strategic decisions throughout the entire platform lifecycle. To begin to address this gap, we explore normative strategies for coordinating pricing and engineering investment decisions on a continuous basis under different ecosystem conditions. We build a simulation model informed by economics and marketing theory and perform extensive sensitivity analyses on key parameters in different ecosystem scenarios over a multi-period lifecycle. We find that pricing and investment strategies must continuously change to perform optimally. In particular, strategies that are most effective at launch often differ from those that are most effective during scaling as well as those most effective at maturity. We also find that the optimal strategy depends strongly on the monetization model and market aversion to price changes. Lastly, we specifically examine four different industry segments: mobile platforms, social media, the sharing economy, and business-to-business. The results provide evidence that the trajectory of platform pricing and investment strategies should greatly differ depending on industrial context.\", 'Model checking for design and assurance of e-Business processes Use of the Internet for electronic business has the potential to revolutionize the way many businesses are conducted. Yet, several businesses have fallen victim to problems in information systems that facilitate e-Business. These problems are characterized by uncertainties due to system complexity, rapid development, interconnectivity, and a lack of familiarity with the new technologically based economy. This paper demonstrates how model checking can aid in the design and assurance of e-Business processes in environments characterized by distributed processing, parallelism, concurrency, communication uncertainties, and continuous operations.', \"Your memory is working against you: How eye tracking and memory explain habituation to security warnings Security warnings are critical to the security of end users and their organizations, often representing the final defense against an attack. Because warnings require users to make a contextual judgment, it is critical that they pay close attention to warnings. However, research shows that users routinely disregard them. A major factor contributing to the ineffectiveness of warnings is habituation, the decreased response to a repeated warning. Although previous research has identified the problem of habituation, the phenomenon has only been observed indirectly through behavioral measures. Therefore, it is unclear how habituation develops in the brain in response to security warnings, and how this in turn influences users' perceptions of these warnings. This paper contributes by using eye tracking to measure the eye movement-based memory (EMM) effect, a neurophysiological manifestation of habituation in which people unconsciously scrutinize previously seen stimuli less than novel stimuli. We show that habituation sets in after only a few exposures to a warning and progresses rapidly with further repetitions. Using guidelines from the warning science literature, we design a polymorphic warning artifact which repeatedly changes its appearance. We demonstrate that our polymorphic warning artifact is substantially more resistant to habituation than conventional security warnings, offering an effective solution for practice. Finally, our results highlight the value of applying neuroscience to the domain of information security behavior.\", 'How users perceive and respond to security messages: a NeuroIS research agenda and empirical study Users are vital to the information security of organizations. In spite of technical safeguards, users make many critical security decisions. An example is users’ responses to security messages – discrete communication designed to persuade users to either impair or improve their security status. Research shows that although users are highly susceptible to malicious messages (e.g., phishing attacks), they are highly resistant to protective messages such as security warnings. Research is therefore needed to better understand how users perceive and respond to security messages. In this article, we argue for the potential of NeuroIS – cognitive neuroscience applied to Information Systems – to shed new light on users’ reception of security messages in the areas of (1) habituation, (2) stress, (3) fear, and (4) dual-task interference. We present an illustrative study that shows the value of using NeuroIS to investigate one of our research questions. This example uses eye tracking to gain unique insight into how habituation occurs when people repeatedly view security messages, allowing us to design more effective security messages. Our results indicate that the eye movement-based memory (EMM) effect is a cause of habituation to security messages – a phenomenon in which people unconsciously scrutinize stimuli that they have previously seen less than other stimuli. We show that after only a few exposures to a warning, this neural aspect of habituation sets in rapidly, and continues with further repetitions. We also created a polymorphic warning that continually updates its appearance and found that it is effective in substantially reducing the rate of habituation as measured by the EMM effect. Our research agenda and empirical example demonstrate the promise of using NeuroIS to gain novel insight into users’ responses to security messages that will encourage more secure user behaviors and facilitate more effective security message designs.', 'From Warning to Wallpaper: Why the Brain Habituates to Security Warnings and What Can Be Done About It Warning messages are fundamental to users’ security interactions. Unfortunately, they are largely ineffective, as shown by prior research. A key contributor to this failure is habituation: decreased response to a repeated warning. Previous research has only inferred the occurrence of habituation to warnings, or measured it indirectly, such as through the proxy of a related behavior. Therefore, there is a gap in our understanding of how habituation to security warnings develops in the brain. Without direct measures of habituation, we are limited in designing warnings that can mitigate its effects. In this study, we use neurophysiological measures to directly observe habituation as it occurs in the brain and behaviorally. We also design a polymorphic warning artifact that repeatedly changes its appearance in order to resist the effects of habituation. In an experiment using functional magnetic resonance imaging (fMRI; n = 25), we found that our polymorphic warning was significantly more resistant to habituation than were conventional warnings in regions of the brain related to attention. In a second experiment (n = 80), we implemented the four most resistant polymorphic warnings in a realistic setting. Using mouse cursor tracking as a surrogate for attention to unobtrusively measure habituation on participants’ personal computers, we found that polymorphic warnings reduced habituation compared to conventional warnings. Together, our findings reveal the substantial influence of neurobiology on users’ habituation to security warnings and security behavior in general, and we offer our polymorphic warning design as an effective solution to practice', \"Practicing Safe Computing: A Multimethod Empirical Examination of Home Computer User Security Behavioral Intentions Although firms are expending substantial resources to develop technology and processes that can help safeguard the security of their computing assets, increased attention is being focused on the role people play in maintaining a safe computing environment. Unlike employees in a work setting, home users are not subject to training, nor are they protected by a technical staff dedicated to keeping security software and hardware current. Thus, with over one billion people with access to the Internet, individual home computer users represent a significant point of weakness in achieving the security of the cyber infrastructure. We study the phenomenon of conscientious cybercitizens, defined as individuals who are motivated to take the necessary precautions under their direct control to secure their own computer and the Internet in a home setting. Using a multidisciplinary, phased approach, we develop a conceptual model of the conscientious cybercitizen. We present results from two studies--a survey and an experiment--conducted to understand the drivers of intentions to perform security-related behavior, and the interventions that can positively influence these drivers. In the first study, we use protection motivation theory as the underlying conceptual foundation and extend the theory by drawing upon the public goods literature and the concept of psychological ownership. Results from a survey of 594 home computer users from a wide range of demographic and socioeconomic backgrounds suggest that a home computer user's intention to perform security-related behavior is influenced by a combination of cognitive, social, and psychological components. In the second study, we draw upon the concepts of goal framing and self-view to examine how the proximal drivers of intentions to perform security-related behavior identified in the first study can be influenced by appropriate messaging. An experiment with 101 subjects is used to test the research hypotheses. Overall, the two studies shed important new light on creating more conscientious cybercitizens. Theoretical and practical implications of the findings are discussed.\", \"The Digitization of Healthcare: Boundary Risks, Emotion, and Consumer Willingness to Disclose Personal Health Information As healthcare becomes increasingly digitized, the promise of improved care enabled by technological advances inevitably must be traded off against any unintended negative consequences. There is little else that is as consequential to an individual as his or her health. In this context, the privacy of one's personal health information has escalated as a matter of significant concern for the public. We pose the question: under what circumstances will individuals be willing to disclose identified personal health information and permit it to be digitized? Using privacy boundary theory and recent developments in the literature related to risk-as-feelings as the core conceptual foundation, we propose and test a model explicating the role played by type of information requested (general health, mental health, genetic), the purpose for which it is to be used (patient care, research, marketing), and the requesting stakeholder (doctors/hospitals, the government, pharmaceutical companies) in an individual's willingness to disclose personal health information. Furthermore, we explore the impact of emotion linked to one's health condition on willingness to disclose. Results from a nationally representative sample of over 1,000 adults underscore the complexity of the health information disclosure decision and show that emotion plays a significant role, highlighting the need for re-examining the timing of consent. Theoretically, the study extends the dominant cognitive-consequentialist approach to privacy by incorporating the role of emotion. It further refines the privacy calculus to incorporate the moderating influence of contextual factors salient in the healthcare setting. The practical implications of this study include an improved understanding of consumer concerns and potential impacts regarding the electronic storage of health information that can be used to craft policy.\", 'Information Security Control Theory: Achieving a Sustainable Reconciliation Between Sharing and Protecting the Privacy of Information Contemporary organizations operate in highly interconnected environments where they are frequently confronted by the challenge of balancing the protection of information resources with the need for sharing information. This tension between the expected benefits and the potential security risks inherent in the information sharing process, exists in many domains, including business, health care, law enforcement, and military—yet it is not well-understood. We propose an information security control theory to explain and manage this tension. We evaluate this theory through a longitudinal case study of the iterative development of the information security policies for a health information exchange in the western United States. Our study shows that the theory offers a good framework through which to understand the information security policy development process, and a way to reconcile the tension between information sharing and information protection. The theory has practical applicability to many business domains.', 'Managing compliance with privacy regulations through translation guardrails: A health information exchange case study Information privacy is increasingly important in our digitally connected world, particularly in healthcare, and privacy regulations are ramping up to promote appropriate privacy practices. As a digital platform that enables healthcare providers to exchange protected health information (PHI), a health information exchange (HIE) is governed by health information privacy regula\\xad tions. The challenge for HIEs is to operate in a way that will maximize information exchange while maintaining compliance with regulations that may constrain the sharing of PHI. Regula\\xad tions impose a measure of universality through compliance requirements, while being flexible to allow adaptation to the local context. However, our longitudinal case study into the privacy policies of an HIE, demonstrates that the journey of privacy ideas from their original formulation in regulations, to their ultimate enactment in an organizational setting, is accompanied by translations, such that the final implementation may vary extensively from its original form. Such variability often results in interpretations that differ from what the regulators intended. Conse\\xad quently, translation guardrails are necessary to protect against problematic translations of reg\\xad ulatory ideas which could lead to compliance issues and loss of platform participation. Our findings offer two contributions. First, we contribute to the compliance literature by explaining how guardrails can balance the use of permission and obligation schemas which are necessary to translate regulations into effective organizational policies for the success of HIEs and other in\\xad formation exchange platforms. Second, we contribute to extending translation theory by explaining how pragmatic reasoning schemas function as the mechanism through which trans\\xad lation of regulations occurs.', \"Blockchain innovation for consent self-management in health information exchanges With the increasing digitalization of health data, patients need to make informed decisions about the online sharing of their protected health information. This necessitates a robust technical infrastructure that enables patients to self-manage consent and the trusted exchange of this information across sharing entities. Unfortu\\xad nately, current health information exchange systems in the U.S. are limited in both these regards. While there is recent work on digital patient consent management, there is limited work that provides effective solutions for patient self-management of consent. Further, interoperability issues in the way health information exchanges are currently architected and differences in regulations across localities exacerbate the challenges of consent man\\xad agement. In this research, we survey potential patients' willingness to self-manage healthcare-related consent. Having established the desire for consent self-management, we propose a solution that enables the seamless sharing of patient consent across different healthcare providers and health information exchanges. Specifically, we use a rigorous design science approach to create a blockchain-based, self-managed patient consent system and we evaluate the design through an instantiated prototype. The results of our study should be useful to researchers in healthcare information management as well as to practitioners designing consent management systems. Our research contributes to design science research with an innovative, rigorously evaluated, design principles-based artifact that addresses a critical problem of sharing protected health information.\", \"Affordance potency: Explaining the actualization of technology affordances Given the importance of information technology (IT) in effecting organizational change, scholars have strived for many years to theorize the ways in which IT can produce the changes intended for it. Recent arguments claim that most information systems (IS) research has taken a limited theoretical focus on the information technology (IT) artifact, which arguably should be at the core of the IS discipline (Benbasat & Zmud, 2003). This research engages directly with the IT artifact by evaluating the use of an electronic medical records system and its relation to actualization of technology affordances. We conducted a case study at a large urban acute care hospital in the Midwestern United States with registered nurses working on inpatient care units as the clinicians of interest. Through interviews with nurses and other clinical stakeholders, observation of nurse's work practices on three patient care units in the hospital, and direct examination of the medical records system, we develop theoretical insights into the role of IT in work practices. The novel concept of affordance potency is introduced as an integral theoretical construct in our model of affordances, helping to explain actualizations of IT in use. Our contribution provides a nuanced yet powerful way of understanding the nature of IT artifacts and their relationships to technology users and work practice.\", 'Managing Distributed Product Development Projects: Integration Strategies for Time-Zone and Language Barriers Distributed product development projects encompass product and process development activities that span organizational and country boundaries. The increasing trend toward globalizing projects requires firms to coordinate development efforts made by team members from various functions within the firm, speaking multiple languages, and working in various time zones. We analyze qualitative data from 70 distributed product development projects that span 14 countries and involve cross-functional team members speaking 10 different languages. We find that commonly discussed integration strategies such as modular product designs and colocating team members by themselves are insufficient to coordinate project work. Rather, our field interviews suggest that firms invest in design information systems (DIS) with specific features to facilitate product design and empower their project managers to integrate the development efforts. Specifically, our interviews suggest that firms often modify the organization by “unifying” the engineering and purchasing functions into a single supply chain integrator function to increase the scope of responsibilities for these managers. We then test our hypotheses on the benefits of these strategies on project outcomes by using survey data from 55 distributed product development projects in 20 firms. Results indicate that the use of DIS is associated with higher quality and relationship performance when there are differences between focal and supplier firm personnel languages. Unifying engineering and purchasing functions into a supply chain integrator is associated with improved response time in the presence of time zone differences. We also find that a unifying strategy is associated with lower cost in the presence of language differences, but is also associated with a worsening of response time. These results provide guidance to product designers in organizations that must coordinate complex work across time zone barriers and languages. The results also provide guidance to researchers, by showing that different integration mechanisms may have differential effects across various coordination barriers and across multiple dimensions of project performance. We conclude by linking these results to integration mechanisms previously discussed in the coordination literature.', 'Microcomputer software evaluation: An econometric model Microcomputer software selection is made difficult by the multiplicity of products, variation in product performance, and the uncertainties of user needs. This paper presents a methodology for the empirical evaluation of competing software packages. The process proposed identifies the most relevant performance attribute set and, through a simultaneous system of equations, the relative importance of each attribute in explaining the satisfaction of users. The methodology is illustrated using sample data derived from user evaluations of five different software types: word processing, spreadsheet, data base management systems, communications, and graphics. The applicability of the methodology and the implications of the findings are discussed.', 'Platform Performance Investment in the Presence of Network Externalities Managers of emerging platforms must decide what level of platform performance to invest in at each product development cycle in markets that exhibit two-sided network externalities. High performance is a selling point for consumers, but in many cases it requires developers to make large investments to participate. Abstracting from an example drawn from the video game industry, we build a strategic model to investigate the trade-off between investing in high platform performance versus reducing investment in order to facilitate third party content development. We carry out a full analysis of three distinct settings: monopoly, price-setting duopoly, and price-taking duopoly. We provide insights on the optimum investment in platform performance and demonstrate how conventional wisdom about product development may be misleading in the presence of strong cross-network externalities. In particular, we show that, contrary to the conventional wisdom about “winner-take-all” markets, heavily investing in the core performance of a platform does not always yield a competitive edge. We characterize the conditions under which offering a platform with lower performance but greater availability of content can be a winning strategy. Keywords: two-sided markets; network externality; product development; video game industry', '“Standardizing information security – a structurational analysis” Given that there are an increasing number of information security breaches, organizations are being driven to adopt best practice for coping with attacks. Information security standards are designed to embody best practice and the legitimacy of these standards is a core issue for standardizing organizations. This study uncovers how structures at play in de jure standard development affect the input and throughput legitimacy of standards. We participated as members responsible for standards on information security and our analysis revealed two structures: consensus and warfare. A major implication of the combination of these structures is that legitimacy claims based on appeals to best practice are futile because it is difficult to know which the best practice is.', 'Architectural knowledge in inter-organizational IT innovation This paper examines the front-end process of inter-organizational IT innovation. In particular, it focuses on the nature and role of architectural knowledge. Such knowledge is important for development of architectures capable of serving the goals of heterogeneous actors and technologies. Yet, surprisingly little research has been done on how architectural knowledge may be developed through collective achievements. This paper presents a theoretical model of architectural knowledge development in inter-organizational IT innovation. Applying this model throughout an action research project within the Swedish transport industry, the paper identifies four dimensions of architectural knowledge that proved important for facilitating an industry-wide ubiquitous computing environment. The four dimensions are technology capability awareness, use context sensitivity, business model understanding, and boundary-spanning competence. We conclude the paper by outlining the theoretical and strategy implications of the model and the four dimensions of architectural knowledge.', 'Examining the state of empirical business intelligence and analytics research: A poly-theoretic approach Interest in Business Intelligence and Analytics (BI&A) has led to a growing body of impactful scholarly articles. We investigate the state of BI&A research by answering what is the state of BI&A research in terms of constructs studied, article’s macrostructure, and theoretical contributions, and how do the constructs studied, macro\\xad structure, and theoretical contributions, influence an article’s impact? We propose a poly-theoretic framework that classifies articles from top IS journals and conferences by studied construct, macrostructure, contribution, distribution, and impact. Findings provide an understanding of how articles’ components influence the impact of BI&A research. Implications and future research areas are discussed.', \"Competitive Brokerage: How Information Management Capability And Collaboration Networks Act As Substitutes IT-based information management and collaboration networks are both important sources of competitive information. Despite anecdotal evidence, limited research examines their contemporaneous impact on firms' ability to compete effectively. We take an information asymmetry perspective to examine the mechanisms through which the firm's information management capability and the structure of its collaboration network influence the structure of its competition network in product markets. We argue that taking a brokerage position in competition networks has a positive influence on a firm's performance. We then explain how the firm's information management capability and its position in collaboration networks each have a direct positive influence on its brokerage position in the competition network. Finally, we propose that information management capability and a central position in collaboration networks act as substitutes. We empirically test our model using a longitudinal competition network, a longitudinal collaboration network, and longitudinal secondary data about firms' information management capability, drawn from 262 firms over a 15-year period. Our findings, robust to endogeneity concerns and alternative model specifications, reveal the direct and substitution effects of information management capability and collaboration networks on competition networks. This research contributes to the information systems and strategy literatures by offering insights into how IT enables firms to design competitive strategies by facilitating analyses of competitors' information, and how the information gained by firms as they collaborate can enhance their ability to compete.\", \"Temporal enactment of resettled refugees' ICT-mediated information practices “In this paper, we explain how resettled refugees use information and communication technology (ICT) to respond to their changed circumstances and, in doing so, enhance their well-being and effective participation in a new society. Focusing on three modes of ICT-mediated information practices (ie, orienting, instrumental, and expressive), we identify eight patterns of ICT use: learning about a new environment, keeping informed, transacting online, communicating with others, managing everyday life, sustaining support networks, maintaining transnational ties, and expressing cultural identity. Further, we draw on a temporal theory of human agency to explain how current dilemmas and contingencies, cultural identities and connections to the past, and future expectations and aspirations shape resettled refugees' enactment of these patterns of ICT-mediated information practices. We show that, as resettled refugees move between multiple and overlapping temporal-relational contexts, ICT use makes a difference to managing their bifurcated lives.”\", 'Digital enforcement: Rethinking the pursuit of a digitally-enabled society In this article, we aim to sensitise the information systems community about the dispossession of choice that the extended reliance on Internet technology creates for individuals. The overemphasis of digital inclusion as a solution to the digital divide problem frames Internet use as desirable in a progressive society but labels non-use as problematic or a deficiency that needs to be remedied. This situation, we argue, creates a new modality of inequality that we term digital enforcement, defined as the process of dispossession that reduces choices for individuals who prefer to minimise their reliance on the Internet if given the opportunity or those who want to live their lives offline altogether. We present digital enforcement as an ethical problem and draw on the concepts of governmentality and technologies of power to explain how practices around Internet use in society result in digital enforcement. We conclude with a hopeful perspective to call for an ethical agenda to develop desirable futures.', 'Seeing for Understanding: Unlocking the Potential of Visual Research in Information Systems In this paper, we argue that information researchers should use images as a source of data. The information systems field is overwhelmingly visual in nature. Not only is the Internet crammed with images, but also almost every detail observed during fieldwork in different research settings can be captured in the form of digital images. Yet, we rarely engage with those images. Except for sporadic video recordings in analyzing human-computer interaction and, more recently, neurophysiological imaging, using images in information systems research has been sparse and non-systematic. Where images are used, the purpose of using them has been largely restricted to visually representing the context of the research setting. This approach underuses the knowledge embedded in visual material, which needs to be unpacked in a systematic fashion. We discuss the theoretical underpinnings of visual research and illustrate via a three-step framework how images in information systems research can be collected, analyzed, and presented. We conclude with four considerations for researchers that can help them develop a visual research capacity in information systems and encourage researchers to engage with the images that are now a major feature of the information systems environment.', 'Formal conceptualisation as a basis for a more procedural knowledge management Knowledge management at an organisational level can only be brought into practice if a corporate memory is defined. Unfortunately, at this moment there is no complete and procedural specification on how to build it. This paper presents a complete and generic knowledge representation scheme that makes it possible to conceptualise/represent the knowledge of any domain in a systematic way, guiding the definition of a corporate memory and allowing us to reach a more procedural level in knowledge management discipline. The conclusions of our study, which follows the generic and formal definition of any conceptualisation, are illustrated by a real project.', \"A decision support system for the collaborative selection of strategies in enterprise networks Collaborative networks (CN) consist of autonomous and heterogeneous partners, and each defines its own objectives and formulates its own strategies, which are selected and activated to achieve these objectives. The heterogeneity that characterises network partners could lead to contradictions appearing among the strategies formulated in each CN enterprise. Consequently, the strategies formulated in one enterprise could negatively influence the achievement of the objectives defined in other enterprises of the same network. These contradictions lead to strategies misalignments, which worsens the network performance. In order to deal with these misalignments, a DSS is proposed to support the process of selecting the strategies among all those formulated, with the aim of achieving higher alignment levels. The proposed DSS considers the impacts that each strategy formulated in each enterprise has on the performance of the objectives defined by each network partner. This allows enterprises to select a set of aligned strategies. The selection of proper strategies to be activated in each enterprise strongly influences the CN's performance level, and higher levels of network adaptability, agility and competitiveness are achieved. The proposed DSS is validated under real conditions in a food industry network. The DSS is evaluated by emulating real collaborative conditions and is compared with the equivalent non-collaborative decision making perspective used for selecting strategies. The results demonstrate that the collaborative approach outperforms the performance level of the non-collaborative one and is more effective for handling the robustness and the long-term operation of the CN.\", 'A Contingency Approach to Software Project Coordination Before software project managers can enhance productivity and satisfaction of the software project team member, the effect of task characteristics, goal orientations, and coordination strategies on design and coding-task outcomes must be understood. A research model, which suggests that task interdependence, goal conflict, and coordination strategies significantly affect productivity and satisfaction associated with software design and coding activities, is presented. Issues such as contingency/design misfit, conflicting contingencies, and the extent of deviation to theoretically prescribed coordination mechanisms applied to contingencies are used to make predictions on productivity and process satisfaction. A 2x2x2 factorial experiment was utilized. Overall, projects characterized by low task interdependence exhibited greater productivity than projects with high task interdependence. Also, in general, organic coordination was more productive than mechanistic coordination. There was also a significant interaction between task interdependence and coordination strategy. Low goal conflict and organic coordination each lead to greater process satisfaction. Productivity results for the goal conflict manipulation was opposite to the hypothesized direction. Unconflicted contingencies addressed with consistent coordination and partially conflicted contingencies, regardless of the coordination used, exhibited significant gains in productivity. In comparison, unconflicted contingencies with inconsistent coordination and conflicted contingencies, regardless of the coordination applied, resulted in lower productivity. This suggests that there are instances where multiple contingencies, which warrant the use of different coordination strategies, can be adequately addressed with a specific coordination strategy.', \"Organisational learning and core capabilities development: the role of IT The resource-based view of the firm (RBVF) focuses on the firm's resources and capabilities to understand business strategy and to provide direction to strategy formulation. This paper emphasizes the learning aspects of capability development and explores how information technology (IT) can contribute to it. As a standardized resource widely available, IT can participate in the fundamental process that transforms resources into capabilities and eventually into core capabilities. In this way, IT can become — embedded in core capabilities — an active component of the firm's competitive advantages. The process by which resources end up being components of core capabilities in firms is a learning process that can be described and understood using RBVF concepts. Furthermore, the development of IT strategic applications (also called ‘strategic information systems’, or SIS) follows patterns that closely parallel the structure of that learning process. For this reason we propose an organizational learning model based on the RBVF, and use it to derive guidelines for management action aimed at improving IT effectiveness in organizations. The paper is organized as follows: the RBVF framework is summarized, including the concepts of capabilities and core capabilities and the organizational processes that lead to them. Next, an organizational learning model is presented: an interpretation of capability development that emphasizes situated learning and knowledge accumulation. The model is then used to show how IT can contribute to core capability formation in a firm: management action can mold the process to some extent, although it often unfolds ‘naturally’ embedded in an organizational context that is both determined by and determinant of learning. Finally, guidelines are discussed to come up and build strategic IT applications, based on the previous analysis. Short conclusions follow.\", 'The strategic dimension of transactional information systems: some organizational implications Abstract. Often information systems (IS) are classified in three groups: (a) transactional, used mainly for co-ordination and resource allocation purposes at the operational level of a company; (b) tactical, often employed to support the resource procurement activities typical of middle management; and (c) information systems for strategic decision making, designed to help in the planning and strategy design processes which are the direct responsibility of top management. In general, the amount of care and management attention that companies give to these different types of systems is proportional to their position in this hierarchy: little attention is devoted to the mundane transaction-pushing systems and exquisite care is put into developing the sophisticated decision making aid for the CEO and his/her staff. The IS/IT literature has been reporting quite commonly cases in which companies have attained or lost great competitive advantages by way of their transactional information systems [for example, Emery Worldwide, Baxter Healthcare ASAP system, and Frontier Airlines]. The aim of this paper is to identify actions that companies can take to realize potential benefits of their IS, in particular from their low-level, transactional IS. Among other actions, we will conclude that companies would be better off if they: (a) have the IS department at the right place in the organization, staffed with people knowledgeable about the basic nature of the business in which the company is engaged; (b) are sensible to what can be called ‘strategic maintenance’ of systems, (c) set up a formal procedure for IS planning to ensure coherence between IS plans and business plans, derived, in turn, from business strategy, and (d) keep abreast of the relevant technology. Several examples taken from European companies are used to illustrate these conclusions.', \"Quality-informed semi-automated event log generation for process mining Process mining, as with any form of data analysis, relies heavily on the quality of input data to generate accurate and reliable results. A fit-for-purpose event log nearly always requires time-consuming, manual pre-processing to extract events from source data, with data quality dependent on the analyst's domain knowledge and skills. Despite much being written about data quality in general, a generalisable framework for analysing event data quality issues when extracting logs for process mining remains unrealised. Following the DSR paradigm, we present RDB2Log, a quality-aware, semi-automated approach for extracting event logs from relational data. We validated RDB2Log's design against design objectives extracted from literature and competing artifacts, evaluated its design and performance with process mining experts, implemented a prototype with a defined set of quality metrics, and applied it in laboratory settings and in a real-world case study. The evaluation shows that RDB2Log is understandable, of relevance in current research, and supports process mining in practice.\", \"An empirical model of IT usage in the Malaysian public sector Whilst there have been many studies to determine the factors that influence the use of information technology (IT) in organisations, few have considered how these factors change with the level of IT use. This paper presents the results of such a study involving the use of IT to support Total Quality Management (TQM). The population studied consisted of those organisations in the Malaysian public sector that had applied for the Malaysian Prime Minister's Quality Award during the period 1992–1997. Three sets of factors were investigated for their impact on the use of IT to support TQM in this setting: external, organisational, and technological factors. Overall, the organisational and technological factors had more influence on IT usage than did the external factors. However, as organisations became more experienced in their use of IT, the major contextual influences on IT usage levels changed. At low levels of IT usage the major contextual influences were organisational. At medium levels of IT usage a combination of technological and organisational factors became important, whilst at high IT usage levels, the dominant factors were technological.\", \"User information satisfaction, job satisfaction and computer background: An exploratory study The relationships among user information satisfaction (UIS), job satisfaction and the users' computer background were examined. UIS was measured using a modified version of the short-form of UIS, while job satisfaction was measured using the short-form Minnesota satisfaction questionnaire (MSQ). We found that UIS provides a sound indication of job satisfaction. However, none of the user computer-background parameters has any significant effect on UIS and job satisfaction. Data for the study were collected from three large organizations which had similar organizational structure and comparable information systems maturity; people who used computer as part of their jobs were randomly selected to take part in the study. A study with more organizations would yield better results.\", 'Critical success factors in implementing MRP and government assistance: A Singapore context It is generally acknowledged that Manufacturing Resource Planning (MRP) could revolutionize manufacturing operations. Making use of data from a recent survey of MRP practices in Singapore conducted jointly by the National Computer Board (NCB) and the National University of Singapore (NUS), this paper provides a profile of manufacturing companies in Singapore that both have and have not implemented MRP. The Critical Success Factors involved in implementing MRP are identified and the theoretical justification behind each factor examined. Finally, the ways in which the Singapore Government can help local companies implement, operate, and maintain MRP systems are discussed. It is an expensive investment, and difficult to implement, due to its complexity. It is hoped that knowledge and understanding of these factors will assist firms in successfully implementing MRP and enable them to further improve their systems in order to maximize returns.', 'A multiple-case design methodology for studying MRP success and CSFs We used a multiple-case design to study materials requirements planning (MRP) implementation outcome in 10 manufacturing companies in Singapore. Using a two-phased data collection approach (pre-interview questionnaires and personal interviews), we sought to develop a comprehensive and operationally acceptable measure of MRP success. Our measure consists of two linked components. They are a satisfaction score (a quantitative measure) and a complementary measure based on comments from the interviewees regarding the level of usage and acceptance of the system. We also extended and consolidated a seven-factor critical success factor (CSF) framework using this methodology. CSFs are important, but knowing the linkages between them is even more important, because these linkages tell us which CSFs to emphasize at various stages of the project.', 'CSFs and sources of assistance and expertise in strategic IS planning: a Singapore perspective Strategic information systems (IS) planning is not an easy task and knowing which critical areas to manage certainly enhances IS planning success. Studies of critical success factors (CSFs) usually dealt with specific systems or management technique implementation, such as manufacturing resource planning (MRP) and total quality management (TQM). There exists little empirical research on CSFs per se in strategic IS planning. This paper is an effort to enhance existing knowledge on how strategic IS planning should be effectively managed. Using data from a survey on IS planning conducted in 1996 by the National University of Singapore, we identified and rank-ordered the CSFs in strategic IS planning in the Singapore context. We also examined the sources of assistance and expertise that companies undertaking IS planning in Singapore can tap.', \"Management issues in data warehousing: insights from the Housing and Development Board Data warehousing has emerged as one of the most powerful tools in delivering information to users. In this paper, we examine data warehousing at the Housing and Development Board (HDB), which is responsible for providing affordable, high-quality public housing to Singapore citizens. The HDB embarked on building a data warehouse because access to the diverse and large amount of data in its operational systems, was becoming increasingly cumbersome and time consuming. By building a data warehouse, the HDB aims to facilitate users' access to corporate information for planning and decision making. The experiences and lessons learned from building a data warehouse at HDB are discussed.\", 'Work Outcomes and Job Design for Contract Versus Permanent Information: Systems Professionals on Software Development Teams  Organizations have significantly increased their use of contracting in information systems (IS), hiring contractors to work with permanent professionals. Based on theories of social exchange and social comparison, we hypothesize differences in work attitudes, behaviors, and performance across ', 'Production and Transaction Economies and IS Outsourcing: A Study of the U.S. Banking Industry  outsourcing decision. To empirically test these relationships, information was gathered from senior 17~ managers in 243 U.S. banks. Financial indices from the archives of the Federal Reserve Bank were a second important source of data.Results of the study show that IS outsourcing in banks was strongly influenced by production cost advantages offered by vendors. Transaction costs played a role in the outsourcing decision, but they were much smaller than production costs. Finally, financial slack was not found to be a significant explanator, although firm size was a significant control factor. The paper has important implications for research and practice. For researchers, the findings provide evidence that financial criteria can be key factors in outsourcing decisions and compare the relative effects of production and transaction costs. For practitioners, the findings suggest that managerial sourcing strategies need to weigh both costs when hiring systems integrators. AbstractThis paper studies economic determinants of IS outsourcing. It argues that a focus on comparative economic theories and models can improve our ability to explain outsourcing within the larger context of organizational strategy and environment. Specifically, the research constructs of production cost, transaction cost, and financial slack are examined simultaneously to understand what influences the 1 Robert Zmud was the accepting senior editor for this paper ', nan, 'DSS research and practice in perspective The aim of this paper is to assess the state-of-the-art in the Decision Support Systems (DSS) field from both a research and a practice perspective. Three main dimensions of DSS research and practice are addressed: 1) supporting human decision-making processes, 2) integrating DSS into the organizational context, and 3) identifying new application domains. The related analysis and discussion provides a better understanding of past developments in the DSS field and insights into future evolution patterns.', 'An empirical study of EDI trading partner selection criteria in customer-supplier relationships Electronic data interchange (EDI)-enabled trading partnerships are even more important now that EDI and electronic commerce-based technologies are underlying long-term strategic business partnerships. This study investigates the trading partner selection criteria used by firms in a customer-supplier dyad and their relative importance according to EDI implementation level is also established. Using the survey method implementing paired questionnaires for a dyad of customer-supplier firms, the study gathered data from 152 respondent firms. Factor analysis yielded six factors in trading partner selection: strategic commitment, trading partner flexibility, joint partnering for EDI, readiness for high-level EDI, EDI infrastructure, and communications. MANOVA and t-tests were used to test differences in the means of the responses of customer and supplier firms to the selection criteria. Overall, customer firms assigned higher means to all six factors than did the supplier firms. The gap between the two groups of firms were widest for the factors readiness for high-level EDI, trading partner flexibility, and communications.', 'Eight scenarios of national information superhighway development Leading industrialized nations are now involved in the full implementation of national information superhighways. Their efforts will depend largely on the roles and levels of involvement of public institutions and how this is perceived and reacted to by the private sector. Through the evolution of eight scenarios of public institution involvement, this paper extrapolates the consequences that each brings about in the development of a national information superhighway. In particular, this paper examines the consequences of the development of an information superhighway for the UK within the context of each scenario and draws comparisons with developments in other countries. Contrary to popular belief, we show that there is no full solution for the shortcomings experienced in the development of the UK’s information superhighway; rather, each scenario offers solutions for a subset of the shortcomings.', 'A methodology for specific, total enterprise, role-playing, intelligent gaming-simulation environment development This paper contributes a methodology for integrating intelligent tutoring into a specific, total enterprise, role-playing gaming-simulation environment. The result of the application of the methodology is a prototype intelligent gaming-simulation environment.', \"‘Though this be madness, yet there is method in't’ The paper is a synthesis of the growing doubt and dissatisfaction felt by the authors about the direction that the study of ‘information systems’ (IS) seems to be taking. The first author has over 20 years' experience, as an information technology/IS academic, as a consultant, and as a co-author of a large and commercially successful software package. The second author is relatively new to the subject, but is working full-time in a commercial IS environment. His views reflect the real uncertainties felt by many practitioners who put current IS theory into practice. Together the two authors confront some of the many methodical features that the discipline considers self-evidently true, but which they have come to see as bizarre.\", \"A model and a performance measurement system for collaborative supply chains Modeling the constituents of a collaborative supply chain, the key parameters they influence, and the appropriate performance measures in a decision support environment enables prior understanding of the impact on the performance of a collaborative supply chain as a result of changes in the constituents and key parameters. In turn, this allows pinpointing of those areas where the actual supply chain can be improved and hence manage the chain's performance. This paper shows how the constituents, key parameters and performance indicators are modelled into the environment and through a case study illustrates how the decision support environment may be used to improve the performance of a collaborative supply chain by pinpointing areas for improvement.\", 'Taxo-Semantics: Assessing similarity between multi-word expressions for extending e-catalogs Taxonomies, also named directories, are utilized in e-catalogs to classify goods in a hierarchical manner with the help of concepts. If there is a need to create new concepts when modifying the taxonomy, the semantic similarity between the provided concepts has to be assessed properly. Existing semantic similarity assessment techniques lack in a comprehensive support for e-commerce, as those are not supporting multi-word expressions, multilingualism, the import/export to relational databases, and supervised user-involvement. This paper proposes Taxo-Semantics, a decision support system that is based on the progress in taxonomy matching to match each expression against various sources of background knowledge. The similarity assessment is based on providing three different matching strategies: a lexical-based strategy named Taxo-Semantics-Label, the strategy Taxo-Semantics-Bk, which is using different sources of background knowledge, and the strategy Taxo-Semantics-User that is providing user-involvement. The proposed system includes a translating service to analyze non-English concepts with the help of the WordNet lexicon, can parse taxonomies of relational databases, supports user-involvement to match single sequences with WordNet, and is capable to analyze each sequence as (sub)-taxonomy. The three proposed matching strategies significantly outperformed existing techniques. Taxo-Semantics-Label could improve the accuracy result by more than 7% as compared to state-of-the-art lexical techniques. Taxo-Semantics-Bk could improve the accuracy compared to structure-based techniques by more than 8%. And, Taxo-Semantics-User could additionally increase the accuracy by on average 23%.', 'Information technology and voluntary quality disclosure by hospitals Information asymmetry between consumers and health care providers is a well-known phenomenon in health care systems. Disclosure of health care quality information is one important mechanism through which hospitals can signal performance to potential patients and competitors, yet little is known about the organizational factors that contribute to voluntary disclosure. In this study we develop an empirical model to investigate the factors associated with choosing to participate in a voluntary quality disclosure initiative, specifically isolating the importance of information technology (IT) in facilitating disclosure. We extend the scope of prior work on the quality disclosure choice by augmenting it with an important decision variable: the operational costs of collecting and reporting quality data. We suggest that IT can facilitate disclosure by reducing these costs, thereby extending the literature on the value of IT. Empirical findings using data from a major voluntary quality disclosure program in California hospitals support our assertion related to the role of IT. Our results further highlight other hospital characteristics contributing to disclosure. We discuss implications of these findings for research and practice.', \"Adoption of Electronic Health Records in the Presence of Privacy Concerns: The Elaboration Likelihood Model and Individual Persuasion Within the emerging context of the digitization of health care, electronic health records (EHRs) constitute a significant technological advance in the way medical information is stored, communicated, and processed by the multiple parties involved in health care delivery. However, in spite of the anticipated value potential of this technology, there is widespread concern that consumer privacy issues may impede its diffusion. In this study, we pose the question: Can individuals be persuaded to change their attitudes and opt-in behavioral intentions toward EHRs, and allow their medical information to be digitized even in the presence of significant privacy concerns? To investigate this question, we integrate an individual's concern for information privacy (CFIP) with the elaboration likelihood model (ELM) to examine attitude change and likelihood of opting-in to an EHR system. We theorize that issue involvement and argument framing interact to influence attitude change, and that concern for information privacy further moderates the effects of these variables. We also propose that likelihood of adoption is driven by concern for information privacy and attitude. We test our predictions using an experiment with 366 subjects where we manipulate the framing of the arguments supporting EHRs. We find that an individual's CFIP interacts with argument framing and issue involvement to affect attitudes toward the use of EHRs. In addition, results suggest that attitude toward EHR use and CFIP directly influence opt-in behavioral intentions. An important finding for both theory and practice is that even when people have high concerns for privacy, their attitudes can be positively altered with appropriate message framing. These results as well as other theoretical and practical implications are discussed.\", 'When Do It Security Investments Matter? Accounting for the Influence of Institutional Factors in the Context of Healthcare Data Breaches In this study, we argue that institutional factors determine the extent to which hospitals are symbolic or substantive adopters of information technology (IT) specific organizational practices. We then propose that symbolic and substantive adoption will moderate the effect that IT security investments have on reducing the incidence of data security breaches over time. Using data from three different sources, we create a matched panel of over 5,000 U.S. hospitals and 938 breaches over the 2005–2013 time frame. Using a growth mixture model approach to model the heterogeneity in likelihood of breach, we use a two class solution in which hospitals that (1) belong to smaller health systems, (2) are older, (3) smaller in size, (4) for-profit, (5) nonacademic, (6) faith-based, and (7) less entrepreneurial with IT are classified as symbolic adopters. We find that symbolic adoption diminishes the effectiveness of IT security investments, resulting in an increased likelihood of breach. Contrary to our theorizing, the use of more IT security is not directly responsible for reducing breaches, but instead, institutional factors create the conditions under which IT security investments can be more effective. Implications of these findings are significant for policy and practice, the most important of which may be the discovery that firms need to consider how adoption is influenced by institutional factors and how this should be balanced with technological solutions. In particular, our results support the notion that deeper integration of security into IT-related processes and routines leads to fewer breaches, with the caveat that it takes time for these benefits to be realized.', 'Dual Role of IT-Assisted Communication in Patient Care: A Validated Structure-Process-Outcome Framework Despite the fact that about 90 percent of information transactions in hospitals are communications between patients, doctors, nurses, and other staff, little research has addressed the role that information technology (IT) plays in improving the efficiency and effectiveness of these communications-based transactions. Addressing this research gap is important considering that a substantial number of adverse hospital events stem from communication failures. Furthermore, effective communication is a major driver of patient satisfaction in hospitals. Using a structure-process-outcome (SPO) framework and guided by the strategic role of IT literature, we develop a model that includes \"structure,\" operationalized as organizational characteristics and two different categories of IT; \"process,\" two different communication-based processes; and \"outcomes,\" quantified as case-mix adjusted mortality, patient loyalty, and patient ratings. Specifically, we hypothesize that a subset of clinical IT (cardiology IT) will affect technical protocols of patient care, which in turn affects mortality, while administrative IT will affect interpersonal patient care, which relates to patient loyalty and ratings. Thus, IT can serve as a double-edged sword affecting both technical and interpersonal processes of care, but possibly independently and differentially. We test our hypotheses on 2,179 hospitals using data collected and matched from three different sources. Our findings suggest that different types of IT differentially affect hospital processes and these same processes influence performance metrics such as mortality and patient satisfaction. For example, cardiology IT has a greater effect on objective patient health status through improvements in the technical protocols of care. Surprisingly, administrative IT was shown to adversely affect interpersonal care processes. It could be true that the IT is intrusive and interferes in the doctor-patient relationship; however, a post hoc analysis suggests the possibility of curvilinear impacts. Thus, managers should recognize that over- and underinvestment in IT can potentially have negative effects on performance and these results vary by IT type. Both technical and interpersonal processes yielded significant relationships to their respective outcomes and some cross-outcome effects were found, further suggesting that the mediating role of processes is an important link between IT and value.', 'Antecedents of Information Systems Sourcing Strategies in U.s. Hospitals: A Longitudinal Study The popular press has long used the terms single-sourcing and multisourcing (also known as best of breed) to describe organizations’ sourcing strategies. Whereas there is an implicit understanding of these terms, no research has quantified what distinguishes one sourcing configuration from another or what institutional factors contribute to the pursuit of one strategy over the other. We leverage institutional theory to examine how key organizational antecedents such as strategic orientation (mission), formal structure (size), and internal dynamics (patient case mix complexity) influence the rate at which organizations move toward or away from a single-sourcing configuration. Employing longitudinal modeling combined with sequence analysis techniques, we empirically evaluate IS sourcing strategies of nearly all U.S. hospitals operating continuously over a 9-year time frame from 2005 to 2013. We find that hospitals are generally trending toward a single-sourcing configuration and that formal structure and internal dynamics serve as predictors of this trend. Contrary to the predictions of institutional theory, we find that strategic orientation is not predictive of IS sourcing strategy. These results have important implications for research and practice. Notably, we are the first to quantify sourcing strategies, and, by doing so, are able to inform practitioners and academics of the key organizational characteristics that lead hospitals to move more quickly toward single-sourcing configurations.', \"An Odyssey into Virtual Worlds: Exploring the Impacts of Technological and Spatial Environments on Intention to Purchase Virtual Products1 Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus-organism-response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants' virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants' virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.\", 'Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.', 'Competing \"Creatively\" in Sponsored Search Markets: The Effect of Rank, Differentiation Strategy, and Competition on Performance Although efficiency-enhancing features of online markets have been well studied, much less is known about firms\\' differentiation strategies in these competitive markets or the outcomes of such differentiation. This study examines competition among firms in online sponsored search markets—one of the fastest growing and most competitive of online markets. We develop and test a model that predicts the clickthrough rate (CTR) of a seller\\'s listing in a sponsored search setting. Drawing on consumer search theory and competitive positioning strategies, we theorize that CTR is jointly driven by a seller\\'s positioning strategy as reflected by the unique selling proposition (USP) in its \"ad creative,\" by its rank in a sponsored search listing, and by the nature of competition around the focal firm\\'s listing. We use data from a field experiment conducted by a leading firm in the mortgage industry where the firm varied its rank and USP dynamically. Results suggest that sponsored search listings can act as effective customer segmentation mechanisms, consistent with a model of consumer search in directional markets. We further find that the effect on CTR of a firm\\'s positioning strategy and its rank in a listing is strongly moderated by its ability to differentiate itself from adjacent rivals. We discuss the implications of our findings for sellers\\' strategies in sponsored search markets and for extending the understanding of consumer search behavior in directional markets.', 'Support Structures and Their Impacts on Employee Outcomes: A Longitudinal Field Study of an Enterprise System Implementation Despite the impressive progress in understanding the benefits and challenges related to enterprise system (ES) implementations—such as enterprise resource planning (ERP) systems—little is known about how the support structures traditionally used by organizations to help employees cope with a new ES affect employee outcomes related to the system and their jobs. Likewise, little is known about how existing peer advice ties in the business unit influence these outcomes after an ES implementation. Understanding employee outcomes is critical because of their ramifications for long-term ES success. This paper examines the impacts of four traditional support structures (namely, training, online support, help desk support, and change management support), and peer advice ties on four key employee outcomes (namely, system satisfaction, job stress, job satisfaction, and job performance). This paper also seeks to show that it is peer advice ties that best fill the complex informational needs of employees after an ES implementation by providing the right information at the right time and in the right context. The proposed model was tested in a field study conducted in one business unit of a large telecommunications company and gathered data from 120 supplier liaisons over the course of a year. Both traditional support structures and peer advice ties were found to influence the various outcomes, even after controlling for pre-implementation levels of the dependent variables. In all cases, peer advice ties was the strongest predictor, thus underscoring the importance of this critical internal resource.', 'Improving the retention of women in the IT workforce: An investigation of gender diversity interventions in the USA To meet the high demand for information technology (IT) professionals, organizations must become more effective at attracting and retaining women. Ninety-seven percent of companies surveyed by Forbes in 2011 had implemented diversity and inclusion interventions. Despite these efforts, the percentage of women working in IT continues to decline, raising questions about the effectiveness of current organizational interventions aimed at increasing gender diversity. This study sought to gain a better understanding of these organizational interventions by developing a comprehensive framework based on comparative case studies of 9 organizations. The framework integrates intervention characteristics and barriers IT women experience and the coping methods they use to address barriers. This paper presents propositions based on this theoretical framework to guide further research on the effectiveness of gender diversity and inclusion interventions in IT.', \"Logics' shift and depletion of innovation: A multi-level study of agile use in a multinational telco company The use of Agile practices is typically associated to a wide array of benefits for organizations. This paper extends growing research on the ‘dark’ side of Agile by investigating the depletion of innovation in a large telco company following the large-scale implementation of Agile in R&D units. Our qualitative study reveals a shift in the organizational logics underpinning new product development, from a “navigating through unchartered waters” to a “putting out fires” logic. We tracked the change in key components of logics (goals of teams, source of legitimacy of team members and support and control systems) and explained the multi-level mechanisms through which the shift occurred, i.e., changes in processes of workflow management, work allocation, and performance management. We found that the new organizational logic negatively impacted individual attitudes towards the generation of new ideas by promoting the internalization of short-termism, a perceived drain in competences and confidence, and the lack of accountability for innovation. By focusing on changes in organizational logics, our insights expand current knowledge about the relationship between Agile implementation and individual attitudes. We also explain why unexpected effects of Agile implementation may go undetected in organizations, because they derive from multi-level, diffused, changes in the organization.\", 'SYNOPSE: a model-based decision support system for the evaluation of flight schedules for cargo airlines The ability to evaluate flight schedules, with respect to cost, revenue and contribution to profit, is essential for cargo airlines to respond properly to changing environments in this highly competitive and consumer-oriented market. In this paper, we introduce SYNOPSE, our model-based decision support system for the evaluation of flight schedules for cargo airlines, by describing the underlying planning situation, the data model and the decision models used, and the implementation, i.e., the development process, as well as the architecture. A small example illustrates the high complexity of the analysis process and the supporting qualities of our decision-support system (DSS).', 'The leveraging influence of strategic alignment on IT investment: An empirical examination Businesses have invested enormous sums in information technology (IT). The challenge now is to optimize these investments. We empirically examined the influence of the alignment between IS strategy and business strategy (strategic alignment) on the payoff of IT investment. Many studies have been performed on the value of IT investment and strategic alignment separately, in the past, but here we combined them by investigating the moderating affect of strategic alignment on the relationship between IT investment and firm performance for a group of manufacturing firms. The results indicated that there is a synergistic coupling between strategic alignment and IT investment with firm performance. Firms that have aligned IT and business strategies can invest in additional IT resources with some assurance that they will be leveraged substantially. One of our main contributions was in the examination of four differing perspectives of strategic alignment and their relationship with the payoff of IT investment.', 'The property of being causal – The conduct of qualitative comparative analysis in information systems research Grounded in configuration and complexity theory, qualitative comparative analysis (QCA) combines the ad\\xad vantages of case-based and variable-oriented methods for rendering complex information systems (IS) phe\\xad nomena comprehensible. Given its manifold benefits, the QCA method has attracted considerable attention in IS research, with an increasing number of studies employing it as their methodological approach. Based on a comprehensive review and synthesis of recent QCA practices from the IS field, covering 12 years of research, we outline the most prevalent research gaps and limitations concerning QCA’s methodological application prior to identifying issues for further improvement as well as highlighting future research directions.', 'The use of a knowledge-based system in conceptual data modeling Based on a study of the data modeling process of novice designers, and the errors they commit, a knowledge-based system (KBS) was designed and developed. It was found that the performance of novice designers was significantly better when they utilized the KBS instead of a system with no knowledge base. Two versions of the KBS—one with a guidance interface that advised the designer on appropriate design choices and another with a restrictive interface that restricted the design choices available to the designer—were developed. The restrictive interface was rated as being significantly easier to use than the guidance interface.', \"Determinants of escrow service adoption in consumer-to-consumer online auction market: An experimental study Risk relief services (RRSs), as complementary to online trust promoting services, are becoming versatile options for risk reduction in online consumer-to-consumer auctions. In this paper, we identify factors that affect the behavior of buyers in an online auction market who had to either adopt or not adopt online escrow services (OES). An experimental C2C auction system with embedded decision support features was used to collect data. Results show that market factors, such as fraud rate, product price, and seller's reputation are important in determining buyers' OES adoption. This study also finds that sellers' reputation has a significant effect on buyer's risk perception, which influences his OES adoption decision. Furthermore, the buyers' OES adoption decisions were found to be congruent with the implied recommendations that were based on expected utility calculations.\", 'Could the use of a knowledge-based system lead to implicit learning? The primary objective of a knowledge-based system (KBS) is to use stored knowledge to provide support for decision-making activities. Empirical studies identify improvements in decision processes and outcomes with the use of such knowledge-based systems. This research suggests that though a KBS is primarily developed to help users in their decision-making activities, as an unintentional consequence, it may induce them to implicitly learn more about a problem. Implicit learning occurs when a person learns unconsciously or unintentionally, without being explicitly instructed or tutored. To test these ideas, a laboratory-based experiment was conducted with a KBS that could provide support for data modeling activities. Results indicated support for implicit learning because subjects who interacted with the KBS exhibited better knowledge on data modeling concepts than those who did not interact with the KBS. Two versions of the KBS were tested, one with a restrictive interface and the other with a guidance interface, and both versions of the interface supported implicit learning. Implications for future research on the design and development of KBSs are proposed.', nan, 'TEA-IS: A hybrid DEA-TOPSIS approach for assessing performance and synergy in Chinese health care This paper presents an assessment of the Chinese healthcare system in 31 provinces for a 10-year period in light of relevant physical and human resource variables. First, a novel TEA-IS (Trigonometric Envelopment Analysis for Ideal Solutions) model is developed to assess healthcare efficiency at the province level. Machine learning methods are also employed to predict high-low performance and the synergistic Chinese healthcare province in terms of contextual variables. The results indicate that synergy has played a pivotal role in the Chinese healthcare systems, not only by triggering higher performance levels due to the progressive adoption of best practices over the course of time, but also by being closely related to different socioeconomic and demographic variables, such as the illiteracy rate. It is possible to claim that healthcare performance has remained stable in China over the past two decades, performance and synergy at the province level are still heterogeneous.', 'Gamification: A key determinant of massive open online course (MOOC) success Massive open online courses (MOOCs), contribute significantly to individual empowerment because they can help people learn about a wide range of topics. To realize the full potential of MOOCs, we need to understand their factors of success, here defined as the use, user satisfaction, along the individual and organizational performance resulting from the user involvement. We propose a theoretical framework to identify the determinants of successful MOOCs, and empirically measure these factors in a real MOOC context. We put forward the role of gamification and suggest that, together with information system (IS) theory, gamification proved to play a crucial role in the success of MOOCs.', \"Strategic profiles and Internet Performance: An empirical investigation into the development of a strategic Internet system Organizations continue to work on defining and developing better strategic Internet systems. The use of the Internet should not, however, adversely affect their existing business processes but rather incorporate and support them. Therefore, developing a strategic Internet system should support their strategic profile while providing high performance from its use. We took a new approach to Internet development by analyzing an organization's Internet use and performance by adopting Miles and Snow's classifications of business strategy: Defenders, Analyzers, and Prospectors. A sample of 257 IT managers and professionals helped in suggesting relationships between Internet use and performance; the consequent model was then validated. Further analysis was then conducted at the dimension level to highlight the differences between strategic profiles and the appropriate Internet use. The results indicated identifiable approaches for different strategic profiles and their desired performance levels.\", 'Investigating Retrieval-Induced Forgetting During Information Requirements Determination Successful systems development requires that appropriate and accurate information be gathered from people who use or will use the system. One critical issue in information gathering is the recall of relevant information by users and other stakeholders. Prior research has shown that users do not recall all the relevant information they have about the requirements for systems, and we suggest that this problem is exacerbated by current systems development practice, in which the same users are often interviewed multiple times by analysts. A potential theoretical explanation for recall failure in requirements determination is the psychological phenomenon known as Retrieval-Induced Forgetting. Retrieval-Induced Forgetting (RIF) theory and empirical findings show that when people are asked to recall information about a situation multiple times, they are likely to recall the same information on subsequent attempts as they recalled on the first attempt (to the exclusion of other relevant information). Although the RIF phenomenon has been investigated in several contexts, such as eyewitness testimony, there have been no studies that have examined the issue in applied contexts such as systems development, in which prior domain knowledge exists and has been learned over a period of time by users and other stakeholders. In the current study, experimental results showed the presence of RIF in both short-term and longer-term information requirements determination (IRD) contexts, thereby providing a memory-based explanation for missing requirements in IRD. Our results have strong implications for the type and sequencing of requirements elicitation techniques and demonstrate threats to both traditional and iterative development methodologies.', \"The Impact of Analyst-Induced Misinformation on the Requirements Elicitation Process Information requirements determination (IRD) is concerned with developing accurate requirements for a proposed system, primarily by eliciting information from users and other organizational stakeholders. In this paper we build and test theory concerning a significant threat to the accuracy of information requirements, termed the misinformation effect. Misinformation is distorted, false, or other erroneous or misleading information that does not reflect the true state of the world or state of mind of the person communicating the information. The misinformation effect refers to the tendency of people to recall misleading or false information introduced to them following an event instead of original material learned or observed at the time the event occurred. During user-analyst communication in the IRD process, analysts may introduce misinformation in their discussions with users. We use the misinformation effect literature to hypothesize that in such circumstance users are likely to recall misinformation introduced by analysts rather than their true beliefs and knowledge of facts. Additionally, we use literature in social psychology to hypothesize that the misinformation effect will be stronger when misinformation is introduced using a social technique rather than a nonsocial technique. We conducted an experiment to test the misinformation effect in the requirements elicitation process. Results indicated that (1) introduction of misinformation reduces the accuracy of requirements provided by users, and (2) social techniques (interviews) are more vulnerable to the misinformation effect than nonsocial techniques (surveys). Our research contributes to the information systems literature by identifying an important reason that requirements provided by users may be inaccurate, and to IRD practice by identifying important dilemmas caused by the misinformation effect as well as potential solutions. We also contribute to the psychology literature by demonstrating the existence of the misinformation effect with users' experiential factual knowledge and beliefs in a business context, and by aiding in understanding the underlying causes of the misinformation effect. We discuss implications of our findings and directions for future research to address challenges resulting from the misinformation effect.\", 'New Information Systems Leaders: A Changing Role in a Changing World  It is widely argued that the information systems (IS) leadership function has undergone fundamental changes over the past decade. To better understand the changes, this study compares the backgrounds, responsibilities, reporting relationships, and power of newly appointed IS executives (who had been in their position for two years or less) with established IS executives (who had been in their position for five years or more). The study found that approximately half of the new IS executives were external hires, whereas almost all of the established IS executives were promoted from within the company. More than two-thirds of the new IS executives had more than five years\\' experience managing a non-IS function within the past 15 years. Established IS executives had spent the majority of their career within the IS function. The activities receiving the most attention from new IS executives were information technology (IT) strategic planning and control, IT architecture management and stan-\\' Portions of this study were previously reported in: Applegate, L.M. and Elam, J.J. \"CIO and SuperCIO.\" ClO, April 1991.dards development, and human resource management. For established IS executives, the activities receiving the most attention were IT architecture management and standards development, human resource management, and operations. An increasing number of new IS executives reported directly to the CEO, and almost half were members of the senior management/ strategic policy committee. These findings have several important implications. First, the senior IS executive must be able to bring a broad business perspective to the position. Current senior IS executives who have not broadened their own knowledge, skills, and experiences in business strategy, management, and operations should immediately develop a personal career development program to gain these valuable perspectives. Second, senior IS executives should implement career development strategies within their own organizations that ensure that IS professionals have the opportunity to acquire the business management experience necessary to advance to higher IS management levels. Third, graduate and executive programs designed to prepare future IS managers and leaders must provide both a business and IT perspective throughout the curriculum. ', \"Reusability-Based Strategy for Development of Information Systems: Implementation Experience of a Bank  This paper describes the experience of a large bank in designing and implementing an information systems strategy that is based on the concept of resuability. The design and implementation was performed in two stages: (1) building prototype to investigate the feasibility and attractiveness of reusability concept for the bank; and (2) its subsequent implementation using a library of reusable entities and a programmer's workbench. The implementation experience confirmed that applying the reusability concept to all stages of the system's life cycle results in both strategic (e.g., improving programmer productivity and increasing the bank's capacity for timely response to market opportunities) and operational (e.g., reducing and controlling system development and maintenance costs) benefits. It is estimated that the library of reusable entities embedded within the programmer workbench saved the bank over $1.5 million in development costs in 1989 alone. Two of the most important lessons learned in implementing the reusabilitybased strategy are: (1) reusability comes in many flavors and should be applied to all stages of systems fife cycle; and (2) major challenges implementing the reusability-based strategy are managerial, not technical. \", 'High tech or high touch? Efficient channel strategies for delivering financial services With the progress of information technology, financial service institutions are restructuring their delivery channels. The delivery channel applications now range from direct sales and agency systems to all-electronic, customer-accessed networks. A key issue in the design of a delivery channel is achieving a proper mix of new technology and traditional human-centred delivery approach. Moreover, the delivery channel design should be properly matched with the characteristics of the service being offered. Based on a model of economic trade-offs, a theoretical framework, called the service channel strategies (SCS) approach, is proposed for identifying, analysing and designing appropriate delivery channel applications in the financial services industry. An example from banking is discussed to highlight the scope and normative focus of the proposed theoretical framework.', 'Exploring contributions of public resources in social bookmarking systems Our study examines whether users’ contributions of public resources to social bookmarking sites are circumstantial (a side effect of bookmarking for oneself), or motivational (intentional bookmarking for others). We develop a research model based on these two explanations and test it using survey data from users of two bookmarking sites. Our results suggest that public contributions are mainly driven by intentional bookmarking of resources for other users. In addition, we found that users deliberately bookmark resources for others when they believe that their bookmarks are valuable to other users and when they perceive that other users are contributing as well.', 'Digital Consumer Networks and Producer-Consumer Collaboration: Innovation and Product Development in the Video Game Industry This paper examines new forms of collaboration between producers and consumers that are emerging in the digital entertainment space. Taking the case of the video game industry, we show how some firms have opened a portion of their proprietary content for transformation by consumers and allowed the development of consumer-designed and consumer-implemented derivative products. By reappropriating these derivatives, video game firms are successfully outsourcing parts of their game design and development process to digital consumer networks. Applying economic analysis, we explore the potential benefits and risks associated with outsourcing to networks of consumers. We also derive the optimal combination of copyright enforcement and consumer compensation. Our results suggest that profit-maximizing producers of video games have incentive to partially open game content to their users and to remunerate the most innovative ones, under the condition that the derivatives constitute complements to, and not substitutes for, the original product. We discuss the implications on firm strategy for innovation.', \"Information, Technology, and Information Worker Productivity We econometrically evaluate information worker productivity at a midsize executive recruiting firm and assess whether the knowledge that workers accessed through their electronic communication networks enabled them to multitask more productively. We estimate dynamic panel data models of multitasking, knowledge networks, and productivity using several types of micro-level data: (a) direct observation of more than 125,000 email messages over a period of 10 months; (b) detailed accounting data on individuals' project output and team membership for more than 1,300 projects spanning five years; and (c) survey and interview data about the same workers' IT skills, IT use, and information sharing. We find that (1) more multitasking is associated with more project output, but diminishing marginal returns, and (2) recruiters whose network contacts have heterogeneous knowledge—an even distribution of expertise over many project types—are less productive on average but more productive when juggling diverse multitasking portfolios. These results show how multitasking affects productivity and how knowledge networks, enabled by IT, can improve worker performance. The methods developed can be replicated in other settings, opening new frontiers for research on social networks and IT value.\", 'Introduction to the Special Issue—Social Media and Business Transformation: A Framework for Research Social media are fundamentally changing the way we communicate, collaborate, consume, and create. They represent one of the most transformative impacts of information technology on business, both within and outside firm boundaries. This special issue was designed to stimulate innovative investigations of the relationship between social media and business transformation. In this paper we outline a broad research agenda for understanding the relationships among social media, business, and society. We place the papers comprising the special issue within this research framework and identify areas where further research is needed. We hope that the flexible framework we outline will help guide future research and develop a cumulative research tradition in this area.', \"Simulation modeling for pandemic decision making: A case study with bi-criteria analysis on school closures Pandemic influenza continues to be a national and international public health concern, and has received significant attention worldwide with the A/H1N1 influenza outbreak in 2009. Many countries, including the United States, have developed preparedness plans for an influenza pandemic. Preparedness plans are falling under renewed scrutiny as decision-makers apply new findings and seek key leverage points for more effective preparedness and response. School closure has been recommended by the World Health Organization as one of the best ways to protect children and other susceptible individuals at the early stages of the pandemic. However, school closure is a difficult mitigation policy to implement from both strategic and operational points of view. Challenges include impacts on alternative education delivery services, such as student meals and after-school oversight, as well as direct and indirect economic outfalls. To help public health decision makers address these issues, we developed an epidemiological simulation tool for pandemic influenza which enables users to make decisions during a simulated pandemic. We then designed a school closure tabletop exercise using our simulation model as a decision-support tool for evaluating the effectiveness of school closure as a community mitigation strategy for pandemic influenza. We conducted two exercises in February 2009 for the Arizona Department of Health and Human Services including high-ranking health and education administrators from across the state. The purpose of these exercises was to test the state's pandemic preparedness plans with respect to school closure timing and impact. The exercises required participants to make (hypothetical) strategic and operational decisions to mitigate the impacts of pandemic influenza at the state and local levels. Our simulation and decision analysis tool was used to assess the impact of key decisions in the exercises. This paper presents the technical details involved in the design and evaluation of this pandemic decision-support tool. Based on the decisions made in the exercises, we present a bi-criteria decision analysis framework to evaluate analytical results obtained from the simulation model. Our analyses show that sequential school closure and re-opening strategy with a specific decision rule gives the best compromised solution in terms of minimizing the total number of infections and providing minimal educational discontinuity.\", 'Turbulent Stability of Emergent Roles: The Dualistic Nature of Self-Organizing Knowledge Coproduction Increasingly, new forms of organizing for knowledge production are built around self-organizing coproduction community models with ambiguous role definitions. Current theories struggle to explain how high-quality knowledge is developed in these settings and how participants self-organize in the absence of role definitions, traditional organizational controls, or formal coordination mechanisms. In this article, we engage the puzzle by investigating the temporal dynamics underlying emergent roles on individual and organizational levels. Comprised of a multilevel large-scale empirical study of Wikipedia stretching over a decade, our study investigates emergent roles in terms of prototypical activity patterns that organically emerge from individuals’ knowledge production actions. Employing a stratified sample of 1,000 Wikipedia articles, we tracked 200,000 distinct participants and 700,000 coproduction activities, and recorded each activity’s type. We found that participants’ role-taking behavior is turbulent across roles, with substantial flow in and out of coproduction work. Our findings at the organizational level, however, show that work is organized around a highly stable set of emergent roles, despite the absence of traditional stabilizing mechanisms such as predefined work procedures or role expectations. This dualism in emergent work is conceptualized as “turbulent stability.” We attribute the stabilizing factor to the artifact-centric production process and present evidence to illustrate the mutual adjustment of role taking according to the artifact’s needs and stage. We discuss the importance of the affordances of Wikipedia in enabling such tacit coordination. This study advances our theoretical understanding of the nature of emergent roles and self-organizing knowledge coproduction. We discuss the implications for custodians of online communities as well as for managers of firms engaging in self-organized knowledge collaboration.', \"Corporate Wikis: The Effects of Owners' Motivation and Behavior on Group Members' Engagement Originally designed as a tool to alleviate bottlenecks associated with knowledge management, the suitability of wikis for corporate settings has been questioned given the inherent tensions between wiki affordances and the realities of organizational life. Drawing on regulatory focus theory and social cognitive theory, we developed and tested a model of the motivational dynamics underlying corporate wikis. We examined leaders (owners) and users of 187 wiki-based projects within a large multinational firm. Our findings revealed two countervailing motivational forces, one oriented toward accomplishment and achievement (promotion focus) and one oriented toward safety and security (prevention focus), that not only predicted owners' participation but also the overall level of engagement within the wiki groups. Our primary contribution is in showing that, notwithstanding the potential benefits to users, wikis can trigger risk-avoidance motives that potentially impede engagement. Practically, our findings call for an alignment between organizational procedures surrounding wiki deployment and the technology's affordances.\", 'Heuristic Principles and Differential Judgments in the Assessment of Information Quality Information quality (IQ) is a multidimensional construct and includes dimensions such as accuracy, completeness, objectivity, and representation that are difficult to measure. Recently, research has shown that independent assessors who rated IQ yielded high inter-rater agreement for some information quality dimensions as opposed to others. In this paper, we explore the reasons that underlie the differences in the “measurability” of IQ. Employing Gigerenzer’s “building blocks” framework, we conjecture that the feasibility of using a set of heuristic principles consistently when assessing different dimensions of IQ is a key factor driving inter-rater agreement in IQ judgments. We report on two studies. In the first study, we qualitatively explored the manner in which participants applied the heuristic principles of search rules, stopping rules, and decision rules in assessing the IQ dimensions of accuracy, completeness, objectivity, and representation. In the second study, we investigated the extent to which participants could reach an agreement in rating the quality of Wikipedia articles along these dimensions. Our findings show an alignment between the consistent application of heuristic principles and inter-rater agreement levels found on particular dimensions of IQ judgments. Specifically, on the dimensions of completeness and representation, assessors applied the heuristic principles consistently and tended to agree in their ratings, whereas, on the dimensions of accuracy and objectivity, they not apply the heuristic principles in a uniform manner and inter-rater agreement was relatively low. We discuss our findings implications for research and practice.', 'A Theory-Driven Design Framework for Social Recommender Systems Social recommender systems utilize data regarding users\\' social relationships in filtering relevant information to users. To date, results show that incorporating social relationship data - beyond consumption profile similarity - is beneficial only in a very limited set of cases. The main conjecture of this study is that the inconclusive results are, at least to some extent, due to an under-specification of the nature of the social relations. To date, there exist no clear guidelines for using behavioral theory to guide systems design. Our primary objective is to propose a methodology for theory-driven design. We enhance Walls et al.\\'s (1992) IS Design Theory by introducing the notion of \"applied behavioral theory,\" as a means of better linking theory and system design. Our second objective is to apply our theory-driven design methodology to social recommender systems, with the aim of improving prediction accuracy. A behavioral study found that some social relationships (e.g., competence, benevolence) are most likely to affect a recipient\\'s advice-taking decision. We designed, developed, and tested a recommender system based on these principles, and found that the same types of relationships yield the best recommendation accuracy. This striking correspondence highlights the importance of behavioral theory in guiding system design. We discuss implications for design science and for research on recommender systems.', 'The Evolutionary Trajectories of Peer-Produced Artifacts: Group Composition, the Trajectories’ Exploration, and the Quality of Artifacts Members of an online community peer-produce digital artifacts by negotiating different perspectives and personal knowledge bases. These negotiations are manifested in the temporal evolution of the peer-produced artifact. In this study, we conceptualize the evolution of a digital artifact as a trajectory in a feature space. Our theoretical frame suggests that, through negotiations, contributors’ actions “pull” the trajectory and shape its movement in the feature space. We hypothesize that the type of contributors that work on a focal article influences the extent to which that article’s trajectory explores alternative positions within that space, and that the trajectory’s exploration is, in turn, associated with the artifact’s quality. To test these hypotheses, we analyzed the trajectories of wiki articles drawn from two peer-production communities, Wikipedia and Wikia, tracking the evolution of 242 paired articles for over a decade during which the articles went through 536,745 revisions. We found that the contributors who are the most likely to increase the trajectory’s exploration are those that (1) return to work on the focal artifact and (2) are unregistered members in the broader online community. Further, our results show that the trajectory’s exploration has a curvilinear association with article quality, indicating that exploration contributes positively to quality, but that the effect is reversed when exploration exceeds a certain level. The insights derived from this study highlight the value of an artifact-centric approach to increasing our understanding of the dynamics underlying peer-production.', 'Information Quality in Wikipedia: The Effects of Group Composition and Task Conflict The success of Wikipedia demonstrates that self-organizing production communities can produce high-quality information-based products. Research on Wikipedia has proceeded largely atheoretically, focusing on (1) the diversity in members’ knowledge bases as a determinant of Wikipedia’s content quality, (2) the task-related conflicts that occur during the collaborative authoring process, and (3) the different roles members play in Wikipedia. We develop a theoretical model that explains how these three factors interact to determine the quality of Wikipedia articles. The results from the empirical study of 96 Wikipedia articles suggest that (1) diversity should be encouraged, as the creative abrasion that is generated when cognitively diverse members engage in task-related conflict leads to higher-quality articles, (2) task conflict should be managed, as conflict —notwithstanding its contribution to creative abrasion—can negatively affect group output, and (3) groups should maintain a balance of both administrative- and content-oriented members, as both contribute to the collaborative process.', 'Enhancing Information Retrieval Through Statistical Natural Language Processing: A Study of Collocation Indexing Although the management of information assets--specifically, of text documents that make up 80 percent of these assets--an provide organizations with a competitive advantage, the ability of information retrieval (IR) systems to deliver relevant information to users is severely hampered by the difficulty of disambiguating natural language. The word ambiguity problem is addressed with moderate success in restricted settings, but continues to be the main challenge for general settings, characterized by large, heterogeneous document collections. In this paper, we provide preliminary evidence for the usefulness of statistical natural language processing (NLP) techniques, and specifically of collocation indexing, for IR in general settings. We investigate the effect of three key parameters on collocation indexing performance: directionality, distance, and weighting. We build on previous work in IR to (1) advance our knowledge of key design elements for collocation indexing, (2) demonstrate gains in retrieval precision from the use of statistical NLP for general-settings IR, and, finally, (3) provide practitioners with a useful cost-benefit analysis of the methods under investigation.', 'The importance of participant interaction in online environments An emerging body of research suggests that participant interaction is one of the strongest predictors of success in online environments. However, studies about the effects of participant interaction in a large sample of multiple online environments are rather limited. Using hierarchical modeling techniques, we examine a sample of 40 online MBA courses to determine whether learner–instructor, learner–learner, or learner–system interaction is most significantly related to online course outcomes. Our findings suggest that while collaborative environments were associated with higher levels of learner–learner and learner–system interaction, only learner–instructor and learner–system interaction were significantly associated with increased perceived learning.', 'The Role of Signaling Identity in the Adoption of Personal Technologies We explore symbolic determinants of technology acceptance to complement more functional frameworks and better predict decisions to adopt information appliances. Previous research has investigated such variables as \"need for uniqueness\" and \"status gains\" to capture relevant aspects of technology acceptance. However, the more we move toward personal and ubiquitous technologies, the more we need to broaden and deepen our understanding of the symbolic aspects of adoption. This study reinterprets the symbolic dimension of adoption by broadening its scope to include the self-concept. Results support a prominent role for self-identity in predicting intentions to adopt mobile TVs. Self-identity is shown to complement the effects of \"need for uniqueness\" and \"status gains\" in this regard.', 'An analytic approach to assessing organizational citizenship behavior This study examines the organizational citizenship behavior (OCB) of employees by designing and developing an analytic network process (ANP) methodology. The viability of the proposed methodology is demonstrated via the sales representatives of Beko, a brand name controlled by Koç Group. We first develop a conceptual framework based on qualitative research methods – in-depth interviews and focus group sessions. We employ the principles of ANP methodology to examine and discover the inter-relationships among the OCBs. This process results in a descriptive model that encapsulates the findings from both qualitative and analytics methods. Necessity, altruism, departmental, compliance, and independence are the underlying dimensions of OCBs found to be the most influential/important. The key novelty of this study resides in designing and developing a prescriptive analytics (i.e. ANP) methodology to evaluate the OCBs, which is rare in the area of organizational behavior (a managerial field of study that have been dominated by traditional statistical methods), and thus serves as a useful contribution/augmentation to the business/managerial research methods, and also extends the reach/coverage of analytics-based decision support systems research and practice into a new direction.', 'A cost-oriented approach for the design of IT architectures Multiple combinations of hardware and network components can be selected to design an information technology (IT) infrastructure that satisfies organizational requirements. The professional criterion to deal with these degrees of freedom is cost minimization. However, a scientific approach has been rarely applied to cost minimization and a rigorous verification of professional design guidelines is still lacking. The methodological contribution of this paper is the representation of complex infrastructural design issues as a single cost-minimization problem. The approach to cost-minimization is empirically verified with a database of costs that has also been built as part of this research. The paper shows how an overall cost-minimization approach can provide significant cost reductions and indicates that infrastructural design rules previously identified by the professional literature can lead to sub-optimal solutions.', 'Knowledge entrepreneurship: institutionalising wiki-based knowledge-management processes in competitive and hierarchical organisations Social media in general and wikis in particular offer unique opportunities for knowledge management. Despite widely publicised successes in public settings, wikis in businesses evince mixed results; enterprises struggle to apply wikis to institutionalise knowledge-management practices. We investigate the inherent tensions underlying knowledge-sharing in competitive and hierarchical organisations. Our application of the multi-level organisational learning framework demonstrates that, although wikis facilitate some important learning stages, other critical challenges remain. A unique blend of project leadership can facilitate the institutionalisation of wiki-based knowledge-management processes. To observe the leadership archetype, we use a longitudinal case study of wiki use within a division of NBC Universal. On the basis of our observations, we propose a new archetype of project leadership called Knowledge Entrepreneurship that integrates managerial skills, technology affordances, and critical factors in knowledge-management processes.', nan, 'Computer monitoring: benefits and pitfalls facing management The Information Age has enabled businesses to improve their efficiency through the use of advanced technology. The increase in the use of computers in the workplace has led to the ease of electronic monitoring of employees. Many feel that monitoring is important for the survival of their business. However, some employees regard this action as negatively impacting their work habits and privacy. This article examines the benefits and pitfalls of computer monitoring and recommends specific steps that need to be taken to monitor employees safely and ethically.', 'Key organizational factors in data warehouse architecture selection Even though data warehousing has been in existence for over a decade, companies are still uncertain about a critical decision — which data warehouse architecture to implement? Based on the existing literature, theory, and interviews with experts, a research model was created that identifies the various contextual factors that affect the selection decision. The results from the field survey and multinomial logistic regression suggest that various combinations of organizational factors influence data warehouse architecture selection. The strategic view of the data warehouse prior to implementation emerged as a key determinant. The research suggests an overall model for predicting the data warehouse architecture selection decision.', 'A dynamic simulation approach to support the evaluation of cyber risks and security investments in SMEs The growing amount of cyberspace threats highlights the need to evaluate cybersecurity risks and to plan for effective investments. One internationally recognized document for cybersecurity risk management is the framework for Improving Critical Infrastructure Cybersecurity by the US National Institute of Standards and Technology (NIST). It provides guidelines, best practices and standards for cybersecurity risk management. Nevertheless, as other self-assessment frameworks, it produces a static view of an organization’s cyber posture and does not capture the dynamics of organizational changes and cyberattacks. Moreover, the current situation sees small and medium enterprises (SMEs) in a critical position since they need to manage their cybersecurity while usually not being skilled or equipped enough to internalize this process. Therefore, there is a need for a practical and easily applicable model able to identify a cybersecurity risk profile and its dynamics. This study proposes a system dynamics methodology and tool (SMECRA - SME Cyber Risk Assessment) for supporting cybersecurity investment decisions for SMEs through the evaluation of cyber risk and previous investments. SMECRA addresses dynamic organizational complexity and can be used to assess cyber risks and related dy\\xad namics over time. Three case studies demonstrate its capability to assess a SME’s cybersecurity status and to evaluate investments impacts on an organization’s risk profile, raising cybersecurity awareness. This study is important for SMEs wishing to manage their own cybersecurity risk and for insurance companies in their eco\\xad nomic evaluation of residual risks that SMEs wish to externalize.', \"Information Technology Assimilation in Firms: The Influence of Senior Leadership and IT Infrastructures IT assimilation is regarded as an important outcome in the efforts of firms to leverage the potential of information technologies in their business activities and strategies. Despite significant investments in information technology, considerable diversity exists in how well firms have been able to assimilate IT and leverage the business value of IT. This research draws upon the emerging knowledge-based and resource-based views of the firm to examine the influence of three factors on IT assimilation: (i) quality of senior leadership, (ii) sophistication of IT infrastructures, and (iii) organizational size. Drawing upon a large-scale sample survey where responses were obtained from CIOs and senior business executives who were members of the firms' top management teams, the study examines a variety of mostly normative prescriptions. The findings provide robust evidence about the impacts of CIOs' business and IT knowledge on IT assimilation. Further, we find that CIOs' membership in top management teams and their informal interactions with TMT members enhance their knowledge, particularly their business knowledge. We find that the intensity of the relationship between CIO's interactions with the top management team and their level of IT and business knowledge is much stronger in firms that articulate a transformational IT vision. The sophistication of IT infrastructures was also found to significantly impact IT assimilation. Surprisingly, the IT knowledge of senior business executives was not found to be a significant influence on IT assimilation. The implications of these findings for evolving a deeper understanding of the dynamics underlying IT assimilation are presented.\", 'CD-ROM: an effective use of technology? Abstract. The correct use of information systems implies, in part, the effective delivery and use of information. In situations where, as has frequently been the case in recent years, CD-ROMs form a part of an information system, there is very little that users can do to regulate or control the quality of the information being supplied. Poor information cannot be effectively used. This paper discusses the effective use of the medium and questions whether all publishers are paying sufficient attention to quality.', \"Exhaustion from Information System Career Experience: Implications for Turn-Away Intention While the U.S. economy is recovering slowly, reports tell us that the supply of information systems (IS) professionals is declining and demand is once again on the rise. With organizations challenged in their efforts to hire additional staff, IS professionals are being asked to do even more, often leading to burnout, turnover, and turnaway intentions. Building on Ahuja et al.’s (2007) work on turnover intentions and using the job demands–resources model of burnout as an organizing framework for the antecedents to exhaustion from IS career experience (EISCE), this illustrative research note draws attention to exhaustion in IS professionals that spans an individual’s professional career. Findings indicate that IS professionals’ perceived workload (demand) was associated with higher levels of EISCE, whereas fairness and perceived control of career (resources) were associated with lower levels of EISCE. The influence of EISCE on affective commitment to the IS profession (ACISP) was found to be negative and, ultimately, ACISP fully mediated the effect of EISCE on the intention to turn away from an IS career. The results suggest the importance of studying IS professionals' perceptions regarding the demands and resources associated with working in the IS field when testing exhaustion across IS career experience.\", \"Exhaustion from Information System Career Experience: Implications for Turn-Away Intention While the U.S. economy is recovering slowly, reports tell us that the supply of information systems (IS) professionals is declining and demand is once again on the rise. With organizations challenged in their efforts to hire additional staff, IS professionals are being asked to do even more, often leading to burnout, turnover, and turn-away intentions. Building on Ahuja et al.'s (2007) work on turnover intentions and using the job demands- resources model of burnout as an organizing framework for the antecedents to exhaustion from IS career experience (EISCE), this illustrative research note draws attention to exhaustion in IS professionals that spans an individual's professional career. Findings indicate that IS professionals' perceived workload (demand) was associated with higher levels of EISCE, whereas fairness and perceived control of career (resources) were associated with lower levels of EISCE. The influence of EISCE on affective commitment to the IS profession (ACISP) was found to be negative and, ultimately, ACISP fully mediated the effect of EISCE on the intention to turn away from an IS career. The results suggest the importance of studying IS professionals' perceptions regarding the demands and resources associated with working in the IS field when testing exhaustion across IS career experience.\", 'Understanding Mindshift Learning: The Transition to Object-Oriented Development Information systems professionals increasingly face changes in their work environment. Some of these changes are incremental, but many require fundamental shifts in mindset (referred to as a mindshift). Within the domain of software development, previous research has determined that veteran developers experience difficulty making the transition to new forms of development. Although prior research has brought awareness to the problems caused by a mindshift and has provided some insight, it has not answered the question of why software developers have difficulty making the transition. This study begins to answer that question by positing and examining the mindshift learning theory (MLT). The MLT suggests that the degree of perceived novelty of the fundamental concepts that characterize the new mindset will impact learning. Specifically, concepts may be perceived as novel (i.e., not familiar to the learner), changed (i.e., similar to a known concept, but a different meaning in the new context), or carryover (i.e., known concept with a similar meaning in the new context). As an exemplar mindshift learning situation, this study explores the phenomenon in the context of software developers transitioning from traditional to object-oriented (OO) software development. Findings indicate that software developers had higher knowledge scores on the OO concepts they perceived as novel or carryover compared to those they perceived as changed. Thus, developers experienced detrimental interference from their existing traditional software development knowledge structure when trying to learn OO software development. The findings have implications for organizations and individuals as an understanding  of mindshifts could mean an easier transition through decreased frustration and a more effective learning process.', \"Advancement, voluntary turnover and women in IT: A cognitive study of work–family conflict We used quality of work life theory and the causal mapping method to evoke the concepts and linkages of women's cognitions about work–family conflict in order to better understand the issues contributing to advancement barriers and voluntary turnover of women in IT. The major concepts (Managing Family Responsibilities, Work Stress, Work Schedule Flexibility, and Job Qualities) were found to not only impact each other but also were key factors influencing women's advancement opportunities and voluntary turnover. Organizations may use these insights to mitigate voluntary turnover and increase workforce diversity by addressing female IT professionals’ concerns regarding work–family conflict issues.\", \"The advancement and persistence of women in the information technology profession: An extension of Ahuja's gendered theory of IT career stages In 2002, Manju Ahuja articulated the challenges women face in the information technology (IT) profession with the goal of developing a theoretical model of factors influencing career choice, career advancement, and career persistence for women in the IT profession. While Ahuja's work has been regularly cited in the IT workforce literature (citation count was around 120 using ISI Web of Science and around 425 using Google Scholar as of September 30, 2017), women continue to leave the IT profession at a disturbing rate. Using Ahuja's theoretical model as the foundation, this study asked women working in IT what workplace challenges they face. The findings from this study validate many of Ahuja's propositions and suggest an extended theoretical model that could be used to further explore the challenges women face at various career stages in the IT field. In addition, the extended theoretical model might be used to galvanize the discussion around developing a more inclusive IT work environment.\", 'Firms that choose outsourcing: A profile A nationwide survey of senior Information Systems (IS) managers in U.S. organizations reveals several structural and managerial characteristics of organizations that outsource one or more IS activities. The characteristics include organizational position of the IS manager, CEO involvement in IS (e.g., presence on an IS steering committee and personal use of computers), and IS performance. Outsourcing activities examined are hardware (e.g., network, PC, workstation, minicomputer, and mainframe maintenance and support); software (e.g., contract programming and software support/ training) and comprehensive management activities (e.g., facility management and systems integration). Apparently CEOs who are heavily involved in a steering committee are the least likely to outsource. CEOs that actively use computers are more likely to outsource specific hardware and software activities, whereas CEOs who do not personally use a computer are more likely to outsource comprehensive management activities. In addition, the distance between the CEO and the IS manager is a factor: further distance makes it more likely that IS functions are outsourced. Industry leaders are among the smallest proportion of outsourcing firms, whereas close followers are the largest.', 'On the phenomenology of technology: the “Janus-faces” of mobile phones This paper argues that technologies perform in Janus faced ways; that is, in ways that are ironic, perverse and paradoxical, and it is argued that these qualities are important to apprehend if we are to more fully understand the role of technology in organizations and in our daily lives. The argument opens with an account of Janus as a metaphorical evocation of irony and paradox, and general examples of Janus faced technologies are given. Prominent philosophies of technology and theoretical approaches to technology are discussed in terms of their capacity to account for generalized examples of irony and paradox. Of these, it is argued that the most satisfactory account is provided by (a) Heidegger’s suggestion that our world is enframed by technology, taken together with (b) a logic of sociotechnical systems based in relational and hybrid ontologies. This sketch of the philosophical landscape occupied by Janus is followed by a interpretation of the specific case of mobile phones, which provides concrete and hopefully vivid examples of the Janus faced performance of technology. The conclusion reached is that the Janus faced metaphor and its philosophical context provides the researcher with the analytic advantages of foregrounding uncertainty, avoiding an essentialist or determinist role for technology, and allowing for the possibility of the presence of tension and contradiction in accounts of sociotechnical outcomes.', \"Competing pressures of risk and absorptive capacity potential on commitment and information sharing in global supply chains Organizations’ competitiveness and success are no longer dependent solely on their own performance, but rather are dependent on the competitiveness of the supply chains in which they participate. Increasingly, these supply chains are globally distributed introducing the possibility of greater benefits, as well as greater risk. This study examines the countervailing impact of a global supply chain partner's business-to-business e-commerce business risk and absorptive capacity on an organization's willingness to commit to and share information with that supply chain partner. We survey 207 organizations on their perceptions of specific offshore outsourcing and supply chain partners across dimensions of risk, absorptive capacity, commitment, and information sharing. The results support the theorized relationships indicating that a supply chain partner's increased levels of perceived risk has a strong negative effect on an organization's commitment and information sharing; conjointly, increases in a supply chain partner's absorptive capacity has a strong positive effect on commitment and information sharing. For both risk and absorptive capacity, commitment partially mediates the relationship with information sharing. Testing for systemic effects from geographical/cultural location on the relationship factors provides no evidence of a regional effect on measured items.\", 'The Differential Use and Effect of Knowledge-Based System Explanations in Novice and Expert Judgment Decisions Explanation facilities are considered essential in facilitating user interaction with knowledge-based systems (KBS). Research on explanation provision and the impact on KBS users has shown that the domain expertise affects the type of explanations selected by the user and the basis for seeking such explanations. The prior literature has been limited, however, by the use of simulated KBS that generally provide only feedback explanations (i.e., ex post to the recommendation of the KBS being presented to the user). The purpose of this study is to examine the way users with varying levels of expertise use alternative types of KBS explanations and the impact of that use on decision making. A total of 64 partner/ manager-level and 82 senior/staff-level insolvency professionals participated in an experiment involving the use of a fully functioning KBS to complete a complex judgment task. In addition to feedback explanations, the KBS also provided feedforward explanations (i.e., general explanations during user input about the relationships between information cues in the KBS) and included definition type explanations (i.e., declarative-level knowledge). The results show that users were more likely to adhere to recommendations of the KBS when an explanation facility was available. Choice patterns in using explanations indicated that novices used feedforward explanations more than experts did, while experts were more likely than novices to use feedback explanations. Novices also used more declarative knowledge and initial problem solving type explanations, while experts used more procedural knowledge explanations. Finally, use of feedback explanations led to greater adherence to the KBS recommendation by experts--a condition that was even more prevalent as the use of feedback explanations increased. The results have several implications for the design and use of KBS in a professional decision-making environment.', \"Behavioral economics for decision support systems researchers Theories of decision-making, both prescriptive and descriptive, have long been important to decision support systems (DSS). Currently, the field of behavioral economics (BE) provides the dominant descriptive approach for understanding human decision-making. An indication of the field's standing is that three Nobel Prizes have been awarded to behavioral economics. Contemporary BE has two major theory foundations – the dual process theory of decision-making cognition and a set of judgment heuristics and cognitive biases. These foundations have been combined to create important theories like prospect theory and action strategies like nudging. Previous research has found that DSS has been slow to adopt recent advances in BE, even to the extent that some projects continue to use older theories like the phase model of decision making. This paper aims to make DSS researchers aware of contemporary BE, its nature, and its differences with early BE. We believe that behavioral economics is a useful and productive foundation for DSS research and that the use of BE in DSS should be significantly expanded.\", 'Behavioral economics in information systems research: Critical analysis and research strategies Theories of decision-making have long been important foundations for information systems research and much of the information system is concerned with information processing for decision-making. The discipline of behavioral economics provides the dominant contemporary approach for understanding human decision-making. Therefore, it is logical that information systems research that involves decision-making should consider behavioral economics as a foundation or reference theory. Surprisingly, and despite calls for greater use of behavioral economics in information systems research, it seems that information systems has been slow to adopt contemporary behavioral economics as reference theory. This article reports a critical analysis of behavioral economics in all fields of information systems based on an intensive investigation of quality information systems research using bibliometric content analysis. The analysis shows that information systems researchers have a general understanding of behavioral economics, but their use of the theories has an ad hoc feel where only a narrow range of behavioral economic concepts and theories tend to form the foundation of information systems research. The factors constraining the adoption of behavioral economic theories in information systems are discussed and strategies for the use of this influential foundation theory are proposed. Guidance is provided on how behavioral economics could be used in various aspects of information systems. The article concludes with the view that behavioral economic reference theory has the potential to transform significant areas of information systems research.', 'Executive information systems development in an emerging economy This paper addresses executive information systems (EIS) development in an emerging economy. In particular, it examines EIS development in Thailand, a nation that is more representative of the majority of emerging economies in the South East Asian region than the four Asian Tigers (Singapore, Hong Kong, Taiwan and South Korea). Case studies of the development of four systems in large Thai organizations are presented. The analysis of the cases and their comparison to a benchmark study gives rise to the concept of EIS cultural fit, a concept that adds to our understanding of the reasons for the success and failure of EIS projects in emerging economies. The cases also raise questions about using outsourcing as a development strategy for EIS in emerging economies.', 'Patterns of business intelligence systems use in organizations Business intelligence (BI) is often used as the umbrella term for large-scale decision support systems (DSS) in organizations. BI is currently the largest area of IT investment in organizations and has been rated as the top technology priority by CIOs worldwide for many years. The most important use patterns in decision support are concerned with the type of decision to be supported and the type of manager that makes the decision. The seminal Gorry and Scott Morton MIS/DSS framework remains the most popular framework to describe these use patterns. It is widely believed that DSS theory like this framework can be transferred to BI. This paper investigates BI systems use patterns using the Gorry and Scott Morton framework and contemporary decision-making theory from behavioral economics. The paper presents secondary case study research that analyzes eight BI systems and 86 decisions supported by these systems. Based on the results of the case studies a framework to describe BI use patterns is developed. The framework provides both a theoretical and empirically based foundation for the development of high quality BI theory. It also provides a guide for developing organizational strategy for BI provision. The framework shows that enterprise and smaller functional BI systems exist together in an organization to support different decisions and different decision makers. The framework shows that personal DSS theory cannot be applied to BI systems without specific empirical support.', 'A note on an experimental study of DSS and forecasting exponential growth Many managers need to make forecasts of variables that are growing rapidly. Business variables that increase or decrease exponentially are common in turbulent and complex markets, and misjudging the exponential nature of these variables in an important decision could have major adverse consequences for an organization. This paper reports on an experiment that investigated the use of DSS in an exponential decision task. It found that the use of a simple DSS significantly improved decision performance. This study forms the start of a series of investigations into DSS and complexity.', \"A critical analysis of decision support systems research This paper critically analyses the nature and state of decision support systems (DSS) research. To provide context for the analysis, a history of DSS is presented which focuses on the evolution of a number of sub-groupings of research and practice: personal DSS, group support systems, negotiation support systems, intelligent DSS, knowledge management-based DSS, executive information systems/business intelligence, and data warehousing. To understand the state of DSS research an empirical investigation of published DSS research is presented. This investigation is based on the detailed analysis of 1,020 DSS articles published in 14 major journals from 1990 to 2003. The analysis found that DSS publication has been falling steadily since its peak in 1994 and the current publication rate is at early 1990s levels. Other findings include that personal DSS and group support systems dominate research activity and data warehousing is the least published type of DSS. The journal DSS is the major publishing outlet; US 'Other' journals dominate DSS publishing and there is very low exposure of DSS in European journals. Around two-thirds of DSS research is empirical, a much higher proportion than general IS research. DSS empirical research is overwhelming positivist, and is more dominated by positivism than IS research in general. Design science is a major DSS research category. The decision support focus of the sample shows a well-balanced mix of development, technology, process, and outcome studies. Almost half of DSS papers did not use judgement and decision-making reference research in the design and analysis of their projects and most cited reference works are relatively old. A major omission in DSS scholarship is the poor identification of the clients and users of the various DSS applications that are the focus of investigation. The analysis of the professional or practical contribution of DSS research shows a field that is facing a crisis of relevance. Using the history and empirical study as a foundation, a number of strategies for improving DSS research are suggested.\", 'Eight key issues for the decision support systems discipline This paper integrates a number of strands of a long-term project that is critically analysing the academic field of decision support systems (DSS). The project is based on the content analysis of 1093 DSS articles published in 14 major journals from 1990 to 2004. An examination of the findings of each part of the project yields eight key issues that the DSS field should address for it to continue to play an important part in information systems scholarship. These eight issues are: the relevance of DSS research, DSS research methods and paradigms, the judgement and decision-making theoretical foundations of DSS research, the role of the IT artifact in DSS research, the funding of DSS research, inertia and conservatism of DSS research agendas, DSS exposure in general “A” journals, and discipline coherence. The discussion of each issue is based on the data derived from the article content analysis. A number of suggestions are made for the improvement of DSS research. These relate to case study research, design science, professional relevance, industry funding, theoretical foundations, data warehousing, and business intelligence. The suggestions should help DSS researchers construct high quality research agendas that are relevant and rigorous.', \"Design Science in Decision Support Systems Research: An Assessment using the Hevner, March, Park, and Ram Guidelines Design science has been an important strategy in decision support systems (DSS) research since the field's inception in the early 1970s. Recent reviews of DSS research have indicated a need to improve its quality and relevance. DSS design-science research has an important role in this improvement because design-science research can engage industry and the profession in intellectually important projects. The Hevner, March, Park, and Ram's (HMPR) guidelines for the conduct and assessment of information systems design-science research, published in MIS Quarterly in 2004, provides a vehicle for assessing DSS design-science research. This paper presents research that used bibliometric content analysis to apply the HMPR guidelines to a representative sample of 362 DSS design-science research papers in 14 journals. The analysis highlights major issues in DSS research that need attention: research design, evaluation, relevance, strategic focus, and theorizing.\", 'A critical analysis of decision support systems research revisited: the rise of design science In 2005 the Journal of Information Technology article ‘A critical analysis of decision support systems research’ analyzed 1020 decision support systems (DSS) articles from 1990 to 2003. Since 2003 business intelligence (BI) and business analytics have gained popularity in practice. In theory and research the period since 2003 has seen a change in the decision-making theory orthodoxy and the codification and acceptance of design science. To investigate the changes in the DSS field, a number of expectations were derived from previous literature analyses. These expectations were assessed using bibliometric content analysis. The article sample to 2010 now includes 1466 articles from 16 journals. The analysis of the expectations yields mixed results for the DSS field. On the negative side, there has been an overall decline in DSS publishing, the relevance of DSS research published in journals to IT professionals has declined, and the rigor of DSS research designs has not improved. On the positive side, there has been improvement in relevance to managers, grant funding of DSS research has increased, there has been a positive shift in judgment and decision-making foundations, BI publishing has increased, and group support systems publishing has reduced to a more balanced level. An important result from the analysis of the last 7 years of DSS research is the significant increase in DSS design-science research (DSR) to almost half of published articles. It is clear from the analysis that DSS is undergoing a transition from a field based on statistical hypothesis testing and conceptual studies to one where DSR is the most popular method.', \"Decision support systems evolution: framework, case study and research agenda Evolutionary development has been central to the theory and practice of decision support systems (DSS) since the inception of the field. Terms such as 'adaptive' and 'evolutionary' capture the organic nature of the development of a decision support system. However, the terms are rarely defined and their meaning varies widely in the research literature. The aim of this paper is to contribute to decision support systems theory by investigating and clearly specifying the nature of the evolutionary process of a DSS. Using insights from other disciplines and prior DSS research, a framework for understanding DSS evolution is developed based on the aetiology, lineage, and tempo of evolution. The descriptive validity of the framework is demonstrated by applying it to published DSS studies and to an intensive case study of DSS development. The framework and the case study findings are used to define a research agenda that is important for evolutionary DSS development.\", 'Cognitive biases and decision support systems development: a design science approach This paper presents design science research that aims to improve decision support systems (DSS) development in organizations. Evolutionary development has been central to DSS theory and practice for decades, but a significant problem for DSS analysts remains how to conceptualize the improvement of a decision task during evolutionary DSS development. The objective of a DSS project is to improve the decision process and outcome for a manager making an important decision. The DSS analyst needs to have a clear idea of the nature of the target decision task and a clear strategy of how to support the decision process. Existing psychological research was examined for help with the conceptualization problem, and the theory of cognitive bias is proposed as a candidate for this assistance. A taxonomy of 37 cognitive biases that codifies a complex area of psychological research is developed. The core of the project involves the construction of a design artefact – an evolutionary DSS development methodology that uses cognitive bias theory as a focusing construct, especially in its analysis cycles. The methodology is the major contribution of the project. The feasibility and effectiveness of the development methodology are evaluated in a participatory case study of a strategic DSS project where a managing director is supported in a decision about whether to close a division of a company.', \"New ways of working (NWW): Workplace transformation in the digital age In the introductory paper of this special issue on new ways of working (NWW) the editors first reflect on the meaning of the ‘new’, finding inspiration in Hannes Meyer's essay “The New World” (1926). The ‘new’ is always relative, of course, closely associated with technological innovation, in our case digitalization, and integrates spatiotemporal, technological and socio-cultural di\\xad mensions of life and organizing. This SI seeks to offer a reflection on and contribution to deeper understanding of ongoing flexibilization, virtualization and mediation of work practices. The authors go on to contextualize and discuss the contributions of the papers included in this special issue, focussing on significant technological, spatiotemporal, organizational and individual de\\xad velopments associated with new ways of working. Finally, they reflect on the possible relevance of the recent Covid-19 pandemic for the future of work, arguing that this pandemic accelerated NWW in many ways and – given the many paradoxical NWW dynamics and developments – that there could very well be unexpected and adverse consequences, including a turn away from formal ways of working.\", 'Dynamic collaboration: A personal reflection This paper explores the nature of, and possibilities arising from, dynamic collaboration, where large numbers of people can collaborate on an evolving set of initiatives, without prior knowledge of each other. It references early examples of dynamic collaboration including Topcoder, Innocentive, Zopa, and Wikipedia. It then speculates about the future of dynamic collaboration.', 'Just Right Outsourcing: Understanding and Managing Risk The risks associated with outsourcing have been the principal limitation on the growth of business process outsourcing, especially cross-border outsourcing. In addition to technological improvements in risk management, it is possible to reduce the risk of opportunistic behavior faced by the buyer by redesigning work flows and dividing work among multiple vendors, increasing the range of tasks that are now appropriate candidates for outsourcing. We provide a taxonomy of risks associated with the outsourcing of business processes. We focus on strategic risks and identify the components of this risk and the means by which it can be mitigated.', \"Achieving the Optimal Balance Between Investment in Quality and Investment in Self-Promotion for Information Products When producers of goods (or services) are confronted by a situation in which their offerings no longer perfectly match consumer preferences, they must determine the extent to which the advertised features of the product reflect the product's actual attributes. We find that the two important determinants of sellers' advertising strategy are the Repeg Cost Ratio, and the Repeat Sales Coefficient. The interplay of these two factors gives rise to four possible strategic scenarios. In the ambiguous fourth scenario, we show that sellers' strategy for information production goods will differ considerably from information consumption goods based on product complexity and cost of product return (borne by the buyer). Finally, we demonstrate that markets are often characterized by self-reinforcing limits on the extent of opportunistic advertising by sellers.\", 'The Impact of Automation of Systems on Medical Errors: Evidence from Field Research We use panel data from multiple wards from two hospitals spanning a three-year period to investigate the impact of automation of the core error prevention functions in hospitals on medical error rates. Although there are studies based on anecdotal evidence and self-reported data on how automation impacts medical errors, no systematic studies exist that are based on actual error rates from hospitals. Further, there is no systematic evidence on how incremental automation over time and across multiple wards impacts the rate of medical errors. The primary objective of our study is to fill this gap in the literature by empirically examining how the automation of core error prevention functions affects two types of medical errors. We draw on the medical informatics literature and principal-agency theory and use a unique panel data set of actual documented medical errors from two major hospitals to analyze the interplay between automation and medical errors.We hypothesize that the automation of the sensing function (recording and observing agent actions) will have the greatest impact on reducing error rates. We show that there are significant complementarities between quality management training imparted to hospital staff and the automation of control systems in reducing interpretative medical errors. We also offer insights to practitioners and theoreticians alike on how the automation of error prevention functions can be combined with training in quality management to yield better outcomes. Our results suggest an optimal implementation path for the automation of error prevention functions in hospitals.', \"Disaggregating the Differential Impact of Healthcare IT in Complex Care Delivery:  Insights from Field Research in Chronic Care  This study focuses on the impact of digitizing medical information on the efficiency and perceived quality of chronic care delivery at the individual physician level. This study extends the theory of task technology fit to activity systems consisting of highly interdependent tasks. We find that the outcomes of efficiency and quality gains are driven by the structure of interdependencies between tasks that physicians perform. While structured information plays a key role in enabling both decision-making and task execution, we find that physician-created semistructured information is also an important predictor of both efficiency and quality gains. We show that the structure of activity systems (task interdependencies) has a strong moderating influence on the factors that drive efficiency and quality gains. We find that digitization enables physicians to preprocess patients' records prior to their visit which in turn drives gains in both the efficiency and the perceived quality of care delivered. \", 'Intelligent agents in electronic markets for information goods: customization, preference revelation and pricing Electronic commerce has enabled the use of intelligent agent technologies that can evaluate buyers, customize products, and price in real-time. Our model of an electronic market with customizable products analyzes the pricing, profitability and welfare implications of agent-based technologies that price dynamically based on product preference information revealed by consumers. We find that in making the trade-off between better prices and better customization, consumers invariably choose less-than-ideal products. Furthermore, this trade-off has a higher impact on buyers on the higher end of the market and causes a transfer of consumer surplus towards buyers with a lower willingness to pay. As buyers adjust their product choices in response to better demand agent technologies, seller revenues decrease since the gains from better buyer information are dominated by the lowering of the total value created from the transactions. We study the strategic and welfare implications of these findings, and discuss managerial and technology development guidelines.', nan, 'Proximity and Information Technology Outsourcing: How Local Are IT Services Markets? We examine the question of which services are tradable within a concrete setting: the outsourcing of information technology (IT) services across a broad cross-section of establishments in the United States. If markets for IT services are local, then we should expect increases in local supply would increase the likelihood of outsourcing by lowering the cost of outsourcing. If markets are not local, then local supply will not affect outsourcing demand. We analyze the outsourcing decisions of a large sample of 99,775 establishments in 2002 and 2004, for two types of IT services—programming and design and hosting. Programming and design projects require communication of detailed user requirements whereas hosting requires less coordination between client and service provider than programming and design. Our empirical results bear out this intuition: the probability of outsourcing programming and design is increasing in the local supply of outsourcing, and this sensitivity to local supply conditions has been increasing over time. This suggests there is some nontradable or \"local\" component to programming and design services that cannot be easily removed. In contrast, the decision to outsource hosting is sensitive to local supply only for firms for which network uptime and security concerns are particularly acute.', \"An Empirical Analysis of Software Vendors' Patch Release Behavior: Impact of Vulnerability Disclosure A key aspect of better and more secure software is timely patch release by software vendors for the vulnerabilities in their products. Software vulnerability disclosure, which refers to the publication of vulnerability information, has generated intense debate. An important consideration in this debate is the behavior of software vendors. How quickly do vendors patch vulnerabilities and how does disclosure affect patch release time? This paper compiles a unique data set from the Computer Emergency Response Team/Coordination Center (CERT) and SecurityFocus to answer this question. Our results suggest that disclosure accelerates patch release. The instantaneous probability of releasing the patch rises by nearly two and a half times because of disclosure. Open source vendors release patches more quickly than closed source vendors. Vendors are more responsive to more severe vulnerabilities. We also find that vendors respond more slowly to vulnerabilities not disclosed by CERT. We verify our results by using another publicly available data set and find that results are consistent. We also show how our estimates can aid policy makers in their decision making.\", 'Resource allocation for demand surge mitigation during disaster response Large-scale public health emergencies can result in an overwhelming demand for healthcare resources. Regional aid in the form of central stockpiles and resource redistribution can help mitigate the resulting demand surge. This paper discusses a resource allocation approach for optimizing regional aid during public health emergencies. We find that, optimal response involves delaying the distribution of resources from the central stockpile as much as possible. Also, smaller counties stand to benefit the most from mutual aid. And finally, policy level decisions that alter the objectives of pandemic relief efforts can significantly impact the allocations to affected regions.', 'Static R&D project portfolio selection in public organizations The problem of static research and development (R&D) project portfolio selection arises when a public organization opens a call for proposals and then builds a portfolio choosing which ones to fund in terms of impact measures including social objectives, emphasis areas, geographical influence, and other non-monetary factors. The funds are classified into types of expenses, depending on the goals and preferences of the organization: equipment purchases, travel expenses, scholarships, publication fees, etc. We propose a mathematical model framework in which each project proposal comprised tasks with a specific type of expense and the assigned funding may be a fraction of the requested amount, accounting for possible inter- and intra-proposal dependencies. We present computational experiments that show our models to be efficiently resolvable.', 'Application of KM measures to the impact of a specialized groupware system on corporate productivity and operations Here we describe an experiment to show the improvement in productivity resulting from use of a specialized groupware system, using quantitative data and observations of the knowledge management (KM) processes, styles, and critical success factors. Data describing the work process before and after the deployment of the system was applied to measure the impact on performance, operations, and knowledge sharing behavior. The attitude of the organization toward knowledge sharing and the deployed groupware system was then studied along several dimensions that represent KM styles, by assessing the KM orientation and motivation of the organization. Lessons learned were presented and used for directing the attention of management to the importance of supporting collaborative and KM technologies for corporate strategic competitiveness.', \"An empirical investigation of judgment feedback and computerized decision support in a prediction task This study examines the effects on judgment accuracy of cognitive and outcome feedback provided using a computerized decision support tool. Five feedback conditions were examined in a two-stage experiment utilizing 294 participants: an outcome feedback condition, two cognitive feedback conditions (judgment policy feedback and model predictions feedback), and two joint feedback conditions (judgment policy plus outcome feedback, and model predictions plus outcome feedback). In the first stage, decision makers specified the judgment policies (i.e. cue weights and function forms) that they believed they would use in making their earnings predictions. They were then asked to forecast earnings per share for several companies based on average earnings for the last three years, current year gross margin percentage, quick ratio and eamings yield. Using appropriately modified end-user software, feedback was then provided to all participants, except those receiving outcome feedback only. Judgment policy feedback consisted of informing decision makers of the cue weights and function forms underlying their actual predictions, while model predictions feedback consisted of earnings predictions generated from the decision makers' stated judgment policies. In the second stage, decision makers revised or retained their original judgment policies and then made another set of earnings predictions. Outcome feedback, consisting of information about the actual earnings attained by the companies, was then provided to participants in the outcome feedback and joint feedback conditions. This process was then repeated for a new set of companies to determine how the various forms of feedback influenced judgment accuracy. Results indicated that providing decision makers with either type of cognitive feedback, relative to providing outcome feedback, contributed to improvements in judgment accuracy. There were no significant differences between the judgment accuracy of the cognitive feedback conditions and of the respective joint feedback conditions, indicating that adding outcome feedback did not enhance judgment accuracy. Results also suggested that model predictions feedback may be more effective than judgment policy feedback, which in turn is superior to outcome feedback. All cognitive feedback conditions, relative to outcome feedback only, also demonstrated convergence between stated model predictions and actual predictions. These results are discussed in terms of implications for the design of decision support systems for individual judgment tasks.\", 'An economic analysis of electronic secondary markets: installed base, technology, durability and firm profitability The Internet has spawned a number of partially structured electronic secondary markets, which enable the trading of secondary goods between consumers. Many of these, such as Usenet groups, or WWW sites for niche products, tend to be self-administering; however, there has been significant recent growth in the number of more general web-based markets of this kind. These electronic secondary markets, while facilitating reliable and liquid trade of used goods, could also have an impact on the desirability of new products, as well as products that are complementary/compatible to those traded. We present an economic framework for analyzing how these markets affect the demand for a primary product. We examine when it is optimal for a firm to operate a market of this kind, and when its presence is socially optimal. Surprisingly, we find that in a number of cases, the presence of these markets has a primary positive effect on the profitability of a new good; this leads us to conjecture that there will soon be a number of such trading forums operated by manufacturers of primary goods. We also find that in a majority of cases, it is feasible for a third-party intermediary to profitably operate such a market. Key parameters that affect the desirability of the market are the existing installed customer base, the cost of information technology, the durability of the products in question, their rate of technological obsolescence and the nature of customer preferences.', 'Information systems use as strategy practice: A multi-dimensional view of strategic information system implementation and use Information systems (IS) are strategic in so far as they are used to realize strategic intent. Yet, while much has been said about aligning IS functionality with the strategic intent and how to organizationally implement strategically aligned systems, less is known of how to successfully implement strategic change associated with system use – a truly critical challenge within strategic IS implementation. Drawing on a strategy-as-practice perspective we address this gap by developing a multi-dimensional view of IS strategy, conceptualizing three key challenges in the IS strategy process, to explain how and why a paper mill, despite successfully implementing a strategic production management system, failed to produce intended strategic change. We call this outcome strategy blindness: organizational incapability to realize the strategic intent of implemented, available system capabilities. Using a longitudinal case study we investigate how cognitive rigidity of key actors and fixed, interrelated practices shaped the implementation of the new production system. We also identify core components and dynamics that constitute a richer multi-dimensional view of the IS strategy implementation (alignment) process. In particular, we identify three salient factors that contribute to strategy blindness – mistranslation of intent, flexibility of the IT artifact and cognitive entrenchment – and discuss how they affect strategic implementation processes. We conclude by discussing implications of our findings for IS strategy theory and practice, especially the contribution of strategy-as-practice to this stream of research.', 'Generating innovation potential: How digital entrepreneurs conceal, sequence, anchor, and propagate new technology Organizations that fail to innovate become disrupted by those that do. Digital technology makes corporate entrepreneurship increasingly potent and prolific but simultaneously blurs the link between entrepreneurial processes and innovation outcomes. Our understanding of how corporate entrepreneurship with digital technology unfolds in organizations is thus limited. We develop a framework that captures four tactics that digital entrepreneurs may use to generate innovation potential. Specifically, we report how these tactics helped employees at a Norwegian hospital to develop and scale an application for time planning and resource analytics. Our study shows that managing digital entrepreneurship strategically requires organizations to harness the multiplicity in information systems (IS) use that increasingly malleable digital technology affords.', 'Lessons from enterprise systems competency centers in adopting digital transformation initiatives: An assemblage approach Firms are increasingly adopting digital transformation as a strategic priority. However, the path to successful transformation remains uncertain for many organizations. This paper examines the establishment and evolution of competency centers in two case study organizations, historically used in enterprise systems, in addressing the complexity and challenges of digital transformation. The interactions within these competency centers are analyzed through assemblage theory to understand the emergent relations between heterogeneous parts (technology, people, and orga\\xad nization) and the dynamic processes of new configurations. The insights from this research show the critical role of the competency center in any enterprise system’s success and how it could continue playing a central role in future digital transformation initiatives. By providing a new lens to examine these issues, the assemblage theory provided a new theoretical perspective to the IS field and a new alternative empirical setting to the organizational literature.', \"The ambiguous proposal evaluation problem A complex decision-making challenge senior managers commonly face is the selection of the winning bid from multiple project proposals. Project selection decisions become more complex when providers deliberately choose to introduce ambiguity to their project proposals rather than address the client's predetermined set of desired specifications. In particular, providers suggest in their proposals a range of values for some product specifications. Providers introduce ambiguity for various reasons, such as future technological advances and strategic misrepresentation. Further, such disruptive behaviour is tolerated by clients for reasons such as ambition, lack of knowledge, uncertain needs, complexity and a lack of competition. This paper defines the ambiguous proposal evaluation problem and develops a solution which enables such proposals to be compared and ranked. The solution is developed through the utilisation of fuzzy logic in combination with a multi-criteria decision-making method, and is illustrated on a procurement project. The contribution of this paper is firstly to define a practical problem in the literature and secondly to develop a solution which enables ranking ambiguous options.\", 'Transforming society by transforming technology: the science and politics of participatory design This article attempts to shed historical light on some of the social, political, and ethical issues that have arisen from two disparate perspectives on technology which have both come to integrate an explicit consideration of social factors into systems design. It presents two distinct historical traditions which have contributed to the current field of participatory design methodologies—Joint Application Design (JAD®), and the British “socio-technical systems” and Scandinavian “collective resources” approaches—and which in practice integrated the end-users in different ways consequent upon their differing perspectives on workers, professional relationships to technology, and stated goals. One interest in examining the independent development of methodologies from these two perspectives is that, despite their differences, the approaches ultimately converged on a set of shared concerns and very similar practices. The paper also examines the relation of these traditions to transformations in the theorization of business organization and trends of corporate restructuring which helped to secure a place for variants of related methodologies in major US and multinational corporations. It concludes with an examination of some broader issues in the relationship between technology and society and the prospects for the critical study of technology. I argue that participatory design and its related methodologies are best understood as a model for involving users, designers and the technology itself in a process of technological development. Rather than seeing participatory design as merely the insertion of public dialog within technological design practices, as several observers have done, we should see it as a model for the critical practice of developing technological designs.', 'Constructing continuity across the organisational culture boundary in a highly virtual work environment While remote work allows organisations to offer their employees flexibility and harness global talent and markets for business growth, inability to rely on physical interactions between employees imposes challenges specific to operations in highly virtual work environments. Among these characteristic issues are challenges associated with organisational socialisation and organisational culture. Accordingly, an action design research project was carried out for building a socialisation substitute (an information artefact in the form of a digital organisational culture handbook) to support synthesis of symbolic and pragmatic components of organisational culture at case company Smartly.io, a highly virtual organisation experiencing rapid growth. The paper contributes to the literature on socialisation and organisational culture by demonstrating one approach to designing a surrogate for socialisation that acts as a conduit between the symbolic aspects of organisational culture (such as values) and the pragmatic ones (such as toolkits). The work contributes to organisational discontinuity theory also, via theory-generating descriptive analysis of the process of building continuity across the organisational culture boundary through creation of an information artefact. The resulting artefact was found to deliver practical utility to the case company and encapsulate generalisable design principles for this building process.', 'Sociotechnical Envelopment of Artificial Intelligence:  An Approach to Organizational Deployment  of Inscrutable Artificial Intelligence Systems  The paper presents an approach for implementing inscrutable (i.e., nonexplainable) artificial intelligence (AI) such as neural networks in an accountable and safe manner in organizational settings. Drawing on an exploratory case study and the recently proposed concept of envelopment, it describes a case of an organization successfully \"enveloping\" its AI solutions to balance the performance benefits of flexible AI models with the risks that inscrutable models can entail. The authors present several envelopment methods-establishing clear boundaries within which the AI is to interact with its surroundings, choosing and curating the training data well, and appropriately managing input and output sources-alongside their influence on the choice of AI models within the organization. This work makes two key contributions: It introduces the concept of sociotechnical envelopment by demonstrating the ways in which an organization\\'s successful AI envelopment depends on the interaction of social and technical factors, thus extending the literature\\'s focus beyond mere technical issues. Secondly, the empirical examples illustrate how operationalizing a sociotechnical envelopment enables an organization to manage the trade-off between low explainability and high performance presented by inscrutable models. These contributions pave the way for more responsible, accountable AI implementations in organizations, whereby humans can gain better control of even inscrutable machine-learning models.Hind Benbya was the accepting senior editor. This research article was submitted on February 29, 2020 and underwent three revisions. About the AuthorsAleksandre Asatiani is an assistant professor in information systems at the Department of Applied Information Technology, at the University of Gothenburg. He is also an affiliated researcher with the Swedish Center for Digital Innovation (SCDI). His research focuses on artificial intelligence, robotic process automation, virtual organizations, and IS sourcing. His work has previously appeared in leading IS journals such as Information Systems Journal, Journal of Information Technology, and MIS Quarterly Executive.Pekka Malo is a tenured associate professor of statistics at Aalto University School of Business. His research has been published in leading journals in operations research, information science, and artificial intelligence. Pekka is considered as one of the pioneers in the development of evolutionary optimization algorithms for solving challenging bilevel programming problems. His research interests include business analytics, computational statistics, machine learning, optimization and evolutionary computation, and their applications to marketing, finance, and healthcare.Per Rådberg Nagbøl is a PhD fellow at the IT University of Copenhagen doing a collaborative PhD with the Danish Business Authority within the field of information systems. He uses action design research to design systems and procedures for quality assurance and evaluation of machine learning, focusing on accurate, transparent, and responsible use in the public sector from a risk management perspective.Esko Penttinen is a professor of practice in information systems at Aalto University School of Business in Helsinki.He holds a PhD in information systems science and an MSc in Economics from Helsinki School of Economics. Esko leads the Real-Time Economy Competence Center and is the co-founder and chairman of XBRL Finland. He studies the interplay between humans and machines, organizational implementation of artificial intelligence, and governance issues related to outsourcing and virtual organizing. His main practical expertise lies in the assimilation and economic implications of interorganizational information systems, focusing on application areas such as electronic financial systems, government reporting, and electronic invoicing. Esko\\'s research has appeared in leading IS outlets such as ', 'Information systems for sustainable remote workplaces This review discusses the challenges associated with the sustainability of remote workplaces, which have become more prevalent due to the growing trend of work digitalization and the pandemic-induced push to remote work. These challenges are highlighted in literature across various disciplines, including information systems, but these discourses have remained isolated from each other. In this review, we consolidated and synthesized research on remote work from the perspective of individual workers by reviewing 187 articles published between 1999 and 2020 in recognized academic journals from fields including information systems, organizational studies, economics, human resources, sociology, and psychology. We identified five key themes that concern opportunities and challenges to sustainable remote workplaces: (1) key character\\xad istics, (2) work-life boundaries; (3) health and well-being; (4) social interaction, and (5) lead\\xad ership. Building on our findings we created a framework that recognizes two interrelated categories of factors influencing remote workplace sustainability – rigid base characteristics and contextual remote workplace variables – that together shape the trajectory of remote workplace sustainability in the long term. The framework also identifies the potential role of information systems in modulating the impact of the base characteristics to build continuities that encourage more sustainable remote workplaces. The paper concludes by offering a research agenda for in\\xad formation systems for sustainable remote workplaces based on the three IS theoretical frames: inclusion, dignity, and boundary objects.', 'Uncovering the nature of the relationship between outsourcing motivations and the degree of outsourcing: An empirical study on Finnish small and medium-sized enterprises Prior literature has identified several outsourcing motivations, such as cost reduction and access to expertise, and deciphered the influence of these variables on outsourcing decisions. In another stream of outsourcing studies, researchers have gauged the degree of outsourcing, unearthing how companies may choose to outsource a set or processes instead of the whole business function. In this article, we draw on both of these streams of outsourcing research to study the relationship between outsourcing motivations and the degree of outsourcing within a particular business function. We probe the effect of nine motivation items on outsourcing decision through an empirical study using survey data gathered from 337 small and medium-sized enterprises. We find that cost reduction, a focus on core competence and business/process improvements are all associated with a higher degree of outsourcing, but interestingly, access to expertise is negatively associated with the degree of outsourcing. This finding suggests that companies that outsource mainly to acquire external expertise outsource only a limited number of processes within a specific business function. Our main theoretical contribution lies in uncovering the dynamic nature of outsourcing motivations, meaning that as companies outsource a larger degree of their business processes, some motivation items become more accentuated and others fade in importance.', 'Constructing continuities in virtual work environments: A multiple case study of two firms with differing degrees of virtuality In this paper, we study how continuities are constructed in virtual work environments by comparing two firms with differing degrees of virtuality. Using Organizational Discontinuity Theory and drawing on a qualitative study of two accounting firms operating in Finland, we observe virtual work discontinuities in the two firms and identify constructed continuities. We find that in constructing continuities, virtual organizations need to balance rigid and flexible approaches regarding governance structure, the role of technology, communication management, and workflow management. Our main contributions are an empirical application of Organizational Discontinuity Theory to the comparison of virtual work environments and a set of propositions regarding how firms approach continuity construction in different virtuality contexts.', 'Pricing Models for Online Advertising: CPM vs. CPC Online advertising has transformed the advertising industry with its measurability and accountability. Online software and services supported by online advertising is becoming a reality as evidenced by the success of Google and its initiatives. Therefore, the choice of a pricing model for advertising becomes a critical issue for these firms. We present a formal model of pricing models in online advertising using the principal–agent framework to study the two most popular pricing models: input-based cost per thousand impressions (CPM) and performance-based cost per click-through (CPC). We identify four important factors that affect the preference of CPM to the CPC model, and vice versa. In particular, we highlight the interplay between uncertainty in the decision environment, value of advertising, cost of mistargeting advertisements, and alignment of incentives. These factors shed light on the preferred online-advertising pricing model for publishers and advertisers under different market conditions.', 'Ad-Blockers: A Blessing or a Curse? Users who have an ad-blocker installed present a genuine predicament for a website (also known as the publisher): On the one hand, these users do not generate revenue for the website; on the other hand, denying them access can shrink the user base and adversely affect the popularity of the website, ultimately reducing traffic over the long run. This has led some websites to require that ad-block users “white-list” them for obtaining access to an “ad-light” experience. We model the decision problem for a website facing two user segments: regular users and ad-block users. The first-level decision or gating strategy is whether to allow ad-free access to ad-block users or require them to white-list the website for gaining access. When ad-block users are allowed ad-free access, the second-level decision is the level of advertising (or ad-intensity) for regular users. When ad-block users are required to white-list, the second-level decisions are the ad-intensities for regular users and ad-block users. The net utility of a user from visiting the website depends on the intrinsic value of the website’s content, the value obtained due to network effects driven by the amount of traffic/popularity of the website, and the cost incurred due to the presence of ads. We derive an optimal gating and ad-intensity strategy for the website and also solve an identical model for a world without ad-block software. We show that the website can increase its revenue by discriminating between regular and ad-block users via the ad-intensities shown to them. More interestingly, we find that the discriminatory power bestowed on the website by ad-blockers can increase the social surplus and, in particular, increase the surplus of both user segments, namely, regular users and ad-block users, when the utility from their outside option is below a threshold. Thus, the advent of ad-blockers can lead to a win-win for both the website and its users. Finally, we propose a superior selective-gating strategy in which only a fraction of ad-block users are gated. We establish the robustness of our conclusions under several enhancements to our base setting: (a) heterogeneous profitabilities from regular users and ad-block users, (b) endogenous adoption of ad-blockers, (c) the presence of a subscription option, and (d) negative externality due to increased traffic. Our analysis ends with recommendations for three stakeholders in this problem, namely, publishers, web-browser developers, and policy makers.', 'Assessing the benefits from e-business transformation through effective enterprise management This paper reports on research carried out in 1999–2001 on the use of e-business applications in enterprise resource planning (ERP)-based organisations. Multiple structured interviews were used to collect data on 11 established organisations from a diverse range of industries. The findings are analysed according to the level of sophistication of e-business models and their transformational impact on the organisation. Early adopters of e-business show a trend towards cost reductions and administrative efficiencies from e-procurement and self-service applications used by customers and employees. More mature users focus on strategic advantage and generate this through an evolutionary model of organisational change. Two complex case studies of e-business integration with global suppliers and their corporate customers are analysed to identify specific stages of benefits accrual through the e-business transformation process. Collectively, the set of case studies is used to demonstrate the increased benefits derived from an e-business architecture based on a network of ERP-enabled organisations.', 'Trust and technologies: Implications for organizational work practices In this paper, we empirically investigate the concept of trust across organizational work practices by examining three groups: within the team, between teams and when interacting with technology. This study adopts Repertory Grid methodology as an interview based technique to elicit important constructs of trust to engineering teams working in two organizations within the energy distribution industry. Thirteen key constructs of trust were identified using content analysis. Drawing on the understanding gained, this paper discusses the implications for theories on trust within teams working with technology across organizations and provides a grounded perspective that could be used as a basis for further research.', 'The ethical and social implications of personalization technologies for e-learning Personalization in information systems can be considered beneficial but also ethically and socially harmful. Like many other technologies, the uptake of personalization has been rapid, with inadequate consideration given to its effects. Personalization in e-learning systems also has potential for both harmful and beneficial outcomes, but less is known about its effects. The ethical and social hazards include privacy compromise, lack of control, reduced individual capability, and the commodification of education. Personalization is appearing in many systems already; thus, these hazards may already be occurring. Solutions, more research and community discussion of the issues are needed.']\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a persist direcotry here will be the vector database stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'chroma_db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null values, they cause errors when we are creating vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None value found for abstract in row with article_id=113 and title=Editors' Preface\n",
      "None value found for abstract in row with article_id=124 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=125 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=127 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=128 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=129 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=247 and title=Letting living intelligence put the artificial version in its place\n",
      "None value found for abstract in row with article_id=249 and title=An introduction to qualitative research\n",
      "None value found for abstract in row with article_id=250 and title=Software process improvement: Concepts and practices\n",
      "None value found for abstract in row with article_id=251 and title=Handbook of Action Research Participative Inquiry and Practice\n",
      "None value found for abstract in row with article_id=252 and title=Managing Industrial Knowledge; Creation, Transfer and Utilization\n",
      "None value found for abstract in row with article_id=269 and title=Some systems implications of EU Data Protection Directive\n",
      "None value found for abstract in row with article_id=299 and title=Oracle8i Data Warehousing\n",
      "None value found for abstract in row with article_id=347 and title=An economic view of information systems\n",
      "None value found for abstract in row with article_id=394 and title=A comment on the intellectual structures of information systems development\n",
      "None value found for abstract in row with article_id=419 and title=Managing uncertainty in decision support models foreword to the special issue\n",
      "None value found for abstract in row with article_id=445 and title=Preface\n",
      "None value found for abstract in row with article_id=478 and title=Risk and the future of AI: Algorithmic bias, data colonialism, and marginalization\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.iterrows():\n",
    "    article_id = row['article_id']\n",
    "    title = row['title']\n",
    "    abstract = row['abstract']\n",
    "    \n",
    "    if article_id is None:\n",
    "        print(f\"None value found for article_id in row with title={title} and abstract={abstract}\")\n",
    "    if title is None:\n",
    "        print(f\"None value found for title in row with article_id={article_id} and abstract={abstract}\")\n",
    "    if abstract is None:\n",
    "        print(f\"None value found for abstract in row with article_id={article_id} and title={title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_id'].fillna('Unknown article_id', inplace=True)\n",
    "df['title'].fillna('No title available', inplace=True)\n",
    "df['abstract'].fillna('No abstract available', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(THIS PART I HAVE TO CHECK NOT SURE IF IT IS WORKING PROPERLY)\n",
    "Creating document object. Page content is the concatenated text and we add the metadata for improved similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(page_content=text, metadata={'id': row['article_id'], 'title': row['title'], 'abstract': row['abstract']})\n",
    "    for text, (_, row) in zip(texts, df.iterrows())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 1\n",
      "Title: Examining interdependence between product users and employees in online user communities: The role of employee-generated content\n",
      "Abstract: Firm-sponsored online user communities have become product innovation and support hubs of strategic importance to firms. Product users and host firm employees comprise the participants of firm-sponsored online user communities. The online user community provides a forum wherein the product users and firm employees discuss questions, problems or issues resulting from the use of host firms’ products. Extant research on online user communities has largely focused on either product users or employees and has examined the various dynamics that ensue from each entity’s community participation. This paper seeks to investigate the interdependence between the two entities in the communities and, in particular, how product users’ reading of employee-generated content influences subsequent knowledge contribution by product users as well as employees. Analyzing data from an online user community over a two-year period, our study shows that employees whose content is read by product users generate additional content and product users who read employee content themselves contribute more knowledge to the community. Thus, the reading of content is not entirely a passive, individual action that only affects the reader. On the contrary, reading sparks additional knowledge contribution by the reader and having readers sparks additional knowledge contribution by the original source of the content, thereby creating a sustainable online user community.\n",
      "Content: Examining interdependence between product users and employees in online user communities: The role of employee-generated content Firm-sponsored online user communities have become product innovation and support hubs of strategic importance to firms. Product users and host firm employees comprise the participants of firm-sponsored online user communities. The online user community provides a forum wherein the product users and firm employees discuss questions, problems or issues resulting from the use of host firms’ products. Extant research on online user communities has largely focused on either product users or employees and has examined the various dynamics that ensue from each entity’s community participation. This paper seeks to investigate the interdependence between the two entities in the communities and, in particular, how product users’ reading of employee-generated content influences subsequent knowledge contribution by product users as well as employees. Analyzing data from an online user community over a two-year period, our study shows that employees whose content is read by product users generate additional content and product users who read employee content themselves contribute more knowledge to the community. Thus, the reading of content is not entirely a passive, individual action that only affects the reader. On the contrary, reading sparks additional knowledge contribution by the reader and having readers sparks additional knowledge contribution by the original source of the content, thereby creating a sustainable online user community.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 2\n",
      "Title: Computer support for strategic organizational decision-making\n",
      "Abstract: While information systems continue to be promoted within organizations as tools to support strategic decision-making, there is growing concern over the ability of such systems to model the social and political complexity of the situations to which they are being applied. This paper examines the nature of organizational decision-making and the use of computer-based systems to support this activity. The debate queries the extent to which such artifacts should be allowed to become enmeshed and embedded within the strategic decision-making activities of organizations which operate within increasingly complex environments.\n",
      "Content: Computer support for strategic organizational decision-making While information systems continue to be promoted within organizations as tools to support strategic decision-making, there is growing concern over the ability of such systems to model the social and political complexity of the situations to which they are being applied. This paper examines the nature of organizational decision-making and the use of computer-based systems to support this activity. The debate queries the extent to which such artifacts should be allowed to become enmeshed and embedded within the strategic decision-making activities of organizations which operate within increasingly complex environments.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 3\n",
      "Title: Essence: facilitating software innovation\n",
      "Abstract: This paper suggests ways to facilitate creativity and innovation in software development. The paper applies four perspectives – Product, Project, Process, and People – to identify an outlook for software innovation. The paper then describes a new facility – Software Innovation Research Lab (SIRL) – and a new method concept for software innovation – Essence – based on views, modes, and team roles. Finally, the paper reports from an early experiment using SIRL and Essence and identifies further research.\n",
      "Content: Essence: facilitating software innovation This paper suggests ways to facilitate creativity and innovation in software development. The paper applies four perspectives – Product, Project, Process, and People – to identify an outlook for software innovation. The paper then describes a new facility – Software Innovation Research Lab (SIRL) – and a new method concept for software innovation – Essence – based on views, modes, and team roles. Finally, the paper reports from an early experiment using SIRL and Essence and identifies further research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 4\n",
      "Title: The dark side of data ecosystems: A longitudinal study of the DAMD project\n",
      "Abstract: Data are often vividly depicted as strategic assets that organisations can (re)use to create value for myriad purposes. However, the same qualities that make data so appreciated – that is, their volume, their value for a plurality of stakeholders and their indefinite reuse capacity – also have a dark side: data reuse can lead to deviant data use that undermines the legitimacy of data analytics initiatives. To investigate this dynamic, we build on the notion of data ecosystems and provide empirical evidence from a longitudinal, 15-year case study of the emergence, expansion and eventual collapse of a large-scale data analytics project – called DAMD – in the Danish health care sector. We demonstrate that data reuse in the evolving data ecosystem elicited a data reuse dark side that was so dominant that it eventually resulted in the project’s demise. We conceptualise the reuse dark side’s three major mechanisms as function creep, stakeholder creep and data creep. Based on these insights, we develop an empirically grounded Data Analytics Ecosystem Model that extends the current understanding of data ecosystems and provides a view of these ecosystems as having both a bright and a dark side.\n",
      "Content: The dark side of data ecosystems: A longitudinal study of the DAMD project Data are often vividly depicted as strategic assets that organisations can (re)use to create value for myriad purposes. However, the same qualities that make data so appreciated – that is, their volume, their value for a plurality of stakeholders and their indefinite reuse capacity – also have a dark side: data reuse can lead to deviant data use that undermines the legitimacy of data analytics initiatives. To investigate this dynamic, we build on the notion of data ecosystems and provide empirical evidence from a longitudinal, 15-year case study of the emergence, expansion and eventual collapse of a large-scale data analytics project – called DAMD – in the Danish health care sector. We demonstrate that data reuse in the evolving data ecosystem elicited a data reuse dark side that was so dominant that it eventually resulted in the project’s demise. We conceptualise the reuse dark side’s three major mechanisms as function creep, stakeholder creep and data creep. Based on these insights, we develop an empirically grounded Data Analytics Ecosystem Model that extends the current understanding of data ecosystems and provides a view of these ecosystems as having both a bright and a dark side.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 5\n",
      "Title: Symbolic Action Research in Information Systems: Introduction to the Special Issue1\n",
      "Abstract: An essay is presented as an introduction to the special issue that focuses on symbolic action research in information systems and highlights papers in the issue that offer expanded research of symbolic action research within Information Systems (IS) discourse. It discusses addressing the tendency to view communicative processes in information systems as psychological processes and suggests the need for better design theories.\n",
      "Content: Symbolic Action Research in Information Systems: Introduction to the Special Issue1 An essay is presented as an introduction to the special issue that focuses on symbolic action research in information systems and highlights papers in the issue that offer expanded research of symbolic action research within Information Systems (IS) discourse. It discusses addressing the tendency to view communicative processes in information systems as psychological processes and suggests the need for better design theories.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 6\n",
      "Title: The Making of Data Commodities: Data Analytics as an Embedded Process\n",
      "Abstract: This paper studies the process by which data are generated, managed, and assembled into tradable objects we call data commodities. We link the making of such objects to the open and editable nature of digital data and to the emerging big data industry in which they are diffused items of exchange, repurposing, and aggregation. We empirically investigate the making of data commodities in the context of an innovative telecommunications operator, analyzing its efforts to produce advertising audiences by repurposing data from the network infrastructure. The analysis unpacks the processes by which data are repurposed and aggregated into novel data-based objects that acquire organizational and industry relevance through carefully maintained metrics and practices of data management and interpretation. Building from our findings, we develop a process theory that explains the transformations data undergo on their way to becoming commodities and shows how these transformations are related to organizational practices and to the editable, portable, and recontextualizable attributes of data. The theory complements the standard picture of data encountered in data science and analytics, and renews and extends the promise of a constructivist Information Systems (IS) research into the age of datafication. The results provide practitioners, regulators included, vital insights concerning data management practices that produce commodities from data.\n",
      "Content: The Making of Data Commodities: Data Analytics as an Embedded Process This paper studies the process by which data are generated, managed, and assembled into tradable objects we call data commodities. We link the making of such objects to the open and editable nature of digital data and to the emerging big data industry in which they are diffused items of exchange, repurposing, and aggregation. We empirically investigate the making of data commodities in the context of an innovative telecommunications operator, analyzing its efforts to produce advertising audiences by repurposing data from the network infrastructure. The analysis unpacks the processes by which data are repurposed and aggregated into novel data-based objects that acquire organizational and industry relevance through carefully maintained metrics and practices of data management and interpretation. Building from our findings, we develop a process theory that explains the transformations data undergo on their way to becoming commodities and shows how these transformations are related to organizational practices and to the editable, portable, and recontextualizable attributes of data. The theory complements the standard picture of data encountered in data science and analytics, and renews and extends the promise of a constructivist Information Systems (IS) research into the age of datafication. The results provide practitioners, regulators included, vital insights concerning data management practices that produce commodities from data.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 7\n",
      "Title: Everything counts in large amounts: a critical realist case study on data-based production\n",
      "Abstract: Contemporary digital ecosystems produce vast amounts of data every day. The data are often no more than microscopic log entries generated by the elements of an information infrastructure or system. Although such records may represent a variety of things outside the system, their powers go beyond the capacity to carry semantic content. In this article, we harness critical realism to explain how such data come to matter in specific business operations. We analyse the production of an advertising audience from data tokens extracted from a telecommunications network. The research is based on an intensive case study of a mobile network operator that tries to turn its subscribers into an advertising audience. We identify three mechanisms that shape data-based production and three properties that characterize the underlying pool of data. The findings advance the understanding of many organizational settings that are centred on data processing.\n",
      "Content: Everything counts in large amounts: a critical realist case study on data-based production Contemporary digital ecosystems produce vast amounts of data every day. The data are often no more than microscopic log entries generated by the elements of an information infrastructure or system. Although such records may represent a variety of things outside the system, their powers go beyond the capacity to carry semantic content. In this article, we harness critical realism to explain how such data come to matter in specific business operations. We analyse the production of an advertising audience from data tokens extracted from a telecommunications network. The research is based on an intensive case study of a mobile network operator that tries to turn its subscribers into an advertising audience. We identify three mechanisms that shape data-based production and three properties that characterize the underlying pool of data. The findings advance the understanding of many organizational settings that are centred on data processing.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 8\n",
      "Title: Building nation-wide information infrastructures in healthcare through modular implementation strategies\n",
      "Abstract: Initiatives that seek to realize the vision of nation-wide information infrastructures (II) in healthcare have often failed to achieve their goals. In this paper, we focus on approaches used to plan, conduct, and manage the realization of such visions. Our empirical material describes two Danish initiatives, where a national project failed to deliver interoperable Electronic Patient Record (EPR) systems while a small, local solution grew and now offers a nation-wide solution for sharing patient record information. We apply II theory, specifically the five design principles proposed by Hanseth and Lyytinen, to contrast the organization and implementation strategies of the two projects. Our findings highlight how implementation strategies differ with respect to how stakeholders are mobilized. We argue that the realization of nation-wide IIs for healthcare not only requires a gradual transition of the installed base, which current II theory advocates. Here we articulate and exemplify a modular implementation strategy as an approach that also addresses the challenges related to mobilization and organization of multiple stakeholders.\n",
      "Content: Building nation-wide information infrastructures in healthcare through modular implementation strategies Initiatives that seek to realize the vision of nation-wide information infrastructures (II) in healthcare have often failed to achieve their goals. In this paper, we focus on approaches used to plan, conduct, and manage the realization of such visions. Our empirical material describes two Danish initiatives, where a national project failed to deliver interoperable Electronic Patient Record (EPR) systems while a small, local solution grew and now offers a nation-wide solution for sharing patient record information. We apply II theory, specifically the five design principles proposed by Hanseth and Lyytinen, to contrast the organization and implementation strategies of the two projects. Our findings highlight how implementation strategies differ with respect to how stakeholders are mobilized. We argue that the realization of nation-wide IIs for healthcare not only requires a gradual transition of the installed base, which current II theory advocates. Here we articulate and exemplify a modular implementation strategy as an approach that also addresses the challenges related to mobilization and organization of multiple stakeholders.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 9\n",
      "Title: Collective mindfulness in post-implementation IS adaptation processes\n",
      "Abstract: The organizational consequences of implementing information systems (IS) in organizations have primarily been studied during the implementation or early post-implementation phase. We argue for the need to study the continuous organizational adaptation of evolving IS because of the challenges such processes pose for users, as well as the organizational capabilities they demand. We report from a qualitative study in a hospital setting in which a scanning project was initiated two years after the initial implementation of an Electronic Health Record system. The project was initially conceived to be minor, but led to thorough redesign of work processes and routines. We give a detailed account of the challenges encountered and the actions taken as part of the users' sensemaking in this project. By describing how the making, giving, demanding, specification, and breaking of sense were carried out, we identify the way in which the organizational capability we call “collective mindfulness” was achieved. Being aware of how to practically achieve collective mindfulness, managers may be able to better facilitate mindful handling of post-implementation IS adaptation processes.\n",
      "Content: Collective mindfulness in post-implementation IS adaptation processes The organizational consequences of implementing information systems (IS) in organizations have primarily been studied during the implementation or early post-implementation phase. We argue for the need to study the continuous organizational adaptation of evolving IS because of the challenges such processes pose for users, as well as the organizational capabilities they demand. We report from a qualitative study in a hospital setting in which a scanning project was initiated two years after the initial implementation of an Electronic Health Record system. The project was initially conceived to be minor, but led to thorough redesign of work processes and routines. We give a detailed account of the challenges encountered and the actions taken as part of the users' sensemaking in this project. By describing how the making, giving, demanding, specification, and breaking of sense were carried out, we identify the way in which the organizational capability we call “collective mindfulness” was achieved. Being aware of how to practically achieve collective mindfulness, managers may be able to better facilitate mindful handling of post-implementation IS adaptation processes.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 10\n",
      "Title: Infrastructuring Work: Building a State-Wide Hospital Information Infrastructure in India\n",
      "Abstract: Information and communication technologies that strengthen knowledge-based governance in low and middle-income countries (LMIC) will affect work processes and organizations on a massive scale. This paper draws attention to demands on public sector organizations in resource-constrained contexts that face different challenges than in high-income societies. This paper from the Indian public healthcare sector reports on design, development, implementation, and scaling of a free and open-source software-based hospital information system for district hospitals. The paper focuses on the implications for work, competencies, and organization, building on and extending the concepts of “automate” and “informate.” The paper focuses on the emerging and recursive interplay between information infrastructure and work within the context of organizational realities of a district hospital in an LMIC context, captured by the concepts of “infrastructuring of work” and “work of infrastructuring.”\n",
      "Content: Infrastructuring Work: Building a State-Wide Hospital Information Infrastructure in India Information and communication technologies that strengthen knowledge-based governance in low and middle-income countries (LMIC) will affect work processes and organizations on a massive scale. This paper draws attention to demands on public sector organizations in resource-constrained contexts that face different challenges than in high-income societies. This paper from the Indian public healthcare sector reports on design, development, implementation, and scaling of a free and open-source software-based hospital information system for district hospitals. The paper focuses on the implications for work, competencies, and organization, building on and extending the concepts of “automate” and “informate.” The paper focuses on the emerging and recursive interplay between information infrastructure and work within the context of organizational realities of a district hospital in an LMIC context, captured by the concepts of “infrastructuring of work” and “work of infrastructuring.”\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 11\n",
      "Title: Scanning World Wide Web documents with the vector space model\n",
      "Abstract: The vector space model used in Information Retrieval is combined with discriminant analysis to provide an automated WWW environment scanning system to detect signals of interest to an organization. The vector space model converts text-based information to numerical vectors that are then used in discriminant analysis. We illustrate the methodology using news articles pertaining to a predefined randomly selected set of stocks to test whether they provide predictive signals on whether the stock's return will increase or decrease relative to the market in the target period following the report or whether the stock's trading volume will increase or decrease.\n",
      "Content: Scanning World Wide Web documents with the vector space model The vector space model used in Information Retrieval is combined with discriminant analysis to provide an automated WWW environment scanning system to detect signals of interest to an organization. The vector space model converts text-based information to numerical vectors that are then used in discriminant analysis. We illustrate the methodology using news articles pertaining to a predefined randomly selected set of stocks to test whether they provide predictive signals on whether the stock's return will increase or decrease relative to the market in the target period following the report or whether the stock's trading volume will increase or decrease.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 12\n",
      "Title: A new dynamic integrated framework for surgical patients' prioritization considering risks and uncertainties\n",
      "Abstract: This study reviews current patients' prioritization systems and presents an innovative integrated three-step decisional framework in an attempt to overcome their limitations. In its first step, the proposed framework encompasses fuzzy logic, analytic hierarchy process (AHP) to formalize stakeholders' goals and objectives. In the second step, the assessments made on each patient's condition are integrated by data envelopment analysis (DEA) and compared among them by a min–max regret approach (MRA) to obtain a primary prioritization of patients. The third step uses the delay ratio, the risk criteria score, and a profile matrix to introduce dynamic aspects related to the evolution of patients' condition and changes in the patient's list to the prioritization process. This three-step framework not only considers the surgery team members' opinions but also considers the patient's opinions in the decision-making process. The new framework has been implemented in the Orthopedic Surgery Ward, Shohada University Hospital, Iran, showing very promising results and advantages.\n",
      "Content: A new dynamic integrated framework for surgical patients' prioritization considering risks and uncertainties This study reviews current patients' prioritization systems and presents an innovative integrated three-step decisional framework in an attempt to overcome their limitations. In its first step, the proposed framework encompasses fuzzy logic, analytic hierarchy process (AHP) to formalize stakeholders' goals and objectives. In the second step, the assessments made on each patient's condition are integrated by data envelopment analysis (DEA) and compared among them by a min–max regret approach (MRA) to obtain a primary prioritization of patients. The third step uses the delay ratio, the risk criteria score, and a profile matrix to introduce dynamic aspects related to the evolution of patients' condition and changes in the patient's list to the prioritization process. This three-step framework not only considers the surgery team members' opinions but also considers the patient's opinions in the decision-making process. The new framework has been implemented in the Orthopedic Surgery Ward, Shohada University Hospital, Iran, showing very promising results and advantages.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 13\n",
      "Title: Metafraud: A Meta-Learning Framework for Detecting Financial Fraud\n",
      "Abstract: Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies. In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.\n",
      "Content: Metafraud: A Meta-Learning Framework for Detecting Financial Fraud Financial fraud can have serious ramifications for the long-term sustainability of an organization, as well as adverse effects on its employees and investors, and on the economy as a whole. Several of the largest bankruptcies in U.S. history involved firms that engaged in major fraud. Accordingly, there has been considerable emphasis on the development of automated approaches for detecting financial fraud. However, most methods have yielded performance results that are less than ideal. In consequence, financial fraud detection continues as an important challenge for business intelligence technologies. In light of the need for more robust identification methods, we use a design science approach to develop MetaFraud, a novel meta-learning framework for enhanced financial fraud detection. To evaluate the proposed framework, a series of experiments are conducted on a test bed encompassing thousands of legitimate and fraudulent firms. The results reveal that each component of the framework significantly contributes to its overall effectiveness. Additional experiments demonstrate the effectiveness of the meta-learning framework over state-of-the-art financial fraud detection methods. Moreover, the MetaFraud framework generates confidence scores associated with each prediction that can facilitate unprecedented financial fraud detection performance and serve as a useful decision-making aid. The results have important implications for several stakeholder groups, including compliance officers, investors, audit firms, and regulators.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 14\n",
      "Title: Stylometric Identification in Electronic Markets: Scalability and Robustness\n",
      "Abstract: Online reputation systems are intended to facilitate the propagation of word of mouth as a credibility scoring mechanism for improved trust in electronic marketplaces. However, they experience two problems attributable to anonymity abuse—easy identity changes and reputation manipulation. In this study, we propose the use of stylometric analysis to help identify online traders based on the writing style traces inherent in their posted feedback comments. We incorporated a rich stylistic feature set and developed the Writeprint technique for detection of anonymous trader identities. The technique and extended feature set were evaluated on a test bed encompassing thousands of feedback comments posted by 200 eBay traders. Experiments conducted to assess the scalability (number of traders) and robustness (against intentional obfuscation) of the proposed approach found it to significantly outperform benchmark stylometric techniques. The results indicate that the proposed method may help militate against easy identity changes and reputation manipulation in electronic markets.\n",
      "Content: Stylometric Identification in Electronic Markets: Scalability and Robustness Online reputation systems are intended to facilitate the propagation of word of mouth as a credibility scoring mechanism for improved trust in electronic marketplaces. However, they experience two problems attributable to anonymity abuse—easy identity changes and reputation manipulation. In this study, we propose the use of stylometric analysis to help identify online traders based on the writing style traces inherent in their posted feedback comments. We incorporated a rich stylistic feature set and developed the Writeprint technique for detection of anonymous trader identities. The technique and extended feature set were evaluated on a test bed encompassing thousands of feedback comments posted by 200 eBay traders. Experiments conducted to assess the scalability (number of traders) and robustness (against intentional obfuscation) of the proposed approach found it to significantly outperform benchmark stylometric techniques. The results indicate that the proposed method may help militate against easy identity changes and reputation manipulation in electronic markets.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 15\n",
      "Title: Cybergate: A Design Framework and System for Text Analysis of Computer-Mediated Communication\n",
      "Abstract: Content analysis of computer-mediated communication (CMC) is important for evaluating the effectiveness of electronic communication in various organizational settings. CMC text analysis relies on systems capable of providing suitable navigation and knowledge discovery functionalities. However, existing CMC systems focus on structural features, with little support for features derived from message text. This deficiency is attributable to the informational richness and representational complexities associated with CMC text. In order to address this shortcoming, we propose a design framework for CMC text analysis systems. Grounded in systemic functional linguistic theory, the proposed framework advocates the development of systems capable of representing the rich array of information types inherent in CMC text. It also provides guidelines regarding the choice of features, feature selection, and visualization techniques that CMC text analysis systems should employ. The CyberGate system was developed as an instantiation of the design framework. CyberGate incorporates a rich feature set and complementary feature selection and visualization methods, including the writeprints and ink blots techniques. An application example was used to illustrate the system's ability to discern important patterns in CMC text. Furthermore, results from numerous experiments conducted in comparison with benchmark methods confirmed the viability of CyberGate's features and techniques. The results revealed that the CyberGate system and its underlying design framework can dramatically improve CMC text analysis capabilities over those provided by existing systems.\n",
      "Content: Cybergate: A Design Framework and System for Text Analysis of Computer-Mediated Communication Content analysis of computer-mediated communication (CMC) is important for evaluating the effectiveness of electronic communication in various organizational settings. CMC text analysis relies on systems capable of providing suitable navigation and knowledge discovery functionalities. However, existing CMC systems focus on structural features, with little support for features derived from message text. This deficiency is attributable to the informational richness and representational complexities associated with CMC text. In order to address this shortcoming, we propose a design framework for CMC text analysis systems. Grounded in systemic functional linguistic theory, the proposed framework advocates the development of systems capable of representing the rich array of information types inherent in CMC text. It also provides guidelines regarding the choice of features, feature selection, and visualization techniques that CMC text analysis systems should employ. The CyberGate system was developed as an instantiation of the design framework. CyberGate incorporates a rich feature set and complementary feature selection and visualization methods, including the writeprints and ink blots techniques. An application example was used to illustrate the system's ability to discern important patterns in CMC text. Furthermore, results from numerous experiments conducted in comparison with benchmark methods confirmed the viability of CyberGate's features and techniques. The results revealed that the CyberGate system and its underlying design framework can dramatically improve CMC text analysis capabilities over those provided by existing systems.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 16\n",
      "Title: Data Science for Social Good\n",
      "Abstract:  Data science has been described as the fourth paradigm of scientific discovery. The latest wave of data science research, pertaining to machine learning and artificial intelligence (AI), is growing exponentially and garnering millions of annual citations. However, this growth has been accompanied by a diminishing emphasis on social good challenges-our analysis reveals that the proportion of data science research focusing on social good is less than it has ever been. At the same time, the proliferation of machine learning and generative AI has sparked debates about the sociotechnical prospects and challenges associated with data science for human flourishing, organizations, and society. Against this backdrop, we present a framework for \"data science for social good\" (DSSG) research that considers the interplay between relevant data science research genres, social good challenges, and different levels of sociotechnical abstraction. We perform an analysis of the literature to empirically demonstrate the paucity of work on DSSG in information systems (and other related disciplines) and highlight current impediments. We then use our proposed framework to introduce the articles appearing in the JAIS special issue on data science for social good. We hope that this editorial and the special issue will spur future DSSG research and help reverse the alarming trend across data science research over the past 30-plus years in which social good challenges are attracting proportionately less attention with each passing day. \n",
      "Content: Data Science for Social Good  Data science has been described as the fourth paradigm of scientific discovery. The latest wave of data science research, pertaining to machine learning and artificial intelligence (AI), is growing exponentially and garnering millions of annual citations. However, this growth has been accompanied by a diminishing emphasis on social good challenges-our analysis reveals that the proportion of data science research focusing on social good is less than it has ever been. At the same time, the proliferation of machine learning and generative AI has sparked debates about the sociotechnical prospects and challenges associated with data science for human flourishing, organizations, and society. Against this backdrop, we present a framework for \"data science for social good\" (DSSG) research that considers the interplay between relevant data science research genres, social good challenges, and different levels of sociotechnical abstraction. We perform an analysis of the literature to empirically demonstrate the paucity of work on DSSG in information systems (and other related disciplines) and highlight current impediments. We then use our proposed framework to introduce the articles appearing in the JAIS special issue on data science for social good. We hope that this editorial and the special issue will spur future DSSG research and help reverse the alarming trend across data science research over the past 30-plus years in which social good challenges are attracting proportionately less attention with each passing day. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 17\n",
      "Title: The Phishing Funnel Model: A Design Artifact to Predict User Susceptibility to Phishing Websites\n",
      "Abstract: Phishing is a significant security concern for organizations, threatening employees and members of the public. Phishing threats against employees can lead to severe security incidents, whereas those against the public can undermine trust, satisfaction, and brand equity. At the root of the problem is the inability of Internet users to identify phishing attacks even when using anti-phishing tools. We propose the phishing funnel model (PFM), a design artifact for predicting user susceptibility to phishing websites. PFM incorporates user, threat, and tool-related factors to predict actions during four key stages of the phishing process: visit, browse, consider legitimate, and intention to transact. We used a support vector ordinal regression with a custom kernel encompassing a cumulative-link mixed model for representing users’ decisions across funnel stages. We evaluated the efficacy of PFM in a 12-month longitudinal field experiment in two organizations involving 1,278 employees and 49,373 phishing interactions. PFM significantly outperformed competing models/methods by 8%–52% in area under the curve, correctly predicting visits to high-severity threats 96% of the time—a result 10% higher than the nearest competitor. A follow-up three-month field study revealed that employees using PFM were significantly less likely to interact with phishing threats relative to comparison models and baseline warnings. Furthermore, a cost-benefit analysis showed that interventions guided by PFM resulted in phishing-related cost reductions of nearly $1,900 per employee more than comparison prediction methods. These results indicate strong external validity for PFM. Our findings have important implications for practice by demonstrating (1) the effectiveness of predicting user susceptibility to phishing as a real-time protection strategy, (2) the value of modeling each stage of the phishing process together, rather than focusing on a single user action, and (3) the considerable impact of anti-phishing tool and threat-related factors on susceptibility to phishing.\n",
      "Content: The Phishing Funnel Model: A Design Artifact to Predict User Susceptibility to Phishing Websites Phishing is a significant security concern for organizations, threatening employees and members of the public. Phishing threats against employees can lead to severe security incidents, whereas those against the public can undermine trust, satisfaction, and brand equity. At the root of the problem is the inability of Internet users to identify phishing attacks even when using anti-phishing tools. We propose the phishing funnel model (PFM), a design artifact for predicting user susceptibility to phishing websites. PFM incorporates user, threat, and tool-related factors to predict actions during four key stages of the phishing process: visit, browse, consider legitimate, and intention to transact. We used a support vector ordinal regression with a custom kernel encompassing a cumulative-link mixed model for representing users’ decisions across funnel stages. We evaluated the efficacy of PFM in a 12-month longitudinal field experiment in two organizations involving 1,278 employees and 49,373 phishing interactions. PFM significantly outperformed competing models/methods by 8%–52% in area under the curve, correctly predicting visits to high-severity threats 96% of the time—a result 10% higher than the nearest competitor. A follow-up three-month field study revealed that employees using PFM were significantly less likely to interact with phishing threats relative to comparison models and baseline warnings. Furthermore, a cost-benefit analysis showed that interventions guided by PFM resulted in phishing-related cost reductions of nearly $1,900 per employee more than comparison prediction methods. These results indicate strong external validity for PFM. Our findings have important implications for practice by demonstrating (1) the effectiveness of predicting user susceptibility to phishing as a real-time protection strategy, (2) the value of modeling each stage of the phishing process together, rather than focusing on a single user action, and (3) the considerable impact of anti-phishing tool and threat-related factors on susceptibility to phishing.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 18\n",
      "Title: Don’t Mention It? Analyzing User-Generated Content Signals for Early Adverse Event Warnings\n",
      "Abstract: With greater impetus on broad postmarket surveillance, the Voice of the Customer (VoC) has emerged as an important source of information for understanding consumer experiences and identifying potential issues. In organizations, risk management groups are increasingly interested in working with their information technology teams to develop robust VoC listening platforms. Two key challenges have impeded success. First, prior work has leveraged diverse sets of channels, adverse event types, and modeling methods, resulting in diverging conclusions regarding the viability and efﬁcacy of various user-generated channels and accompanying modeling methods. Second, many existing detection methods rely on “mention models” that have low detection rates, have high false positives, and lack timeliness. Following the information systems design science approach, in this research note we propose a framework for examining key design elements for VoC listening platforms. As part of our framework, we also develop a novel heuristic-based method for detecting adverse events. We evaluate our framework and method on two large test beds each encompassing millions of tweets, forums postings, and search query logs pertaining to hundreds of adverse events related to the pharmaceutical and automotive industries. The results shed light on the interplay between user-generated channels and event types, as well as the potential for more robust event modeling methods that go beyond basic mention models. Our analysis framework reveals that user-generated content channels can facilitate timelier detection of adverse events: on average, two to three years or earlier than commonly used databases. The inclusion of negative sentiment polarity in the models can further reduce false-positive rates. Additionally, we ﬁnd social media channels provide higher detection rates but lower precision than do search-based signals. The search and web forum channels are timelier than Twitter. The proposed heuristic-based method attains markedly better results than do existing methods—with earlier detection rates of 50%–80% and far fewer false positives across an array of VoC channels and event types. The heuristic method is also well suited for signal fusion across channels. Our note makes several contributions to research. The results also have important implications for various practitioner groups, including regulatory agencies and risk management teams at product manufacturing ﬁrms.\n",
      "Content: Don’t Mention It? Analyzing User-Generated Content Signals for Early Adverse Event Warnings With greater impetus on broad postmarket surveillance, the Voice of the Customer (VoC) has emerged as an important source of information for understanding consumer experiences and identifying potential issues. In organizations, risk management groups are increasingly interested in working with their information technology teams to develop robust VoC listening platforms. Two key challenges have impeded success. First, prior work has leveraged diverse sets of channels, adverse event types, and modeling methods, resulting in diverging conclusions regarding the viability and efﬁcacy of various user-generated channels and accompanying modeling methods. Second, many existing detection methods rely on “mention models” that have low detection rates, have high false positives, and lack timeliness. Following the information systems design science approach, in this research note we propose a framework for examining key design elements for VoC listening platforms. As part of our framework, we also develop a novel heuristic-based method for detecting adverse events. We evaluate our framework and method on two large test beds each encompassing millions of tweets, forums postings, and search query logs pertaining to hundreds of adverse events related to the pharmaceutical and automotive industries. The results shed light on the interplay between user-generated channels and event types, as well as the potential for more robust event modeling methods that go beyond basic mention models. Our analysis framework reveals that user-generated content channels can facilitate timelier detection of adverse events: on average, two to three years or earlier than commonly used databases. The inclusion of negative sentiment polarity in the models can further reduce false-positive rates. Additionally, we ﬁnd social media channels provide higher detection rates but lower precision than do search-based signals. The search and web forum channels are timelier than Twitter. The proposed heuristic-based method attains markedly better results than do existing methods—with earlier detection rates of 50%–80% and far fewer false positives across an array of VoC channels and event types. The heuristic method is also well suited for signal fusion across channels. Our note makes several contributions to research. The results also have important implications for various practitioner groups, including regulatory agencies and risk management teams at product manufacturing ﬁrms.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 19\n",
      "Title: Big Data Research in Information Systems: Toward an Inclusive Research Agenda\n",
      "Abstract: Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data's characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data's disruptive effects.\n",
      "Content: Big Data Research in Information Systems: Toward an Inclusive Research Agenda Big data has received considerable attention from the information systems (IS) discipline over the past few years, with several recent commentaries, editorials, and special issue introductions on the topic appearing in leading IS outlets. These papers present varying perspectives on promising big data research topics and highlight some of the challenges that big data poses. In this editorial, we synthesize and contribute further to this discourse. We offer a first step toward an inclusive big data research agenda for IS by focusing on the interplay between big data's characteristics, the information value chain encompassing people-process-technology, and the three dominant IS research traditions (behavioral, design, and economics of IS). We view big data as a disruption to the value chain that has widespread impacts, which include but are not limited to changing the way academics conduct scholarly work. Importantly, we critically discuss the opportunities and challenges for behavioral, design science, and economics of IS research and the emerging implications for theory and methodology arising due to big data's disruptive effects.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 20\n",
      "Title: Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information\n",
      "Abstract: Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools’ warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the “what” but rather on operationalizing the “why” into salient detection cues.\n",
      "Content: Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools’ warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the “what” but rather on operationalizing the “why” into salient detection cues.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 21\n",
      "Title: Detecting Fake Websites: The Contribution of Statistical Learning Theory\n",
      "Abstract: Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for e-commerce and online security.\n",
      "Content: Detecting Fake Websites: The Contribution of Statistical Learning Theory Fake websites have become increasingly pervasive, generating billions of dollars in fraudulent revenue at the expense of unsuspecting Internet users. The design and appearance of these websites makes it difficult for users to manually identify them as fake. Automated detection systems have emerged as a mechanism for combating fake websites, however most are fairly simplistic in terms of their fraud cues and detection methods employed. Consequently, existing systems are susceptible to the myriad of obfuscation tactics used by fraudsters, resulting in highly ineffective fake website detection performance. In light of these deficiencies, we propose the development of a new class of fake website detection systems that are based on statistical learning theory (SLT). Using a design science approach, a prototype system was developed to demonstrate the potential utility of this class of systems. We conducted a series of experiments, comparing the proposed system against several existing fake website detection systems on a test bed encompassing 900 websites. The results indicate that systems grounded in SLT can more accurately detect various categories of fake websites by utilizing richer sets of fraud cues in combination with problem-specific knowledge. Given the hefty cost exacted by fake websites, the results have important implications for e-commerce and online security.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 22\n",
      "Title: Text Analytics to Support Sense-Making in Social Media: A Language-Action Perspective\n",
      "Abstract: Social media and online communities provide organizations with new opportunities to support their businessrelated functions. Despite their various benefits, social media technologies present two important challenges for sense-making. First, online discourse is plagued by incoherent, intertwined conversations that are often difficult to comprehend. Moreover, organizations are increasingly interested in understanding social media participants’ actions and intentions; however, existing text analytics tools mostly focus on the semantic dimension of language. The language-action perspective (LAP) emphasizes pragmatics; not what people say but, rather, what they do with language. Adopting the design science paradigm, we propose a LAP-based text analytics framework to support sense-making in online discourse. The proposed framework is specifically intended to address the two aforementioned challenges associated with sense-making in online discourse: the need for greater coherence and better understanding of actions. We rigorously evaluate a system that is developed based on the framework in a series of experiments using a test bed encompassing social media data from multiple channels and industries. The results demonstrate the utility of each individual component of the system, and its underlying framework, in comparison with existing benchmark methods. Furthermore, the results of a user experiment involving hundreds of practitioners, and a four-month field experiment in a large organization, underscore the enhanced sense-making capabilities afforded by text analytics grounded in LAP principles. The results have important implications for online sense-making and social media analytics.\n",
      "Content: Text Analytics to Support Sense-Making in Social Media: A Language-Action Perspective Social media and online communities provide organizations with new opportunities to support their businessrelated functions. Despite their various benefits, social media technologies present two important challenges for sense-making. First, online discourse is plagued by incoherent, intertwined conversations that are often difficult to comprehend. Moreover, organizations are increasingly interested in understanding social media participants’ actions and intentions; however, existing text analytics tools mostly focus on the semantic dimension of language. The language-action perspective (LAP) emphasizes pragmatics; not what people say but, rather, what they do with language. Adopting the design science paradigm, we propose a LAP-based text analytics framework to support sense-making in online discourse. The proposed framework is specifically intended to address the two aforementioned challenges associated with sense-making in online discourse: the need for greater coherence and better understanding of actions. We rigorously evaluate a system that is developed based on the framework in a series of experiments using a test bed encompassing social media data from multiple channels and industries. The results demonstrate the utility of each individual component of the system, and its underlying framework, in comparison with existing benchmark methods. Furthermore, the results of a user experiment involving hundreds of practitioners, and a four-month field experiment in a large organization, underscore the enhanced sense-making capabilities afforded by text analytics grounded in LAP principles. The results have important implications for online sense-making and social media analytics.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 23\n",
      "Title: Everywhere and nowhere: nearshore software development in the context of globalisation†\n",
      "Abstract: Offshore software development has been identified as one of the most striking manifestations of contemporary globalisation and as evidence of placelessness, the idea that information and communication technologies have rendered location irrelevant. Research in the International Business and Information Systems fields, in contrast, has suggested that all locations are not equal and has identified a number of characteristics that may influence the attractiveness of a location for multinational investment and offshoring, respectively. These literatures, however, focus almost exclusively on quantitative, economic characteristics that are seen as fixed and applying uniformly throughout a whole country. They therefore offer little guidance on the suitability of particular locations as offshoring destinations, especially in countries without a track record in offshore software development. Drawing on two cases of nearshore software development centres set up by offshore service providers in the Caribbean, this paper illustrates that, while the initial decision to establish the ventures reflected a logic of placelessness, characteristics of these particular locations affected their subsequent success. Through the findings, we therefore develop a typology of espoused, unanticipated and remediable locational characteristics, which illustrates that locational attractiveness may vary significantly within countries and that offshore service providers and government agencies can modify locational characteristics to their advantage.\n",
      "Content: Everywhere and nowhere: nearshore software development in the context of globalisation† Offshore software development has been identified as one of the most striking manifestations of contemporary globalisation and as evidence of placelessness, the idea that information and communication technologies have rendered location irrelevant. Research in the International Business and Information Systems fields, in contrast, has suggested that all locations are not equal and has identified a number of characteristics that may influence the attractiveness of a location for multinational investment and offshoring, respectively. These literatures, however, focus almost exclusively on quantitative, economic characteristics that are seen as fixed and applying uniformly throughout a whole country. They therefore offer little guidance on the suitability of particular locations as offshoring destinations, especially in countries without a track record in offshore software development. Drawing on two cases of nearshore software development centres set up by offshore service providers in the Caribbean, this paper illustrates that, while the initial decision to establish the ventures reflected a logic of placelessness, characteristics of these particular locations affected their subsequent success. Through the findings, we therefore develop a typology of espoused, unanticipated and remediable locational characteristics, which illustrates that locational attractiveness may vary significantly within countries and that offshore service providers and government agencies can modify locational characteristics to their advantage.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 24\n",
      "Title: From boundary spanning to creolization: A study of Chinese software and services outsourcing vendors\n",
      "Abstract: In achieving success in global sourcing arrangements, the role of a cultural liaison, boundary spanner or transnational intermediary is frequently highlighted as being critical. This paper critiques, builds upon and synthesizes relevant streams of ideas in relation to boundary-spanning and cross-cultural management across a number of disciplines, and constructs a multi-layered creolization framework, encompassing processes at the individual, intra- and inter-organizational and inter-national levels which, we argue, are entangled and interrelated. Viewed as a vital and innovative phenomenon, creolization embodies the interactive, contentious and creative processes of network expansion, mutual sensemaking, cultural hybridity and identity multiplicity. Qualitative empirical data from the software and services outsourcing industry in Northwest China is used to demonstrate the complexity of cross-cultural practices in offshore collaborations and illustrate creolization processes. Potentials for theoretical development are outlined and implications for cross-cultural practices are discussed.\n",
      "Content: From boundary spanning to creolization: A study of Chinese software and services outsourcing vendors In achieving success in global sourcing arrangements, the role of a cultural liaison, boundary spanner or transnational intermediary is frequently highlighted as being critical. This paper critiques, builds upon and synthesizes relevant streams of ideas in relation to boundary-spanning and cross-cultural management across a number of disciplines, and constructs a multi-layered creolization framework, encompassing processes at the individual, intra- and inter-organizational and inter-national levels which, we argue, are entangled and interrelated. Viewed as a vital and innovative phenomenon, creolization embodies the interactive, contentious and creative processes of network expansion, mutual sensemaking, cultural hybridity and identity multiplicity. Qualitative empirical data from the software and services outsourcing industry in Northwest China is used to demonstrate the complexity of cross-cultural practices in offshore collaborations and illustrate creolization processes. Potentials for theoretical development are outlined and implications for cross-cultural practices are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 25\n",
      "Title: Scad-elastic net and the estimation of individual tourism expenditure determinants\n",
      "Abstract: This paper introduces the use of scad-elastic net in the assessment of the determinants of individual tourist spending. This technique approaches two main estimation-related issues of primary importance. So far studies of tourism literature have made a wide use of classic regressions, whose results might be affected by multicollinearity. In addition, because of the absence of robust economic theory on tourism behavior, regressor selection is often left to researcher's choice when not driven by non-optimal automatic criteria. Scad-elastic net is an OLS model that accounts for both these problems by including two types of parameters constraints, namely the smoothly clipped absolute deviation (scad) and the ℓ2-norm. We analyze an official dataset of incoming tourists to Uruguay. Socio-demographic, psychographic and trip-related variables are used as explanatory of per capita per day tourist expenditure. Significant impact on tourism expenditure of some accommodation facilities such as expensive one and second dwellings, personal experiences rather than the number of past visits, doing certain activities, place of stay, and seasonality are the main conclusions that are drawn from the analysis.\n",
      "Content: Scad-elastic net and the estimation of individual tourism expenditure determinants This paper introduces the use of scad-elastic net in the assessment of the determinants of individual tourist spending. This technique approaches two main estimation-related issues of primary importance. So far studies of tourism literature have made a wide use of classic regressions, whose results might be affected by multicollinearity. In addition, because of the absence of robust economic theory on tourism behavior, regressor selection is often left to researcher's choice when not driven by non-optimal automatic criteria. Scad-elastic net is an OLS model that accounts for both these problems by including two types of parameters constraints, namely the smoothly clipped absolute deviation (scad) and the ℓ2-norm. We analyze an official dataset of incoming tourists to Uruguay. Socio-demographic, psychographic and trip-related variables are used as explanatory of per capita per day tourist expenditure. Significant impact on tourism expenditure of some accommodation facilities such as expensive one and second dwellings, personal experiences rather than the number of past visits, doing certain activities, place of stay, and seasonality are the main conclusions that are drawn from the analysis.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 26\n",
      "Title: Using semiotics to analyze representational complexity in social media\n",
      "Abstract: Data from social media offer us multimedia data brimming with multiple layers of meanings. Social media enable rapid-fire digital communications. These communications are incredibly complex in content, form and meaning. This representational complexity is a stumbling block in data analysis that stands in the way of deeper explanations. These unstructured data, rich in social meanings, are as complex as the phenomena they represent. While it is possible to formulate an entire research methodology around semiotics, it is not always necessary. We can adapt semiotic analysis within existing methodologies. This paper offers and illustrates an analytical technique to address representational complexity that can be used in conjunction with other methodologies such as case study, ethnography, etc. This analytical technique espouses a critical realist philosophy to develop much needed, deeper explanations from qualitative data.\n",
      "Content: Using semiotics to analyze representational complexity in social media Data from social media offer us multimedia data brimming with multiple layers of meanings. Social media enable rapid-fire digital communications. These communications are incredibly complex in content, form and meaning. This representational complexity is a stumbling block in data analysis that stands in the way of deeper explanations. These unstructured data, rich in social meanings, are as complex as the phenomena they represent. While it is possible to formulate an entire research methodology around semiotics, it is not always necessary. We can adapt semiotic analysis within existing methodologies. This paper offers and illustrates an analytical technique to address representational complexity that can be used in conjunction with other methodologies such as case study, ethnography, etc. This analytical technique espouses a critical realist philosophy to develop much needed, deeper explanations from qualitative data.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 27\n",
      "Title: Examining the case of French hesitancy toward IDaaS solutions: Technical and social contextual factors of the organizational IDaaS privacy calculus\n",
      "Abstract: Identity-as-a-service (IDaaS) is a cloud security service to which companies can outsource the identity and access management (IAM) functions that administer their employee’s access to organizational resources. Engaging with the information systems (IS) privacy literature, our qualitative analysis develops a framework for an organiza­ tional privacy calculus that informs French organizational consumers’ decisions to pursue IDaaS solutions. We collect data from employees of a multinational IDaaS provider operating in Europe but headquartered in the US. Our case study reveals the organizational privacy calculus associated with transferring control of a primary security control to a multinational cloud service provider.\n",
      "Content: Examining the case of French hesitancy toward IDaaS solutions: Technical and social contextual factors of the organizational IDaaS privacy calculus Identity-as-a-service (IDaaS) is a cloud security service to which companies can outsource the identity and access management (IAM) functions that administer their employee’s access to organizational resources. Engaging with the information systems (IS) privacy literature, our qualitative analysis develops a framework for an organiza­ tional privacy calculus that informs French organizational consumers’ decisions to pursue IDaaS solutions. We collect data from employees of a multinational IDaaS provider operating in Europe but headquartered in the US. Our case study reveals the organizational privacy calculus associated with transferring control of a primary security control to a multinational cloud service provider.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 28\n",
      "Title: The Impact of Goals on Software Project Management: An Experimental Investigation\n",
      "Abstract:  Over the last three decades, a significant stream of research in organizational behavior has established the importance of goals in regulating human behavior. The precise degree of association between goals and action, however, remains an empirical question since people may, for example, make errors and~or lack the ability to attain their goals. This may be particularly true in dynamically complex task environments, such as the management of software development.To date, goal setting research in the software engineering field has emphasized the development of Robert Zmud was the accepting, senior editor for this paper. tools to identify, structure, and measure software development goals. In contrast, there has been little microempirical analysis of how goals affect managerial decision behavior. The current study attempts to address this research problem. It investigated the impact of different project goals on software project planning and resource allocation decisions and, in turn, on project performance. The research question was explored through a role-playing project simulation game in which subjects played the role of software project managers. Two multigoal structures were tested, one for cost~schedule and the other quality/schedule. The cost~schedule group opted for smaller cost adjustments and was more willing to extend the project completion time. The quality/schedule group, on the other hand, acquired a larger staff level in the later stages of the project and allocated a higher percentage of the larger staff level to quality assurance. A cost/schedule goal led to lower cost, while a quality/schedule goal led to higher quality. These findings suggest that given specific software project goals, managers do make planning and resource allocation choices in such a way that will meet those goals. The implications of the results for project management practice and research are discussed. Introduction and Motivation mManagers widely rely on goal setting as a motivational technique to regulate task performance, \n",
      "Content: The Impact of Goals on Software Project Management: An Experimental Investigation  Over the last three decades, a significant stream of research in organizational behavior has established the importance of goals in regulating human behavior. The precise degree of association between goals and action, however, remains an empirical question since people may, for example, make errors and~or lack the ability to attain their goals. This may be particularly true in dynamically complex task environments, such as the management of software development.To date, goal setting research in the software engineering field has emphasized the development of Robert Zmud was the accepting, senior editor for this paper. tools to identify, structure, and measure software development goals. In contrast, there has been little microempirical analysis of how goals affect managerial decision behavior. The current study attempts to address this research problem. It investigated the impact of different project goals on software project planning and resource allocation decisions and, in turn, on project performance. The research question was explored through a role-playing project simulation game in which subjects played the role of software project managers. Two multigoal structures were tested, one for cost~schedule and the other quality/schedule. The cost~schedule group opted for smaller cost adjustments and was more willing to extend the project completion time. The quality/schedule group, on the other hand, acquired a larger staff level in the later stages of the project and allocated a higher percentage of the larger staff level to quality assurance. A cost/schedule goal led to lower cost, while a quality/schedule goal led to higher quality. These findings suggest that given specific software project goals, managers do make planning and resource allocation choices in such a way that will meet those goals. The implications of the results for project management practice and research are discussed. Introduction and Motivation mManagers widely rely on goal setting as a motivational technique to regulate task performance, \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 29\n",
      "Title: The Economics of Software Quality Assurance: A Simulation-Based Case Study\n",
      "Abstract:  Software quafity assurance (QA) is a critical function in the successful development and maintenance of software systems. Because the QA activity adds significantly to the cost of developing software, the cost-effectiveness of QA has been a pressing concern to software quality managers. As of yet, though, this concern has not been adequately addressed in the fiterature.The objective of this article is to investigate the tradeoffs between the economic benefits and costs of QA. A comprehensive system dynamics model of the software development process was developed that serves as an experimentation vehicle for QA poficy. One such experiment, involving a NASA software project, is discussed in detail. In this experiment, the level of QA expenditure was found to have a significant impact on the project's total cost. The model was also used to identify, the optimal QA expenditure level and its distribution throughout the project's lifecycle. \n",
      "Content: The Economics of Software Quality Assurance: A Simulation-Based Case Study  Software quafity assurance (QA) is a critical function in the successful development and maintenance of software systems. Because the QA activity adds significantly to the cost of developing software, the cost-effectiveness of QA has been a pressing concern to software quality managers. As of yet, though, this concern has not been adequately addressed in the fiterature.The objective of this article is to investigate the tradeoffs between the economic benefits and costs of QA. A comprehensive system dynamics model of the software development process was developed that serves as an experimentation vehicle for QA poficy. One such experiment, involving a NASA software project, is discussed in detail. In this experiment, the level of QA expenditure was found to have a significant impact on the project's total cost. The model was also used to identify, the optimal QA expenditure level and its distribution throughout the project's lifecycle. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 30\n",
      "Title: How AI-Based Systems Can Induce Reflections: The Case of AI-Augmented Diagnostic Work\n",
      "Abstract:  This paper addresses a thus-far neglected dimension in human-artificial intelligence (AI) augmentation: machine-induced reflections. By establishing a grounded theoretical-informed model of machine-induced reflection, we contribute to the ongoing discussion in information systems (IS) regarding AI and research on reflection theories. In our multistage study, physicians used a machine learning-based (ML) clinical decision support system (CDSS) to see if and how this interaction can stimulate reflective practice in the context of an X-ray diagnosis task. By analyzing verbal protocols, performance metrics, and survey data, we developed an integrative theoretical foundation to explain how ML-based systems can help stimulate reflective practice. Individuals engage in more critical or shallower modes depending on whether they perceive a conflict or agreement with these CDSS systems, which in turn leads to different levels of reflection depth. By uncovering the process of machine-induced reflections, we offer IS research a different perspective on how such AI-based systems can help individuals become more reflective, and consequently more effective, professionals. This perspective stands in stark contrast to the traditional, efficiency-focused view of MLbased decision support systems and also enriches theories on human-AI augmentation. \n",
      "Content: How AI-Based Systems Can Induce Reflections: The Case of AI-Augmented Diagnostic Work  This paper addresses a thus-far neglected dimension in human-artificial intelligence (AI) augmentation: machine-induced reflections. By establishing a grounded theoretical-informed model of machine-induced reflection, we contribute to the ongoing discussion in information systems (IS) regarding AI and research on reflection theories. In our multistage study, physicians used a machine learning-based (ML) clinical decision support system (CDSS) to see if and how this interaction can stimulate reflective practice in the context of an X-ray diagnosis task. By analyzing verbal protocols, performance metrics, and survey data, we developed an integrative theoretical foundation to explain how ML-based systems can help stimulate reflective practice. Individuals engage in more critical or shallower modes depending on whether they perceive a conflict or agreement with these CDSS systems, which in turn leads to different levels of reflection depth. By uncovering the process of machine-induced reflections, we offer IS research a different perspective on how such AI-based systems can help individuals become more reflective, and consequently more effective, professionals. This perspective stands in stark contrast to the traditional, efficiency-focused view of MLbased decision support systems and also enriches theories on human-AI augmentation. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 31\n",
      "Title: The Impact of Computer Alienation on Information Technology Investment Decisions: An Exploratory Cross-National Analysis\n",
      "Abstract:  Organizations in both developed and developing countries use information technology to support their operational, tactical, and strategic  processes (cf., Bogod, 1979;Cooper and Zmud, 1990). Any strategic competitive advantage of information technology, however, is contingent on acquisition and assimilation of information technology products and applications into organizational processes. Using a value expectancy approach, this study proposes an expanded model to examine the variables that correlate with information technology investment decisions. The theory of alienation from social psychology is used as a basis to systematically define and measure decision makers' attitudes and internal beliefs toward information technology in an investment context. Detailed discussion of the development of a computer alienation measurement scale is presented. The scale was used to collect data from 97 decision makers in the United States, a developed country, and Saudi Arabia, a developing country. Results provide empirical evidence on the appropriateness of applying the computer alienation construct to computer purchase decisions. Computer-alienated decision makers were found to be more inclined to resist information technology adoption by refraining from buying computers. This resistance was evident in both the U.S. and the Saudi samples. The study findings also indicate that decision-maker computer knowledge, computer experience, and education level are closely associated with alienated beliefs and attitudes toward information technology. Alienated decision makers reported paying less attention to information technology information sources. Assuming technologies can provide advantages, these findings point to the need for change agents to minimize afienating befiefs and attitudes. \n",
      "Content: The Impact of Computer Alienation on Information Technology Investment Decisions: An Exploratory Cross-National Analysis  Organizations in both developed and developing countries use information technology to support their operational, tactical, and strategic  processes (cf., Bogod, 1979;Cooper and Zmud, 1990). Any strategic competitive advantage of information technology, however, is contingent on acquisition and assimilation of information technology products and applications into organizational processes. Using a value expectancy approach, this study proposes an expanded model to examine the variables that correlate with information technology investment decisions. The theory of alienation from social psychology is used as a basis to systematically define and measure decision makers' attitudes and internal beliefs toward information technology in an investment context. Detailed discussion of the development of a computer alienation measurement scale is presented. The scale was used to collect data from 97 decision makers in the United States, a developed country, and Saudi Arabia, a developing country. Results provide empirical evidence on the appropriateness of applying the computer alienation construct to computer purchase decisions. Computer-alienated decision makers were found to be more inclined to resist information technology adoption by refraining from buying computers. This resistance was evident in both the U.S. and the Saudi samples. The study findings also indicate that decision-maker computer knowledge, computer experience, and education level are closely associated with alienated beliefs and attitudes toward information technology. Alienated decision makers reported paying less attention to information technology information sources. Assuming technologies can provide advantages, these findings point to the need for change agents to minimize afienating befiefs and attitudes. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 32\n",
      "Title: Policies for a single-vendor multi-buyer system with finite production rate\n",
      "Abstract: We deal with a multi-echelon inventory system in which one vendor supplies an item to multiple buyers. The vendor produces the item at a finite rate and customer demand occurs at each buyer at a constant rate. There is a holding cost per unit stored per unit time at the vendor and at each buyer. Each time a production is carried out the vendor incurs a setup cost. Moreover, placing an order at a buyer entails a fixed ordering cost. Shortages are not allowed. The goal is to determine the order quantities at the buyers and the production and shipment schedule at the vendor in order to minimize the average total cost per unit time. We formulate the problem in terms of integer-ratio policies and we develop a heuristic procedure. We also show how the problem should be addressed in case of independence among the vendor and the buyers. Both solution procedures are illustrated with a numerical example. Finally, we present the results of a numerical study which illustrates the performance of the heuristic for computing integer-ratio policies. Additionally, we compare the integer-ratio policies with the decentralized policies, and a sensitivity analysis of parameters is also reported.\n",
      "Content: Policies for a single-vendor multi-buyer system with finite production rate We deal with a multi-echelon inventory system in which one vendor supplies an item to multiple buyers. The vendor produces the item at a finite rate and customer demand occurs at each buyer at a constant rate. There is a holding cost per unit stored per unit time at the vendor and at each buyer. Each time a production is carried out the vendor incurs a setup cost. Moreover, placing an order at a buyer entails a fixed ordering cost. Shortages are not allowed. The goal is to determine the order quantities at the buyers and the production and shipment schedule at the vendor in order to minimize the average total cost per unit time. We formulate the problem in terms of integer-ratio policies and we develop a heuristic procedure. We also show how the problem should be addressed in case of independence among the vendor and the buyers. Both solution procedures are illustrated with a numerical example. Finally, we present the results of a numerical study which illustrates the performance of the heuristic for computing integer-ratio policies. Additionally, we compare the integer-ratio policies with the decentralized policies, and a sensitivity analysis of parameters is also reported.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 33\n",
      "Title: Inventing Together: The Role of Actor Goals and Platform Affordances in Open Innovation\n",
      "Abstract:  With the ubiquity of the internet and social media platforms, open innovation (OI) opportunities now extend to individuals with creative ideas and interests in innovation. Understanding why individuals are willing to engage in open innovation and how their diverse goals affect their participation is important for assessing the viability of various OI models and to inform platform design. In this paper, we develop a theoretical model that examines the impact of three categories of human goalsextrinsic, intrinsic, and internalized extrinsic--on actors' continuous intentions to participate in three general categories of open innovation behaviors: ideation, collaboration, and socialization. The model also considers how perceived platform participation affordances mediate the influence of goals on these innovation behaviors. We validate this goals-affordances-behavior model via a field survey of participants on a social product development (SPD) platform. By theorizing and empirically examining how goals influence participation in the SPD context, our study advances knowledge about open innovation behaviors, provides a foundation for future research across various OI models, and highlights practical insights for OI platform design.Likoebe Maruping was the accepting senior editor. This research article was submitted on March 19, 2018 and underwent three revisions.Marisa has a million ideas but only a few minutes to spare. She had an idea for a brand-new product her kids would love, so-naturally-she shared it on the Quirky invention platform. Talented renderers, sketchers, and toy enthusiasts in our community helped strengthen her idea submission. In turn, she shared some of her Influence (i.e., a cut of the product revenue) with the people that helped out the most.-Quirky.com \n",
      "Content: Inventing Together: The Role of Actor Goals and Platform Affordances in Open Innovation  With the ubiquity of the internet and social media platforms, open innovation (OI) opportunities now extend to individuals with creative ideas and interests in innovation. Understanding why individuals are willing to engage in open innovation and how their diverse goals affect their participation is important for assessing the viability of various OI models and to inform platform design. In this paper, we develop a theoretical model that examines the impact of three categories of human goalsextrinsic, intrinsic, and internalized extrinsic--on actors' continuous intentions to participate in three general categories of open innovation behaviors: ideation, collaboration, and socialization. The model also considers how perceived platform participation affordances mediate the influence of goals on these innovation behaviors. We validate this goals-affordances-behavior model via a field survey of participants on a social product development (SPD) platform. By theorizing and empirically examining how goals influence participation in the SPD context, our study advances knowledge about open innovation behaviors, provides a foundation for future research across various OI models, and highlights practical insights for OI platform design.Likoebe Maruping was the accepting senior editor. This research article was submitted on March 19, 2018 and underwent three revisions.Marisa has a million ideas but only a few minutes to spare. She had an idea for a brand-new product her kids would love, so-naturally-she shared it on the Quirky invention platform. Talented renderers, sketchers, and toy enthusiasts in our community helped strengthen her idea submission. In turn, she shared some of her Influence (i.e., a cut of the product revenue) with the people that helped out the most.-Quirky.com \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 34\n",
      "Title: Business Models in the Sharing Economy: Manufacturing Durable Goods in the Presence of Peer-to-Peer Rental Markets\n",
      "Abstract: Business models that provide access to assets rather than transfer ownership of goods have become an important industry trend, representing a challenge for incumbent firms. This paper analyzes the interaction of a peer-to-peer (P2P) rental market and a manufacturer of durable goods, and highlights the important role of consumer heterogeneity in usage rates in determining which business model would be preferred by the manufacturer. The introduction of a P2P rental market creates an equalizing effect, which leads to purchases from low-usage consumers. P2P rentals act as a discrimination device, allowing the manufacturer to segment consumers and extract a larger fraction of surplus, which might hurt consumers. The manufacturer is better off with P2P rentals when the heterogeneity in usage rates is intermediate, whereas the consumers are better off with P2P rentals when the heterogeneity is sufficiently high. This paper examines different business models such as the manufacturer with only sales, with rentals in addition to sales (the “dual” firm), with its own P2P rentals platform alongside sales (the “P2P-sponsoring” firm), and with a mixed structure in which the manufacturer competes against P2P rentals by introducing its own direct rentals (the “dual-plus-P2P” firm). Consumer heterogeneity in usage rates plays a fundamental role in business model outcomes. When usage rates and heterogeneity in usage rates are sufficiently large, the manufacturer is better off offering sales and facilitating a P2P rental market. In contrast, if heterogeneity in usage rates is too low, the manufacturer prefers to offer only sales. If heterogeneity is too high but usage rates are below a threshold, the manufacturer prefers to operate as a dual firm that offers both sales and rentals directly to consumers. If P2P rentals are unavoidable, introducing its own rentals to compete against P2P rentals might not be the best strategy for the manufacturer under certain conditions. Overall, contrary to what could be expected, the manufacturer has an incentive to facilitate P2P rentals in a large variety of cases.\n",
      "Content: Business Models in the Sharing Economy: Manufacturing Durable Goods in the Presence of Peer-to-Peer Rental Markets Business models that provide access to assets rather than transfer ownership of goods have become an important industry trend, representing a challenge for incumbent firms. This paper analyzes the interaction of a peer-to-peer (P2P) rental market and a manufacturer of durable goods, and highlights the important role of consumer heterogeneity in usage rates in determining which business model would be preferred by the manufacturer. The introduction of a P2P rental market creates an equalizing effect, which leads to purchases from low-usage consumers. P2P rentals act as a discrimination device, allowing the manufacturer to segment consumers and extract a larger fraction of surplus, which might hurt consumers. The manufacturer is better off with P2P rentals when the heterogeneity in usage rates is intermediate, whereas the consumers are better off with P2P rentals when the heterogeneity is sufficiently high. This paper examines different business models such as the manufacturer with only sales, with rentals in addition to sales (the “dual” firm), with its own P2P rentals platform alongside sales (the “P2P-sponsoring” firm), and with a mixed structure in which the manufacturer competes against P2P rentals by introducing its own direct rentals (the “dual-plus-P2P” firm). Consumer heterogeneity in usage rates plays a fundamental role in business model outcomes. When usage rates and heterogeneity in usage rates are sufficiently large, the manufacturer is better off offering sales and facilitating a P2P rental market. In contrast, if heterogeneity in usage rates is too low, the manufacturer prefers to offer only sales. If heterogeneity is too high but usage rates are below a threshold, the manufacturer prefers to operate as a dual firm that offers both sales and rentals directly to consumers. If P2P rentals are unavoidable, introducing its own rentals to compete against P2P rentals might not be the best strategy for the manufacturer under certain conditions. Overall, contrary to what could be expected, the manufacturer has an incentive to facilitate P2P rentals in a large variety of cases.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 35\n",
      "Title: Effective use of information technologies by seniors: the case of wearable device use\n",
      "Abstract: Healthcare is an area that has benefitted from the developments in wearable device technology. Seniors, who usually suffer from multiple comorbidities, are among the target users of these devices, and research has shown potential health benefits for seniors when they use these devices effectively. However, the adoption rate of wearable devices is low, especially among seniors, preventing the full utilisation of their data in healthcare. In this study, we interviewed forty-four seniors across North America and collected data from their wearable devices to develop a theoretical affordance network-based model to explain seniors’ effective use of wearable devices. Our model indicates that despite the apparent simplicity of wearable devices, they have multiple affordances that help seniors achieve several goals, including activity monitoring, activity planning, and activity improvement. Furthermore, we identified factors that enable seniors to actualise the affordances of wearable devices and achieve their goals. The results of this study suggest a strong relationship between seniors’ mental and physical capabilities and their willingness to use and benefit from wearable devices. We join other researchers in their call for a contextual study on consumer technology use.\n",
      "Content: Effective use of information technologies by seniors: the case of wearable device use Healthcare is an area that has benefitted from the developments in wearable device technology. Seniors, who usually suffer from multiple comorbidities, are among the target users of these devices, and research has shown potential health benefits for seniors when they use these devices effectively. However, the adoption rate of wearable devices is low, especially among seniors, preventing the full utilisation of their data in healthcare. In this study, we interviewed forty-four seniors across North America and collected data from their wearable devices to develop a theoretical affordance network-based model to explain seniors’ effective use of wearable devices. Our model indicates that despite the apparent simplicity of wearable devices, they have multiple affordances that help seniors achieve several goals, including activity monitoring, activity planning, and activity improvement. Furthermore, we identified factors that enable seniors to actualise the affordances of wearable devices and achieve their goals. The results of this study suggest a strong relationship between seniors’ mental and physical capabilities and their willingness to use and benefit from wearable devices. We join other researchers in their call for a contextual study on consumer technology use.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 36\n",
      "Title: Enriching our theoretical repertoire: the role of evolutionary psychology in technology acceptance\n",
      "Abstract: Information systems (IS) research has drawn heavily on social and cognitive psychology to explain technology adoption. Indeed, the many variations of the technology acceptance model all share these same theoretical foundations. Focusing exclusively on the socio-cognitive lens can lead to overlooking enhanced explanations of technology acceptance, such that new theoretical perspectives may be warranted. In this qualitative grounded theory study, we discovered how the lens of evolutionary psychology, as embodied in the Four-Drive model, was helpful in understanding technology acceptance across three organizational sites. We contend that evolutionary psychology is an important addition to the theoretical repertoire of IS researchers, and propose including ‘evolved psychological mechanisms’ within traditional models of technology acceptance.\n",
      "Content: Enriching our theoretical repertoire: the role of evolutionary psychology in technology acceptance Information systems (IS) research has drawn heavily on social and cognitive psychology to explain technology adoption. Indeed, the many variations of the technology acceptance model all share these same theoretical foundations. Focusing exclusively on the socio-cognitive lens can lead to overlooking enhanced explanations of technology acceptance, such that new theoretical perspectives may be warranted. In this qualitative grounded theory study, we discovered how the lens of evolutionary psychology, as embodied in the Four-Drive model, was helpful in understanding technology acceptance across three organizational sites. We contend that evolutionary psychology is an important addition to the theoretical repertoire of IS researchers, and propose including ‘evolved psychological mechanisms’ within traditional models of technology acceptance.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 37\n",
      "Title: From cacophony to harmony: A case study about the IS implementation process as an opportunity for organizational transformation at Sentara Healthcare\n",
      "Abstract: The cacophony of criticisms emanating from an organization facing an information technology-enabled transformation can be deafening and deleterious. This is especially true in healthcare in the US, where information systems investments are typically huge and often perceived by change resistant stakeholders as disruptive or even potentially life threatening. We describe how the IS implementation process itself contributed to organizational transformation in terms of changes in coordination, culture, and learning at a successful organization, Sentara Healthcare, which transformed the discordant cacophony of the change process into a harmonious implementation.\n",
      "Content: From cacophony to harmony: A case study about the IS implementation process as an opportunity for organizational transformation at Sentara Healthcare The cacophony of criticisms emanating from an organization facing an information technology-enabled transformation can be deafening and deleterious. This is especially true in healthcare in the US, where information systems investments are typically huge and often perceived by change resistant stakeholders as disruptive or even potentially life threatening. We describe how the IS implementation process itself contributed to organizational transformation in terms of changes in coordination, culture, and learning at a successful organization, Sentara Healthcare, which transformed the discordant cacophony of the change process into a harmonious implementation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 38\n",
      "Title: Supporting decision support: Where information on DSS is located\n",
      "Abstract: DSS professionals may differ in their opinion and practice as to where they locate the most useful information relevant to their work. Online and other electronic form databases are increasingly becoming the key resource for literature searches. This study empirically compared 31 online databases identified as promising for DSS relevant information according to their coverage of DSS. Rankings for recent years and temporally unconstrained conditions were obtained and discussed. INSPEC was the highest ranked database overall and for recent information. INSPEC was also the highest rated database for coverage of major DSS journals. However, there are many other databases that also provide coverage of DSS materials. It is hoped that DSS professionals will use these results to improve the effectiveness of their information search process.\n",
      "Content: Supporting decision support: Where information on DSS is located DSS professionals may differ in their opinion and practice as to where they locate the most useful information relevant to their work. Online and other electronic form databases are increasingly becoming the key resource for literature searches. This study empirically compared 31 online databases identified as promising for DSS relevant information according to their coverage of DSS. Rankings for recent years and temporally unconstrained conditions were obtained and discussed. INSPEC was the highest ranked database overall and for recent information. INSPEC was also the highest rated database for coverage of major DSS journals. However, there are many other databases that also provide coverage of DSS materials. It is hoped that DSS professionals will use these results to improve the effectiveness of their information search process.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 39\n",
      "Title: Concept comparison engines: A new frontier of search\n",
      "Abstract: In a traditional search engine interaction scenario, a user begins with a certain concept and finds documents that are similar to their concept. However, the user may wish to compare alternatives and a search capability should compare concepts and present the best alternatives. This task can be difficult without proper decision aids. We propose a concept comparison engine as a decision support tool that may be used to compare attributes of different alternatives and aid in making an informed selection. We describe an architecture and an interaction scenario and implement a prototype. We propose a number of evaluation metrics for measuring the viability of different terms for the purpose of comparing concepts. In scripted experiments, orderings for candidate terms from the prototype are compared to gold standard ranking lists from structured external sources. Our results indicate that a Rankor analysis may be promising as a measure of the differentiating power of candidate terms a user might choose to support concept comparison.\n",
      "Content: Concept comparison engines: A new frontier of search In a traditional search engine interaction scenario, a user begins with a certain concept and finds documents that are similar to their concept. However, the user may wish to compare alternatives and a search capability should compare concepts and present the best alternatives. This task can be difficult without proper decision aids. We propose a concept comparison engine as a decision support tool that may be used to compare attributes of different alternatives and aid in making an informed selection. We describe an architecture and an interaction scenario and implement a prototype. We propose a number of evaluation metrics for measuring the viability of different terms for the purpose of comparing concepts. In scripted experiments, orderings for candidate terms from the prototype are compared to gold standard ranking lists from structured external sources. Our results indicate that a Rankor analysis may be promising as a measure of the differentiating power of candidate terms a user might choose to support concept comparison.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 40\n",
      "Title: What's buzzing in the blizzard of buzz? Automotive component isolation in social media postings\n",
      "Abstract: In the blizzard of social media postings, isolating what is important to a corporation is a huge challenge. In the consumer-related manufacturing industry, for instance, manufacturers and distributors are faced with an unrelenting, accumulating snow of millions of discussion forum postings. In this paper, we describe and evaluate text mining tools for categorizing this user-generated content and distilling valuable intelligence frozen in the mound of postings. Using the automotive industry as an example, we implement and tune the parameters of a text-mining model for component diagnostics from social media. Our model can automatically and accurately isolate the vehicle component that is the subject of a user discussion. The procedure described also rapidly identifies the most distinctive terms for each component category, which provides further marketing and competitive intelligence to manufacturers, distributors, service centers, and suppliers.\n",
      "Content: What's buzzing in the blizzard of buzz? Automotive component isolation in social media postings In the blizzard of social media postings, isolating what is important to a corporation is a huge challenge. In the consumer-related manufacturing industry, for instance, manufacturers and distributors are faced with an unrelenting, accumulating snow of millions of discussion forum postings. In this paper, we describe and evaluate text mining tools for categorizing this user-generated content and distilling valuable intelligence frozen in the mound of postings. Using the automotive industry as an example, we implement and tune the parameters of a text-mining model for component diagnostics from social media. Our model can automatically and accurately isolate the vehicle component that is the subject of a user discussion. The procedure described also rapidly identifies the most distinctive terms for each component category, which provides further marketing and competitive intelligence to manufacturers, distributors, service centers, and suppliers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 41\n",
      "Title: Vehicle defect discovery from social media\n",
      "Abstract: A pressing need of vehicle quality management professionals is decision support for the vehicle defect discovery and classification process. In this paper, we employ text mining on a popular social medium used by vehicle enthusiasts: online discussion forums. We find that sentiment analysis, a conventional technique for consumer complaint detection, is insufficient for finding, categorizing, and prioritizing vehicle defects discussed in online forums, and we describe and evaluate a new process and decision support system for automotive defect identification and prioritization. Our findings provide managerial insights into how social media analytics can improve automotive quality management.\n",
      "Content: Vehicle defect discovery from social media A pressing need of vehicle quality management professionals is decision support for the vehicle defect discovery and classification process. In this paper, we employ text mining on a popular social medium used by vehicle enthusiasts: online discussion forums. We find that sentiment analysis, a conventional technique for consumer complaint detection, is insufficient for finding, categorizing, and prioritizing vehicle defects discussed in online forums, and we describe and evaluate a new process and decision support system for automotive defect identification and prioritization. Our findings provide managerial insights into how social media analytics can improve automotive quality management.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 42\n",
      "Title: A decision support system for patient scheduling in travel vaccine administration\n",
      "Abstract: The administration of travel vaccines presents a number of operations management challenges. The interplay between shared consumption of multi-dose vaccine packages, rapid spoilage upon opening, the high cost of wastage, and the unique vaccination needs of the patients makes for a very interesting and complex scheduling problem that could benefit from computerized decision support. We compare the performance of a novel binary integer programming model and a genetic algorithm solution technique with conventional scheduling approaches. Computational results show that significant cost savings can be achieved with the DSS while simultaneously considering scheduling preferences of patients and mitigating scheduling inconvenience.\n",
      "Content: A decision support system for patient scheduling in travel vaccine administration The administration of travel vaccines presents a number of operations management challenges. The interplay between shared consumption of multi-dose vaccine packages, rapid spoilage upon opening, the high cost of wastage, and the unique vaccination needs of the patients makes for a very interesting and complex scheduling problem that could benefit from computerized decision support. We compare the performance of a novel binary integer programming model and a genetic algorithm solution technique with conventional scheduling approaches. Computational results show that significant cost savings can be achieved with the DSS while simultaneously considering scheduling preferences of patients and mitigating scheduling inconvenience.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 43\n",
      "Title: ‘Lots done, more to do’: the current state of agile systems development research\n",
      "Abstract: The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "Content: ‘Lots done, more to do’: the current state of agile systems development research The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 44\n",
      "Title: The role of users and customers in digital innovation: Insights from B2B manufacturing firms\n",
      "Abstract: Diffusion of digital technologies into the manufacturing industry has created new opportunities for innovation that firms must address to remain competitive. We investigate the role of customer and user knowledge in the digital innovation processes of three global B2B manufacturing companies. We find that the B2B manufacturing industry's characteristics influence how users and customers may be leveraged. Customers making the purchasing decisions are considered for knowledge about short-term changes in market needs, while users working directly with the products provide long-term guidance for digital innovation. We identify practices for acquiring, distributing, and using customer and user knowledge for digital innovation.\n",
      "Content: The role of users and customers in digital innovation: Insights from B2B manufacturing firms Diffusion of digital technologies into the manufacturing industry has created new opportunities for innovation that firms must address to remain competitive. We investigate the role of customer and user knowledge in the digital innovation processes of three global B2B manufacturing companies. We find that the B2B manufacturing industry's characteristics influence how users and customers may be leveraged. Customers making the purchasing decisions are considered for knowledge about short-term changes in market needs, while users working directly with the products provide long-term guidance for digital innovation. We identify practices for acquiring, distributing, and using customer and user knowledge for digital innovation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 45\n",
      "Title: Digital entrepreneurship and indigenous value systems: An Ubuntu perspective\n",
      "Abstract: This paper investigates the embeddedness of digital entrepreneurship in the entrepreneurs' indigenous value system by examining the influence of ‘Ubuntu’ on digital entrepreneurship activities in the South African context. We do so through an interpretive field study of two innovation clusters in South Africa. Our findings reveal Ubuntu as the basis of a community orientation to digital entrepreneurship that offers an alternative to the prevalent heroic view in which digital entrepreneurship narratives are centred around the individual entrepreneur(s). They also highlight the tensions faced by digital entrepreneurs as they attempt to uphold the Ubuntu values of humility, reciprocity, and benevolence while operating in a competitive and fast-paced environment. In addition, our study indicates that the way entrepreneurs draw on their indigenous value system is dynamic, giving rise to what we call digital Ubuntu, reflecting a reworking of Ubuntu values into their increasingly digital reality. The concept of digital Ubuntu brings to light how indigenous values can become entangled with the capabilities of digital technologies and highlights the need for indigenous perspectives to advance our understanding of the diversity of digital phenomena, such as digital entrepreneurship, across cultural contexts.\n",
      "Content: Digital entrepreneurship and indigenous value systems: An Ubuntu perspective This paper investigates the embeddedness of digital entrepreneurship in the entrepreneurs' indigenous value system by examining the influence of ‘Ubuntu’ on digital entrepreneurship activities in the South African context. We do so through an interpretive field study of two innovation clusters in South Africa. Our findings reveal Ubuntu as the basis of a community orientation to digital entrepreneurship that offers an alternative to the prevalent heroic view in which digital entrepreneurship narratives are centred around the individual entrepreneur(s). They also highlight the tensions faced by digital entrepreneurs as they attempt to uphold the Ubuntu values of humility, reciprocity, and benevolence while operating in a competitive and fast-paced environment. In addition, our study indicates that the way entrepreneurs draw on their indigenous value system is dynamic, giving rise to what we call digital Ubuntu, reflecting a reworking of Ubuntu values into their increasingly digital reality. The concept of digital Ubuntu brings to light how indigenous values can become entangled with the capabilities of digital technologies and highlights the need for indigenous perspectives to advance our understanding of the diversity of digital phenomena, such as digital entrepreneurship, across cultural contexts.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 46\n",
      "Title: How do technologists do “ICT for development”? A contextualised perspective on ICT4D in South Africa\n",
      "Abstract: We take a layered approach to contextualise Information Communication Technology for Development (ICT4D) to understand digital technologists’ motivations to implement technologies to address socio-economic issues based on their capabilities and kinship affiliations. We adopt an interpretive approach to conducting an inductive qualitative study of digital technologists based in South Africa. We propose three mechanisms (emotional connectedness, user-centred technologies, and symbiotic relations) through which digital technologists undertake ICT4D to exercise their agency and enhance the socio-economic well-being of disadvantaged members of society. Taking the kinship perspective and capability approach as underlying motivations for undertaking ICT4D projects allows us to contribute to the ICT4D literature.\n",
      "Content: How do technologists do “ICT for development”? A contextualised perspective on ICT4D in South Africa We take a layered approach to contextualise Information Communication Technology for Development (ICT4D) to understand digital technologists’ motivations to implement technologies to address socio-economic issues based on their capabilities and kinship affiliations. We adopt an interpretive approach to conducting an inductive qualitative study of digital technologists based in South Africa. We propose three mechanisms (emotional connectedness, user-centred technologies, and symbiotic relations) through which digital technologists undertake ICT4D to exercise their agency and enhance the socio-economic well-being of disadvantaged members of society. Taking the kinship perspective and capability approach as underlying motivations for undertaking ICT4D projects allows us to contribute to the ICT4D literature.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 47\n",
      "Title: The role of formal controls in facilitating information system diffusion\n",
      "Abstract: Information system (IS) studies highlight that IS usage, a pre-requisite for IS diffusion, may be difficult to attain when usage is voluntary because users can resist using the system. User resistance may be overcome through the application of organizational controls. Control theory explains how users’ actions and practices are shaped in line with organizational guidelines and procedures. This paper reports on a qualitative case study and shows how formal control mechanisms (behavior and outcome controls) can have a positive and conclusive impact on IS diffusion. The paper makes three contributions to knowledge. First, it explains how the application of outcome control mechanisms can lead to IS diffusion despite user resistance. Second, it suggests that IS diffusion paths are iterative rather than smooth and linear. Finally, the paper demonstrates that in some contexts, despite a lack of reward expectancy, sanction expectancy can be an effective force during the IS diffusion process.\n",
      "Content: The role of formal controls in facilitating information system diffusion Information system (IS) studies highlight that IS usage, a pre-requisite for IS diffusion, may be difficult to attain when usage is voluntary because users can resist using the system. User resistance may be overcome through the application of organizational controls. Control theory explains how users’ actions and practices are shaped in line with organizational guidelines and procedures. This paper reports on a qualitative case study and shows how formal control mechanisms (behavior and outcome controls) can have a positive and conclusive impact on IS diffusion. The paper makes three contributions to knowledge. First, it explains how the application of outcome control mechanisms can lead to IS diffusion despite user resistance. Second, it suggests that IS diffusion paths are iterative rather than smooth and linear. Finally, the paper demonstrates that in some contexts, despite a lack of reward expectancy, sanction expectancy can be an effective force during the IS diffusion process.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 48\n",
      "Title: Issues in computer and non-computer supported GDSSs\n",
      "Abstract: Considerable attention in recent years, especially in the US, has been paid to computer based Group Decision Support Systems (GDSS). These systems expect to capitalise on the benefits provided by computers and networks. These computer based GDSSs are heralded as being relatively new, however systems for supporting group decision making developed in the UK which use only manual methods and techniques have been available for much longer. This paper aims to examine these two types of group decision support system, and suggest a third type — that of the partially computer based system. The paper considers aspects of GDSS design such as location, flexibility of design, levels of participation in data capture, presentational difficulties, managing complexity of data, client control, and management of group dynamics. By so doing the paper aims to demonstrate that all three types of system can benefit from consideration of the other types, that each type has both positive and negative features, and that some combination of all of the types could provide groups with the best form of support.\n",
      "Content: Issues in computer and non-computer supported GDSSs Considerable attention in recent years, especially in the US, has been paid to computer based Group Decision Support Systems (GDSS). These systems expect to capitalise on the benefits provided by computers and networks. These computer based GDSSs are heralded as being relatively new, however systems for supporting group decision making developed in the UK which use only manual methods and techniques have been available for much longer. This paper aims to examine these two types of group decision support system, and suggest a third type — that of the partially computer based system. The paper considers aspects of GDSS design such as location, flexibility of design, levels of participation in data capture, presentational difficulties, managing complexity of data, client control, and management of group dynamics. By so doing the paper aims to demonstrate that all three types of system can benefit from consideration of the other types, that each type has both positive and negative features, and that some combination of all of the types could provide groups with the best form of support.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 49\n",
      "Title: Impact of meta-analytic decisions on the conclusions drawn on the business value of information technology\n",
      "Abstract: Meta-analysis is a quantitative methodology that allows for summarizing the results of primary research studies in a field to provide new insights in terms of the phenomenon observed or the outcomes reported. This paper attempts to answer the fundamental question, “Do methodological decisions in a meta-analytic study affect the conclusions drawn from that study?” Specifically, this paper examines the effects of meta-analytic decisions when applied to the business value of information technology (BVIT) research stream. A closer examination of the variation in the methodological decisions informs us that, with each different decision alternative, we are examining slightly different phenomenon. The findings reveal that study outcomes do change, depending on the meta-analytic decisions that are tested. In other words, methodological decisions matter. Based on the data from 99 primary studies and 531 effect sizes, we tested seven hypotheses, in the BVIT research stream, using a comprehensive set of different methodological conditions. We find support for two findings that are consistent across all the different conditions. First, investing in information technology (IT) is positively associated with the firm's performance. We also find that large firms get more benefits from IT than small firms. These and the overall findings suggest that researchers should be cognizant of their methodological decisions, as they may be observing the phenomenon under different boundary conditions with different methodologies.\n",
      "Content: Impact of meta-analytic decisions on the conclusions drawn on the business value of information technology Meta-analysis is a quantitative methodology that allows for summarizing the results of primary research studies in a field to provide new insights in terms of the phenomenon observed or the outcomes reported. This paper attempts to answer the fundamental question, “Do methodological decisions in a meta-analytic study affect the conclusions drawn from that study?” Specifically, this paper examines the effects of meta-analytic decisions when applied to the business value of information technology (BVIT) research stream. A closer examination of the variation in the methodological decisions informs us that, with each different decision alternative, we are examining slightly different phenomenon. The findings reveal that study outcomes do change, depending on the meta-analytic decisions that are tested. In other words, methodological decisions matter. Based on the data from 99 primary studies and 531 effect sizes, we tested seven hypotheses, in the BVIT research stream, using a comprehensive set of different methodological conditions. We find support for two findings that are consistent across all the different conditions. First, investing in information technology (IT) is positively associated with the firm's performance. We also find that large firms get more benefits from IT than small firms. These and the overall findings suggest that researchers should be cognizant of their methodological decisions, as they may be observing the phenomenon under different boundary conditions with different methodologies.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 50\n",
      "Title: Being an ‘it’ in IT: gendered identities in IT work\n",
      "Abstract: This paper reflects on aspects of gender and IT work. The core hypothesis is that, if technical skill and masculinity are fundamentally related, then women working in IT jobs who are, in effect, challenging masculine skills by gaining them themselves, must develop a number of strategies to cope with the challenge that they feel is being made to their own gender identities and those of the men with whom they work. One strategy is for women to distance themselves from IT work; a second strategy is for women to distance themselves from their identities as women. Our results are drawn from a set of semi-structured interviews. We adopt the approach of critical research that seeks to expose asymmetric power relations in the organization and to let silenced voices be heard. This is related to the literature on silence in organizations. Within the critical approach, we chose a feminist methodology that looks towards identifying practices that are problematic for women and that acknowledges our biases and interests as researchers. Additionally, we draw upon the theoretical constructs of the gender and technology literature to theorize the relationship between gender and technical skill and how this impacts conceptions of gender identity.\n",
      "Content: Being an ‘it’ in IT: gendered identities in IT work This paper reflects on aspects of gender and IT work. The core hypothesis is that, if technical skill and masculinity are fundamentally related, then women working in IT jobs who are, in effect, challenging masculine skills by gaining them themselves, must develop a number of strategies to cope with the challenge that they feel is being made to their own gender identities and those of the men with whom they work. One strategy is for women to distance themselves from IT work; a second strategy is for women to distance themselves from their identities as women. Our results are drawn from a set of semi-structured interviews. We adopt the approach of critical research that seeks to expose asymmetric power relations in the organization and to let silenced voices be heard. This is related to the literature on silence in organizations. Within the critical approach, we chose a feminist methodology that looks towards identifying practices that are problematic for women and that acknowledges our biases and interests as researchers. Additionally, we draw upon the theoretical constructs of the gender and technology literature to theorize the relationship between gender and technical skill and how this impacts conceptions of gender identity.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 51\n",
      "Title: Computer ethics in a different voice\n",
      "Abstract: This paper argues that the potential of writing on computer ethics to contribute to a deeper understanding of inequalities surrounding the use of information and communications technologies is threatened by forms of technological determinism and liberalism. Such views are prevalent in professional and more popular literature, and even in policy documents, albeit expressed tacitly. Adopting this standpoint substantially reduces explanatory power in relation to certain computer ethics topics, especially equality and participation, particularly in relation to gender. Research on gender and information and communications technologies has analyzed inequalities between men and women both inside and outside the workplace, drawing heavily from feminist theory. The paper argues that feminist ethics, coupled with aspects of feminist legal and political theory, may offer a fruitful, novel direction for analyzing computer ethics problems, and certainly those that contain substantial differences, and therefore inequalities, in men's and women's experiences on-line. Furthermore, feminist ethics can offer a more collectivist approach toward computer ethics problems. Emerging themes in existing research on gender and computer ethics are discussed before exploring some of the outcomes of applying feminist theory to a problem of privacy in the extreme form of Internet-based harassment known as “cyberstalking”, where traditional liberal and determinist views have proved problematic.\n",
      "Content: Computer ethics in a different voice This paper argues that the potential of writing on computer ethics to contribute to a deeper understanding of inequalities surrounding the use of information and communications technologies is threatened by forms of technological determinism and liberalism. Such views are prevalent in professional and more popular literature, and even in policy documents, albeit expressed tacitly. Adopting this standpoint substantially reduces explanatory power in relation to certain computer ethics topics, especially equality and participation, particularly in relation to gender. Research on gender and information and communications technologies has analyzed inequalities between men and women both inside and outside the workplace, drawing heavily from feminist theory. The paper argues that feminist ethics, coupled with aspects of feminist legal and political theory, may offer a fruitful, novel direction for analyzing computer ethics problems, and certainly those that contain substantial differences, and therefore inequalities, in men's and women's experiences on-line. Furthermore, feminist ethics can offer a more collectivist approach toward computer ethics problems. Emerging themes in existing research on gender and computer ethics are discussed before exploring some of the outcomes of applying feminist theory to a problem of privacy in the extreme form of Internet-based harassment known as “cyberstalking”, where traditional liberal and determinist views have proved problematic.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 52\n",
      "Title: Exploring the gender question in critical information systems\n",
      "Abstract: This paper addresses ways in which theorizing gender may be important in forming an understanding of the topic of emancipation, which is central to the new critical information systems (IS) based on the thinking of Habermas. After briefly discussing some problems with current research on gender and IS the paper argues that appropriate feminist theory may be useful in augmenting our understanding of foundational issues such as emancipation. The development of feminist philosophy and epistemology is briefly introduced. Habermas' 'ideal speech situation' is problematized in relation to feminist writing on male and female communication juxtaposed with recent research in computer-mediated communications. The paper continues by exploring the concept of emancipation through feminist epistemology and it closes with a preliminary consideration of how these concerns may be applied to critical IS.\n",
      "Content: Exploring the gender question in critical information systems This paper addresses ways in which theorizing gender may be important in forming an understanding of the topic of emancipation, which is central to the new critical information systems (IS) based on the thinking of Habermas. After briefly discussing some problems with current research on gender and IS the paper argues that appropriate feminist theory may be useful in augmenting our understanding of foundational issues such as emancipation. The development of feminist philosophy and epistemology is briefly introduced. Habermas' 'ideal speech situation' is problematized in relation to feminist writing on male and female communication juxtaposed with recent research in computer-mediated communications. The paper continues by exploring the concept of emancipation through feminist epistemology and it closes with a preliminary consideration of how these concerns may be applied to critical IS.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 53\n",
      "Title: IS and its agenda\n",
      "Abstract: The Journal of Information Technology is of interest to academics, scholars, advanced students and reflective practitioners in management science, information systems and computer science disciplines. The journal will also inform those seeking an update on current experience and future prospects in the areas of contemporary information and communications technology.\n",
      "Content: IS and its agenda The Journal of Information Technology is of interest to academics, scholars, advanced students and reflective practitioners in management science, information systems and computer science disciplines. The journal will also inform those seeking an update on current experience and future prospects in the areas of contemporary information and communications technology.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 54\n",
      "Title: A framework for the classification of DSS usage across organizations\n",
      "Abstract: The research project reported in this paper constitutes an attempt to build upon existing research in the DSS area, namely the well-known framework for DSS developed by Gorry and Scott Morton. The authors put forward a revised framework which researchers in the field can use to classify DSS applications meaningfully and make comparisons across large samples of systems and organizations. The findings of a three year action research study were used to develop a framework for the classification of DSS usage which was then tested empirically. The organizations studied were classified based on the extent to which they used DSS for different decision situations using two specific measurements: DSS spread and DSS complexity. The results obtained suggest that the framework which was developed in this research is a useful vehicle for categorizing the degree of maturity of organizations regarding their usage and development of actual DSS applications.\n",
      "Content: A framework for the classification of DSS usage across organizations The research project reported in this paper constitutes an attempt to build upon existing research in the DSS area, namely the well-known framework for DSS developed by Gorry and Scott Morton. The authors put forward a revised framework which researchers in the field can use to classify DSS applications meaningfully and make comparisons across large samples of systems and organizations. The findings of a three year action research study were used to develop a framework for the classification of DSS usage which was then tested empirically. The organizations studied were classified based on the extent to which they used DSS for different decision situations using two specific measurements: DSS spread and DSS complexity. The results obtained suggest that the framework which was developed in this research is a useful vehicle for categorizing the degree of maturity of organizations regarding their usage and development of actual DSS applications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 55\n",
      "Title: Information flows amongst executives: their implications for systems development\n",
      "Abstract: Senior managers have tended to resist the incursion into their personal domain of computer systems meant for their use. Their main criticism is that technical solutions are being imposed on them without an adequate analysis of the problems at hand. This suggests that the way in which executives obtain and exchange information may not be adequately understood. With the help of a framework designed to identify top executives' networks of information flows, the study reported in this paper analysed the information practices of 16 executives from four organizations. The findings of the research indicated that executives use a combination of communication flows and information flows in a proportion which varies depending upon the context of their different activities. It also revealed that executives initiated information and communication flows of a different nature depending upon the role they play and the level of those with whom they deal within the organization. The results of the study suggest that very specific approaches are needed when identifying executives' needs in terms of developing systems aimed at supporting top managers' strategic activities.\n",
      "Content: Information flows amongst executives: their implications for systems development Senior managers have tended to resist the incursion into their personal domain of computer systems meant for their use. Their main criticism is that technical solutions are being imposed on them without an adequate analysis of the problems at hand. This suggests that the way in which executives obtain and exchange information may not be adequately understood. With the help of a framework designed to identify top executives' networks of information flows, the study reported in this paper analysed the information practices of 16 executives from four organizations. The findings of the research indicated that executives use a combination of communication flows and information flows in a proportion which varies depending upon the context of their different activities. It also revealed that executives initiated information and communication flows of a different nature depending upon the role they play and the level of those with whom they deal within the organization. The results of the study suggest that very specific approaches are needed when identifying executives' needs in terms of developing systems aimed at supporting top managers' strategic activities.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 56\n",
      "Title: Lessons from enterprise resource planning implementations in Ireland – towards smaller and shorter ERP projects\n",
      "Abstract: The enterprise resource planning (ERP) software market has been growing at a very fast pace over the last few years and has been predicted to keep growing rapidly in the long term. This has led to an abundance of media reports on the subject of ERP and to managers wondering whether their companies should implement ERP systems. In order to separate the reality of the ERP phenomenon from the hype that surrounds it, we studied 14 ERP implementation projects in Irish organizations and focused on the key relationships between organizations which attempt to implement ERP systems and their implementing partners. We found that the ERP implementations that are going on in Ireland at the moment are different to the projects that have been reported elsewhere in two key respects. Firstly, the organizations interested in ERP software are, on average, far smaller than the case studies reported in the literature and the majority of the cases we reviewed were small and medium enterprises (SMEs). Secondly, the durations of implementation were far shorter than reported elsewhere. These results are not surprising if one considers the smaller average size of Irish organizations, but they indicate that the ERP movement is truly ready for an extension towards the SME market. They also indicate that the duration of the implementation of ERP software may be related to the size and complexity of the client organization and that SMEs can expect to have an easier time implementing ERPs than the current literature suggests. We also found that software implementers play a key role, not only in technical terms, but also in managerial and political terms, because they can help their clients in correcting their expectations and perceptions of ERP systems and ERP implementations.\n",
      "Content: Lessons from enterprise resource planning implementations in Ireland – towards smaller and shorter ERP projects The enterprise resource planning (ERP) software market has been growing at a very fast pace over the last few years and has been predicted to keep growing rapidly in the long term. This has led to an abundance of media reports on the subject of ERP and to managers wondering whether their companies should implement ERP systems. In order to separate the reality of the ERP phenomenon from the hype that surrounds it, we studied 14 ERP implementation projects in Irish organizations and focused on the key relationships between organizations which attempt to implement ERP systems and their implementing partners. We found that the ERP implementations that are going on in Ireland at the moment are different to the projects that have been reported elsewhere in two key respects. Firstly, the organizations interested in ERP software are, on average, far smaller than the case studies reported in the literature and the majority of the cases we reviewed were small and medium enterprises (SMEs). Secondly, the durations of implementation were far shorter than reported elsewhere. These results are not surprising if one considers the smaller average size of Irish organizations, but they indicate that the ERP movement is truly ready for an extension towards the SME market. They also indicate that the duration of the implementation of ERP software may be related to the size and complexity of the client organization and that SMEs can expect to have an easier time implementing ERPs than the current literature suggests. We also found that software implementers play a key role, not only in technical terms, but also in managerial and political terms, because they can help their clients in correcting their expectations and perceptions of ERP systems and ERP implementations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 57\n",
      "Title: Input control and its signalling effects for complementors' intention to join digital platforms\n",
      "Abstract: Existing information systems (IS) research on platform control has largely focused on examining how input control (i.e., the mechanisms used to control platform access) affects complementors' intentions and behaviours after their decision to join a digital platform. Yet, our understanding of how input control is perceived before this decision and how such perceptions influence prospective complementors' intention to join a platform is still nascent. In this regard, our study views input control as a salient signal that shapes prospective complementors' expected benefits and costs (i.e., their performance and effort expectancy), and ultimately their decision to join a digital platform. Drawing on signalling theory and the antecedent-benefit-cost (ABC) framework, we conducted a randomized online experiment in the context of donation-based crowdfunding. The experiment results offer empirical support for this view by showing that input control has distinct and complex signalling effects for prospective complementors. In particular, our findings reveal curvilinear and competing signalling effects, with perceived input control increasing both performance expectancy (at a decreasing rate) and effort expectancy (at an increasing rate). Also, we find that performance expectancy linearly increases prospective complementors' intention to join a platform, whereas effort expectancy linearly decreases their intention to do so. These findings imply that the overall relationship between perceived input control and intention to join follows an inverted U-shape curve, which means that neither a low nor a high, but a moderate degree of perceived input control maximizes prospective complementors' intention to join. In sum, the results of our study provide novel and important insights into the signalling role that perceived input control plays in shaping prospective complementors' decision to join a digital platform.\n",
      "Content: Input control and its signalling effects for complementors' intention to join digital platforms Existing information systems (IS) research on platform control has largely focused on examining how input control (i.e., the mechanisms used to control platform access) affects complementors' intentions and behaviours after their decision to join a digital platform. Yet, our understanding of how input control is perceived before this decision and how such perceptions influence prospective complementors' intention to join a platform is still nascent. In this regard, our study views input control as a salient signal that shapes prospective complementors' expected benefits and costs (i.e., their performance and effort expectancy), and ultimately their decision to join a digital platform. Drawing on signalling theory and the antecedent-benefit-cost (ABC) framework, we conducted a randomized online experiment in the context of donation-based crowdfunding. The experiment results offer empirical support for this view by showing that input control has distinct and complex signalling effects for prospective complementors. In particular, our findings reveal curvilinear and competing signalling effects, with perceived input control increasing both performance expectancy (at a decreasing rate) and effort expectancy (at an increasing rate). Also, we find that performance expectancy linearly increases prospective complementors' intention to join a platform, whereas effort expectancy linearly decreases their intention to do so. These findings imply that the overall relationship between perceived input control and intention to join follows an inverted U-shape curve, which means that neither a low nor a high, but a moderate degree of perceived input control maximizes prospective complementors' intention to join. In sum, the results of our study provide novel and important insights into the signalling role that perceived input control plays in shaping prospective complementors' decision to join a digital platform.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 58\n",
      "Title: Gamified monetary reward designs: Offering certain versus chance-based rewards\n",
      "Abstract: To motivate visitors to engage with websites, e-tailers widely employ monetary rewards (e.g., vouchers, discounts) in their website designs. With advances in user interface technologies, many e-tailers have started to offer gamified monetary reward designs (MRDs), which require visitors to earn the monetary reward by playing a game, rather than simply claiming the reward. However, little is known about whether and why gamified MRDs engage visitors compared to their non-gamified counterpart. Even less is known about the effectiveness of gamified MRDs when providing certain or chance-based rewards, in that visitors do or do not know what reward they will gain for successfully performing in the game. Drawing on cognitive evaluation theory, we investigate gamified MRDs with certain or chance-based rewards and contrast them to non-gamified MRDs with certain rewards in user registration systems. Our results from a multi-method approach encompassing the complementary features of a randomised field experiment (N = 651) and a randomised online experiment (N = 330) demonstrate differential effects of the three investigated MRDs on user registration. Visitors encountering either type of gamified MRD are more likely to register than those encountering a non-gamified MRD. Moreover, gamified MRDs with chance-based rewards have the highest likelihood of user registrations. We also show that MRDs have distinct indirect effects on user registration via anticipated experiences of competence and sensation. Overall, the paper offers theoretical insights and practical guidance on how and why gamified MRDs are effective for e-tailers.\n",
      "Content: Gamified monetary reward designs: Offering certain versus chance-based rewards To motivate visitors to engage with websites, e-tailers widely employ monetary rewards (e.g., vouchers, discounts) in their website designs. With advances in user interface technologies, many e-tailers have started to offer gamified monetary reward designs (MRDs), which require visitors to earn the monetary reward by playing a game, rather than simply claiming the reward. However, little is known about whether and why gamified MRDs engage visitors compared to their non-gamified counterpart. Even less is known about the effectiveness of gamified MRDs when providing certain or chance-based rewards, in that visitors do or do not know what reward they will gain for successfully performing in the game. Drawing on cognitive evaluation theory, we investigate gamified MRDs with certain or chance-based rewards and contrast them to non-gamified MRDs with certain rewards in user registration systems. Our results from a multi-method approach encompassing the complementary features of a randomised field experiment (N = 651) and a randomised online experiment (N = 330) demonstrate differential effects of the three investigated MRDs on user registration. Visitors encountering either type of gamified MRD are more likely to register than those encountering a non-gamified MRD. Moreover, gamified MRDs with chance-based rewards have the highest likelihood of user registrations. We also show that MRDs have distinct indirect effects on user registration via anticipated experiences of competence and sensation. Overall, the paper offers theoretical insights and practical guidance on how and why gamified MRDs are effective for e-tailers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 59\n",
      "Title: Human vs. Automated Sales Agents: How and Why Customer Responses Shift Across Sales Stages\n",
      "Abstract: Customers in sales processes increasingly encounter automated sales agents (ASAs) that complement or replace human sales agents (HSAs). Yet, little is known about whether, how, and why customers respond to ASAs in contrast to HSAs across successive decision stages of the same sales process. Even less is known about customer responses to HSA-ASA combinations, where both agents assume distinct roles and focus on complementary tasks that are traditionally performed by only one single sales agent. Against this backdrop, this paper explores the influence of increasingly common sales representative (rep) types (i.e., ASA, HSA, and HSA-ASA) on customer decisions across sales stages. Drawing on information processing theory and the literature on hedonic-utilitarian decision making, we investigate customer responses to text-based ASAs from vendor companies in two common early stages of email sales processes (i.e., sales initiation stages) when customers successively decide whether to indicate their initial interest in an offer and then, whether to provide their contact information. Specifically, we conducted two complementary multi-decision experiments, namely (1) a randomized field experiment in a high-stakes sales initiation setting (n = 325) and (2) a subsequent randomized online experiment to complement the real-world insights (n = 408). Our core findings reveal reversing effect patterns of sales rep types across stages: although customers are more likely to indicate their initial interest to HSAs (versus ASAs) because of HSAs’ higher levels of social presence, they are less likely to provide contact information to HSAs because of HSAs’ lower levels of performance expectancy and effort expectancy. We also show that HSA-ASA combinations can be reasonable options for single ASAs, yet contextual features of the sales setting may affect differential customer responses to HSA-ASA combinations (versus ASAs) in each sales stage. Taken together, we uncover shifting effect patterns in customer responses to sales rep types across successive sales stages and shed light on the consecutive underlying mechanisms that explain these shifts. These findings have significant implications for vendor companies seeking to allocate HSAs and/or ASAs effectively across various decision stages in sales processes and beyond. History: Wonseok Oh, Senior Editor and Khim Yong Goh; Associate Editor. Funding: This work was supported by the Center for Responsible Digitality (ZEVEDI) and the German Research Foundation (DFG) [Grant 471168026]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1171.\n",
      "Content: Human vs. Automated Sales Agents: How and Why Customer Responses Shift Across Sales Stages Customers in sales processes increasingly encounter automated sales agents (ASAs) that complement or replace human sales agents (HSAs). Yet, little is known about whether, how, and why customers respond to ASAs in contrast to HSAs across successive decision stages of the same sales process. Even less is known about customer responses to HSA-ASA combinations, where both agents assume distinct roles and focus on complementary tasks that are traditionally performed by only one single sales agent. Against this backdrop, this paper explores the influence of increasingly common sales representative (rep) types (i.e., ASA, HSA, and HSA-ASA) on customer decisions across sales stages. Drawing on information processing theory and the literature on hedonic-utilitarian decision making, we investigate customer responses to text-based ASAs from vendor companies in two common early stages of email sales processes (i.e., sales initiation stages) when customers successively decide whether to indicate their initial interest in an offer and then, whether to provide their contact information. Specifically, we conducted two complementary multi-decision experiments, namely (1) a randomized field experiment in a high-stakes sales initiation setting (n = 325) and (2) a subsequent randomized online experiment to complement the real-world insights (n = 408). Our core findings reveal reversing effect patterns of sales rep types across stages: although customers are more likely to indicate their initial interest to HSAs (versus ASAs) because of HSAs’ higher levels of social presence, they are less likely to provide contact information to HSAs because of HSAs’ lower levels of performance expectancy and effort expectancy. We also show that HSA-ASA combinations can be reasonable options for single ASAs, yet contextual features of the sales setting may affect differential customer responses to HSA-ASA combinations (versus ASAs) in each sales stage. Taken together, we uncover shifting effect patterns in customer responses to sales rep types across successive sales stages and shed light on the consecutive underlying mechanisms that explain these shifts. These findings have significant implications for vendor companies seeking to allocate HSAs and/or ASAs effectively across various decision stages in sales processes and beyond. History: Wonseok Oh, Senior Editor and Khim Yong Goh; Associate Editor. Funding: This work was supported by the Center for Responsible Digitality (ZEVEDI) and the German Research Foundation (DFG) [Grant 471168026]. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1171.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 60\n",
      "Title: No man is an island: Social and human capital in IT capacity building in the Maldives\n",
      "Abstract: In many developing countries, lack of IT skills and human capital impede the potential of IT investments in organizations in developing countries [Lee, J. (2001). Education for technology readiness: Prospects for developing countries. Journal of Human Development, 2(1), 115–151]. This paper draws upon theories of human and social capital, and knowledge, to explain enablers/obstacles for knowledge creation and transfer for IT capacity building in a tourism organization in a developing country – the Maldives. IT capacity building is intimately linked to knowledge and skills at the level of human resource development. Using the Nahapiet and Ghoshal (1998) [Nahapiet, J., & Ghoshal, S. (1998). Social capital, intellectual capital, and the organizational advantage. Academy of Management Review, 23, 242–267] framework for the role of social capital in knowledge creation and transfer, we examine the major issues of IT capacity building for the case organization. We conclude that the role of cognitive capital is the most important for the tourism sector of the Maldives, and may play a vital role in accumulating structural and relational capital, together with appropriate government policies on ICT.\n",
      "Content: No man is an island: Social and human capital in IT capacity building in the Maldives In many developing countries, lack of IT skills and human capital impede the potential of IT investments in organizations in developing countries [Lee, J. (2001). Education for technology readiness: Prospects for developing countries. Journal of Human Development, 2(1), 115–151]. This paper draws upon theories of human and social capital, and knowledge, to explain enablers/obstacles for knowledge creation and transfer for IT capacity building in a tourism organization in a developing country – the Maldives. IT capacity building is intimately linked to knowledge and skills at the level of human resource development. Using the Nahapiet and Ghoshal (1998) [Nahapiet, J., & Ghoshal, S. (1998). Social capital, intellectual capital, and the organizational advantage. Academy of Management Review, 23, 242–267] framework for the role of social capital in knowledge creation and transfer, we examine the major issues of IT capacity building for the case organization. We conclude that the role of cognitive capital is the most important for the tourism sector of the Maldives, and may play a vital role in accumulating structural and relational capital, together with appropriate government policies on ICT.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 61\n",
      "Title: Containing COVID-19 through physical distancing: the impact of real-time crowding information\n",
      "Abstract: With the rise of COVID-19, decision support systems (DSS) increasingly display crowding information (CI) (e.g. how crowded a medical practice is) to encourage physical distancing when users select locations. Despite important implications for containing COVID-19, little is known about the causal effect of CI on user selection behaviour and how the immediacy of CI (e.g. “updated 2 minutes ago“) as well as users’ health anxiety further influence the effect of CI. Drawing on literature on digital choice environments and construal level theory, we conducted a multi-national online experiment to investigate the effect of CI on selecting differently crowded medical practices. Our results demonstrate that present (vs. absent) CI in DSS increases the likelihood of users selecting less crowded medical practices, while the effect is strongest when employed with real-time (vs. historical average) CI and, surprisingly, when users’ health anxiety is low (vs. high). Overall, our study adds to the growing body of research on IS in the age of pandemics and provides actionable insights for DSS providers and policymakers to endow users with information to identify and select less crowded locations, thus containing COVID-19 through improved physical distancing without paternalistically restricting users’ freedom of choice.\n",
      "Content: Containing COVID-19 through physical distancing: the impact of real-time crowding information With the rise of COVID-19, decision support systems (DSS) increasingly display crowding information (CI) (e.g. how crowded a medical practice is) to encourage physical distancing when users select locations. Despite important implications for containing COVID-19, little is known about the causal effect of CI on user selection behaviour and how the immediacy of CI (e.g. “updated 2 minutes ago“) as well as users’ health anxiety further influence the effect of CI. Drawing on literature on digital choice environments and construal level theory, we conducted a multi-national online experiment to investigate the effect of CI on selecting differently crowded medical practices. Our results demonstrate that present (vs. absent) CI in DSS increases the likelihood of users selecting less crowded medical practices, while the effect is strongest when employed with real-time (vs. historical average) CI and, surprisingly, when users’ health anxiety is low (vs. high). Overall, our study adds to the growing body of research on IS in the age of pandemics and provides actionable insights for DSS providers and policymakers to endow users with information to identify and select less crowded locations, thus containing COVID-19 through improved physical distancing without paternalistically restricting users’ freedom of choice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 62\n",
      "Title: A Form-Based Approach to Natural Language Query Processing\n",
      "Abstract: We describe a methodology for processing data retrieval and update queries using a form-based natural language interface. For the purpose of illustration, we use computer integrated manufacturing (CIM) as the application domain. The interface consists of a set of fourth-generation interface tools (SQL forms), a set of form definitions, a lexicon, and a parser. The forms are developed from the functional and data models of the system. A form definition consists of a form name, a form object, a set of form fields, and a set of fragment grammars. A form object is a single or composite entity that uniquely identifies a form. Form fields consist of database fields whose values can be entered by users (user-defined), and others whose values can be derived by the system (system-defined). Fragment grammars are templates that identify the information requested by user queries. The lexicon consists of all words recognized by the system, their grammatical categories, synonyms, and associations (if any) with database objects and forms. The parser scans a natural language query to identify a form in a bottom-up fashion. The information requested by the user query is determined in a top-down manner by matching the fragment grammars associated with a form against the user query. Extragrammatical inputs with limited deviations from the grammar rules are supported. Elliptical queries are supported by deriving the missing information from those specified in previous queries and forms. Combining a natural language processor with SQL forms allows update queries and prevents violation of database integrity constraints, duplication of records, and invalid data entry.\n",
      "Content: A Form-Based Approach to Natural Language Query Processing We describe a methodology for processing data retrieval and update queries using a form-based natural language interface. For the purpose of illustration, we use computer integrated manufacturing (CIM) as the application domain. The interface consists of a set of fourth-generation interface tools (SQL forms), a set of form definitions, a lexicon, and a parser. The forms are developed from the functional and data models of the system. A form definition consists of a form name, a form object, a set of form fields, and a set of fragment grammars. A form object is a single or composite entity that uniquely identifies a form. Form fields consist of database fields whose values can be entered by users (user-defined), and others whose values can be derived by the system (system-defined). Fragment grammars are templates that identify the information requested by user queries. The lexicon consists of all words recognized by the system, their grammatical categories, synonyms, and associations (if any) with database objects and forms. The parser scans a natural language query to identify a form in a bottom-up fashion. The information requested by the user query is determined in a top-down manner by matching the fragment grammars associated with a form against the user query. Extragrammatical inputs with limited deviations from the grammar rules are supported. Elliptical queries are supported by deriving the missing information from those specified in previous queries and forms. Combining a natural language processor with SQL forms allows update queries and prevents violation of database integrity constraints, duplication of records, and invalid data entry.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 63\n",
      "Title: Heterogeneous Demand Effects of Recommendation Strategies in a Mobile Application: Evidence from Econometric Models and Machine-Learning Instruments\n",
      "Abstract: In this paper, we examine the effectiveness of various recommendation strategies in the mobile channel and their impact on consumers’ utility and demand levels for individual products. We find significant differences in effectiveness among various recommendation strategies. Interestingly, recommendation strategies that directly embed social proofs for the recommended alternatives outperform other recommendations. In addition, recommendation strategies combining social proofs with higher levels of induced awareness due to the prescribed temporal diversity have an even stronger effect on the mobile channel. We also examine the heterogeneity of the demand effect across items, users, and contextual settings, further verifying empirically the aforementioned information and persuasion mechanisms and generating rich insights. We also facilitate the estimation of causal effects in the presence of endogeneity using machine-learning methods. Specifically, we develop novel econometric instruments that capture product differentiation (isolation) based on deeplearning models of user-generated reviews. Our empirical findings extend the current knowledge regarding the heterogeneous impact of recommender systems, reconcile contradictory prior results in the related literature, and have significant business implications.\n",
      "Content: Heterogeneous Demand Effects of Recommendation Strategies in a Mobile Application: Evidence from Econometric Models and Machine-Learning Instruments In this paper, we examine the effectiveness of various recommendation strategies in the mobile channel and their impact on consumers’ utility and demand levels for individual products. We find significant differences in effectiveness among various recommendation strategies. Interestingly, recommendation strategies that directly embed social proofs for the recommended alternatives outperform other recommendations. In addition, recommendation strategies combining social proofs with higher levels of induced awareness due to the prescribed temporal diversity have an even stronger effect on the mobile channel. We also examine the heterogeneity of the demand effect across items, users, and contextual settings, further verifying empirically the aforementioned information and persuasion mechanisms and generating rich insights. We also facilitate the estimation of causal effects in the presence of endogeneity using machine-learning methods. Specifically, we develop novel econometric instruments that capture product differentiation (isolation) based on deeplearning models of user-generated reviews. Our empirical findings extend the current knowledge regarding the heterogeneous impact of recommender systems, reconcile contradictory prior results in the related literature, and have significant business implications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 64\n",
      "Title: Demand Effects of the Internet-of-Things Sales Channel: Evidence from Automating the Purchase Process\n",
      "Abstract: The internet of things (IoT) is rapidly becoming one of the most popular emerging technologies in business and society. One of the major verticals that has recently begun to effectively use IoT technologies is the retail industry. Given the unprecedented opportunities IoT generates for brands and retailers, it is important to glean timely insights regarding the business value of IoT and understand whether the introduction of an IoT technology as an alternative purchase channel for consumers affects the sales of physical products. In this paper, using empirical data from a multinational online retailer that adopted an IoT technology that largely automates consumers’ purchases and a quasi-experimental framework, we study the effect of the introduction of IoT on product sales. Our analyses reveal a statistically and economically significant increase in sales as a result of adopting an IoT technology and demonstrate the business value of the IoT channel for retailers and brands. In addition, we conduct analyses of the IoT phenomenon to also delve into the effect heterogeneity and empirically validate the underlying mechanism by examining the impact of IoT for products in different price ranges, levels of substitutability, and product categories (e.g., search versus experience goods and hedonic versus utilitarian), drawing on mental accounting and automaticity theory. For instance, our analyses reveal that less expensive and more differentiated products as well as experience and utilitarian goods can accrue higher benefits leveraging more effectively novel IoT technologies. We validate the robustness of our findings using an extensive set of robustness checks and falsification tests. This is the first paper to study the impact of an IoT technology on product sales, drawing important theoretical and managerial implications and seeding new future research directions for devices and technologies largely automating the purchase process.\n",
      "Content: Demand Effects of the Internet-of-Things Sales Channel: Evidence from Automating the Purchase Process The internet of things (IoT) is rapidly becoming one of the most popular emerging technologies in business and society. One of the major verticals that has recently begun to effectively use IoT technologies is the retail industry. Given the unprecedented opportunities IoT generates for brands and retailers, it is important to glean timely insights regarding the business value of IoT and understand whether the introduction of an IoT technology as an alternative purchase channel for consumers affects the sales of physical products. In this paper, using empirical data from a multinational online retailer that adopted an IoT technology that largely automates consumers’ purchases and a quasi-experimental framework, we study the effect of the introduction of IoT on product sales. Our analyses reveal a statistically and economically significant increase in sales as a result of adopting an IoT technology and demonstrate the business value of the IoT channel for retailers and brands. In addition, we conduct analyses of the IoT phenomenon to also delve into the effect heterogeneity and empirically validate the underlying mechanism by examining the impact of IoT for products in different price ranges, levels of substitutability, and product categories (e.g., search versus experience goods and hedonic versus utilitarian), drawing on mental accounting and automaticity theory. For instance, our analyses reveal that less expensive and more differentiated products as well as experience and utilitarian goods can accrue higher benefits leveraging more effectively novel IoT technologies. We validate the robustness of our findings using an extensive set of robustness checks and falsification tests. This is the first paper to study the impact of an IoT technology on product sales, drawing important theoretical and managerial implications and seeding new future research directions for devices and technologies largely automating the purchase process.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 65\n",
      "Title: Integrating Decision Technologies: Implications for Management Curriculum\n",
      "Abstract:  The essence of management is decision making. Decision making requires the availability and proper use of data. Three evolving technologies relate to the support of decision making: information processing; decision science methods; and organization of decision makers and decision processes. More and more, because of technological developments and increased understanding of complex decision situations, these technologies must be seen as an integrated whole in order to support efficient and effective decision making. Within organizations, different groups are often charged with each of the three technologies that should support decision making. As a result, we often experience problems where the data is unavailable, is not compatible with the analyses desired, or is not relevant to the decision-making processes that managers wish to use. A substantive integration of these support groups and their expertise would help.Acing as a change agent, business and management schools can develop a conceptualiza; tion of the three technology areas that integrates them with respect to terminology. They can also develop a unified set of general constructs that carry throughout each of the areas. Further, a new sedes of core courses can be developed that present the integrated technology subjects in a logical sequence.Management schools have been concerned th teaching concepts and skills related to data acquisition, manipulation and presentation to their students for some Eme. However, in organizing the delivery of data-handling concepts and ski/Is, these schools have relied on courses from vadous discipline bases such as decision sciences (DSci), management information systems (M/S), managerial decision making (MDM). These courses in management school curricula are generally redundant, use similar terminology differently, and are not mutually supportive.This a~cle focuses on the design and delivery of an integrated sequence of core courses that addresses the three technology areas. Drawing on experience at our own schools, we address genera/course design principles, resolution of issues on content requirements and delivery, implementation issues, and current and future problems related to the effective integration of topics that have traditionally been addressed in DSci, MIS, and MDM courses. \n",
      "Content: Integrating Decision Technologies: Implications for Management Curriculum  The essence of management is decision making. Decision making requires the availability and proper use of data. Three evolving technologies relate to the support of decision making: information processing; decision science methods; and organization of decision makers and decision processes. More and more, because of technological developments and increased understanding of complex decision situations, these technologies must be seen as an integrated whole in order to support efficient and effective decision making. Within organizations, different groups are often charged with each of the three technologies that should support decision making. As a result, we often experience problems where the data is unavailable, is not compatible with the analyses desired, or is not relevant to the decision-making processes that managers wish to use. A substantive integration of these support groups and their expertise would help.Acing as a change agent, business and management schools can develop a conceptualiza; tion of the three technology areas that integrates them with respect to terminology. They can also develop a unified set of general constructs that carry throughout each of the areas. Further, a new sedes of core courses can be developed that present the integrated technology subjects in a logical sequence.Management schools have been concerned th teaching concepts and skills related to data acquisition, manipulation and presentation to their students for some Eme. However, in organizing the delivery of data-handling concepts and ski/Is, these schools have relied on courses from vadous discipline bases such as decision sciences (DSci), management information systems (M/S), managerial decision making (MDM). These courses in management school curricula are generally redundant, use similar terminology differently, and are not mutually supportive.This a~cle focuses on the design and delivery of an integrated sequence of core courses that addresses the three technology areas. Drawing on experience at our own schools, we address genera/course design principles, resolution of issues on content requirements and delivery, implementation issues, and current and future problems related to the effective integration of topics that have traditionally been addressed in DSci, MIS, and MDM courses. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 66\n",
      "Title: The journal list and its use: motivation, perceptions, and reality\n",
      "Abstract: The Bauer College of Business at the University of Houston uses lists of journals as part of its process to evaluate and reward the research productivity of its faculty. The creation of the journal lists was proposed for three reasons: to target faculty research as to which journal outlets the College deemed acceptable, to encourage cross-disciplinary research, and to decrease the internal politicizing of the journals themselves. This article discusses the history and use of the lists as well as surveying faculty perceptions. The use of the journal list is imperfect. Faculty seem unclear as to the use of the lists and there does not seem to have been a substantial increase in cross-discipline research. While the tenor of the discussions about the merits of a journal has changed, interdepartmental politicizing of journals continues. Consistency of annual reviews, however, has increased substantially.\n",
      "Content: The journal list and its use: motivation, perceptions, and reality The Bauer College of Business at the University of Houston uses lists of journals as part of its process to evaluate and reward the research productivity of its faculty. The creation of the journal lists was proposed for three reasons: to target faculty research as to which journal outlets the College deemed acceptable, to encourage cross-disciplinary research, and to decrease the internal politicizing of the journals themselves. This article discusses the history and use of the lists as well as surveying faculty perceptions. The use of the journal list is imperfect. Faculty seem unclear as to the use of the lists and there does not seem to have been a substantial increase in cross-discipline research. While the tenor of the discussions about the merits of a journal has changed, interdepartmental politicizing of journals continues. Consistency of annual reviews, however, has increased substantially.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 67\n",
      "Title: Perceived Usefulness, Ease of Use, and Usage of Information Technology: A Replication\n",
      "Abstract:  This paper presents the findings of two studies that replicate previous work by Fred Davis on the subject of perceived usefulness, ease of use, and usage of information technology. The two studies focus on evaluating the psychometric properties of the ease of use and usefulness scales, while examining the relationship between ease of use, usefulness, and system usage. Study 1 provides a strong assessment of the convergent validity of the two scales by examining heterogeneous user groups dealing with heterogeneous implementations of messaging technology. In addition, because one might expect users to share similar perspectives about voice and electronic mail, the study also represents a strong test of discriminant validity. In this study a total of 118 respondents from 10 different organizations were surveyed for their attitudes toward two messaging technologies: voice and electronic mail. Study 2 complements the approach taken in Study I by focusing on the ability to demonstrate discriminant validity. Three popular software applications (WordPerfect, and Harvard  Graphics)  were examined based on the expectation that they would all be rated highly on both scales. In this study a total of 73 users rated the three packages in terms of ease of use and usefulness.The results of the studies demonstrate reliable and valid scales for measurement of perceived ease of use and usefulness. In addition, the paper tests: the relationships between ease of use,.. usefulness, and usage using structural equation modelling. The results of this model are consistent with previous research for Study 1, suggesting that usefulness is an important determinant of system use. For Study 2 the results are somewhat mixed, but indicate the importahce of both ease of use and usefulness. Differences.in conditions of usage are explored to explain these findings. \n",
      "Content: Perceived Usefulness, Ease of Use, and Usage of Information Technology: A Replication  This paper presents the findings of two studies that replicate previous work by Fred Davis on the subject of perceived usefulness, ease of use, and usage of information technology. The two studies focus on evaluating the psychometric properties of the ease of use and usefulness scales, while examining the relationship between ease of use, usefulness, and system usage. Study 1 provides a strong assessment of the convergent validity of the two scales by examining heterogeneous user groups dealing with heterogeneous implementations of messaging technology. In addition, because one might expect users to share similar perspectives about voice and electronic mail, the study also represents a strong test of discriminant validity. In this study a total of 118 respondents from 10 different organizations were surveyed for their attitudes toward two messaging technologies: voice and electronic mail. Study 2 complements the approach taken in Study I by focusing on the ability to demonstrate discriminant validity. Three popular software applications (WordPerfect, and Harvard  Graphics)  were examined based on the expectation that they would all be rated highly on both scales. In this study a total of 73 users rated the three packages in terms of ease of use and usefulness.The results of the studies demonstrate reliable and valid scales for measurement of perceived ease of use and usefulness. In addition, the paper tests: the relationships between ease of use,.. usefulness, and usage using structural equation modelling. The results of this model are consistent with previous research for Study 1, suggesting that usefulness is an important determinant of system use. For Study 2 the results are somewhat mixed, but indicate the importahce of both ease of use and usefulness. Differences.in conditions of usage are explored to explain these findings. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 68\n",
      "Title: The many faces of information technology interruptions: a taxonomy and preliminary investigation of their performance effects\n",
      "Abstract: Despite the growing importance of information technology (IT) interruptions for individual work, very little is known about their nature and consequences. This paper develops a taxonomy that classifies interruptions based on the relevance and structure of their content, and propositions that relate different interruption types to individual performance. A qualitative approach combining the use of log diaries of professional workers and semi-structured interviews with product development workers provide a preliminary validation of the taxonomy and propositions and allow for the discovery of a continuum of interruption events that fall in-between the extreme types in the taxonomy. The results show that some IT interruptions have positive effects on individual performance, whilst others have negative effects, or both. The taxonomy developed in the paper allows for a better understanding of the nature of different types of IT interruption and their consequences on individual work. By showing that different types of interruptions have different effects, the paper helps to explain and shed light on the inconsistent results of past research.\n",
      "Content: The many faces of information technology interruptions: a taxonomy and preliminary investigation of their performance effects Despite the growing importance of information technology (IT) interruptions for individual work, very little is known about their nature and consequences. This paper develops a taxonomy that classifies interruptions based on the relevance and structure of their content, and propositions that relate different interruption types to individual performance. A qualitative approach combining the use of log diaries of professional workers and semi-structured interviews with product development workers provide a preliminary validation of the taxonomy and propositions and allow for the discovery of a continuum of interruption events that fall in-between the extreme types in the taxonomy. The results show that some IT interruptions have positive effects on individual performance, whilst others have negative effects, or both. The taxonomy developed in the paper allows for a better understanding of the nature of different types of IT interruption and their consequences on individual work. By showing that different types of interruptions have different effects, the paper helps to explain and shed light on the inconsistent results of past research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 69\n",
      "Title: E-Mail Interruptions and Individual Performance: Is There a Silver Lining?\n",
      "Abstract: The article presents a study on the impact of interruptions of work e-mail and other communication technologies on the performance of individuals. Topics include the use of action regulation theory to determine the direct and indirect impact of incongruent and congruent e-mail interruptions, and the positive impact of technology capabilities such as rehearsing and reprocessing on mindfulness of individuals.\n",
      "Content: E-Mail Interruptions and Individual Performance: Is There a Silver Lining? The article presents a study on the impact of interruptions of work e-mail and other communication technologies on the performance of individuals. Topics include the use of action regulation theory to determine the direct and indirect impact of incongruent and congruent e-mail interruptions, and the positive impact of technology capabilities such as rehearsing and reprocessing on mindfulness of individuals.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 70\n",
      "Title: Theorizing the Multilevel Effects of Interruptions and the Role of Communication Technology\n",
      "Abstract: Our understanding of how interrupting the work of an individual affects group outcomes and the role of communication technologies (CT) in shaping these effects is limited. Drawing upon coordination theory and the literatures on computer-mediated communication and interruptions, this paper develops a multilevel theory of work interruptions. It suggests that interruptions that target individuals can also affect other group members through various ripple effects and a crosslevel direct effect. We also discuss how the usage of five CT capabilities during interruption episodes can moderate the impact of interruptions at the individual and group levels. Our theoretical model draws attention to the importance of examining the individual-to-group processes to better understand the impact of interruptions in group environments. Additionally, by accounting for the role of the use of CT capabilities during interruption episodes, our work contributes to both the interruptions literature, which dedicates scant attention to the interrupting media, and to IS research on media use and media effects.\n",
      "Content: Theorizing the Multilevel Effects of Interruptions and the Role of Communication Technology Our understanding of how interrupting the work of an individual affects group outcomes and the role of communication technologies (CT) in shaping these effects is limited. Drawing upon coordination theory and the literatures on computer-mediated communication and interruptions, this paper develops a multilevel theory of work interruptions. It suggests that interruptions that target individuals can also affect other group members through various ripple effects and a crosslevel direct effect. We also discuss how the usage of five CT capabilities during interruption episodes can moderate the impact of interruptions at the individual and group levels. Our theoretical model draws attention to the importance of examining the individual-to-group processes to better understand the impact of interruptions in group environments. Additionally, by accounting for the role of the use of CT capabilities during interruption episodes, our work contributes to both the interruptions literature, which dedicates scant attention to the interrupting media, and to IS research on media use and media effects.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 71\n",
      "Title: Information Technology and Government Corruption in Developing Countries: Evidence from Ghana Customs\n",
      "Abstract: The literature on information technology (IT) and government corruption in developing countries indicates contradictory evidence about the realization of anti-corruption effects. So far, there is no theoretical explanation of why the anti-corruption potential of IT demonstrated in some countries is not realized in many other countries. Drawing evidence from a case study of information systems interventions at Ghana customs over 35 years, we investigate how and why IT’s anti-corruption potential may be curtailed in the context of developing countries’ governments and societies. We focus on IT-mediated petty corruption practices of street-level officers, which we consider to be socially embedded and institutionally conditioned phenomena. We find that conditions of possibility for the IT-mediated petty corruption practices are created during the implementation of information systems. The configuration of IT and organizational processes of a government agency are constrained by the broader government administration system and influenced by the vested interests of government officers, politicians, and businesses. Subsequently, the co-optation of IT for petty corruption practices is enabled by networks of relationships and institutions of patronage that extend across government, business, and society. We thus explain the often limited effects of IT on petty corruption as the inability of localized information systems implementations to change modes of government administration that are embedded in the enduring neopatrimonial institutions and politics of many developing countries.\n",
      "Content: Information Technology and Government Corruption in Developing Countries: Evidence from Ghana Customs The literature on information technology (IT) and government corruption in developing countries indicates contradictory evidence about the realization of anti-corruption effects. So far, there is no theoretical explanation of why the anti-corruption potential of IT demonstrated in some countries is not realized in many other countries. Drawing evidence from a case study of information systems interventions at Ghana customs over 35 years, we investigate how and why IT’s anti-corruption potential may be curtailed in the context of developing countries’ governments and societies. We focus on IT-mediated petty corruption practices of street-level officers, which we consider to be socially embedded and institutionally conditioned phenomena. We find that conditions of possibility for the IT-mediated petty corruption practices are created during the implementation of information systems. The configuration of IT and organizational processes of a government agency are constrained by the broader government administration system and influenced by the vested interests of government officers, politicians, and businesses. Subsequently, the co-optation of IT for petty corruption practices is enabled by networks of relationships and institutions of patronage that extend across government, business, and society. We thus explain the often limited effects of IT on petty corruption as the inability of localized information systems implementations to change modes of government administration that are embedded in the enduring neopatrimonial institutions and politics of many developing countries.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 72\n",
      "Title: Information technology and public administration modernization in a developing country: Pursuing paperless clearance at Ghana customs\n",
      "Abstract: Despite significant information technology (IT) implementations in public administrations of developing countries to change their dysfunctional traditional practices towards modern forms, the outcomes are typically disappointing relative to the potential of IT for organizational change. With a case study of the customs clearance process when importing goods through Ghana's main port, the Tema Harbour, we explore why IT struggles to modernize traditional practices of public administration in a developing country context. We focus on explaining the persistence of paper use at Ghana customs despite more than a decade of digitalization and automation to establish paperless processes that eliminate malpractices of traditional clearance. We view modernization as a process of long-term institutional change and therefore draw from the literature on IT and institutional change to focus our investigation on the dynamics of incongruent institutional logics of IT and public administration. We show that in the administration context of a developing country like Ghana, the endurance of patrimonial logics in the state and broader society, coupled with high levels of administrative discretion and ambivalent or weak compliance pressure limit the realization of IT for modernization and allows hybrid practices to emerge.\n",
      "Content: Information technology and public administration modernization in a developing country: Pursuing paperless clearance at Ghana customs Despite significant information technology (IT) implementations in public administrations of developing countries to change their dysfunctional traditional practices towards modern forms, the outcomes are typically disappointing relative to the potential of IT for organizational change. With a case study of the customs clearance process when importing goods through Ghana's main port, the Tema Harbour, we explore why IT struggles to modernize traditional practices of public administration in a developing country context. We focus on explaining the persistence of paper use at Ghana customs despite more than a decade of digitalization and automation to establish paperless processes that eliminate malpractices of traditional clearance. We view modernization as a process of long-term institutional change and therefore draw from the literature on IT and institutional change to focus our investigation on the dynamics of incongruent institutional logics of IT and public administration. We show that in the administration context of a developing country like Ghana, the endurance of patrimonial logics in the state and broader society, coupled with high levels of administrative discretion and ambivalent or weak compliance pressure limit the realization of IT for modernization and allows hybrid practices to emerge.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 73\n",
      "Title: Orchestrating a digital platform ecosystem to address societal challenges: A robust action perspective\n",
      "Abstract: Orchestration of digital platform ecosystems has been well examined in the context of markets and the private sector where an orchestrator is a resourceful firm exploiting commercial opportunities in an industry. However, little is known about how it occurs when a government organization orchestrates to address societal challenges. We build upon the construct of ?robust action??identified with chess masters who play to advance a broad strategy while simultaneously maintaining flexibility?to explain how orchestration by a government organization might overcome initiation, stabilization, expansion, and meta-governance challenges through digital platform-enabled participative architecture, multivocal inscriptions, and distributed experimentation. Evidence for our theoretical framework is drawn from empirical studies of the Aadhaar, a platform ecosystem orchestrated to address multiple societal challenges stemming from identity and its management.\n",
      "Content: Orchestrating a digital platform ecosystem to address societal challenges: A robust action perspective Orchestration of digital platform ecosystems has been well examined in the context of markets and the private sector where an orchestrator is a resourceful firm exploiting commercial opportunities in an industry. However, little is known about how it occurs when a government organization orchestrates to address societal challenges. We build upon the construct of ?robust action??identified with chess masters who play to advance a broad strategy while simultaneously maintaining flexibility?to explain how orchestration by a government organization might overcome initiation, stabilization, expansion, and meta-governance challenges through digital platform-enabled participative architecture, multivocal inscriptions, and distributed experimentation. Evidence for our theoretical framework is drawn from empirical studies of the Aadhaar, a platform ecosystem orchestrated to address multiple societal challenges stemming from identity and its management.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 74\n",
      "Title: Effects of media formats on emotions and impulse buying intent\n",
      "Abstract: One way of generating revenue from broadband media content rests upon the assumption that multi-media content may trigger a greater intent to buy products and services impulsively. An experiment was performed in order to explore the effects of media formats on the emotions and impulse buying intentions for music compact discs (CDs). Three distinct media formats of World Wide Web pages were set up: (1) the text of the lyrics, (2) still images from the song's music video and (3) the music video itself. Each had a varying degree of visual/verbal intensity while simultaneously playing the soundtrack in all three conditions. The results of this study indicate that displaying the text of the lyrics had a greater effect on the impulse buying intent than showing still images of the music video. In addition, different media formats caused emotional responses that can explain the participant's impulse buying intent to buy the CD. Unexpectedly, the still images and video did not necessarily generate more buying intention than combinations of the text and music. Therefore, it is recommended that electronic commerce and marketing managers explore innovative ways of integrating visual and verbal media formats for eliciting an effective consumer response.\n",
      "Content: Effects of media formats on emotions and impulse buying intent One way of generating revenue from broadband media content rests upon the assumption that multi-media content may trigger a greater intent to buy products and services impulsively. An experiment was performed in order to explore the effects of media formats on the emotions and impulse buying intentions for music compact discs (CDs). Three distinct media formats of World Wide Web pages were set up: (1) the text of the lyrics, (2) still images from the song's music video and (3) the music video itself. Each had a varying degree of visual/verbal intensity while simultaneously playing the soundtrack in all three conditions. The results of this study indicate that displaying the text of the lyrics had a greater effect on the impulse buying intent than showing still images of the music video. In addition, different media formats caused emotional responses that can explain the participant's impulse buying intent to buy the CD. Unexpectedly, the still images and video did not necessarily generate more buying intention than combinations of the text and music. Therefore, it is recommended that electronic commerce and marketing managers explore innovative ways of integrating visual and verbal media formats for eliciting an effective consumer response.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 75\n",
      "Title: Towards an evidence-based decision making healthcare system management: Modelling patient pathways to improve clinical outcomes\n",
      "Abstract: The concept of patient flow modelling has attracted managers, commissioners and clinicians to better understand the operational and clinical functions of the healthcare system. In this context, the current study has two objectives: First, to introduce a random effects continuation-ratio logit model, suitable for detecting stage wise transitions, to patient pathways modelling. Second, we aim at advancing our knowledge with regard to the application of modelling techniques to patient pathways. We study individual clinical pathways of chronic obstructive pulmonary disease (COPD) patients, a source of concern for major stakeholders. Data on COPD patients were extracted from the national English Hospital Episodes Statistics dataset. Individual patient pathways from initial admission through to more than four readmissions are captured. We notice that as patients are frequently readmitted, males are more likely to be in the higher risk group than females. Furthermore, the number of previous readmissions has a direct impact on the propensity of experiencing a further readmission. This model is very useful in detecting the most critical threshold at which multiple readmissions are more probable. Clinicians should note that a first readmission signifies a problem in the process of care and if care is not taken this may be the beginning of many subsequent readmissions. Our method could easily be implemented as a decision support tool to determine disease specific probabilities of multiple readmissions. Therefore, this could be a valuable tool for clinicians, health care managers, and policy makers for informed decision making in the management of diseases, which ultimately contributes to improved measures for hospital performance management.\n",
      "Content: Towards an evidence-based decision making healthcare system management: Modelling patient pathways to improve clinical outcomes The concept of patient flow modelling has attracted managers, commissioners and clinicians to better understand the operational and clinical functions of the healthcare system. In this context, the current study has two objectives: First, to introduce a random effects continuation-ratio logit model, suitable for detecting stage wise transitions, to patient pathways modelling. Second, we aim at advancing our knowledge with regard to the application of modelling techniques to patient pathways. We study individual clinical pathways of chronic obstructive pulmonary disease (COPD) patients, a source of concern for major stakeholders. Data on COPD patients were extracted from the national English Hospital Episodes Statistics dataset. Individual patient pathways from initial admission through to more than four readmissions are captured. We notice that as patients are frequently readmitted, males are more likely to be in the higher risk group than females. Furthermore, the number of previous readmissions has a direct impact on the propensity of experiencing a further readmission. This model is very useful in detecting the most critical threshold at which multiple readmissions are more probable. Clinicians should note that a first readmission signifies a problem in the process of care and if care is not taken this may be the beginning of many subsequent readmissions. Our method could easily be implemented as a decision support tool to determine disease specific probabilities of multiple readmissions. Therefore, this could be a valuable tool for clinicians, health care managers, and policy makers for informed decision making in the management of diseases, which ultimately contributes to improved measures for hospital performance management.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 76\n",
      "Title: Efficient clustering of databases induced by local patterns\n",
      "Abstract: Many large organizations have multiple large databases as they transact from multiple branches. Most of the previous pieces of work are based on a single database. Thus, it is necessary to study data mining on multiple databases. In this paper, we propose two measures of similarity between a pair of databases. Also, we propose an algorithm for clustering a set of databases. Efficiency of the clustering process has been improved using the following strategies: reducing execution time of clustering algorithm, using more appropriate similarity measure, and storing frequent itemsets space efficiently.\n",
      "Content: Efficient clustering of databases induced by local patterns Many large organizations have multiple large databases as they transact from multiple branches. Most of the previous pieces of work are based on a single database. Thus, it is necessary to study data mining on multiple databases. In this paper, we propose two measures of similarity between a pair of databases. Also, we propose an algorithm for clustering a set of databases. Efficiency of the clustering process has been improved using the following strategies: reducing execution time of clustering algorithm, using more appropriate similarity measure, and storing frequent itemsets space efficiently.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 77\n",
      "Title: The Effects of Tree-View Based Presentation Adaptation on Mobile Web Browsing\n",
      "Abstract: Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.\n",
      "Content: The Effects of Tree-View Based Presentation Adaptation on Mobile Web Browsing Accessing the Web from mobile handheld devices has become increasingly common. However, accomplishing that task remains challenging mainly due to the physical constraints of handheld devices and the static presentation of Web pages. Adapting the presentation of Web pages is, therefore, critical to enabling effective mobile Web browsing and information searching. Based on cognitive fit theory and information foraging theory, we propose a novel hybrid approach to adapting Web page presentation that integrates three types of adaptation techniques, namely tree-view, hierarchical text summarization, and colored keyword highlighting. By following the design science research framework, we implemented the proposed approach on handheld devices and empirically evaluated the effects of presentation adaptation on mobile Web browsing. The results show that presentation adaptation significantly improves user performance and perception of mobile Web browsing. We also discover that the positive impact of presentation adaptation is moderated by the complexity of an information search task. The findings have significant theoretical and practical implications for the design and implementation of mobile Web applications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 78\n",
      "Title: Reducing Medicare Spending Through Electronic Health Information Exchange: The Role of Incentives and Exchange Maturity\n",
      "Abstract: Health information exchanges (HIEs) are entities that have emerged in healthcare delivery markets across the United States. By providing an interorganizational information system (IOIS) and governance over use of this system and the information exchanged through it, HIEs enable more routine and efficient electronic sharing of patient information between disparate and fragmented healthcare providers. This should result in improved quality and efficiency of care. However, significant questions persist about the extent to which HIEs produce these benefits in practice, particularly in terms of reducing healthcare spending. We use transaction cost economics (TCE) to theorize that HIEs establish a quasi-hierarchy that decreases frictions associated with information sharing in ways that reduce healthcare spending, and that the magnitude of reductions is greater when (1) insurer and provider incentives align, and (2) HIE capabilities mature. We can test these conjectures because HIEs, unlike other efforts that provide IOIS, are typically confined to regional markets and develop heterogeneously between these markets, introducing variation in insurer-provider incentive alignment and HIE maturity. Leveraging a unique national panel data set, we evaluate whether HIEs reduce spending for the largest insurer in the United States, i.e., Medicare, and whether incentives and HIE maturity modify the magnitude of reductions. We find significant spending reductions in healthcare markets that have established operational HIEs, with an average reduction of $139 per Medicare beneficiary per year (1.4% decrease) or a $3.12 billion annual reduction in spending if HIEs were nationally implemented in 2015. We also find that these reductions occur disproportionately in healthcare markets where providers have financial incentives to use an HIE to reduce spending and when HIEs are more mature. Our results inform an important open empirical question in the healthcare domain related to the value of HIEs, while also joining perspectives from TCE with the IOIS literature to understand the factors that may be relevant to IOIS value creation more generally.\n",
      "Content: Reducing Medicare Spending Through Electronic Health Information Exchange: The Role of Incentives and Exchange Maturity Health information exchanges (HIEs) are entities that have emerged in healthcare delivery markets across the United States. By providing an interorganizational information system (IOIS) and governance over use of this system and the information exchanged through it, HIEs enable more routine and efficient electronic sharing of patient information between disparate and fragmented healthcare providers. This should result in improved quality and efficiency of care. However, significant questions persist about the extent to which HIEs produce these benefits in practice, particularly in terms of reducing healthcare spending. We use transaction cost economics (TCE) to theorize that HIEs establish a quasi-hierarchy that decreases frictions associated with information sharing in ways that reduce healthcare spending, and that the magnitude of reductions is greater when (1) insurer and provider incentives align, and (2) HIE capabilities mature. We can test these conjectures because HIEs, unlike other efforts that provide IOIS, are typically confined to regional markets and develop heterogeneously between these markets, introducing variation in insurer-provider incentive alignment and HIE maturity. Leveraging a unique national panel data set, we evaluate whether HIEs reduce spending for the largest insurer in the United States, i.e., Medicare, and whether incentives and HIE maturity modify the magnitude of reductions. We find significant spending reductions in healthcare markets that have established operational HIEs, with an average reduction of $139 per Medicare beneficiary per year (1.4% decrease) or a $3.12 billion annual reduction in spending if HIEs were nationally implemented in 2015. We also find that these reductions occur disproportionately in healthcare markets where providers have financial incentives to use an HIE to reduce spending and when HIEs are more mature. Our results inform an important open empirical question in the healthcare domain related to the value of HIEs, while also joining perspectives from TCE with the IOIS literature to understand the factors that may be relevant to IOIS value creation more generally.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 79\n",
      "Title: Does Analytics Help Resolve Equivocality in the Healthcare Context? Contrasting the Effects of Analyzability and Differentiation\n",
      "Abstract:  Organizations are increasingly using data analytics to help make decisions and drive positive outcomes. But organizational scholarship has warned us that the sort of information processing associated with analytic capabilities, while effective for uncertainty reduction, may be less effective in equivocal contexts. Equivocality is evident when tasks are not easily analyzable (task analyzability) or when organizational departments are highly differentiated (differentiation). We hypothesize that analytics will be less effective in driving positive outcomes when equivocality is high because of low task analyzability. However, when an organization is more differentiated, resulting in high equivocality, we anticipate that analytics will be more effective in driving positive outcomes. To test this theory, we studied how clinical healthcare analytics influenced experiential quality (akin to patient satisfaction) in over 3,000 hospitals across nine years. Our results show that analytics capabilities, on average, do improve outcomes in terms of patient experiential quality, suggesting that analytics can reduce uncertainty, but we also found evidence for the moderating role of equivocality. Specifically, as task analyzability decreases (i.e., increasing equivocality), clinical healthcare analytics becomes less effective in improving experiential quality. However, when equivocality is high because of differentiation, there is a positive relationship between clinical healthcare analytics and experiential quality but only in larger hospitals. From a managerial perspective, this study has implications for boundary conditions of data analytics in organizations. \n",
      "Content: Does Analytics Help Resolve Equivocality in the Healthcare Context? Contrasting the Effects of Analyzability and Differentiation  Organizations are increasingly using data analytics to help make decisions and drive positive outcomes. But organizational scholarship has warned us that the sort of information processing associated with analytic capabilities, while effective for uncertainty reduction, may be less effective in equivocal contexts. Equivocality is evident when tasks are not easily analyzable (task analyzability) or when organizational departments are highly differentiated (differentiation). We hypothesize that analytics will be less effective in driving positive outcomes when equivocality is high because of low task analyzability. However, when an organization is more differentiated, resulting in high equivocality, we anticipate that analytics will be more effective in driving positive outcomes. To test this theory, we studied how clinical healthcare analytics influenced experiential quality (akin to patient satisfaction) in over 3,000 hospitals across nine years. Our results show that analytics capabilities, on average, do improve outcomes in terms of patient experiential quality, suggesting that analytics can reduce uncertainty, but we also found evidence for the moderating role of equivocality. Specifically, as task analyzability decreases (i.e., increasing equivocality), clinical healthcare analytics becomes less effective in improving experiential quality. However, when equivocality is high because of differentiation, there is a positive relationship between clinical healthcare analytics and experiential quality but only in larger hospitals. From a managerial perspective, this study has implications for boundary conditions of data analytics in organizations. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 80\n",
      "Title: Beyond the Privacy Paradox: Objective Versus Relative Risk in Privacy Decision Making\n",
      "Abstract: The article presents a study on both the objective and relative risks involved with privacy decision making. Topics include the impact of changes in the objective risk of disclosure and the impact of changes in the relative perceptions of risk of disclosure on hypothetical and actual consumer privacy choices, a decrease in objective risk going from hypothetical to actual choice settings, and an increase in relative risk going from hypothetical to actual choice settings.\n",
      "Content: Beyond the Privacy Paradox: Objective Versus Relative Risk in Privacy Decision Making The article presents a study on both the objective and relative risks involved with privacy decision making. Topics include the impact of changes in the objective risk of disclosure and the impact of changes in the relative perceptions of risk of disclosure on hypothetical and actual consumer privacy choices, a decrease in objective risk going from hypothetical to actual choice settings, and an increase in relative risk going from hypothetical to actual choice settings.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 81\n",
      "Title: Using group support systems for strategic planning with the United States Air Force\n",
      "Abstract: Strategic planning is a critical part of establishing an organization's direction. Although strategic planning is utilized throughout the United States Air Force today in various forms, group sessions can become time-consuming without structured planning and a focus on group communication. Computer-supported strategic planning, making effective use of technology, is one way to improve the strategic planning process. This research implements a group support system (GSS) as a communication tool to facilitate the strategic planning process. The researchers investigate effects of a facilitator's using technology to structure verbal and electronic communication, with the goal of increasing quality output and improving group member satisfaction. This project was completed at Mountain Home Air Force Base with the support of the 366th Wing. As predicted, a GSS facilitator's structuring verbal and electronic communication improved the quality of the strategic plan, reduced time to complete a strategic plan, and increased satisfaction with the strategic planning process. The results did not indicate increased commitment to implement the strategic plans developed by a group using GSS facilitation.\n",
      "Content: Using group support systems for strategic planning with the United States Air Force Strategic planning is a critical part of establishing an organization's direction. Although strategic planning is utilized throughout the United States Air Force today in various forms, group sessions can become time-consuming without structured planning and a focus on group communication. Computer-supported strategic planning, making effective use of technology, is one way to improve the strategic planning process. This research implements a group support system (GSS) as a communication tool to facilitate the strategic planning process. The researchers investigate effects of a facilitator's using technology to structure verbal and electronic communication, with the goal of increasing quality output and improving group member satisfaction. This project was completed at Mountain Home Air Force Base with the support of the 366th Wing. As predicted, a GSS facilitator's structuring verbal and electronic communication improved the quality of the strategic plan, reduced time to complete a strategic plan, and increased satisfaction with the strategic planning process. The results did not indicate increased commitment to implement the strategic plans developed by a group using GSS facilitation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 82\n",
      "Title: A complexity perspective on collaborative decision making in organizations: The ecology of group-performance\n",
      "Abstract: Networks of communication are essential when managing corporate work and performing information exchange; the systems must allow them to be dynamic and well-structured. They help provide high organizational performance and innovative capacity in today's knowledge intense corporations, and this means that organizations must manage the networks strategically. Despite the fact that practitioners are aware of the huge influence of informal communication on decision making, little is known about the underlying principles of efficient employee network collaboration, which is dynamic in nature, especially for complex environments resulting from steady innovation and high competitive pressure. We addressed this issue from a complexity perspective, using an agent based simulation to visualize the key elements of efficient, information-based, collaborative decision making. Our findings suggested that information and communication technologies (ICT) may not be able to leverage corporate performance of the increasingly complex adaptive organizations. There seem to be elementary natural constraints on the cognitive capacities of people dealing with and managing information. Rather than a better technical approach, a more ecologic one is therefore advocated as the best way to improve decision making.\n",
      "Content: A complexity perspective on collaborative decision making in organizations: The ecology of group-performance Networks of communication are essential when managing corporate work and performing information exchange; the systems must allow them to be dynamic and well-structured. They help provide high organizational performance and innovative capacity in today's knowledge intense corporations, and this means that organizations must manage the networks strategically. Despite the fact that practitioners are aware of the huge influence of informal communication on decision making, little is known about the underlying principles of efficient employee network collaboration, which is dynamic in nature, especially for complex environments resulting from steady innovation and high competitive pressure. We addressed this issue from a complexity perspective, using an agent based simulation to visualize the key elements of efficient, information-based, collaborative decision making. Our findings suggested that information and communication technologies (ICT) may not be able to leverage corporate performance of the increasingly complex adaptive organizations. There seem to be elementary natural constraints on the cognitive capacities of people dealing with and managing information. Rather than a better technical approach, a more ecologic one is therefore advocated as the best way to improve decision making.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 83\n",
      "Title: Participatory sociotechnical design of organizations and information systems – an adaptation of ETHICS methodology\n",
      "Abstract: This paper examines a practical adaptation of the ETHICS methodology used in redesigning an information technology (IT) support service in an academic setting. The purpose of the project was to design appropriate organizational structures and functions and an accompanying information system (IS), to increase the effectiveness of the existing service. A participative sociotechnical approach was adopted for the entire design process which was carried out by the practitioners themselves. The staff’s views were elicited during informal participatory group sessions as well as in one-to-one informal discussions. While ETHICS was the overall guiding methodology for the design, QUICKethics was used as a complementary means of analysing the requirements of the new IS. This paper describes the methodology used and the design process; it reflects on the adaptation and its match with the ETHICS methodology, exploring the claimed association with the viable systems methodology and concludes with suggestions for further research.\n",
      "Content: Participatory sociotechnical design of organizations and information systems – an adaptation of ETHICS methodology This paper examines a practical adaptation of the ETHICS methodology used in redesigning an information technology (IT) support service in an academic setting. The purpose of the project was to design appropriate organizational structures and functions and an accompanying information system (IS), to increase the effectiveness of the existing service. A participative sociotechnical approach was adopted for the entire design process which was carried out by the practitioners themselves. The staff’s views were elicited during informal participatory group sessions as well as in one-to-one informal discussions. While ETHICS was the overall guiding methodology for the design, QUICKethics was used as a complementary means of analysing the requirements of the new IS. This paper describes the methodology used and the design process; it reflects on the adaptation and its match with the ETHICS methodology, exploring the claimed association with the viable systems methodology and concludes with suggestions for further research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 84\n",
      "Title: Reducing Recommender System Biases: An Investigation of Rating Display Designs\n",
      "Abstract: Prior research has shown that online recommendations have a significant influence on consumers' preference ratings and economic behavior. Specifically, biases induced by observing personalized system recommendations can lead to distortions in users' self-reported preference ratings after consumption of an item, thus contaminating the users' subsequent inputs to the recommender system. This, in turn, provides the system with an inaccurate view of user preferences and opens up possibilities of rating manipulation. As recommender systems continue to become increasingly popular in today's online environments, preventing or reducing such system-induced biases constitutes a highly important and practical research problem. In this paper, we address this problem via the analysis of different rating display designs for the purpose of proactively preventing biases before they occur (i.e., at rating collection time). We use randomized laboratory experimentation to test how the presentation format of personalized recommendations affects the biases generated in post-consumption preference ratings. We demonstrate that graphical rating display designs of recommender systems are more advantageous than numerical designs in reducing the biases, although none are able to remove biases completely. We also show that scale compatibility is a contributing mechanism operating to create these biases, although not the only one. Together, the results have practical implications for the design and implementation of recommender systems as well as theoretical implications for the study of recommendation biases.\n",
      "Content: Reducing Recommender System Biases: An Investigation of Rating Display Designs Prior research has shown that online recommendations have a significant influence on consumers' preference ratings and economic behavior. Specifically, biases induced by observing personalized system recommendations can lead to distortions in users' self-reported preference ratings after consumption of an item, thus contaminating the users' subsequent inputs to the recommender system. This, in turn, provides the system with an inaccurate view of user preferences and opens up possibilities of rating manipulation. As recommender systems continue to become increasingly popular in today's online environments, preventing or reducing such system-induced biases constitutes a highly important and practical research problem. In this paper, we address this problem via the analysis of different rating display designs for the purpose of proactively preventing biases before they occur (i.e., at rating collection time). We use randomized laboratory experimentation to test how the presentation format of personalized recommendations affects the biases generated in post-consumption preference ratings. We demonstrate that graphical rating display designs of recommender systems are more advantageous than numerical designs in reducing the biases, although none are able to remove biases completely. We also show that scale compatibility is a contributing mechanism operating to create these biases, although not the only one. Together, the results have practical implications for the design and implementation of recommender systems as well as theoretical implications for the study of recommendation biases.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 85\n",
      "Title: Do Recommender Systems Manipulate Consumer Preferences? A Study of Anchoring Effects\n",
      "Abstract: Recommender systems are becoming a salient part of many e-commerce websites. Much research has focused on advancing recommendation technologies to improve accuracy of predictions, although behavioral aspects of using recommender systems are often overlooked. In our studies, we explore how consumer preferences at the time of consumption are impacted by predictions generated by recommender systems. We conducted three controlled laboratory experiments to explore the effects of system recommendations on preferences. Studies 1 and 2 investigated user preferences for television programs across a variety of conditions, which were surveyed immediately following program viewing. Study 3 investigated the granularity of the observed effects within individual participants. Results provide strong evidence that the rating presented by a recommender system serves as an anchor for the consumer's constructed preference. Viewers' preference ratings are malleable and can be significantly influenced by the recommendation received. The effect is sensitive to the perceived reliability of a recommender system and, thus, not a purely numerical or priming-based effect. Finally, the effect of anchoring is continuous and linear, operating over a range of perturbations of the system. These general findings have a number of important implications (e.g., on recommender systems performance metrics and design, preference bias, potential strategic behavior, and trust), which are discussed.\n",
      "Content: Do Recommender Systems Manipulate Consumer Preferences? A Study of Anchoring Effects Recommender systems are becoming a salient part of many e-commerce websites. Much research has focused on advancing recommendation technologies to improve accuracy of predictions, although behavioral aspects of using recommender systems are often overlooked. In our studies, we explore how consumer preferences at the time of consumption are impacted by predictions generated by recommender systems. We conducted three controlled laboratory experiments to explore the effects of system recommendations on preferences. Studies 1 and 2 investigated user preferences for television programs across a variety of conditions, which were surveyed immediately following program viewing. Study 3 investigated the granularity of the observed effects within individual participants. Results provide strong evidence that the rating presented by a recommender system serves as an anchor for the consumer's constructed preference. Viewers' preference ratings are malleable and can be significantly influenced by the recommendation received. The effect is sensitive to the perceived reliability of a recommender system and, thus, not a purely numerical or priming-based effect. Finally, the effect of anchoring is continuous and linear, operating over a range of perturbations of the system. These general findings have a number of important implications (e.g., on recommender systems performance metrics and design, preference bias, potential strategic behavior, and trust), which are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 86\n",
      "Title: Effects of Online Recommendations on Consumers’ Willingness to Pay\n",
      "Abstract: Recommender systems are an integral part of the online retail environment. Prior research has focused largely on computational approaches to improving recommendation accuracy, and only recently researchers have started to study their behavioral implications and potential side effects. We used three controlled experiments, in the context of purchasing digital songs, to explore the willingness-to-pay judgments of individual consumers after being shown personalized recommendations. In Study 1, we found strong evidence that randomly assigned song recommendations affected participants’ willingness to pay, even when controlling for participants’ preferences and demographics. In Study 2, participants viewed actual system-generated recommendations that were intentionally perturbed (introducing recommendation error), and we observed similar effects. In Study 3, we showed that the influence of personalized recommendations on willingness-to-pay judgments was obtained even when preference uncertainty was reduced through immediate and mandatory song sampling prior to pricing. The results demonstrate the existence of important economic side effects of personalized recommender systems and inform our understanding of how system recommendations can influence our everyday preference judgments. The findings have significant implications for the design and application of recommender systems as well as for online retail practices.The online appendix is available at https://doi-org.tudelft.idm.oclc.org/10.1287/isre.2017.0703.\n",
      "Content: Effects of Online Recommendations on Consumers’ Willingness to Pay Recommender systems are an integral part of the online retail environment. Prior research has focused largely on computational approaches to improving recommendation accuracy, and only recently researchers have started to study their behavioral implications and potential side effects. We used three controlled experiments, in the context of purchasing digital songs, to explore the willingness-to-pay judgments of individual consumers after being shown personalized recommendations. In Study 1, we found strong evidence that randomly assigned song recommendations affected participants’ willingness to pay, even when controlling for participants’ preferences and demographics. In Study 2, participants viewed actual system-generated recommendations that were intentionally perturbed (introducing recommendation error), and we observed similar effects. In Study 3, we showed that the influence of personalized recommendations on willingness-to-pay judgments was obtained even when preference uncertainty was reduced through immediate and mandatory song sampling prior to pricing. The results demonstrate the existence of important economic side effects of personalized recommender systems and inform our understanding of how system recommendations can influence our everyday preference judgments. The findings have significant implications for the design and application of recommender systems as well as for online retail practices.The online appendix is available at https://doi-org.tudelft.idm.oclc.org/10.1287/isre.2017.0703.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 87\n",
      "Title: Effects of Personalized Recommendations Versus Aggregate Ratings on Post-Consumption Preference Responses\n",
      "Abstract: Online retailers use product ratings to signal quality and help consumers identify products for purchase. These ratings commonly take the form of either non-personalized, aggregate product ratings (i.e., the average rating a product received from a number of consumers such as “the average rating is 4.5/5 based on 100 reviews”), or personalized predicted preference ratings for a product (i.e., recommender-system-generated predictions for a consumer’s rating of a product such as “we think you’d rate this product 4.5/5”). Ratings in either format can provide decision aid to the consumer, but the two formats convey different types of product quality information and operate with different psychological mechanisms. Prior research has indicated that each recommendation type can significantly affect consumer’s post-experience preference ratings, constituting a judgmental bias, but has not compared the effects of these two common product-rating formats. Using a laboratory experiment, we show that aggregate ratings and personalized recommendations create similar biases on post-experience preference ratings when shown separately. Shown together, there is no cumulative increase in the effect. Instead, personalized recommendations tend to dominate. Our findings can help retailers determine how to use these different types of product ratings to most effectively serve their customers. Additionally, these results help to educate the consumer on how product-rating displays influence their stated preferences.\n",
      "Content: Effects of Personalized Recommendations Versus Aggregate Ratings on Post-Consumption Preference Responses Online retailers use product ratings to signal quality and help consumers identify products for purchase. These ratings commonly take the form of either non-personalized, aggregate product ratings (i.e., the average rating a product received from a number of consumers such as “the average rating is 4.5/5 based on 100 reviews”), or personalized predicted preference ratings for a product (i.e., recommender-system-generated predictions for a consumer’s rating of a product such as “we think you’d rate this product 4.5/5”). Ratings in either format can provide decision aid to the consumer, but the two formats convey different types of product quality information and operate with different psychological mechanisms. Prior research has indicated that each recommendation type can significantly affect consumer’s post-experience preference ratings, constituting a judgmental bias, but has not compared the effects of these two common product-rating formats. Using a laboratory experiment, we show that aggregate ratings and personalized recommendations create similar biases on post-experience preference ratings when shown separately. Shown together, there is no cumulative increase in the effect. Instead, personalized recommendations tend to dominate. Our findings can help retailers determine how to use these different types of product ratings to most effectively serve their customers. Additionally, these results help to educate the consumer on how product-rating displays influence their stated preferences.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 88\n",
      "Title: Bundling Effects on Variety Seeking for Digital Information Goods\n",
      "Abstract: Prior research with consumable goods has consistently found that consumers have a preference for greater variety when selecting items simultaneously as a bundle, rather than as a sequential series of individual decisions. However, digital information goods have a number of important differences from consumable goods that may impact variety-seeking behavior. In three experiments, we address two general research questions. First, as a precursor to studying digital goods, we disentangle the role of bundle cohesion (i.e., item relatedness) from the role of timing (simultaneous vs. sequential choice) as factors in variety seeking with consumable goods. Next, based on differences between digital and consumable goods, we theorize differences in the behavioral effects of bundle cohesion and timing on variety preferences for digital goods. The results show a reduction of influences upon variety-seeking behavior with digital goods, providing important implications for the sellers of such goods in contrast to what has been suggested for consumable goods. Therefore, a key takeaway is that, for digital goods such as music, the use of consumer-driven bundling variations does not suggest an advantage in terms of their ability to affect consumers’ variety-seeking behavior.\n",
      "Content: Bundling Effects on Variety Seeking for Digital Information Goods Prior research with consumable goods has consistently found that consumers have a preference for greater variety when selecting items simultaneously as a bundle, rather than as a sequential series of individual decisions. However, digital information goods have a number of important differences from consumable goods that may impact variety-seeking behavior. In three experiments, we address two general research questions. First, as a precursor to studying digital goods, we disentangle the role of bundle cohesion (i.e., item relatedness) from the role of timing (simultaneous vs. sequential choice) as factors in variety seeking with consumable goods. Next, based on differences between digital and consumable goods, we theorize differences in the behavioral effects of bundle cohesion and timing on variety preferences for digital goods. The results show a reduction of influences upon variety-seeking behavior with digital goods, providing important implications for the sellers of such goods in contrast to what has been suggested for consumable goods. Therefore, a key takeaway is that, for digital goods such as music, the use of consumer-driven bundling variations does not suggest an advantage in terms of their ability to affect consumers’ variety-seeking behavior.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 89\n",
      "Title: Making Sense of Technology Trends in the Information Technology Landscape: A Design Science Approach\n",
      "Abstract: A major problem for firms making information technology investment decisions is predicting and understanding the effects of future technological developments on the value of present technologies. Failure to adequately address this problem can result in wasted organization resources in acquiring, developing, managing, and training employees in the use of technologies that are short-lived and fail to produce adequate return on investment. The sheer number of available technologies and the complex set of relationships among them make IT landscape analysis extremely challenging. Most IT-consuming firms rely on third parties and suppliers for strategic recommendations on IT investments, which can lead to biased and generic advice. We address this problem by defining a new set of constructs and methodologies upon which we develop an IT ecosystem model. The objective of these artifacts is to provide a formal problem representation structure for the analysis of information technology development trends and to reduce the complexity of the IT landscape for practitioners making IT investment decisions. We adopt a process theory perspective and use a combination of visual mapping and quantification strategies to develop our artifacts and a state diagram-based technique to represent evolutionary transitions over time. We illustrate our approach using two exemplars: digital music technologies and wireless networking technologies. We evaluate the utility of our approach by conducting in-depth interviews with IT industry experts and demonstrate the contribution of our approach relative to existing techniques for technology forecasting.\n",
      "Content: Making Sense of Technology Trends in the Information Technology Landscape: A Design Science Approach A major problem for firms making information technology investment decisions is predicting and understanding the effects of future technological developments on the value of present technologies. Failure to adequately address this problem can result in wasted organization resources in acquiring, developing, managing, and training employees in the use of technologies that are short-lived and fail to produce adequate return on investment. The sheer number of available technologies and the complex set of relationships among them make IT landscape analysis extremely challenging. Most IT-consuming firms rely on third parties and suppliers for strategic recommendations on IT investments, which can lead to biased and generic advice. We address this problem by defining a new set of constructs and methodologies upon which we develop an IT ecosystem model. The objective of these artifacts is to provide a formal problem representation structure for the analysis of information technology development trends and to reduce the complexity of the IT landscape for practitioners making IT investment decisions. We adopt a process theory perspective and use a combination of visual mapping and quantification strategies to develop our artifacts and a state diagram-based technique to represent evolutionary transitions over time. We illustrate our approach using two exemplars: digital music technologies and wireless networking technologies. We evaluate the utility of our approach by conducting in-depth interviews with IT industry experts and demonstrate the contribution of our approach relative to existing techniques for technology forecasting.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 90\n",
      "Title: Modeling Supply-Side Dynamics of IT Components, Products, and Infrastructure: An Empirical Analysis Using Vector Autoregression\n",
      "Abstract: Prior IS research on technological change has focused primarily on organizational information systems and technology innovation; however, there is a growing need to understand the dynamics of supply-side forces in the introduction of new technologies. In this paper we investigate how the interdependencies among information technology components, products, and infrastructure affect the release of new technologies. Going beyond the ad hoc heuristic approaches applied in previous studies, we empirically validate the existence of several patterns of supply-side technology relationships in the context of wireless networking. We use vector autoregression (VAR) to model the comovements of new component, product, and infrastructure introductions and provide evidence of strong Granger-causal interdependencies. We also demonstrate that substantial improvements in forecasting can be gained by incorporating these cross-level effects into models of technological change. This paper provides some of the first research that empirically demonstrates these cross-level effects and also provides an exposition of VAR methodology for both analysis and forecasting in IS research.\n",
      "Content: Modeling Supply-Side Dynamics of IT Components, Products, and Infrastructure: An Empirical Analysis Using Vector Autoregression Prior IS research on technological change has focused primarily on organizational information systems and technology innovation; however, there is a growing need to understand the dynamics of supply-side forces in the introduction of new technologies. In this paper we investigate how the interdependencies among information technology components, products, and infrastructure affect the release of new technologies. Going beyond the ad hoc heuristic approaches applied in previous studies, we empirically validate the existence of several patterns of supply-side technology relationships in the context of wireless networking. We use vector autoregression (VAR) to model the comovements of new component, product, and infrastructure introductions and provide evidence of strong Granger-causal interdependencies. We also demonstrate that substantial improvements in forecasting can be gained by incorporating these cross-level effects into models of technological change. This paper provides some of the first research that empirically demonstrates these cross-level effects and also provides an exposition of VAR methodology for both analysis and forecasting in IS research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 91\n",
      "Title: Impact of Information Feedback in Continuous Combinatorial Auctions: An Experimental Study of Economic Performance\n",
      "Abstract: Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions-in which bidders can bid on combinations of goods-have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidder scan submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism\n",
      "Content: Impact of Information Feedback in Continuous Combinatorial Auctions: An Experimental Study of Economic Performance Advancements in information technology offer opportunities for designing and deploying innovative market mechanisms that can improve the allocation and procurement processes of businesses. For example, combinatorial auctions-in which bidders can bid on combinations of goods-have been shown to increase the economic efficiency of a trade when goods have complementarities. However, the lack of real-time decision support tools for bidders has prevented this mechanism from reaching its full potential. With the objective of facilitating bidder participation in combinatorial auctions, this study, using recent research in real-time bidder support metrics, discusses several novel feedback schemes that can aid bidders in formulating combinatorial bids in real-time. The feedback schemes allow us to conduct continuous combinatorial auctions, where bidder scan submit bids at any time. Using laboratory experiments with two different setups, we compare the economic performance of the continuous mechanism under three progressively advanced levels of feedback. Our findings indicate that information feedback plays a major role in influencing the economic outcomes of combinatorial auctions. We compare several important bid characteristics to explain the observed differences in aggregate measures. This study advances the ongoing research on combinatorial auctions by developing continuous auctions that differentiate themselves from earlier combinatorial auction mechanisms by facilitating free flowing participation of bidders and providing exact prices of bundles on demand in real time. For practitioners, the study provides insights on how the nature of feedback can influence the economic outcomes of a complex trading mechanism\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 92\n",
      "Title: Designing Real-Time Feedback for Bidders in Homogeneous-Item Continuous Combinatorial Auctions\n",
      "Abstract: Although combinatorial auctions are important mechanisms for many specialized applications, their adoption in general-purpose marketplaces is still fairly limited, partly due to the inherent difficulty in evaluating the efficacy of bids without the availability of comprehensive bidder support. In this paper, we present both theoretical results and computational designs to support real-time feedback to bidders in continuous combinatorial auctions, where bidders are free to join and leave the auction at any time. In particular, we focus on the broad class of single-item multi-unit (SIMU) combinatorial auctions, where multiple identical units of one homogenous item are being auctioned. We also consider two common ways to express bidding preferences: OR bids and XOR bids. For SIMU auctions with each of the two bid types, we present comprehensive analyses of auction dynamics, which can determine winning bids that satisfy allocative fairness, and compute critical evaluative metrics needed to provide bidder support, including bid winning and deadness levels. We also design the data structures and algorithms needed to provide bidder support in real time for SIMU auctions of practically relevant sizes. The computational tools proposed in this paper can facilitate the efficient and more transparent implementation of SIMU combinatorial auctions in business- and consumer-oriented markets.\n",
      "Content: Designing Real-Time Feedback for Bidders in Homogeneous-Item Continuous Combinatorial Auctions Although combinatorial auctions are important mechanisms for many specialized applications, their adoption in general-purpose marketplaces is still fairly limited, partly due to the inherent difficulty in evaluating the efficacy of bids without the availability of comprehensive bidder support. In this paper, we present both theoretical results and computational designs to support real-time feedback to bidders in continuous combinatorial auctions, where bidders are free to join and leave the auction at any time. In particular, we focus on the broad class of single-item multi-unit (SIMU) combinatorial auctions, where multiple identical units of one homogenous item are being auctioned. We also consider two common ways to express bidding preferences: OR bids and XOR bids. For SIMU auctions with each of the two bid types, we present comprehensive analyses of auction dynamics, which can determine winning bids that satisfy allocative fairness, and compute critical evaluative metrics needed to provide bidder support, including bid winning and deadness levels. We also design the data structures and algorithms needed to provide bidder support in real time for SIMU auctions of practically relevant sizes. The computational tools proposed in this paper can facilitate the efficient and more transparent implementation of SIMU combinatorial auctions in business- and consumer-oriented markets.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 93\n",
      "Title: Effect of Information Feedback on the Outcomes and Dynamics of Multisourcing Multiattribute Procurement Auctions\n",
      "Abstract: Electronic auctions are increasingly being used to facilitate the procurement of goods and services in organizations. Multiattribute auctions, which allow bids on multiple dimensions of the product and not just price, are information technology-enabled sourcing mechanisms that can increase the efficiency of procurement for configurable goods and services compared to price-only auctions. Given the strategic nature of procurement auctions, the amount of information concerning the buyer's preferences that is disclosed to the suppliers has implications on the profits of the buyer and the suppliers and, consequently, on the long-term relationship between them. This study explores novel feedback schemes for multisourcing multiattribute auctions that require limited exchange of strategic information between the buyer and the suppliers. To study the impact of feedback on the outcomes and dynamics of the auctions, we conduct laboratory experiments wherein we analyze bidder behavior and economic outcomes under three different treatment conditions with different types of information feedback. Our results indicate that, in contrast to winner-take-all multiattribute auctions, multisourcing multiattribute auctions, with potentially multiple winners, allow bidders (i.e., suppliers) to extract more profit when greater transparency in terms of provisional allocations and prices is provided. We develop several insights for mechanism designers toward developing sustainable procurement auctions that efficiently allocate multiple units of an asset with multiple negotiable attributes among multiple suppliers.\n",
      "Content: Effect of Information Feedback on the Outcomes and Dynamics of Multisourcing Multiattribute Procurement Auctions Electronic auctions are increasingly being used to facilitate the procurement of goods and services in organizations. Multiattribute auctions, which allow bids on multiple dimensions of the product and not just price, are information technology-enabled sourcing mechanisms that can increase the efficiency of procurement for configurable goods and services compared to price-only auctions. Given the strategic nature of procurement auctions, the amount of information concerning the buyer's preferences that is disclosed to the suppliers has implications on the profits of the buyer and the suppliers and, consequently, on the long-term relationship between them. This study explores novel feedback schemes for multisourcing multiattribute auctions that require limited exchange of strategic information between the buyer and the suppliers. To study the impact of feedback on the outcomes and dynamics of the auctions, we conduct laboratory experiments wherein we analyze bidder behavior and economic outcomes under three different treatment conditions with different types of information feedback. Our results indicate that, in contrast to winner-take-all multiattribute auctions, multisourcing multiattribute auctions, with potentially multiple winners, allow bidders (i.e., suppliers) to extract more profit when greater transparency in terms of provisional allocations and prices is provided. We develop several insights for mechanism designers toward developing sustainable procurement auctions that efficiently allocate multiple units of an asset with multiple negotiable attributes among multiple suppliers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 94\n",
      "Title: Bidder Support in Multi-item Multi-unit Continuous Combinatorial Auctions: A Unifying Theoretical Framework\n",
      "Abstract: Despite known advantages of combinatorial auctions, wide adoption of this allocation mechanism, especially in consumer-oriented marketplaces, is limited partially by the lack of effective bidder support information that can assist bidders to make bidding decisions. In this paper, we study the bidder support problem for general multi-item multi-unit (MIMU) combinatorial auctions, where multiple heterogeneous items are being auctioned and multiple homogeneous units are available for each item. Specifically, we consider continuous MIMU auctions, which impose minimal restrictions on bidding activities, thereby reducing the complexity of participation. Two prevalent bidding languages: OR bidding and XOR bidding, are discussed separately. For MIMU auctions with XOR bids, we derive theoretical results to calculate important bidder support metrics. We further demonstrate that bidder support results for MIMU auctions with OR bids can be derived directly from those with XOR bids, by viewing OR bids as XOR bids with each bid submitted by a unique bidder. Consequently, we establish MIMU auctions with XOR bids as the most general case, and unify the theoretical insights on bidder support problem for different bidding languages as well as different special cases of general MIMU auctions, namely single-item multi-unit (SIMU) auctions and multi-item single-unit (MISU) auctions. The derived theoretical results lead to algorithmic procedures that are capable of providing bidder support information efficiently in practice, and that outperform the commonly used integer programming approach. Theoretical insights of the general MIMU auctions also extend to auctions with additional bidding constraints, including batch-based combinatorial auctions, hierarchical combinatorial auctions, and combinatorial reverse auctions. History: This paper has been accepted for the Information Systems Research Special Section on Market Design and Analytics. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2021.1068.\n",
      "Content: Bidder Support in Multi-item Multi-unit Continuous Combinatorial Auctions: A Unifying Theoretical Framework Despite known advantages of combinatorial auctions, wide adoption of this allocation mechanism, especially in consumer-oriented marketplaces, is limited partially by the lack of effective bidder support information that can assist bidders to make bidding decisions. In this paper, we study the bidder support problem for general multi-item multi-unit (MIMU) combinatorial auctions, where multiple heterogeneous items are being auctioned and multiple homogeneous units are available for each item. Specifically, we consider continuous MIMU auctions, which impose minimal restrictions on bidding activities, thereby reducing the complexity of participation. Two prevalent bidding languages: OR bidding and XOR bidding, are discussed separately. For MIMU auctions with XOR bids, we derive theoretical results to calculate important bidder support metrics. We further demonstrate that bidder support results for MIMU auctions with OR bids can be derived directly from those with XOR bids, by viewing OR bids as XOR bids with each bid submitted by a unique bidder. Consequently, we establish MIMU auctions with XOR bids as the most general case, and unify the theoretical insights on bidder support problem for different bidding languages as well as different special cases of general MIMU auctions, namely single-item multi-unit (SIMU) auctions and multi-item single-unit (MISU) auctions. The derived theoretical results lead to algorithmic procedures that are capable of providing bidder support information efficiently in practice, and that outperform the commonly used integer programming approach. Theoretical insights of the general MIMU auctions also extend to auctions with additional bidding constraints, including batch-based combinatorial auctions, hierarchical combinatorial auctions, and combinatorial reverse auctions. History: This paper has been accepted for the Information Systems Research Special Section on Market Design and Analytics. Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2021.1068.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 95\n",
      "Title: Designing Intelligent Software Agents for Auctions with Limited Information Feedback\n",
      "Abstract: This paper presents analytical, computational, and empirical analyses of strategies for intelligent bid formulations in online auctions. We present results related to a weighted-average ascending price auction mechanism that is designed to provide opaque feedback information to bidders and presents a challenge in formulating appropriate bids. Using limited information provided by the mechanism, we design strategies for software agents to make bids intelligently. In particular, we derive analytical results for the important characteristics of the auction, which allow estimation of the key parameters; we then use these theoretical results to design several bidding strategies. We demonstrate the validity of designed strategies using a discrete event simulation model that resembles the mechanisms used in treasury bills auctions, business-to-consumer (B2C) auctions, and auctions for environmental emission allowances. In addition, using the data generated by the simulation model, we show that intelligent strategies can provide a high probability of winning an auction without significant loss in surplus.\n",
      "Content: Designing Intelligent Software Agents for Auctions with Limited Information Feedback This paper presents analytical, computational, and empirical analyses of strategies for intelligent bid formulations in online auctions. We present results related to a weighted-average ascending price auction mechanism that is designed to provide opaque feedback information to bidders and presents a challenge in formulating appropriate bids. Using limited information provided by the mechanism, we design strategies for software agents to make bids intelligently. In particular, we derive analytical results for the important characteristics of the auction, which allow estimation of the key parameters; we then use these theoretical results to design several bidding strategies. We demonstrate the validity of designed strategies using a discrete event simulation model that resembles the mechanisms used in treasury bills auctions, business-to-consumer (B2C) auctions, and auctions for environmental emission allowances. In addition, using the data generated by the simulation model, we show that intelligent strategies can provide a high probability of winning an auction without significant loss in surplus.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 96\n",
      "Title: Toward Comprehensive Real-Time Bidder Support in Iterative Combinatorial Auctions\n",
      "Abstract: Many auctions involve selling several distinct items simultaneously, where bidders can bid on the whole or any part of the lot. Such auctions are referred to as combinatorial auctions. Examples of such auctions include truck delivery routes,industrial procurement, and FCC spectrum. Determining winners in such auctions is an NP-hard problem, and significant research is being conducted in this area. However, multiple- round (iterative) combinatorial auctions present significant challenges in bid formulations as well. Because the combinatorial dynamics in iterative auctions can make a given bid part of a winning and nonwinning set of bids without any changes in the bid, bidders are usually not able to evaluate whether they should revise their bid at a given point in time or not. Therefore, in this paper we address various computational problems that are relevant from the bidder's perspective. In particular, we introduce two bid evaluation metrics that can be used by bidders to determine whether any given bid can be a part of the winning allocation and explore their theoretical properties. Based on these metrics, we also develop efficient data structures and algorithms that provide comprehensive information about the current state of the auction at any time, which can help bidders in evaluating their bids and bidding strategies. Our approach uses exponential memory storage but provides fast incremental update for new bids, thereby facilitating bidder support for real-time iterative combinatorial auctions.\n",
      "Content: Toward Comprehensive Real-Time Bidder Support in Iterative Combinatorial Auctions Many auctions involve selling several distinct items simultaneously, where bidders can bid on the whole or any part of the lot. Such auctions are referred to as combinatorial auctions. Examples of such auctions include truck delivery routes,industrial procurement, and FCC spectrum. Determining winners in such auctions is an NP-hard problem, and significant research is being conducted in this area. However, multiple- round (iterative) combinatorial auctions present significant challenges in bid formulations as well. Because the combinatorial dynamics in iterative auctions can make a given bid part of a winning and nonwinning set of bids without any changes in the bid, bidders are usually not able to evaluate whether they should revise their bid at a given point in time or not. Therefore, in this paper we address various computational problems that are relevant from the bidder's perspective. In particular, we introduce two bid evaluation metrics that can be used by bidders to determine whether any given bid can be a part of the winning allocation and explore their theoretical properties. Based on these metrics, we also develop efficient data structures and algorithms that provide comprehensive information about the current state of the auction at any time, which can help bidders in evaluating their bids and bidding strategies. Our approach uses exponential memory storage but provides fast incremental update for new bids, thereby facilitating bidder support for real-time iterative combinatorial auctions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 97\n",
      "Title: REQUEST: A Query Language for Customizing Recommendations\n",
      "Abstract: Initially popularized by Amazon.com, recommendation technologies have become widespread over the past several years. However, the types of recommendations available to the users in these recommender systems are typically determined by the vendor and therefore are not flexible. In this paper, we address this problem by presenting the recommendation query language REQUEST that allows users to customize recommendations by formulating them in the ways satisfying personalized needs of the users. REQUEST is based on the multidimensional model of recommender systems that supports additional contextual dimensions besides traditional User and Item dimensions and also OLAP-type aggregation and filtering capabilities. This paper also presents the recommendation algebra RA, shows how REQUEST recommendations can be mapped into this algebra, and analyzes the expressive power of the query language and the algebra. This paper also shows how users can customize their recommendations using REQUEST queries through a series of examples.\n",
      "Content: REQUEST: A Query Language for Customizing Recommendations Initially popularized by Amazon.com, recommendation technologies have become widespread over the past several years. However, the types of recommendations available to the users in these recommender systems are typically determined by the vendor and therefore are not flexible. In this paper, we address this problem by presenting the recommendation query language REQUEST that allows users to customize recommendations by formulating them in the ways satisfying personalized needs of the users. REQUEST is based on the multidimensional model of recommender systems that supports additional contextual dimensions besides traditional User and Item dimensions and also OLAP-type aggregation and filtering capabilities. This paper also presents the recommendation algebra RA, shows how REQUEST recommendations can be mapped into this algebra, and analyzes the expressive power of the query language and the algebra. This paper also shows how users can customize their recommendations using REQUEST queries through a series of examples.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 98\n",
      "Title: Centralization as a design consideration for the management of call centers\n",
      "Abstract: A call center and its associated information technology (IT) provide an opportunity to redesign and improve service-delivery operations. Managers at all levels should understand the role of organizational design as call centers are established or expanded, in particular the relative centralization (distribution of authority) associated with delivering services to customers. This article argues that centralization moderates and influences the organization’s efforts to improve customer service through the implementation of the call center and its IT. If managers fail to capitalize on the particular way that centralization moderates between IT and competitive strategy, the organization may not enjoy an important benefit of the call center, which is competitive advantage through increased efficiency and improved customer service. Based on survey responses from 68 call-center managers, the authors found that both centralization and decentralization are associated with call-center service operations. While the call center provides managers with the ability to influence decision-making (centralization), there are also opportunities for agents in the call center to exercise authority in managing the organization’s communications with customers (decentralization). Implications for organizational practice are considered.\n",
      "Content: Centralization as a design consideration for the management of call centers A call center and its associated information technology (IT) provide an opportunity to redesign and improve service-delivery operations. Managers at all levels should understand the role of organizational design as call centers are established or expanded, in particular the relative centralization (distribution of authority) associated with delivering services to customers. This article argues that centralization moderates and influences the organization’s efforts to improve customer service through the implementation of the call center and its IT. If managers fail to capitalize on the particular way that centralization moderates between IT and competitive strategy, the organization may not enjoy an important benefit of the call center, which is competitive advantage through increased efficiency and improved customer service. Based on survey responses from 68 call-center managers, the authors found that both centralization and decentralization are associated with call-center service operations. While the call center provides managers with the ability to influence decision-making (centralization), there are also opportunities for agents in the call center to exercise authority in managing the organization’s communications with customers (decentralization). Implications for organizational practice are considered.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 99\n",
      "Title: Development and validation of a rule-based time series complexity scoring technique to support design of adaptive forecasting DSS\n",
      "Abstract: Evidence from forecasting research gives reason to believe that understanding time series complexity can enable design of adaptive forecasting decision support systems (FDSSs) to positively support forecasting behaviors and accuracy of outcomes. Yet, such FDSS design capabilities have not been formally explored because there exists no systematic approach to identifying series complexity. This study describes the development and validation of a rule-based complexity scoring technique (CST) that generates a complexity score for time series using 12 rules that rely on 14 features of series. The rule-based schema was developed on 74 series and validated on 52 holdback series using well-accepted forecasting methods as benchmarks. A supporting experimental validation was conducted with 14 participants who generated 336 structured judgmental forecasts for sets of series classified as simple or complex by the CST. Benchmark comparisons validated the CST by confirming, as hypothesized, that forecasting accuracy was lower for series scored by the technique as complex when compared to the accuracy of those scored as simple. The study concludes with a comprehensive framework for design of FDSS that can integrate the CST to adaptively support forecasters under varied conditions of series complexity. The framework is founded on the concepts of restrictiveness and guidance and offers specific recommendations on how these elements can be built in FDSS to support complexity.\n",
      "Content: Development and validation of a rule-based time series complexity scoring technique to support design of adaptive forecasting DSS Evidence from forecasting research gives reason to believe that understanding time series complexity can enable design of adaptive forecasting decision support systems (FDSSs) to positively support forecasting behaviors and accuracy of outcomes. Yet, such FDSS design capabilities have not been formally explored because there exists no systematic approach to identifying series complexity. This study describes the development and validation of a rule-based complexity scoring technique (CST) that generates a complexity score for time series using 12 rules that rely on 14 features of series. The rule-based schema was developed on 74 series and validated on 52 holdback series using well-accepted forecasting methods as benchmarks. A supporting experimental validation was conducted with 14 participants who generated 336 structured judgmental forecasts for sets of series classified as simple or complex by the CST. Benchmark comparisons validated the CST by confirming, as hypothesized, that forecasting accuracy was lower for series scored by the technique as complex when compared to the accuracy of those scored as simple. The study concludes with a comprehensive framework for design of FDSS that can integrate the CST to adaptively support forecasters under varied conditions of series complexity. The framework is founded on the concepts of restrictiveness and guidance and offers specific recommendations on how these elements can be built in FDSS to support complexity.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 100\n",
      "Title: Architectures in context: on the evolution of business, application software, and ICT platform architectures\n",
      "Abstract: This paper distinguishes between the business domain, the application software domain, and the Information and Communication Technology (ICT) platform domain. It analyses historical developments in each of these three domains and shows that they experienced parallel development. The parallelism can be explained by mutual influence and alignment. Innovation in one domain may enable or drive developments in another. In order to be able to analyse alignment patterns, the notions of business architecture, application software architecture, and ICT platform architecture are introduced and defined. Interdependent historical developments sometimes demonstrate a radical change. Each can be described as a shift in “dominant design”, and we identify six such changes in the history of the modern enterprise. Professionals and scientific researchers working in Information and Management can benefit from these insights by assuming that radical changes in dominant designs will affect their field in the future according to the same pattern.\n",
      "Content: Architectures in context: on the evolution of business, application software, and ICT platform architectures This paper distinguishes between the business domain, the application software domain, and the Information and Communication Technology (ICT) platform domain. It analyses historical developments in each of these three domains and shows that they experienced parallel development. The parallelism can be explained by mutual influence and alignment. Innovation in one domain may enable or drive developments in another. In order to be able to analyse alignment patterns, the notions of business architecture, application software architecture, and ICT platform architecture are introduced and defined. Interdependent historical developments sometimes demonstrate a radical change. Each can be described as a shift in “dominant design”, and we identify six such changes in the history of the modern enterprise. Professionals and scientific researchers working in Information and Management can benefit from these insights by assuming that radical changes in dominant designs will affect their field in the future according to the same pattern.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 101\n",
      "Title: The platform shapes the message: How website design affects abstraction and valence of online consumer reviews\n",
      "Abstract: Online consumer reviews provide relevant information about products and services for consumers. In today's networked age, the online consumer review platform market is hyper-competitive. These platforms can easily change different design characteristics to get more reviewers and to nudge reviewers to deliver higher quality reviews. This study explored the relation between online consumer review platforms' design characteristics and the reviewers' construal level. A psycholinguistic coding scheme was used to assess which social and physical design characteristics impact the language abstraction in accompanying online consumer reviews. To this end, we content analyzed reviews of services and products posted on eight different online consumer review platforms (N=400). This resulted in a number of key design characteristics (e.g., reviewer identification, reviewer status, order of instructions and length instructions) that led to a decrease in language abstraction used in online consumer reviews. Moreover, results showed that language abstraction mediated the relationship between the four design characteristics and valence. The findings and their broader theoretical, methodological and practical implications are discussed. Online consumer review platforms could capitalize on our findings in adaptive design choices.\n",
      "Content: The platform shapes the message: How website design affects abstraction and valence of online consumer reviews Online consumer reviews provide relevant information about products and services for consumers. In today's networked age, the online consumer review platform market is hyper-competitive. These platforms can easily change different design characteristics to get more reviewers and to nudge reviewers to deliver higher quality reviews. This study explored the relation between online consumer review platforms' design characteristics and the reviewers' construal level. A psycholinguistic coding scheme was used to assess which social and physical design characteristics impact the language abstraction in accompanying online consumer reviews. To this end, we content analyzed reviews of services and products posted on eight different online consumer review platforms (N=400). This resulted in a number of key design characteristics (e.g., reviewer identification, reviewer status, order of instructions and length instructions) that led to a decrease in language abstraction used in online consumer reviews. Moreover, results showed that language abstraction mediated the relationship between the four design characteristics and valence. The findings and their broader theoretical, methodological and practical implications are discussed. Online consumer review platforms could capitalize on our findings in adaptive design choices.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 102\n",
      "Title: Do Organic Results Help or Hurt Sponsored Search Performance?\n",
      "Abstract: We study the impact of changes in the competitors’ listings in organic search results on the performance of sponsored search advertisements. Using data from an online retailer’s keyword advertising campaign, we measure the impact of organic competition on both click-through rate and conversion rate of sponsored search advertisements. We find that an increase in organic competition leads to a decrease in the click performance of sponsored advertisements. However, organic competition helps the conversion performance of sponsored ads and leads to higher revenue. We also find that organic competition has a higher negative effect on click performance than does sponsored competition. Our results inform advertisers on how the presence of organic results influences the performance of their sponsored advertisements. Specifically, we show that organic competition acts as a substitute for clicks, but has a complementary effect on the conversion performance.\n",
      "Content: Do Organic Results Help or Hurt Sponsored Search Performance? We study the impact of changes in the competitors’ listings in organic search results on the performance of sponsored search advertisements. Using data from an online retailer’s keyword advertising campaign, we measure the impact of organic competition on both click-through rate and conversion rate of sponsored search advertisements. We find that an increase in organic competition leads to a decrease in the click performance of sponsored advertisements. However, organic competition helps the conversion performance of sponsored ads and leads to higher revenue. We also find that organic competition has a higher negative effect on click performance than does sponsored competition. Our results inform advertisers on how the presence of organic results influences the performance of their sponsored advertisements. Specifically, we show that organic competition acts as a substitute for clicks, but has a complementary effect on the conversion performance.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 103\n",
      "Title: Cosearch Attention and Stock Return Predictability in Supply Chains\n",
      "Abstract: The ability to make predictions based on online searches in various contexts is gaining substantial interest in both research and practice. This study investigates a novel application of correlated online searches in predicting stock performance across supply chain partners. If two firms are economically dependent through a supply chain relationship and if information related to both firms diffuses in the market slowly or rapidly, then our ability to predict stock returns increases or decreases, respectively. We use online cosearches of stock as a proxy for the extent of information diffusion across supply chain-related firms. We identify publicly traded supply chain partners using Bloomberg data and construct cosearch networks of supply chain partners based on the weekly coviewing pattern of these firms on Yahoo! Finance. Our analyses show that the cosearch intensity across supply chain partners helps determine cross-return predictability. When investors of a focal stock pay less attention to its supply chain partners, we can use lagged partner returns to predict the future return of the focal stock. When investors’ coattention to focal and partner stocks is high, the predictability is low. Our simulated trading strategy using returns of supply chain partners with low coattention generates a significant and positive return above the market returns and performs better than the previously established trading strategy using returns of all supply chain partners. The online appendix is available at https://doi-org.ezproxy2.utwente.nl/10.1287/isre.2016.0656.\n",
      "Content: Cosearch Attention and Stock Return Predictability in Supply Chains The ability to make predictions based on online searches in various contexts is gaining substantial interest in both research and practice. This study investigates a novel application of correlated online searches in predicting stock performance across supply chain partners. If two firms are economically dependent through a supply chain relationship and if information related to both firms diffuses in the market slowly or rapidly, then our ability to predict stock returns increases or decreases, respectively. We use online cosearches of stock as a proxy for the extent of information diffusion across supply chain-related firms. We identify publicly traded supply chain partners using Bloomberg data and construct cosearch networks of supply chain partners based on the weekly coviewing pattern of these firms on Yahoo! Finance. Our analyses show that the cosearch intensity across supply chain partners helps determine cross-return predictability. When investors of a focal stock pay less attention to its supply chain partners, we can use lagged partner returns to predict the future return of the focal stock. When investors’ coattention to focal and partner stocks is high, the predictability is low. Our simulated trading strategy using returns of supply chain partners with low coattention generates a significant and positive return above the market returns and performs better than the previously established trading strategy using returns of all supply chain partners. The online appendix is available at https://doi-org.ezproxy2.utwente.nl/10.1287/isre.2016.0656.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 104\n",
      "Title: The Impact of Competing Ads on Click Performance in Sponsored Search\n",
      "Abstract: Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.\n",
      "Content: The Impact of Competing Ads on Click Performance in Sponsored Search Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 105\n",
      "Title: Managing paradoxical tensions in the development of a telemedicine system\n",
      "Abstract: The global pandemic has escalated the demand for telemedicine systems across the world, particularly for vulnerable populations such as the elderly in nursing homes. However, challenges in implementation and high failure rates continue to affect the sustainability and capability of telemedicine systems. This study therefore addresses the question of how to sustain and develop telemedicine systems, and offers a conceptual model developed from longitudinal study data and paradox theory. We found that in the inter-organizational context of telemedicine systems, par­ adoxical tensions arise from conflict between demands and interests of the telemedicine system versus those of its members. We also identified when the specific tensions of belonging, learning, organizing, and performing are likely to occur. These tensions are addressed through responses, initiated by the hub, that address both system and member level demands and interests through creating collaborative governance, uplifting member capabilities, and targeted resourcing. We further demonstrate the temporal dynamics of how the hub's responses create inter-organizational norms and structures that in turn influence responses to tensions in subsequent phases. We examined variations in members' reactions to the responses, and found that they were influenced by member-specific resource factors, suggesting that while the hub does sustain the development of the telemedicine system through addressing common member demands, there are limits with regard to aspects that are more member-specific. Finally, we show how technology can be both enabler-trigger and enabler-response due to its inherent attributes of malleability and reconfigurability.\n",
      "Content: Managing paradoxical tensions in the development of a telemedicine system The global pandemic has escalated the demand for telemedicine systems across the world, particularly for vulnerable populations such as the elderly in nursing homes. However, challenges in implementation and high failure rates continue to affect the sustainability and capability of telemedicine systems. This study therefore addresses the question of how to sustain and develop telemedicine systems, and offers a conceptual model developed from longitudinal study data and paradox theory. We found that in the inter-organizational context of telemedicine systems, par­ adoxical tensions arise from conflict between demands and interests of the telemedicine system versus those of its members. We also identified when the specific tensions of belonging, learning, organizing, and performing are likely to occur. These tensions are addressed through responses, initiated by the hub, that address both system and member level demands and interests through creating collaborative governance, uplifting member capabilities, and targeted resourcing. We further demonstrate the temporal dynamics of how the hub's responses create inter-organizational norms and structures that in turn influence responses to tensions in subsequent phases. We examined variations in members' reactions to the responses, and found that they were influenced by member-specific resource factors, suggesting that while the hub does sustain the development of the telemedicine system through addressing common member demands, there are limits with regard to aspects that are more member-specific. Finally, we show how technology can be both enabler-trigger and enabler-response due to its inherent attributes of malleability and reconfigurability.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 106\n",
      "Title: Social Interactions and the “Digital Divide”: Explaining Variations in Internet Use\n",
      "Abstract: Given the increasingly important role of the Internet in education, healthcare, and other essential services, it is important that we develop an understanding of the “digital divide.” Despite the widespread diffusion of the Web and related technologies, pockets remain where the Internet is used sparingly, if at all. There are large geographic variations, as well as variations across ethnic and racial lines. Prior research suggests that individual, household, and regional differences are responsible for this disparity. We argue for an alternative explanation: Individual choice is subject to social influence (“peer effects”) that emanates from geographic proximity; this influence is the cause of the excess variation. We test this assertion with empirical analysis of a data set compiled from a number of sources. We find, first, that widespread Internet use among people who live in proximity has a direct effect on an individual's propensity to go online. Using data on residential segregation, we test the proposition that the Internet usage patterns of people who live in more ethnically isolated regions will more closely resemble usage patterns of their ethnic group. Finally, we examine the moderating impact of housing density and directly measured social interactions on the relationship between Internet use and peer effects. Results are consistent across analyses and provide strong evidence of peer effects, suggesting that individual Internet use is influenced by local patterns of usage. Implications for public policy and the diffusion of the Internet are discussed.\n",
      "Content: Social Interactions and the “Digital Divide”: Explaining Variations in Internet Use Given the increasingly important role of the Internet in education, healthcare, and other essential services, it is important that we develop an understanding of the “digital divide.” Despite the widespread diffusion of the Web and related technologies, pockets remain where the Internet is used sparingly, if at all. There are large geographic variations, as well as variations across ethnic and racial lines. Prior research suggests that individual, household, and regional differences are responsible for this disparity. We argue for an alternative explanation: Individual choice is subject to social influence (“peer effects”) that emanates from geographic proximity; this influence is the cause of the excess variation. We test this assertion with empirical analysis of a data set compiled from a number of sources. We find, first, that widespread Internet use among people who live in proximity has a direct effect on an individual's propensity to go online. Using data on residential segregation, we test the proposition that the Internet usage patterns of people who live in more ethnically isolated regions will more closely resemble usage patterns of their ethnic group. Finally, we examine the moderating impact of housing density and directly measured social interactions on the relationship between Internet use and peer effects. Results are consistent across analyses and provide strong evidence of peer effects, suggesting that individual Internet use is influenced by local patterns of usage. Implications for public policy and the diffusion of the Internet are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 107\n",
      "Title: Editorial—Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research\n",
      "Abstract: We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems (IS) community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.\n",
      "Content: Editorial—Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems (IS) community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 108\n",
      "Title: The Digital Transformation of Healthcare: Current Status and the Road Ahead\n",
      "Abstract: As the United States expends extraordinary efforts toward the digitization of its health-care system, and as policy makers across the globe look to information technology (IT) as a means of making health-care systems safer, more affordable, and more accessible, a rare and remarkable opportunity has emerged for the information systems research community to leverage its in-depth knowledge to both advance theory and influence practice and policy. Although health IT (HIT) has tremendous potential to improve quality and reduce costs in healthcare, significant challenges need to be overcome to fully realize this potential. In this commentary, we survey the landscape of existing studies on HIT to provide an overview of the current status of HIT research. We then identify three major areas that warrant further research: (1) HIT design, implementation, and meaningful use; (2) measurement and quantification of HIT payoff and impact; and (3) extending the traditional realm of HIT. We discuss specific research questions in each domain and suggest appropriate methods to approach them. We encourage information systems scholars to become active participants in the global discourse on health-care transformation through IT.\n",
      "Content: The Digital Transformation of Healthcare: Current Status and the Road Ahead As the United States expends extraordinary efforts toward the digitization of its health-care system, and as policy makers across the globe look to information technology (IT) as a means of making health-care systems safer, more affordable, and more accessible, a rare and remarkable opportunity has emerged for the information systems research community to leverage its in-depth knowledge to both advance theory and influence practice and policy. Although health IT (HIT) has tremendous potential to improve quality and reduce costs in healthcare, significant challenges need to be overcome to fully realize this potential. In this commentary, we survey the landscape of existing studies on HIT to provide an overview of the current status of HIT research. We then identify three major areas that warrant further research: (1) HIT design, implementation, and meaningful use; (2) measurement and quantification of HIT payoff and impact; and (3) extending the traditional realm of HIT. We discuss specific research questions in each domain and suggest appropriate methods to approach them. We encourage information systems scholars to become active participants in the global discourse on health-care transformation through IT.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 109\n",
      "Title: Editorial Overview—The Interplay Between Digital and Social Networks\n",
      "Abstract: Social networks constructed on digital platforms are becoming increasingly pervasive in all aspects of individual and organizational life. This special issue of Information Systems Research includes 10 papers that focus on the interplay between digital and social networks. The interplay draws attention to the fact that digital interaction among individuals and organizations is almost always embedded in, influenced by, and in turn influences a social network. The papers in this special issue collectively shed light on the technical, behavioral, and economic challenges and implications of such networks and contribute to our understanding of how the power of such networks can be harnessed.\n",
      "Content: Editorial Overview—The Interplay Between Digital and Social Networks Social networks constructed on digital platforms are becoming increasingly pervasive in all aspects of individual and organizational life. This special issue of Information Systems Research includes 10 papers that focus on the interplay between digital and social networks. The interplay draws attention to the fact that digital interaction among individuals and organizations is almost always embedded in, influenced by, and in turn influences a social network. The papers in this special issue collectively shed light on the technical, behavioral, and economic challenges and implications of such networks and contribute to our understanding of how the power of such networks can be harnessed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 110\n",
      "Title: Time Flies When You’re Having Fun:  Cognitive Absorption and Beliefs About Information Technology Usage\n",
      "Abstract:  Extant explanations of why users behave in particular ways toward information technologies have tended to focus predominantly on instrumental beliefs as drivers of individual usage intentions. Prior work in individual psychology, however, suggests that holistic experiences with technology as captured in constructs such as enjoyment and flow are potentially important explanatory variables in technology acceptance theories. In this paper, we describe a multidimensional construct labeled cognitive absorption and defined as a state of deep involvement with software. Cognitive absorption, theorized as 1 Cynthia Beath was the accepting senior editor for this paper.being exhibited through the five dimensions of temporal dissociation, focused immersion, heightened enjoyment, control, and curiosity, is posited to be a proximal antecedent of two important beliefs about technology use: perceived usefulness and perceived ease of use. In addition, we propose that the individual traits of playfulness and personal innovativeness are important determinants of cognitive absorption. Based on the conceptual definition of this construct, operational measures for each dimension are developed. Using the World Wide Web as the target technology, scale validation indicates that the operational measures have acceptable psychometric properties and confirmatory factor analysis supports the proposed multi-dimensional structure. Structural equation analysis provides evidence for the theorized nomological net of cognitive absorption. Theoretical and practical implications are offered. \n",
      "Content: Time Flies When You’re Having Fun:  Cognitive Absorption and Beliefs About Information Technology Usage  Extant explanations of why users behave in particular ways toward information technologies have tended to focus predominantly on instrumental beliefs as drivers of individual usage intentions. Prior work in individual psychology, however, suggests that holistic experiences with technology as captured in constructs such as enjoyment and flow are potentially important explanatory variables in technology acceptance theories. In this paper, we describe a multidimensional construct labeled cognitive absorption and defined as a state of deep involvement with software. Cognitive absorption, theorized as 1 Cynthia Beath was the accepting senior editor for this paper.being exhibited through the five dimensions of temporal dissociation, focused immersion, heightened enjoyment, control, and curiosity, is posited to be a proximal antecedent of two important beliefs about technology use: perceived usefulness and perceived ease of use. In addition, we propose that the individual traits of playfulness and personal innovativeness are important determinants of cognitive absorption. Based on the conceptual definition of this construct, operational measures for each dimension are developed. Using the World Wide Web as the target technology, scale validation indicates that the operational measures have acceptable psychometric properties and confirmatory factor analysis supports the proposed multi-dimensional structure. Structural equation analysis provides evidence for the theorized nomological net of cognitive absorption. Theoretical and practical implications are offered. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 111\n",
      "Title: Infusing learning into the information systems organization\n",
      "Abstract: Contemporary information systems (IS) organizations need mechanisms to cope with both the complexity that is created by rapid technological change and escalating business demands, and the resulting uncertainty engendered in organizational operations and policies. The IS function will not be in a position to manage such changes effectively unless it undergoes a fundamental restructuring towards becoming a ‘learning’ organization. This paper presents a framework that identifies the core drivers than an IS organization can proactively influence as it attempts to become learning-oriented. The framework identifies three essential components: (1) the learning context that defines various dimensions for measuring organizational and individual performance; (2) procedures and management initiatives that will facilitate individual learning to improve such performance; and (3) norms and culture that are established by the leadership to encourage learning. Using a case study methodology, the actions taken by one specific IS organization in its attempts to infuse learning capabilities among its members are examined. The conceptual framework for examining what it takes to be a learning IS organization and the detailed documented experiences of one specific organization may provide valuable insights to other IS organizations in their efforts to become more adaptive and responsive to change.\n",
      "Content: Infusing learning into the information systems organization Contemporary information systems (IS) organizations need mechanisms to cope with both the complexity that is created by rapid technological change and escalating business demands, and the resulting uncertainty engendered in organizational operations and policies. The IS function will not be in a position to manage such changes effectively unless it undergoes a fundamental restructuring towards becoming a ‘learning’ organization. This paper presents a framework that identifies the core drivers than an IS organization can proactively influence as it attempts to become learning-oriented. The framework identifies three essential components: (1) the learning context that defines various dimensions for measuring organizational and individual performance; (2) procedures and management initiatives that will facilitate individual learning to improve such performance; and (3) norms and culture that are established by the leadership to encourage learning. Using a case study methodology, the actions taken by one specific IS organization in its attempts to infuse learning capabilities among its members are examined. The conceptual framework for examining what it takes to be a learning IS organization and the detailed documented experiences of one specific organization may provide valuable insights to other IS organizations in their efforts to become more adaptive and responsive to change.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 112\n",
      "Title: The Information Systems Identity Crisis: Focusing on High-Visibility and High-Impact Research\n",
      "Abstract: This paper presents an alternative view of the Information Systems identity crisis described recently by Benbasat and Zmud (2003). We agree with many of their observations, but we are concerned with their prescription for IS research. We critique their discussion of errors of inclusion and exclusion in IS research and highlight the potential misinterpretations that are possible from a literal reading of their comments. Our conclusion is that following Benbasat and Zmud's nomological net will result in a micro focus for IS research. The results of such a focus are potentially dangerous for the field. They could result in the elimination of IS from many academic programs. We present an alternative set of heuristics that can be used to assess what lies within the domain of IS scholarship. We argue that the IS community has a powerful story to tell about the transformational impact of information technology. We believe that a significant portion of our research should be macro studies of the impact of IT. It is important for academic colleagues, deans, and managers to understand the transformational power of the technology. As IS researchers with deep knowledge of the underlying artifact, we are best positioned to do such research.\n",
      "Content: The Information Systems Identity Crisis: Focusing on High-Visibility and High-Impact Research This paper presents an alternative view of the Information Systems identity crisis described recently by Benbasat and Zmud (2003). We agree with many of their observations, but we are concerned with their prescription for IS research. We critique their discussion of errors of inclusion and exclusion in IS research and highlight the potential misinterpretations that are possible from a literal reading of their comments. Our conclusion is that following Benbasat and Zmud's nomological net will result in a micro focus for IS research. The results of such a focus are potentially dangerous for the field. They could result in the elimination of IS from many academic programs. We present an alternative set of heuristics that can be used to assess what lies within the domain of IS scholarship. We argue that the IS community has a powerful story to tell about the transformational impact of information technology. We believe that a significant portion of our research should be macro studies of the impact of IT. It is important for academic colleagues, deans, and managers to understand the transformational power of the technology. As IS researchers with deep knowledge of the underlying artifact, we are best positioned to do such research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 113\n",
      "Title: Editors' Preface\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 114\n",
      "Title: A Conceptual and Operational Definition of Personal Innovativeness in the Domain of Information Technology\n",
      "Abstract: The acceptance of new information technologies by their intended users persists as an important issue for researchers and practitioners of information systems. Several models have been developed in the literature to facilitate understanding of the process by which new information technologies are adopted. This paper proposes a new construct that further illuminates the relationships explicit in the technology acceptance models and describes an operational measure for this construct that possesses desirable psychometric properties. The construct, personal innovativeness in the domain of information technology, is hypothesized to exhibit moderating effects on the antecedents as well as the consequences of individual perceptions about a new information technology. The construct was developed and validated in the context of the innovation represented by the World-Wide Web. Implications for theory and practice are discussed.\n",
      "Content: A Conceptual and Operational Definition of Personal Innovativeness in the Domain of Information Technology The acceptance of new information technologies by their intended users persists as an important issue for researchers and practitioners of information systems. Several models have been developed in the literature to facilitate understanding of the process by which new information technologies are adopted. This paper proposes a new construct that further illuminates the relationships explicit in the technology acceptance models and describes an operational measure for this construct that possesses desirable psychometric properties. The construct, personal innovativeness in the domain of information technology, is hypothesized to exhibit moderating effects on the antecedents as well as the consequences of individual perceptions about a new information technology. The construct was developed and validated in the context of the innovation represented by the World-Wide Web. Implications for theory and practice are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 115\n",
      "Title: The antecedents and consequents of user perceptions in information technology adoption\n",
      "Abstract: A common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables. Although a fairly significant body of research that empirically tests these models is now in existence, some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered. This paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application. Two research objectives that have both theoretical and practical relevance motivated and guided the study. One, the study challenges an assumption which is implicit in technology acceptance models: that of the non-existence of moderating influences on the relationship between perceptions and adoption decisions. Specifically, the study examines the effects of an important moderating influence – personal innovativeness – on this relationship. Two, the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development. Theoretical and practical implications that follow from the results are discussed.\n",
      "Content: The antecedents and consequents of user perceptions in information technology adoption A common theme underlying various models that explain information technology adoption is the inclusion of perceptions of an innovation as key independent variables. Although a fairly significant body of research that empirically tests these models is now in existence, some questions with regard to both the antecedents as well as the consequents of perceptions remain unanswered. This paper reports the results of a field study examining adoption of an information technology innovation represented by an expert systems application. Two research objectives that have both theoretical and practical relevance motivated and guided the study. One, the study challenges an assumption which is implicit in technology acceptance models: that of the non-existence of moderating influences on the relationship between perceptions and adoption decisions. Specifically, the study examines the effects of an important moderating influence – personal innovativeness – on this relationship. Two, the study seeks to shed further light on the determinants of perceptions by examining the relative efficacy of mass media and interpersonal communication channels in facilitating perception development. Theoretical and practical implications that follow from the results are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 116\n",
      "Title: MIS planning: A methodology for systems prioritization\n",
      "Abstract: The literature proposes a number of approaches that allow for the consideration of corporate goals and objectives in prioritizing information systems using both financial and non-financial criteria. Research in cognitive psychology suggests that an individual confronted with a simultaneous consideration of both qualitative and quantitative factors tends to assign greater salience to concrete factors than to more abstract criteria. This paper proposes a multi-dimensional methodology that allows for the prioritization of systems proposals based on attributes that are mostly qualitative as the first step in the resource allocation phase of MIS planning. This initial prioritization is used in conjunction with other quantitative factors to arrive at a final system portfolio. The methodology is illustrated with the aid of a case study conducted at a non-profit organization.\n",
      "Content: MIS planning: A methodology for systems prioritization The literature proposes a number of approaches that allow for the consideration of corporate goals and objectives in prioritizing information systems using both financial and non-financial criteria. Research in cognitive psychology suggests that an individual confronted with a simultaneous consideration of both qualitative and quantitative factors tends to assign greater salience to concrete factors than to more abstract criteria. This paper proposes a multi-dimensional methodology that allows for the prioritization of systems proposals based on attributes that are mostly qualitative as the first step in the resource allocation phase of MIS planning. This initial prioritization is used in conjunction with other quantitative factors to arrive at a final system portfolio. The methodology is illustrated with the aid of a case study conducted at a non-profit organization.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 117\n",
      "Title: Research Report: The Evolving Relationship Between General and Specific Computer Self-Efficacy—An Empirical Assessment\n",
      "Abstract: The concept of computer self-efficacy (CSE) recently has been proposed as important to the study of individual behavior toward information technology. This paper extends current understanding about the concept of self-efficacy in the context of computer software. We describe how two broad types of computer self-efficacy beliefs, general self-efficacy and task-specific self-efficacy, are constructed across different computing tasks by suggesting that initial general CSE beliefs will strongly predict subsequent specific CSE beliefs. The theorized causal relationships illustrate the malleability and development of CSE beliefs over time, within a training environment where individuals are progressively provided with greater opportunity for hands-on experience and practice with different software. Consistent with the findings of prior research, judgments of self-efficacy then serve as key antecedents of the perceived cognitive effort (ease of use) associated with technology usage. Further, we theorize that self-efficacy judgments in the task domain of computing are strongly influenced by the extent to which individuals believe that they are personally innovative with respect to information technology. Panel data were collected using a longitudinal research design within a training context where 186 subjects were taught two software packages in a sequential manner over a 14-week period. The emergent patterns of the hypothesized relationships are examined using structural equation modeling techniques. Results largely support the relationships posited.\n",
      "Content: Research Report: The Evolving Relationship Between General and Specific Computer Self-Efficacy—An Empirical Assessment The concept of computer self-efficacy (CSE) recently has been proposed as important to the study of individual behavior toward information technology. This paper extends current understanding about the concept of self-efficacy in the context of computer software. We describe how two broad types of computer self-efficacy beliefs, general self-efficacy and task-specific self-efficacy, are constructed across different computing tasks by suggesting that initial general CSE beliefs will strongly predict subsequent specific CSE beliefs. The theorized causal relationships illustrate the malleability and development of CSE beliefs over time, within a training environment where individuals are progressively provided with greater opportunity for hands-on experience and practice with different software. Consistent with the findings of prior research, judgments of self-efficacy then serve as key antecedents of the perceived cognitive effort (ease of use) associated with technology usage. Further, we theorize that self-efficacy judgments in the task domain of computing are strongly influenced by the extent to which individuals believe that they are personally innovative with respect to information technology. Panel data were collected using a longitudinal research design within a training context where 186 subjects were taught two software packages in a sequential manner over a 14-week period. The emergent patterns of the hypothesized relationships are examined using structural equation modeling techniques. Results largely support the relationships posited.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 118\n",
      "Title: Cognitive Fit in Requirements Modeling: A Study of Object and Process Methodologies\n",
      "Abstract: Requirements modeling constitutes one of the most important phases of the systems development life cycle. Despite the proliferation of methodologies and models for requirements analysis, empirical work examining their relative efficacy is limited. This paper presents an empirical examination of object-oriented and process-oriented methodologies as applied to object-oriented and process-oriented tasks. The conceptual basis of the research model is derived from the theory of cognitive fit, which posits that superior problem-solving performance will result when the problem-solving task and the problem-solving tool emphasize the same type of information. Two groups of subjects participated in an experiment that required them to construct solutions to two requirements-modeling tasks, one process-oriented and the other object-oriented. One group employed the object-oriented tool while the other used the process-oriented tool. As predicted by the theory of cognitive fit, superior performance was observed when the process-oriented tool was applied to the process-oriented task. For the object-oriented task, however, the performance effects of cognitive fit require further investigation since there was no difference in subject performance across the two tools.\n",
      "Content: Cognitive Fit in Requirements Modeling: A Study of Object and Process Methodologies Requirements modeling constitutes one of the most important phases of the systems development life cycle. Despite the proliferation of methodologies and models for requirements analysis, empirical work examining their relative efficacy is limited. This paper presents an empirical examination of object-oriented and process-oriented methodologies as applied to object-oriented and process-oriented tasks. The conceptual basis of the research model is derived from the theory of cognitive fit, which posits that superior problem-solving performance will result when the problem-solving task and the problem-solving tool emphasize the same type of information. Two groups of subjects participated in an experiment that required them to construct solutions to two requirements-modeling tasks, one process-oriented and the other object-oriented. One group employed the object-oriented tool while the other used the process-oriented tool. As predicted by the theory of cognitive fit, superior performance was observed when the process-oriented tool was applied to the process-oriented task. For the object-oriented task, however, the performance effects of cognitive fit require further investigation since there was no difference in subject performance across the two tools.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 119\n",
      "Title: Knowledge-based model validation support for end-user computing environments\n",
      "Abstract: Encouraging individuals to use corporate data and build computer-based decision models locally, while simultaneously ensuring that the modelling activity is consistent with corporate policies and guidelines poses a challenge to many organizations. Although it is desirable to encourage user autonomy in decision making, it is equally imperative to assure appropriate quality of the decisions made. In this paper interdependencies within the organizational decision making activity are used to identify some generic categories of support required to maintain consistency and quality in end-user model construction. Five distinct cases of model building activity in an end-user computing environment are described and support for ensuring consistency in user constructed models for two of these cases is discussed. An object-oriented knowledge-based system that provides such support has been developed; the architecture of this system is described. System implementation and interaction is illustrated with the aid of a financial budgeting application.\n",
      "Content: Knowledge-based model validation support for end-user computing environments Encouraging individuals to use corporate data and build computer-based decision models locally, while simultaneously ensuring that the modelling activity is consistent with corporate policies and guidelines poses a challenge to many organizations. Although it is desirable to encourage user autonomy in decision making, it is equally imperative to assure appropriate quality of the decisions made. In this paper interdependencies within the organizational decision making activity are used to identify some generic categories of support required to maintain consistency and quality in end-user model construction. Five distinct cases of model building activity in an end-user computing environment are described and support for ensuring consistency in user constructed models for two of these cases is discussed. An object-oriented knowledge-based system that provides such support has been developed; the architecture of this system is described. System implementation and interaction is illustrated with the aid of a financial budgeting application.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 120\n",
      "Title: Editorial—Evolvable Systems: Through the Looking Glass of IS\n",
      "Abstract: We explore how “Red Queen” competition is increasing the competitive premium on “evolvable” information systems (IS). Ephemeral market advantage coupled with relentless innovation spawning trends such as the Internet-of-Things and additive manufacturing are amplifying the importance of evolvable systems across all industries. We discuss uncharted theoretical and empirical territory for IS research on evolvable systems. The elusiveness of some of these phenomena to other disciplines offers a unique opportunity for IS scholars.\n",
      "Content: Editorial—Evolvable Systems: Through the Looking Glass of IS We explore how “Red Queen” competition is increasing the competitive premium on “evolvable” information systems (IS). Ephemeral market advantage coupled with relentless innovation spawning trends such as the Internet-of-Things and additive manufacturing are amplifying the importance of evolvable systems across all industries. We discuss uncharted theoretical and empirical territory for IS research on evolvable systems. The elusiveness of some of these phenomena to other disciplines offers a unique opportunity for IS scholars.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 121\n",
      "Title: Assessing a Firm's Web Presence: A Heuristic Evaluation Procedure for the Measurement of Usability\n",
      "Abstract: Web site usability is a critical metric for assessing the quality of a firm's Web presence. A measure of usability must not only provide a global rating for a specific Web site, ideally it should also illuminate specific strengths and weaknesses associated with site design. In this paper, we describe a heuristic evaluation procedure for examining the usability guidelines developed by Microsoft. We present the categories and subcategories comprising these guidelines, and discuss the development of an instrument that operationalizes the measurement of usability. The proposed instrument was tested in a heuristic evaluation study where 1,475 users rated multiple Web sites from four different industry sectors: airlines, online bookstores, automobile manufacturers, and car rental agencies. To enhance the external validity of the study, users were asked to assume the role of a consumer or an investor when assessing usability. Empirical results suggest that the evaluation procedure, the instrument, as well as the usability metric exhibit good properties. Implications of the findings for researchers, for Web site designers, and for heuristic evaluation methods in usability testing are offered.\n",
      "Content: Assessing a Firm's Web Presence: A Heuristic Evaluation Procedure for the Measurement of Usability Web site usability is a critical metric for assessing the quality of a firm's Web presence. A measure of usability must not only provide a global rating for a specific Web site, ideally it should also illuminate specific strengths and weaknesses associated with site design. In this paper, we describe a heuristic evaluation procedure for examining the usability guidelines developed by Microsoft. We present the categories and subcategories comprising these guidelines, and discuss the development of an instrument that operationalizes the measurement of usability. The proposed instrument was tested in a heuristic evaluation study where 1,475 users rated multiple Web sites from four different industry sectors: airlines, online bookstores, automobile manufacturers, and car rental agencies. To enhance the external validity of the study, users were asked to assume the role of a consumer or an investor when assessing usability. Empirical results suggest that the evaluation procedure, the instrument, as well as the usability metric exhibit good properties. Implications of the findings for researchers, for Web site designers, and for heuristic evaluation methods in usability testing are offered.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 122\n",
      "Title: Editorial Notes: Reflections on Year One: A 2011 Retrospective\n",
      "Abstract: The author reflects on the information systems and technology research study papers and notes that were published in the journal. She highlights the dual notions of technological innovation and impact in the field's progress. She also talks about cognitive neuroscience applied to information studies, the improvement in the traditional metrics used to review the quality of the journal, and the influence of the research studies published in the journal and their economic and social values.\n",
      "Content: Editorial Notes: Reflections on Year One: A 2011 Retrospective The author reflects on the information systems and technology research study papers and notes that were published in the journal. She highlights the dual notions of technological innovation and impact in the field's progress. She also talks about cognitive neuroscience applied to information studies, the improvement in the traditional metrics used to review the quality of the journal, and the influence of the research studies published in the journal and their economic and social values.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 123\n",
      "Title: Editorial Notes\n",
      "Abstract: An introduction is presented in which the editor discusses various reports within the issue on topics including the experimental study of online privacy information revelation and consumer transaction behavior, the specification of precise requirements for information technology (IT) artifact design and development and the iterative auction designs with linear and nonlinear pricing schemes.\n",
      "Content: Editorial Notes An introduction is presented in which the editor discusses various reports within the issue on topics including the experimental study of online privacy information revelation and consumer transaction behavior, the specification of precise requirements for information technology (IT) artifact design and development and the iterative auction designs with linear and nonlinear pricing schemes.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 124\n",
      "Title: Editorial Notes\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 125\n",
      "Title: Editorial Notes\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 126\n",
      "Title: Editorial Notes\n",
      "Abstract: In this end of the year editorial commentary ISR's accomplishments for 2013 are summarized. I recognize the contribution of members of the editorial board who are retiring and welcome new editorial team members. Winners of the annual ISR awards for 2012, including best paper, AE of the year, and reviewer of the year are acknowledged and congratulated. An overview of changes to the journal's mission for 2014 is presented. The commentary closes with a discussion of papers in this issue. The research commentaries, articles, and notes span a broad gamut of research domains and theoretical traditions, ranging from digital networks, to information technology value, to online communities, to the design of software for games. The papers also exhibit considerable methodological diversity, including econometric analyses, qualitative approaches, and analytical modeling.\n",
      "Content: Editorial Notes In this end of the year editorial commentary ISR's accomplishments for 2013 are summarized. I recognize the contribution of members of the editorial board who are retiring and welcome new editorial team members. Winners of the annual ISR awards for 2012, including best paper, AE of the year, and reviewer of the year are acknowledged and congratulated. An overview of changes to the journal's mission for 2014 is presented. The commentary closes with a discussion of papers in this issue. The research commentaries, articles, and notes span a broad gamut of research domains and theoretical traditions, ranging from digital networks, to information technology value, to online communities, to the design of software for games. The papers also exhibit considerable methodological diversity, including econometric analyses, qualitative approaches, and analytical modeling.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 127\n",
      "Title: Editorial Notes\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 128\n",
      "Title: Editorial Notes\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 129\n",
      "Title: Editorial Notes\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 130\n",
      "Title: Editorial—On the Intellectual Structure and Evolution of ISR\n",
      "Abstract: Scholars within fields frequently introspect about the evolution of their research disciplines, asking if the discipline is developing a cumulative research tradition, influencing scholarly work in other fields, and expanding its boundaries. In this spirit, I examine the intellectual structure and evolution of one disciplinary journal, ISR, over the most recent four-year period (2012–2015). I use a census of citation data and classify journals citing ISR and cited by ISR into the disciplines they represent. Analyzing 12,833 citations to ISR from 100 journals, and 7,731 citations from ISR to papers published in 67 journals I find that, consistent with prior studies, ISR reflects the multidisciplinarity of the information systems (IS) discipline, its boundaries are expanding to include new disciplines, and the journal draws extensively on other IS journals. I raise some questions for further introspection about the impact and visibility of IS research.\n",
      "Content: Editorial—On the Intellectual Structure and Evolution of ISR Scholars within fields frequently introspect about the evolution of their research disciplines, asking if the discipline is developing a cumulative research tradition, influencing scholarly work in other fields, and expanding its boundaries. In this spirit, I examine the intellectual structure and evolution of one disciplinary journal, ISR, over the most recent four-year period (2012–2015). I use a census of citation data and classify journals citing ISR and cited by ISR into the disciplines they represent. Analyzing 12,833 citations to ISR from 100 journals, and 7,731 citations from ISR to papers published in 67 journals I find that, consistent with prior studies, ISR reflects the multidisciplinarity of the information systems (IS) discipline, its boundaries are expanding to include new disciplines, and the journal draws extensively on other IS journals. I raise some questions for further introspection about the impact and visibility of IS research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 131\n",
      "Title: The More, the Merrier? How the Number of Partners in a Standard-Setting Initiative Affects Shareholder's Risk and Return1\n",
      "Abstract: The article presents research on collaboration between business enterprises to set standards for information technology examining if such cooperation reduces the financial risks faced by stockholders of the individual companies involved. It was found that an increase in the number of companies involved in cooperation on standards decreased the market risk on stockholder rate of return as measured by beta, but increased the idiosyncratic risk to the individual firms' returns. This indicates companies elected to participate in a large standardization project obtain a reduction in abnormal returns on stocks.\n",
      "Content: The More, the Merrier? How the Number of Partners in a Standard-Setting Initiative Affects Shareholder's Risk and Return1 The article presents research on collaboration between business enterprises to set standards for information technology examining if such cooperation reduces the financial risks faced by stockholders of the individual companies involved. It was found that an increase in the number of companies involved in cooperation on standards decreased the market risk on stockholder rate of return as measured by beta, but increased the idiosyncratic risk to the individual firms' returns. This indicates companies elected to participate in a large standardization project obtain a reduction in abnormal returns on stocks.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 132\n",
      "Title: Intellectual Property Bundle (IPB) theory: Managing transaction costs in technology development through network governance\n",
      "Abstract: Technology is a bundle of inventions, which are increasingly protected by intellectual property rights. Typically, these rights are owned by multiple different entities, operating in different industries and countries. Moreover, once an invention protected by intellectual property right is incorporated in a product, it becomes very difficult to substitute it with an alternative technology, especially when the product has been widely adopted. Thus, technology creators must coordinate the disparate interests of various intellectual property owners in order to create useful technology. In this paper we introduce a new theory as an extension of transaction cost economics to explain the relative merits of different governance forms vis-à-vis the creation of technology that is a bundle of inventions. From this theoretical extension, we derive a number of testable hypotheses.\n",
      "Content: Intellectual Property Bundle (IPB) theory: Managing transaction costs in technology development through network governance Technology is a bundle of inventions, which are increasingly protected by intellectual property rights. Typically, these rights are owned by multiple different entities, operating in different industries and countries. Moreover, once an invention protected by intellectual property right is incorporated in a product, it becomes very difficult to substitute it with an alternative technology, especially when the product has been widely adopted. Thus, technology creators must coordinate the disparate interests of various intellectual property owners in order to create useful technology. In this paper we introduce a new theory as an extension of transaction cost economics to explain the relative merits of different governance forms vis-à-vis the creation of technology that is a bundle of inventions. From this theoretical extension, we derive a number of testable hypotheses.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 133\n",
      "Title: Putting Money Where the Mouths Are: The Relation Between Venture Financing and Electronic Word-of-Mouth\n",
      "Abstract: External financing is critical to ventures that do not have a revenue source but need to recruit employees, develop products, pay suppliers, and market their products/services. There is an increasing belief among entrepreneurs that electronic word-of-mouth (eWOM), specifically blog coverage, can aid in achieving venture capital financing. Conflicting findings reported by past studies examining eWOM make it unclear what to make of such beliefs of entrepreneurs. Even if there were generally agreed-upon results, a stream of literature indicates that because of the differences in traits between the prior investigated contexts and venture capital financing, the findings from the prior studies cannot be generalized to venture capital financing. Extant studies also fall short in examining the role of time and the status of entities generating eWOM in determining the influence of eWOM on decision making. To address this dearth of literature in a context that attracts billions of dollars every year, we investigate the effect of eWOM on venture capital financing. This study entails the challenging task of gathering data from hundreds of ventures along with other sources including VentureXpert, surveys, Google Blogsearch, Lexis-Nexis, and Archive.org. The key findings of our econometric analysis are that the impact of negative eWOM is greater than is the impact of positive eWOM and that the effect of eWOM on financing decreases with the progress through the financing stages. We also find that the eWOM of popular bloggers helps ventures in getting higher funding amounts and valuations. The empirical model used in this work accounts for inherent selection biases of entrepreneurs and venture capitalists, and we conduct numerous robustness checks for potential issues of endogeneity, selection bias, nonlinearities, and popularity cutoff for blogs. The findings have important implications for entrepreneurs and suggest ways by which entrepreneurs can take advantage of eWOM.\n",
      "Content: Putting Money Where the Mouths Are: The Relation Between Venture Financing and Electronic Word-of-Mouth External financing is critical to ventures that do not have a revenue source but need to recruit employees, develop products, pay suppliers, and market their products/services. There is an increasing belief among entrepreneurs that electronic word-of-mouth (eWOM), specifically blog coverage, can aid in achieving venture capital financing. Conflicting findings reported by past studies examining eWOM make it unclear what to make of such beliefs of entrepreneurs. Even if there were generally agreed-upon results, a stream of literature indicates that because of the differences in traits between the prior investigated contexts and venture capital financing, the findings from the prior studies cannot be generalized to venture capital financing. Extant studies also fall short in examining the role of time and the status of entities generating eWOM in determining the influence of eWOM on decision making. To address this dearth of literature in a context that attracts billions of dollars every year, we investigate the effect of eWOM on venture capital financing. This study entails the challenging task of gathering data from hundreds of ventures along with other sources including VentureXpert, surveys, Google Blogsearch, Lexis-Nexis, and Archive.org. The key findings of our econometric analysis are that the impact of negative eWOM is greater than is the impact of positive eWOM and that the effect of eWOM on financing decreases with the progress through the financing stages. We also find that the eWOM of popular bloggers helps ventures in getting higher funding amounts and valuations. The empirical model used in this work accounts for inherent selection biases of entrepreneurs and venture capitalists, and we conduct numerous robustness checks for potential issues of endogeneity, selection bias, nonlinearities, and popularity cutoff for blogs. The findings have important implications for entrepreneurs and suggest ways by which entrepreneurs can take advantage of eWOM.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 134\n",
      "Title: Blog, Blogger, and the Firm: Can Negative Employee Posts Lead to Positive Outcomes?\n",
      "Abstract: Consumer-generated media, particularly blogs, can help companies increase the visibility of their products without spending millions of dollars in advertising. Although a number of companies realize the potential of blogs and encourage their employees to blog, a good chunk of them are skeptical about losing control over this new media. Companies fear that employees may write negative things about them and that this may bring significant reputation loss. Overall, companies show mixed response toward negative posts on employee blogs—some companies show complete aversion; others allow some negative posts. Such mixed reactions toward negative posts motivated us to probe for any positive aspects of negative posts. In particular, we investigate the relationship between negative posts and readership of an employee blog. In contrast to the popular perception, our results reveal a potential positive aspect of negative posts. Our analysis suggests that negative posts act as catalyst and can exponentially increase the readership of employee blogs, suggesting that companies should permit employees to make negative posts. Because employees typically write few negative posts and largely write positive posts, the increase in readership of employee blogs generally should be enough to offset the negative effect of few negative posts. Therefore, not restraining negative posts to increase readership should be a good strategy. This raises a logical question: what should a firm's policy be regarding employee blogging? For exposition, we suggest an analytical framework using our empirical model.\n",
      "Content: Blog, Blogger, and the Firm: Can Negative Employee Posts Lead to Positive Outcomes? Consumer-generated media, particularly blogs, can help companies increase the visibility of their products without spending millions of dollars in advertising. Although a number of companies realize the potential of blogs and encourage their employees to blog, a good chunk of them are skeptical about losing control over this new media. Companies fear that employees may write negative things about them and that this may bring significant reputation loss. Overall, companies show mixed response toward negative posts on employee blogs—some companies show complete aversion; others allow some negative posts. Such mixed reactions toward negative posts motivated us to probe for any positive aspects of negative posts. In particular, we investigate the relationship between negative posts and readership of an employee blog. In contrast to the popular perception, our results reveal a potential positive aspect of negative posts. Our analysis suggests that negative posts act as catalyst and can exponentially increase the readership of employee blogs, suggesting that companies should permit employees to make negative posts. Because employees typically write few negative posts and largely write positive posts, the increase in readership of employee blogs generally should be enough to offset the negative effect of few negative posts. Therefore, not restraining negative posts to increase readership should be a good strategy. This raises a logical question: what should a firm's policy be regarding employee blogging? For exposition, we suggest an analytical framework using our empirical model.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 135\n",
      "Title: Early to Adopt and Early to Discontinue: The Impact of Self-Perceived and Actual IT Knowledge on Technology Use Behaviors of End Users\n",
      "Abstract: For organizations to achieve the benefits of new information technology (IT) systems, their users must adopt and then actually use these new systems. Recent models help to articulate the potentially different explanations for why some users will adopt and then continue using new technologies, but these models have not explicitly incorporated IT knowledge. This is particularly important in contexts where the user base may be non-IT professionals—i.e., the users may vary substantially in their basic IT knowledge. We draw on psychology to argue that in situations where there is a wide variance in actual IT knowledge, there will often exist a U-shaped relationship between actual and self-perceived IT knowledge such that the least knowledgeable believe themselves to be highly knowledgeable. We then draw on individual-level adoption theories to argue that users with high self-perceived IT knowledge will be more likely to adopt new technologies and do so faster. We also draw on individual-level continuance theories to argue that users with low actual IT knowledge will be more likely to discontinue using new technologies and do so faster. We test our expectations using a proprietary data set of 225 sales professionals in a large Indian pharmaceutical company that is testing a new customer relationship management system. We find strong support for our hypotheses.\n",
      "Content: Early to Adopt and Early to Discontinue: The Impact of Self-Perceived and Actual IT Knowledge on Technology Use Behaviors of End Users For organizations to achieve the benefits of new information technology (IT) systems, their users must adopt and then actually use these new systems. Recent models help to articulate the potentially different explanations for why some users will adopt and then continue using new technologies, but these models have not explicitly incorporated IT knowledge. This is particularly important in contexts where the user base may be non-IT professionals—i.e., the users may vary substantially in their basic IT knowledge. We draw on psychology to argue that in situations where there is a wide variance in actual IT knowledge, there will often exist a U-shaped relationship between actual and self-perceived IT knowledge such that the least knowledgeable believe themselves to be highly knowledgeable. We then draw on individual-level adoption theories to argue that users with high self-perceived IT knowledge will be more likely to adopt new technologies and do so faster. We also draw on individual-level continuance theories to argue that users with low actual IT knowledge will be more likely to discontinue using new technologies and do so faster. We test our expectations using a proprietary data set of 225 sales professionals in a large Indian pharmaceutical company that is testing a new customer relationship management system. We find strong support for our hypotheses.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 136\n",
      "Title: Differential Impact of Content in Online Communication on Heterogeneous Candidates: A Field Study in Technical Recruitment\n",
      "Abstract: Recruitment is a critical activity for companies, and the research community has also shown significant interest in examining technology recruitment. Companies often communicate how they value their employees along with job requirements to potential candidates in a bid to attract them. However, there is an overall lack of understanding of how candidates react to such information and how their motivation toward the job changes with such online communication. Although there is substantial work that examines the decision-making process of managers who do technical hiring, to the best of our knowledge, there is a paucity of work that investigates the decision-making process of technical candidates. The broad research question studied is how including certain content in online communication about a technical job opportunity may (de)motivate heterogeneous candidates differently in applying for the job. We theorize the underlying process of how including certain content in online job communication may influence candidates’ propensity to apply and their minimum acceptable salary increase if they were to join the company. Additionally, we capture mediating variables that influence the effect of different online content on candidates’ propensity to apply and on candidates’ minimum acceptable salary increase. By testing actual job application behavior in a field study, we confirmed the content that can attract high performers while discouraging low performers from applying.History: Yong Tan, Senior Editor; Yan Huang, Associate Editor.Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2022.1120.\n",
      "Content: Differential Impact of Content in Online Communication on Heterogeneous Candidates: A Field Study in Technical Recruitment Recruitment is a critical activity for companies, and the research community has also shown significant interest in examining technology recruitment. Companies often communicate how they value their employees along with job requirements to potential candidates in a bid to attract them. However, there is an overall lack of understanding of how candidates react to such information and how their motivation toward the job changes with such online communication. Although there is substantial work that examines the decision-making process of managers who do technical hiring, to the best of our knowledge, there is a paucity of work that investigates the decision-making process of technical candidates. The broad research question studied is how including certain content in online communication about a technical job opportunity may (de)motivate heterogeneous candidates differently in applying for the job. We theorize the underlying process of how including certain content in online job communication may influence candidates’ propensity to apply and their minimum acceptable salary increase if they were to join the company. Additionally, we capture mediating variables that influence the effect of different online content on candidates’ propensity to apply and on candidates’ minimum acceptable salary increase. By testing actual job application behavior in a field study, we confirmed the content that can attract high performers while discouraging low performers from applying.History: Yong Tan, Senior Editor; Yan Huang, Associate Editor.Supplemental Material: The online appendices are available at https://doi.org/10.1287/isre.2022.1120.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 137\n",
      "Title: Superlatives and Scope of Improvement in Online Recommendations: Breath of Life or a Kiss of Death?\n",
      "Abstract:  Online professional networks are important tools used by recruiters to find qualified candidates for job openings. Within these networks, professional recommendations are used to supplement profiles and add credibility. These recommendations tend to be overly positive, full of superlatives, and lacking in critical statements (referred to as scope of improvement). We draw on the theory of online trust to argue that having scope of improvement and superlatives may affect various dimensions of trust and to show how online trust, in turn, can affect the usefulness of a recommendation and the likelihood of receiving an interview. We contribute to the body of work on online trust both theoretically and empirically. From a theory perspective, we explain why including scope of improvement and superlatives in recommendations on online professional networks may help certain candidates in getting an interview but hurt others. From an empirical perspective, we provide a unique empirical setting that allows us to observe not only the effect of scope of improvement and superlatives, but also validate the theoretically argued underlying process. Furthermore, through discussion with recruiters, we identify then test contextual factors that differentiate recommendations on online professional networks from traditional recommendations. In this study, we use a scenario-based, quasiexperimental survey to test the effects of superlatives and scope of improvement on the usefulness and effectiveness of recommendations. Further, we test the mediating role of trust and how the experience levels of the recommendee affect the sign and strength of these relationships. Our findings indicate that including scope of improvement increases the effectiveness and usefulness of recommendations for candidates at low-and middle levels of experience. For the most experienced candidates, including scope of improvement has a negative effect on effectiveness. Superlatives negatively affect the perceived competence of the recommender and thus should be avoided. This negative effect is reduced when combined with scope of improvement. \n",
      "Content: Superlatives and Scope of Improvement in Online Recommendations: Breath of Life or a Kiss of Death?  Online professional networks are important tools used by recruiters to find qualified candidates for job openings. Within these networks, professional recommendations are used to supplement profiles and add credibility. These recommendations tend to be overly positive, full of superlatives, and lacking in critical statements (referred to as scope of improvement). We draw on the theory of online trust to argue that having scope of improvement and superlatives may affect various dimensions of trust and to show how online trust, in turn, can affect the usefulness of a recommendation and the likelihood of receiving an interview. We contribute to the body of work on online trust both theoretically and empirically. From a theory perspective, we explain why including scope of improvement and superlatives in recommendations on online professional networks may help certain candidates in getting an interview but hurt others. From an empirical perspective, we provide a unique empirical setting that allows us to observe not only the effect of scope of improvement and superlatives, but also validate the theoretically argued underlying process. Furthermore, through discussion with recruiters, we identify then test contextual factors that differentiate recommendations on online professional networks from traditional recommendations. In this study, we use a scenario-based, quasiexperimental survey to test the effects of superlatives and scope of improvement on the usefulness and effectiveness of recommendations. Further, we test the mediating role of trust and how the experience levels of the recommendee affect the sign and strength of these relationships. Our findings indicate that including scope of improvement increases the effectiveness and usefulness of recommendations for candidates at low-and middle levels of experience. For the most experienced candidates, including scope of improvement has a negative effect on effectiveness. Superlatives negatively affect the perceived competence of the recommender and thus should be avoided. This negative effect is reduced when combined with scope of improvement. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 138\n",
      "Title: Differential Influence of Blogs Across Different Stages of Decision Making: The Case of Venture Capitalists\n",
      "Abstract: In this paper, we study the differential influence of online user-generated content (UGC), specifically blogs, across the multiple stages of decision making of venture capitalists: screening stage, choice stage, and contract stage. We conjecture that, first, blogs are influential at the screening stage; second, after the screening stage, blogs are noninfluential since decision makers evaluate entities closely at later stages; third, blogs increase the interest from multiple decision makers which in turn increases the cost of the deal for a decision maker. This empirical investigation provides support for the hypotheses, which we tested for funding decisions by venture capitalists in information technology ventures. In particular, this study indicates that blogs can help managers in getting their products/services selected at the screening stage, but, beyond that, blogs do not help directly. However, since more decision makers screen products/services that receive blog coverage, the competition among decision makers helps managers in negotiating better contract terms. We advance the boundary of existing studies on the influence of UGC from single stage process to multiple stages.\n",
      "Content: Differential Influence of Blogs Across Different Stages of Decision Making: The Case of Venture Capitalists In this paper, we study the differential influence of online user-generated content (UGC), specifically blogs, across the multiple stages of decision making of venture capitalists: screening stage, choice stage, and contract stage. We conjecture that, first, blogs are influential at the screening stage; second, after the screening stage, blogs are noninfluential since decision makers evaluate entities closely at later stages; third, blogs increase the interest from multiple decision makers which in turn increases the cost of the deal for a decision maker. This empirical investigation provides support for the hypotheses, which we tested for funding decisions by venture capitalists in information technology ventures. In particular, this study indicates that blogs can help managers in getting their products/services selected at the screening stage, but, beyond that, blogs do not help directly. However, since more decision makers screen products/services that receive blog coverage, the competition among decision makers helps managers in negotiating better contract terms. We advance the boundary of existing studies on the influence of UGC from single stage process to multiple stages.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 139\n",
      "Title: Learning to Be Creative: A Mutually Exciting Spatiotemporal Point Process Model for Idea Generation in Open Innovation\n",
      "Abstract: This study investigates the creative idea generation process in an open innovation platform. The idea generation process is simultaneously influenced by multiple activities: knowledge acquisition from participants’ interactions with each other’s ideas, deliberate practice through persistent participation, and learning through failures. Due to the dynamic interplay across these activities, it is challenging to identify each activity’s influence on creative ideation outcomes using reduced-form regression analysis. To overcome these challenges, we employ a comprehensive empirical framework, the mutually exciting spatiotemporal point process model with unobserved heterogeneity, which endogenizes the occurrences of these activities in continuous time and allows for user-dependent effects. By utilizing the activity stream data of 13,028 participants from 2010 to 2016 in an open innovation platform, we uncovered synergistic effects of these activities on creative outcomes. We find that knowledge acquired through interaction with others (i.e., stimulus ideas) plays a vital role in the creative ideation process, but their effect is more nuanced than what we have known so far. In contrast to the prior belief that distant analogies, stimulus ideas outside of a problem domain, spur creativity, we find that distant analogies lead to failures. Yet, we further find that such failures are indispensable to the creative ideation process because failures motivate idea generators (1) to acquire more knowledge by increasing their future interactions with other participants’ ideas (learning from others), and (2) to persist in generating ideas that lead to improvements in their ability to apply the acquired knowledge and to identify innovation tasks that are relevant to their stock of acquired knowledge (learning by doing). Our results indicate that failures are a stronger driver of the learning activities than successes. Based on our findings, we offer insights on how to cultivate creativity in an open innovation setting.\n",
      "Content: Learning to Be Creative: A Mutually Exciting Spatiotemporal Point Process Model for Idea Generation in Open Innovation This study investigates the creative idea generation process in an open innovation platform. The idea generation process is simultaneously influenced by multiple activities: knowledge acquisition from participants’ interactions with each other’s ideas, deliberate practice through persistent participation, and learning through failures. Due to the dynamic interplay across these activities, it is challenging to identify each activity’s influence on creative ideation outcomes using reduced-form regression analysis. To overcome these challenges, we employ a comprehensive empirical framework, the mutually exciting spatiotemporal point process model with unobserved heterogeneity, which endogenizes the occurrences of these activities in continuous time and allows for user-dependent effects. By utilizing the activity stream data of 13,028 participants from 2010 to 2016 in an open innovation platform, we uncovered synergistic effects of these activities on creative outcomes. We find that knowledge acquired through interaction with others (i.e., stimulus ideas) plays a vital role in the creative ideation process, but their effect is more nuanced than what we have known so far. In contrast to the prior belief that distant analogies, stimulus ideas outside of a problem domain, spur creativity, we find that distant analogies lead to failures. Yet, we further find that such failures are indispensable to the creative ideation process because failures motivate idea generators (1) to acquire more knowledge by increasing their future interactions with other participants’ ideas (learning from others), and (2) to persist in generating ideas that lead to improvements in their ability to apply the acquired knowledge and to identify innovation tasks that are relevant to their stock of acquired knowledge (learning by doing). Our results indicate that failures are a stronger driver of the learning activities than successes. Based on our findings, we offer insights on how to cultivate creativity in an open innovation setting.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 140\n",
      "Title: Desktop video conferencing in the organisation\n",
      "Abstract: While the potential advantages of video conferencing are appealing, the technology has not been implemented in more than a handful of organisations. This is likely to change in the future as video conferencing moves to the desktop. This paper examines the issues surrounding the area and, in particular, attempts to determine the impact of video conferencing on organisations. A framework is provided for understanding desktop video conferencing in the organisational context, and the relative benefits of and problems in the use of desktop video conferencing are discussed. Finally, a number of suggestions are made on how organisations may reconcile the implications of utilising desktop video conferencing technology.\n",
      "Content: Desktop video conferencing in the organisation While the potential advantages of video conferencing are appealing, the technology has not been implemented in more than a handful of organisations. This is likely to change in the future as video conferencing moves to the desktop. This paper examines the issues surrounding the area and, in particular, attempts to determine the impact of video conferencing on organisations. A framework is provided for understanding desktop video conferencing in the organisational context, and the relative benefits of and problems in the use of desktop video conferencing are discussed. Finally, a number of suggestions are made on how organisations may reconcile the implications of utilising desktop video conferencing technology.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 141\n",
      "Title: “How does tech make you feel?” a review and examination of negative affective responses to technology use\n",
      "Abstract: The study of individual, affect-related consequences from technology adoption and use is gaining traction in the information systems discipline. Efforts to explore affective reactions to technology have considered positive, affective constructs (e.g., enjoyment, playfulness, and flow), with a more recent focus on the dark side of technology use and constructs such as technostress, technophobia, and computer anxiety. While some research has examined these negative affective responses to technology, construct definitions and relationships are not well-defined or theoretically grounded. In this research, an integrative literature review is conducted on computer anxiety, technophobia and technostress, and the known antecedents, dimensions, and outcomes of each concept are organised into nomological networks. These nomological networks are then combined to identify inconsistencies and omissions in the literature. The Affective Response Model, a recently advanced, theoretically grounded taxonomy of affective responses to technology, is applied to differentiate the three constructs and to introduce technology-induced state anxiety (TISA), a new temporal (state-like) negative response to a specific instance of technology. Two empirical studies are conducted using existing and newly developed scales, and demonstrate that computer anxiety, technophobia, technostress and TISA are conceptually and empirically distinct and provide insight into how these constructs are related. Future research opportunities on affective responses to technology are described based on the integrated nomological network and empirical findings.\n",
      "Content: “How does tech make you feel?” a review and examination of negative affective responses to technology use The study of individual, affect-related consequences from technology adoption and use is gaining traction in the information systems discipline. Efforts to explore affective reactions to technology have considered positive, affective constructs (e.g., enjoyment, playfulness, and flow), with a more recent focus on the dark side of technology use and constructs such as technostress, technophobia, and computer anxiety. While some research has examined these negative affective responses to technology, construct definitions and relationships are not well-defined or theoretically grounded. In this research, an integrative literature review is conducted on computer anxiety, technophobia and technostress, and the known antecedents, dimensions, and outcomes of each concept are organised into nomological networks. These nomological networks are then combined to identify inconsistencies and omissions in the literature. The Affective Response Model, a recently advanced, theoretically grounded taxonomy of affective responses to technology, is applied to differentiate the three constructs and to introduce technology-induced state anxiety (TISA), a new temporal (state-like) negative response to a specific instance of technology. Two empirical studies are conducted using existing and newly developed scales, and demonstrate that computer anxiety, technophobia, technostress and TISA are conceptually and empirically distinct and provide insight into how these constructs are related. Future research opportunities on affective responses to technology are described based on the integrated nomological network and empirical findings.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 142\n",
      "Title: Matching intermediaries for information goods in the presence of direct search: an examination of switching costs and obsolescence of information\n",
      "Abstract: This paper investigates patterns of revenues earned by an intermediary that matches buyers and sellers in the presence of direct search markets. We develop a theoretical structure and a computer simulation model of such a marketplace where vendors are horizontally differentiated, and an intermediary matches clients to the optimal vendor for a fee. The model is applicable to information services such as application service providers (ASPs). The contribution of this paper is the identification of scenarios under which intermediaries that match clients and vendors are likely to be profitable considering switching costs and obsolescence of information.\n",
      "Content: Matching intermediaries for information goods in the presence of direct search: an examination of switching costs and obsolescence of information This paper investigates patterns of revenues earned by an intermediary that matches buyers and sellers in the presence of direct search markets. We develop a theoretical structure and a computer simulation model of such a marketplace where vendors are horizontally differentiated, and an intermediary matches clients to the optimal vendor for a fee. The model is applicable to information services such as application service providers (ASPs). The contribution of this paper is the identification of scenarios under which intermediaries that match clients and vendors are likely to be profitable considering switching costs and obsolescence of information.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 143\n",
      "Title: Market reactions to E-business outsourcing announcements: An event study\n",
      "Abstract: Stock markets have reacted favorably to firms who announced their implementation of E-business projects for commercial exploitation. Those that outsourced E-business projects in order to achieve swift execution also achieved abnormal positive returns. Contrary to expectations, outsourcing E-business projects with high task complexity also led to positive results. We analyzed the process and found that these three factors explained more than 20% of the variance in abnormal returns. The results were obtained from an event study of 96 E-business-related announcements, including those made by firms in the S&P500 index during 1999–2002. This paper contains information that should therefore help firms identify E-business projects for outsourcing.\n",
      "Content: Market reactions to E-business outsourcing announcements: An event study Stock markets have reacted favorably to firms who announced their implementation of E-business projects for commercial exploitation. Those that outsourced E-business projects in order to achieve swift execution also achieved abnormal positive returns. Contrary to expectations, outsourcing E-business projects with high task complexity also led to positive results. We analyzed the process and found that these three factors explained more than 20% of the variance in abnormal returns. The results were obtained from an event study of 96 E-business-related announcements, including those made by firms in the S&P500 index during 1999–2002. This paper contains information that should therefore help firms identify E-business projects for outsourcing.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 144\n",
      "Title: Catch Me If You Can: Effectiveness and Consequences of Online Copyright Enforcement\n",
      "Abstract: We evaluate the unexpected shutdown of kino.to, a major platform for unlicensed video streaming in the German market. Using highly disaggregated clickstream data in a difference-in-differences setting, we compare the web behavior of 20,000 consumers in Germany and three control countries. We find that this intervention was not very effective in reducing unlicensed consumption or encouraging licensed consumption, mainly because users quickly switch to alternative unlicensed sites. We highlight that the shutdown additionally had important unintended externalities. Individuals who never visited kino.to and who additionally clicked on news articles that covered the shutdown increased their visits to piracy websites substantially. We show that this effect largely comes from articles that explicitly mention alternative websites or suggest that users do not have to fear legal consequences from unlicensed streaming. Finally, we document that the unlicensed video streaming market is much more fragmented after the shutdown, potentially affecting future interventions, at least in the short run. We argue that our results can be helpful to understand why online piracy rates are still high, despite a plethora of enforcement efforts.\n",
      "Content: Catch Me If You Can: Effectiveness and Consequences of Online Copyright Enforcement We evaluate the unexpected shutdown of kino.to, a major platform for unlicensed video streaming in the German market. Using highly disaggregated clickstream data in a difference-in-differences setting, we compare the web behavior of 20,000 consumers in Germany and three control countries. We find that this intervention was not very effective in reducing unlicensed consumption or encouraging licensed consumption, mainly because users quickly switch to alternative unlicensed sites. We highlight that the shutdown additionally had important unintended externalities. Individuals who never visited kino.to and who additionally clicked on news articles that covered the shutdown increased their visits to piracy websites substantially. We show that this effect largely comes from articles that explicitly mention alternative websites or suggest that users do not have to fear legal consequences from unlicensed streaming. Finally, we document that the unlicensed video streaming market is much more fragmented after the shutdown, potentially affecting future interventions, at least in the short run. We argue that our results can be helpful to understand why online piracy rates are still high, despite a plethora of enforcement efforts.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 145\n",
      "Title: Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form1\n",
      "Abstract: Researchers in a number of disciplines, including Information Systems, have argued that much of past research may have incorrectly specified the relationship between latent variables and indicators as reflective when an understanding of a construct and its measures indicates that a formative specification would have been warranted. Coupled with the posited severe biasing effects of construct misspecification on structural parameters, these two assertions would lead to concluding that an important portion of our literature is largely invalid. While we do not delve into the issue of when one specification should be employed over another, our work here contends that construct misspecification, but with a particular exception, does not lead to severely biased estimates. We argue, and show through extensive simulations, that a lack of attention to the metric in which relationships are expressed is responsible for the current belief in the negative effects of misspecification.\n",
      "Content: Revisiting Bias Due to Construct Misspecification: Different Results from Considering Coefficients in Standardized Form1 Researchers in a number of disciplines, including Information Systems, have argued that much of past research may have incorrectly specified the relationship between latent variables and indicators as reflective when an understanding of a construct and its measures indicates that a formative specification would have been warranted. Coupled with the posited severe biasing effects of construct misspecification on structural parameters, these two assertions would lead to concluding that an important portion of our literature is largely invalid. While we do not delve into the issue of when one specification should be employed over another, our work here contends that construct misspecification, but with a particular exception, does not lead to severely biased estimates. We argue, and show through extensive simulations, that a lack of attention to the metric in which relationships are expressed is responsible for the current belief in the negative effects of misspecification.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 146\n",
      "Title: A Rejoinder to Rigdon et al. (2014)\n",
      "Abstract: We appreciate the interest shown by Rigdon et al. [Rigdon EE, Becker J-M, Rai A, Ringle CM, Diamantopoulos A, Karahanna E, Straub DW, Dijkstra TK (2014) Conflating antecedents and formative indicators: A comment on Aguirre-Urreta and Marakas. Inform. Systems Res. 25(4):780–784.] in our recent work and for the time and effort spent in carefully considering it and offering their comments and concerns. In what follows, and within the limitations of a short rejoinder, we offer our response to their comments, highlighting points of agreement and noting where more research is necessary.\n",
      "Content: A Rejoinder to Rigdon et al. (2014) We appreciate the interest shown by Rigdon et al. [Rigdon EE, Becker J-M, Rai A, Ringle CM, Diamantopoulos A, Karahanna E, Straub DW, Dijkstra TK (2014) Conflating antecedents and formative indicators: A comment on Aguirre-Urreta and Marakas. Inform. Systems Res. 25(4):780–784.] in our recent work and for the time and effort spent in carefully considering it and offering their comments and concerns. In what follows, and within the limitations of a short rejoinder, we offer our response to their comments, highlighting points of agreement and noting where more research is necessary.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 147\n",
      "Title: Partial Least Squares and Models with Formatively Specified Endogenous Constructs: A Cautionary Note\n",
      "Abstract: Information systems researchers have recently begun to propose models that include formatively specified constructs, and largely rely on partial least squares (PLS) to estimate the parameters of interest in those models. In this research, we focus on those cases where the formatively specified constructs are endogenous to other constructs in the research model in addition to their own manifest indicators, which are quite common in published research in the discipline, and analyze whether PLS is a valid statistical technique for estimating those models. Although there is evidence that covariance-based approaches can accurately estimate them, this is the first research that examines whether PLS can indeed do so. Through a theoretical analysis based on the inner workings of the PLS algorithm, which is later validated and extended through a series of Monte Carlo simulations, we conclude that is not the case. Specifically, estimates obtained from PLS are capturing something other than the relationship of interest when the formatively specified constructs are endogenous to others in the model. We show how our results apply more generally to a class of models, and discuss implications for future research practice.\n",
      "Content: Partial Least Squares and Models with Formatively Specified Endogenous Constructs: A Cautionary Note Information systems researchers have recently begun to propose models that include formatively specified constructs, and largely rely on partial least squares (PLS) to estimate the parameters of interest in those models. In this research, we focus on those cases where the formatively specified constructs are endogenous to other constructs in the research model in addition to their own manifest indicators, which are quite common in published research in the discipline, and analyze whether PLS is a valid statistical technique for estimating those models. Although there is evidence that covariance-based approaches can accurately estimate them, this is the first research that examines whether PLS can indeed do so. Through a theoretical analysis based on the inner workings of the PLS algorithm, which is later validated and extended through a series of Monte Carlo simulations, we conclude that is not the case. Specifically, estimates obtained from PLS are capturing something other than the relationship of interest when the formatively specified constructs are endogenous to others in the model. We show how our results apply more generally to a class of models, and discuss implications for future research practice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 148\n",
      "Title: Statistical Inference with Plsc Using Bootstrap Confidence Intervals\n",
      "Abstract: Partial least squares (PLS) is one of the most popular statistical techniques in use in the Information Systems field. When applied to data originating from a common factor model, as is often the case in the discipline, PLS will produce biased estimates. A recent development, consistent PLS (PLSc), has been introduced to correct for this bias. In addition, the common practice in PLS of comparing the ratio of an estimate to its standard error to a t distribution for the purposes of statistical inference has also been challenged. We contribute to the practice of research in the IS discipline by providing evidence of the value of employing bootstrap confidence intervals in conjunction with PLSc, which is a more appropriate alternative than PLS for many of the research scenarios that are of interest to the field. Such evidence is direly needed before a complete approach to the estimation of SEM that relies on both PLSc and bootstrap CIs can be widely adopted. We also provide recommendations for researchers on the use of confidence intervals with PLSc.\n",
      "Content: Statistical Inference with Plsc Using Bootstrap Confidence Intervals Partial least squares (PLS) is one of the most popular statistical techniques in use in the Information Systems field. When applied to data originating from a common factor model, as is often the case in the discipline, PLS will produce biased estimates. A recent development, consistent PLS (PLSc), has been introduced to correct for this bias. In addition, the common practice in PLS of comparing the ratio of an estimate to its standard error to a t distribution for the purposes of statistical inference has also been challenged. We contribute to the practice of research in the IS discipline by providing evidence of the value of employing bootstrap confidence intervals in conjunction with PLSc, which is a more appropriate alternative than PLS for many of the research scenarios that are of interest to the field. Such evidence is direly needed before a complete approach to the estimation of SEM that relies on both PLSc and bootstrap CIs can be widely adopted. We also provide recommendations for researchers on the use of confidence intervals with PLSc.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 149\n",
      "Title: A two-stage machine learning framework to predict heart transplantation survival probabilities over time with a monotonic probability constraint\n",
      "Abstract: The overarching goal of this paper is to develop a modeling framework that can be used to obtain personalized, data-driven and monotonically constrained probability curves. This research is motivated by the important problem of improving the predictions for organ transplantation outcomes, which can inform updates made to organ allocation protocols, post-transplantation care pathways, and clinical resource utilization. In pursuit of our overarching goal and motivating problem, we propose a novel two-stage machine learning-based framework for obtaining monotonic probabilities over time. The first stage uses the standard approach of using independent machine learning models to predict transplantation outcomes for each time-period of interest. In the second stage, we calibrate the survival probabilities over time using isotonic regression. To show the utility of our framework, we applied it on a national registry of U.S. heart transplants from 1987 to 2016. The first stage produces an area under the receiver operating curve (AUC) between 0.60 and 0.71 for years 1–10. While the 1year prediction AUC result is comparable to the reported results in the literature, our 10-year AUC of 0.70 is higher than the current state-of-the-art results. More importantly, we show that the application of isotonic regression to calibrate the survival probabilities for each patient over the 10-year period guarantees mono­ tonicity, while capitalizing on the data-driven and individualized nature of machine learning models. To pro­ mote future research, our code and analysis are publicly available on GitHub. Furthermore, we created a web app titled “H-TOP: Heart Transplantation Outcome Predictor” to encourage practical applications.\n",
      "Content: A two-stage machine learning framework to predict heart transplantation survival probabilities over time with a monotonic probability constraint The overarching goal of this paper is to develop a modeling framework that can be used to obtain personalized, data-driven and monotonically constrained probability curves. This research is motivated by the important problem of improving the predictions for organ transplantation outcomes, which can inform updates made to organ allocation protocols, post-transplantation care pathways, and clinical resource utilization. In pursuit of our overarching goal and motivating problem, we propose a novel two-stage machine learning-based framework for obtaining monotonic probabilities over time. The first stage uses the standard approach of using independent machine learning models to predict transplantation outcomes for each time-period of interest. In the second stage, we calibrate the survival probabilities over time using isotonic regression. To show the utility of our framework, we applied it on a national registry of U.S. heart transplants from 1987 to 2016. The first stage produces an area under the receiver operating curve (AUC) between 0.60 and 0.71 for years 1–10. While the 1year prediction AUC result is comparable to the reported results in the literature, our 10-year AUC of 0.70 is higher than the current state-of-the-art results. More importantly, we show that the application of isotonic regression to calibrate the survival probabilities for each patient over the 10-year period guarantees mono­ tonicity, while capitalizing on the data-driven and individualized nature of machine learning models. To pro­ mote future research, our code and analysis are publicly available on GitHub. Furthermore, we created a web app titled “H-TOP: Heart Transplantation Outcome Predictor” to encourage practical applications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 150\n",
      "Title: The Effects of Time Pressure and Completeness of Information on Decision Making\n",
      "Abstract: The Israeli Air Force (IAF) has developed a simulation system to train its top commanders in how to use defensive resources in the face of an aerial attack by enemy combat aircraft. During the simulation session, the commander in charge allocates airborne and standby resources and dispatches or diverts aircraft to intercept intruders. Seventy-four simulation sessions were conducted in order to examine the effects of time pressure and completeness of information on the performance of twenty-nine top IAF commanders. Variables examined were: (1) display of complete versus incomplete information, (2) time-constrained decision making versus unlimited decision time, and (3) the difference in performance between top strategic commanders and mid-level field commanders. The authors' results show that complete information usually improved performance. However, field commanders (as opposed to top strategic commanders) did not improve their performance when presented with complete information under pressure of time. Time pressure usually, but not always, impaired performance. Top commanders tended to make fewer changes in previous decisions than did field commanders.\n",
      "Content: The Effects of Time Pressure and Completeness of Information on Decision Making The Israeli Air Force (IAF) has developed a simulation system to train its top commanders in how to use defensive resources in the face of an aerial attack by enemy combat aircraft. During the simulation session, the commander in charge allocates airborne and standby resources and dispatches or diverts aircraft to intercept intruders. Seventy-four simulation sessions were conducted in order to examine the effects of time pressure and completeness of information on the performance of twenty-nine top IAF commanders. Variables examined were: (1) display of complete versus incomplete information, (2) time-constrained decision making versus unlimited decision time, and (3) the difference in performance between top strategic commanders and mid-level field commanders. The authors' results show that complete information usually improved performance. However, field commanders (as opposed to top strategic commanders) did not improve their performance when presented with complete information under pressure of time. Time pressure usually, but not always, impaired performance. Top commanders tended to make fewer changes in previous decisions than did field commanders.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 151\n",
      "Title: Factors Affecting Policy for Distributing Computing Resources\n",
      "Abstract:  This article discusses the findings of an empirical study whose major purpose was to analyze the relationship between various organizational attributes and the deployment of hardware resources. The study used a sample of 303 organizations. The article begins by reviewing some fundamentals of information systems (IS) distribution policy. After providing the basic research hypothesis and discussing the data collection process, it then describes the sample and the distribution of the data and presents the statistical analysis. The last section discusses the findings and provides some concluding remarks. AbstractThis article discusses the findings of an empirical study conducted on 303 organizations. The major purpose of the study was to analyze the relationship between various organizational attributes and the deployment of hardware resources. The salient finding was that the most influential variable is the distribution of decisionmaking processes in the organization. The more decision making is distributed, the more hardware is distributed. No significant relationships were detected between hardware distribution and any of the following variables: organizational structure, economic sectorial association, and the size of the organization. \n",
      "Content: Factors Affecting Policy for Distributing Computing Resources  This article discusses the findings of an empirical study whose major purpose was to analyze the relationship between various organizational attributes and the deployment of hardware resources. The study used a sample of 303 organizations. The article begins by reviewing some fundamentals of information systems (IS) distribution policy. After providing the basic research hypothesis and discussing the data collection process, it then describes the sample and the distribution of the data and presents the statistical analysis. The last section discusses the findings and provides some concluding remarks. AbstractThis article discusses the findings of an empirical study conducted on 303 organizations. The major purpose of the study was to analyze the relationship between various organizational attributes and the deployment of hardware resources. The salient finding was that the most influential variable is the distribution of decisionmaking processes in the organization. The more decision making is distributed, the more hardware is distributed. No significant relationships were detected between hardware distribution and any of the following variables: organizational structure, economic sectorial association, and the size of the organization. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 152\n",
      "Title: A Flexible Approach to Information System Development\n",
      "Abstract:  The Information Systems Development Life Cycle (ISDLC) is usually treated as a rigid sequence of ac. tivities. This article asserts that differences in the nature of development projects should affect the planning of the ISDLC. Two classes of factors affecting the ISDLC are identified: factors relating to the environment and factors relating to the development effort (e.g., inhouse development vs. canned software package). Each step along the ISDLC is decomposed into several dimensions relating to the activities that should be performed, the degree of control that should be exerted, to human resources, to other resources, and to the time factor. The relationship between the six dimensions and the two classes of factors are explained. Finally, a practical approach to ISDLC planning is suggested based on a structured procedure and a number of working forms. It assists in preliminary planning of the development process as well as in periodic reviews and revisions whenever the project reaches a certain milestone. \n",
      "Content: A Flexible Approach to Information System Development  The Information Systems Development Life Cycle (ISDLC) is usually treated as a rigid sequence of ac. tivities. This article asserts that differences in the nature of development projects should affect the planning of the ISDLC. Two classes of factors affecting the ISDLC are identified: factors relating to the environment and factors relating to the development effort (e.g., inhouse development vs. canned software package). Each step along the ISDLC is decomposed into several dimensions relating to the activities that should be performed, the degree of control that should be exerted, to human resources, to other resources, and to the time factor. The relationship between the six dimensions and the two classes of factors are explained. Finally, a practical approach to ISDLC planning is suggested based on a structured procedure and a number of working forms. It assists in preliminary planning of the development process as well as in periodic reviews and revisions whenever the project reaches a certain milestone. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 153\n",
      "Title: Instant Quality Control of Large Batch Processing Jobs\n",
      "Abstract:  The most common way to identify success or failure of a job running in a batch-processing mode is by examining a completion code sent by the job to the host operating system. Yet, for a variety of reasons the completion code may inaccurately indicate a successful termination of the job. This article describes a different approach to monitoring the quality of batch processing jobs while in operation. A pattern of behavior is suggested for a program. The pattern reflects ratios of consumption of various hardware resources. The ratios are determined by collecting historical performance variables of the job and analyzing the data by means of statistical methods. Once a pattern is set, the performance variables of every individual run of the program are compared with the precalculated pattern of behavior and if the deviation is beyond certain limits an alarm is triggered.The proposed quality control technique has been tested on real applications, as well as on some artificial programs. The findings suggest that the technique is reliable in that # successfully distinguishes between proper and malfunctioning runs of a program. \n",
      "Content: Instant Quality Control of Large Batch Processing Jobs  The most common way to identify success or failure of a job running in a batch-processing mode is by examining a completion code sent by the job to the host operating system. Yet, for a variety of reasons the completion code may inaccurately indicate a successful termination of the job. This article describes a different approach to monitoring the quality of batch processing jobs while in operation. A pattern of behavior is suggested for a program. The pattern reflects ratios of consumption of various hardware resources. The ratios are determined by collecting historical performance variables of the job and analyzing the data by means of statistical methods. Once a pattern is set, the performance variables of every individual run of the program are compared with the precalculated pattern of behavior and if the deviation is beyond certain limits an alarm is triggered.The proposed quality control technique has been tested on real applications, as well as on some artificial programs. The findings suggest that the technique is reliable in that # successfully distinguishes between proper and malfunctioning runs of a program. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 154\n",
      "Title: Environmental scanning and information systems in relation to success in introducing new products\n",
      "Abstract: CEOs in 46 firms were interviewed in regard to the pattern of the environmental scanning they performed. The results were analyzed to determine the degree of use of information systems by CEOs in their strategic decision making and to seek a link with the firm's success in introducing new products. The study indicates significant differences in the level of environmental scanning and in the use of information systems between firms that were more successful in introducing new products into the market and firms that were less successful. The differences are in the pattern and the frequency of conducting environmental scanning, in the number of computerized applications, and in the number of advanced marketing information systems.\n",
      "Content: Environmental scanning and information systems in relation to success in introducing new products CEOs in 46 firms were interviewed in regard to the pattern of the environmental scanning they performed. The results were analyzed to determine the degree of use of information systems by CEOs in their strategic decision making and to seek a link with the firm's success in introducing new products. The study indicates significant differences in the level of environmental scanning and in the use of information systems between firms that were more successful in introducing new products into the market and firms that were less successful. The differences are in the pattern and the frequency of conducting environmental scanning, in the number of computerized applications, and in the number of advanced marketing information systems.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 155\n",
      "Title: A Systematic Approach Toward Assessing the Value of an Information System\n",
      "Abstract:  A bstractA multiattribute utility approach is adopted to assess the value of an information system. Various economic analyses of the value of information are reviewed and the conceptual problems regarding the definition of this value and some measurement difficulties are discussed. A list of possible utility attributes is proposed for a reporting system value assessment, and for each attribute a measure and a utility function is suggested. Some techniques which constitute a joint utility function are presented, accompanied by two examples. A real case of minicomputer selection is given in order to illustrate the structured approach. \n",
      "Content: A Systematic Approach Toward Assessing the Value of an Information System  A bstractA multiattribute utility approach is adopted to assess the value of an information system. Various economic analyses of the value of information are reviewed and the conceptual problems regarding the definition of this value and some measurement difficulties are discussed. A list of possible utility attributes is proposed for a reporting system value assessment, and for each attribute a measure and a utility function is suggested. Some techniques which constitute a joint utility function are presented, accompanied by two examples. A real case of minicomputer selection is given in order to illustrate the structured approach. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 156\n",
      "Title: A resource-based perspective of value generation through enterprise architecture management\n",
      "Abstract: Our study contributes to the nascent discourse on enterprise architecture management (EAM) benefits by pro­ posing a theory-led, empirically validated model to explain how EAM benefits unfold. Drawing on the resourcebased theory, and based on empirical insights from 8 case studies, we find that EAM does not create benefits per se, but only creates value if an organization develops four second-order EAM capabilities – EA modeling, EA planning, EA implementation, and EA governance. The discovery that EAM resources only unfold their potential when forming EAM capabilities casts doubt on the established practice of initiating EAM as a modeling and documentation endeavor.\n",
      "Content: A resource-based perspective of value generation through enterprise architecture management Our study contributes to the nascent discourse on enterprise architecture management (EAM) benefits by pro­ posing a theory-led, empirically validated model to explain how EAM benefits unfold. Drawing on the resourcebased theory, and based on empirical insights from 8 case studies, we find that EAM does not create benefits per se, but only creates value if an organization develops four second-order EAM capabilities – EA modeling, EA planning, EA implementation, and EA governance. The discovery that EAM resources only unfold their potential when forming EAM capabilities casts doubt on the established practice of initiating EAM as a modeling and documentation endeavor.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 157\n",
      "Title: Composite quality of service and decision making perspectives in wireless networks\n",
      "Abstract: Supporting quality of service (QoS) in wireless networks has been a very rich and interesting area of research. Many significant advances have been made in supporting QoS in single wireless networks. However, the support for the QoS across multiple heterogeneous wireless networks will be required in the future wireless networks. In connections spanning multiple wireless networks, the end-to-end QoS will depend on several factors such as mobility and connection patterns of users, and the QoS policies in each of the wireless networks. The end-to-end QoS is also affected by multiple decisions that must be made by several different network entities for resource allocation. The paper has two objectives: one is to demonstrate the decision making process for resource allocation in multiple heterogeneous wireless networks and the second is to present a novel concept of composite QoS in such wireless environment. More specifically, we present an architecture for multiple heterogeneous wireless networks, decision making process for resource request and allocation, a simulation model to study composite QoS, and several interesting results. We also present potential implications of composite QoS on users and network service providers. We also show how the QoS ideas presented in this paper can be used by wireless carriers for improved QoS support and management. The paper can form the basis for a significant further research in DSS for emerging 3G/4G wireless networks supporting QoS for a range of sophisticated and resource intensive mobile applications.\n",
      "Content: Composite quality of service and decision making perspectives in wireless networks Supporting quality of service (QoS) in wireless networks has been a very rich and interesting area of research. Many significant advances have been made in supporting QoS in single wireless networks. However, the support for the QoS across multiple heterogeneous wireless networks will be required in the future wireless networks. In connections spanning multiple wireless networks, the end-to-end QoS will depend on several factors such as mobility and connection patterns of users, and the QoS policies in each of the wireless networks. The end-to-end QoS is also affected by multiple decisions that must be made by several different network entities for resource allocation. The paper has two objectives: one is to demonstrate the decision making process for resource allocation in multiple heterogeneous wireless networks and the second is to present a novel concept of composite QoS in such wireless environment. More specifically, we present an architecture for multiple heterogeneous wireless networks, decision making process for resource request and allocation, a simulation model to study composite QoS, and several interesting results. We also present potential implications of composite QoS on users and network service providers. We also show how the QoS ideas presented in this paper can be used by wireless carriers for improved QoS support and management. The paper can form the basis for a significant further research in DSS for emerging 3G/4G wireless networks supporting QoS for a range of sophisticated and resource intensive mobile applications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 158\n",
      "Title: Vulnerability disclosure mechanisms: A synthesis and framework for market-based and non-market-based disclosures\n",
      "Abstract: Vulnerability disclosure has been a controversial topic among scholars and practitioners. Most scholars agree on adopting the responsible disclosure practices for vulnerability disclosures, which give firms a protected period to address the vulnerability before public disclosure is made. However, the firms may not fully utilize the protected period resulting in financial and reputational losses. The recent popularity in market-based disclosure methods such as bug bounty programs has provided new methods to control ethical hackers and effectively manage the disclosure timelines. Through a systematic literature review, we investigate and identify various vulnerability disclosure mechanisms and elaborate the disclosure process of each mechanism. We synthesize and compare the antecedents and consequences of the vulnerability disclosure under market- and non-market-based disclosure mechanisms by proposing two research frameworks. Our analysis suggests that incentivizing hackers in market mechanisms change hackers’ motivations, leading to behavioral changes and eventually giving firms more control over the disclosure process. Additionally, our research frameworks provide a basis for further theorizing in this area. We also identify several open research questions addressing issues and challenges in the marketbased disclosures. The research has important implications for firms, hackers, policymakers, and researchers in this area.\n",
      "Content: Vulnerability disclosure mechanisms: A synthesis and framework for market-based and non-market-based disclosures Vulnerability disclosure has been a controversial topic among scholars and practitioners. Most scholars agree on adopting the responsible disclosure practices for vulnerability disclosures, which give firms a protected period to address the vulnerability before public disclosure is made. However, the firms may not fully utilize the protected period resulting in financial and reputational losses. The recent popularity in market-based disclosure methods such as bug bounty programs has provided new methods to control ethical hackers and effectively manage the disclosure timelines. Through a systematic literature review, we investigate and identify various vulnerability disclosure mechanisms and elaborate the disclosure process of each mechanism. We synthesize and compare the antecedents and consequences of the vulnerability disclosure under market- and non-market-based disclosure mechanisms by proposing two research frameworks. Our analysis suggests that incentivizing hackers in market mechanisms change hackers’ motivations, leading to behavioral changes and eventually giving firms more control over the disclosure process. Additionally, our research frameworks provide a basis for further theorizing in this area. We also identify several open research questions addressing issues and challenges in the marketbased disclosures. The research has important implications for firms, hackers, policymakers, and researchers in this area.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 159\n",
      "Title: Knowledge-based scenario management — Process and support\n",
      "Abstract: Scenario planning is a widely accepted management process for decision support activities. Though conventional decision support systems provide a strong database, modeling and visualization capabilities for the decision maker, they do not explicitly support scenario management. We propose an integrated life cycle approach for knowledge-based scenario-driven decision support incorporating three interrelated frameworks at different abstraction levels to support this process. The macro-level knowledge-based framework guides the Meso-level Scenario-driven framework, and these two in turn guide and inform the micro-level process-oriented framework. We develop a domain independent, component-based, and layered architecture to support the scenario management process and framework. The framework and architecture are realized through a concrete prototype.\n",
      "Content: Knowledge-based scenario management — Process and support Scenario planning is a widely accepted management process for decision support activities. Though conventional decision support systems provide a strong database, modeling and visualization capabilities for the decision maker, they do not explicitly support scenario management. We propose an integrated life cycle approach for knowledge-based scenario-driven decision support incorporating three interrelated frameworks at different abstraction levels to support this process. The macro-level knowledge-based framework guides the Meso-level Scenario-driven framework, and these two in turn guide and inform the micro-level process-oriented framework. We develop a domain independent, component-based, and layered architecture to support the scenario management process and framework. The framework and architecture are realized through a concrete prototype.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 160\n",
      "Title: Sustainability modelling and reporting: From roadmap to implementation\n",
      "Abstract: Sustainable business management aspires towards balancing and integrating social, economic and environmental dimensions. Existing roadmaps, frameworks and systems do not comprehensively support sustainable business transformation nor do they allow decision makers to explore interrelationships and influences between the sustainability dimensions. This leads to silo-based decision making where vision and strategies are not mapped to execution, and sustainability modelling and reporting processes are uncoordinated. This research proposes and implements a generic sustainable business transformation roadmap, which is supported by a framework and architecture for integrated sustainability modelling and reporting. The implementation leverages system dynamics, workflow modelling and adaptable system concepts.\n",
      "Content: Sustainability modelling and reporting: From roadmap to implementation Sustainable business management aspires towards balancing and integrating social, economic and environmental dimensions. Existing roadmaps, frameworks and systems do not comprehensively support sustainable business transformation nor do they allow decision makers to explore interrelationships and influences between the sustainability dimensions. This leads to silo-based decision making where vision and strategies are not mapped to execution, and sustainability modelling and reporting processes are uncoordinated. This research proposes and implements a generic sustainable business transformation roadmap, which is supported by a framework and architecture for integrated sustainability modelling and reporting. The implementation leverages system dynamics, workflow modelling and adaptable system concepts.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 161\n",
      "Title: A design and implementation model for life cycle cost management system\n",
      "Abstract: Design-to-cost is a management philosophy that emphasizes the selection and design of a system based on minimizing life-cycle cost. In some instances, systems alternatives are evaluated using such analysis, but actual implementation of design-to-cost philosophy throughout the entire system life is an exception rather than the rule. Management's lack of planning makes it difficult to implement this important philosophy. This paper analyzes and identifies the issues and provides a framework for design and implementation of a life-cycle cost management system.\n",
      "Content: A design and implementation model for life cycle cost management system Design-to-cost is a management philosophy that emphasizes the selection and design of a system based on minimizing life-cycle cost. In some instances, systems alternatives are evaluated using such analysis, but actual implementation of design-to-cost philosophy throughout the entire system life is an exception rather than the rule. Management's lack of planning makes it difficult to implement this important philosophy. This paper analyzes and identifies the issues and provides a framework for design and implementation of a life-cycle cost management system.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 162\n",
      "Title: IS diffusion: A dynamic control and stakeholder perspective\n",
      "Abstract: Our research uses a novel perspective of control balancing and stakeholder orientation to explore an information system (IS) phenomenon called enterprise IS diffusion. Employing a case-based, grounded theory approach, we analyze four large-scale IS implementation projects in a Canadian government organization. Our findings offer a synergistic approach to IS diffusion by integrating a dynamic control configuration perspective with stakeholder engagement, stakeholder sensitivity, and the impact of shifting stakeholder orientations on IS diffusion stages.We draw from control, control balancing, and stakeholder theories. Moreover, we refer to the IS implementation and IS diffusion literature to develop our argument. In doing so, this research develops a new and vital con­ struct—stakeholder orientation—for IS implementation projects. We also identify the existence of four distinct stakeholder orientations, a combination of stakeholder engagement with stakeholder sensitivity in large ISD projects: (a) strategic, (b) responsibility, (c) best interest, and (d) economic cost.Through analysis and synthesis, we establish vital relationships among shifting control configurations, stakeholder engagement, stakeholder sensitivity, and IS diffusion stages, where maintaining an optimal stakeholder orientation, facilitated by appropriate control configuration, leads to a successful IS diffusion outcome. Furthermore, we propose an extension to the existing control balancing theory by identifying missing links and trigger factors.\n",
      "Content: IS diffusion: A dynamic control and stakeholder perspective Our research uses a novel perspective of control balancing and stakeholder orientation to explore an information system (IS) phenomenon called enterprise IS diffusion. Employing a case-based, grounded theory approach, we analyze four large-scale IS implementation projects in a Canadian government organization. Our findings offer a synergistic approach to IS diffusion by integrating a dynamic control configuration perspective with stakeholder engagement, stakeholder sensitivity, and the impact of shifting stakeholder orientations on IS diffusion stages.We draw from control, control balancing, and stakeholder theories. Moreover, we refer to the IS implementation and IS diffusion literature to develop our argument. In doing so, this research develops a new and vital con­ struct—stakeholder orientation—for IS implementation projects. We also identify the existence of four distinct stakeholder orientations, a combination of stakeholder engagement with stakeholder sensitivity in large ISD projects: (a) strategic, (b) responsibility, (c) best interest, and (d) economic cost.Through analysis and synthesis, we establish vital relationships among shifting control configurations, stakeholder engagement, stakeholder sensitivity, and IS diffusion stages, where maintaining an optimal stakeholder orientation, facilitated by appropriate control configuration, leads to a successful IS diffusion outcome. Furthermore, we propose an extension to the existing control balancing theory by identifying missing links and trigger factors.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 163\n",
      "Title: Utilizing knowledge context in virtual collaborative work\n",
      "Abstract: The understanding of knowledge can be impaired if it is isolated from the proper context. Despite the importance of contextual information, there has been limited support for utilizing context in current knowledge management and collaborative systems. This paper presents a knowledge context model, called KC-V, which facilitates the use of contextual information in virtual collaborative work. Four benefits of using KC-V are suggested: evolutionary accumulation of knowledge aligned with collaborative activities, supporting the virtual team lifecycle, improved understanding by rich navigation paths, and searching for knowledge with similar context. A web-based collaboration system called VWSS is developed using KC-V.\n",
      "Content: Utilizing knowledge context in virtual collaborative work The understanding of knowledge can be impaired if it is isolated from the proper context. Despite the importance of contextual information, there has been limited support for utilizing context in current knowledge management and collaborative systems. This paper presents a knowledge context model, called KC-V, which facilitates the use of contextual information in virtual collaborative work. Four benefits of using KC-V are suggested: evolutionary accumulation of knowledge aligned with collaborative activities, supporting the virtual team lifecycle, improved understanding by rich navigation paths, and searching for knowledge with similar context. A web-based collaboration system called VWSS is developed using KC-V.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 164\n",
      "Title: Attention Adjustment, Renewal, and Equilibrium Seeking in Online Search: An Eye-Tracking Approach\n",
      "Abstract: Using eye-tracking field experiments, we examine the dynamics underlying consumers' attention-allocation behaviors in online search, with focus on attention adjustment, attention renewal, and equilibrium seeking. In particular, we probed into how consumers' e-commerce search behaviors vary when they are exposed to an advertisement during a search and when they are not. The findings from the two separate experiments suggest that consumers' attention span decreases exponentially, instead of linearly, as they maneuver from the top to the bottom of a search result webpage. The total number of available options significantly influences the speed and pattern of attention decay. However, attention decay does not simply move in the direction of depletion but can be refreshed and renewed upon encountering attention-diverting ad stimuli. Although ad stimuli are often considered distracting and worthless, they can produce positive effects when positioned in the middle of a search results listing, where a consumer's attention resources are rejuvenated by ads. Finally, because of consumers' propensity to seek equilibrium, attention decay occurs more rapidly after, rather than before, attention renewal. We extend the literature on the mere categorization effect by investigating how ad stimuli structurally separate search choices into mental categories and diminish on-going attention decay patterns.\n",
      "Content: Attention Adjustment, Renewal, and Equilibrium Seeking in Online Search: An Eye-Tracking Approach Using eye-tracking field experiments, we examine the dynamics underlying consumers' attention-allocation behaviors in online search, with focus on attention adjustment, attention renewal, and equilibrium seeking. In particular, we probed into how consumers' e-commerce search behaviors vary when they are exposed to an advertisement during a search and when they are not. The findings from the two separate experiments suggest that consumers' attention span decreases exponentially, instead of linearly, as they maneuver from the top to the bottom of a search result webpage. The total number of available options significantly influences the speed and pattern of attention decay. However, attention decay does not simply move in the direction of depletion but can be refreshed and renewed upon encountering attention-diverting ad stimuli. Although ad stimuli are often considered distracting and worthless, they can produce positive effects when positioned in the middle of a search results listing, where a consumer's attention resources are rejuvenated by ads. Finally, because of consumers' propensity to seek equilibrium, attention decay occurs more rapidly after, rather than before, attention renewal. We extend the literature on the mere categorization effect by investigating how ad stimuli structurally separate search choices into mental categories and diminish on-going attention decay patterns.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 165\n",
      "Title: Assessing the contribution of knowledge to business performance: the KP3 methodology\n",
      "Abstract: Knowledge is inherently difficult to measure. However, without valid and reliable measurement, it is very difficult to develop a comprehensive theory of knowledge and provide a practical guide for knowledge management. In this paper, we do not measure knowledge directly, but assess how much knowledge contributes to business performance. The KP3 methodology developed in this paper assesses the contribution of knowledge to business performance by employing product and process as intermediaries between the two. The understanding of the contribution is essential because it makes it possible to assess the productivities of knowledge entities, evaluate and compensate knowledge workers, and to allocate and develop human capital.\n",
      "Content: Assessing the contribution of knowledge to business performance: the KP3 methodology Knowledge is inherently difficult to measure. However, without valid and reliable measurement, it is very difficult to develop a comprehensive theory of knowledge and provide a practical guide for knowledge management. In this paper, we do not measure knowledge directly, but assess how much knowledge contributes to business performance. The KP3 methodology developed in this paper assesses the contribution of knowledge to business performance by employing product and process as intermediaries between the two. The understanding of the contribution is essential because it makes it possible to assess the productivities of knowledge entities, evaluate and compensate knowledge workers, and to allocate and develop human capital.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 166\n",
      "Title: Decision support for real-time telemarketing operations through Bayesian network learning\n",
      "Abstract: Many knowledge discovery systems have been developed in diverse areas, but few systems address the use of knowledge in decision problems explicitly. This paper presents a decision support system for real-time telemarketing operations using the information extracted from the Bayesian network learning model. A prototype decision support system was developed for AT&T customer-contact employees to provide a recommendation regarding the promotion of a telephone discount plan. The system integrated a Bayesian network learning model (knowledge discovery process) and decision-making technique (influence diagram) to provide real-time decision support. A Bayesian network learning model was used to predict a probability of the customer's response from the previous promotion/response history. The influence diagram framework was used to integrate the predicted probability with the cost and benefit related to the possible actions. It was demonstrated that decision support by the Bayesian network learning model itself can be misleading. However, by linking the Bayesian network learning model with rigorous decision-making techniques such as influence diagrams, the decision support system developed in this paper was shown to provide an intelligent decision advice.\n",
      "Content: Decision support for real-time telemarketing operations through Bayesian network learning Many knowledge discovery systems have been developed in diverse areas, but few systems address the use of knowledge in decision problems explicitly. This paper presents a decision support system for real-time telemarketing operations using the information extracted from the Bayesian network learning model. A prototype decision support system was developed for AT&T customer-contact employees to provide a recommendation regarding the promotion of a telephone discount plan. The system integrated a Bayesian network learning model (knowledge discovery process) and decision-making technique (influence diagram) to provide real-time decision support. A Bayesian network learning model was used to predict a probability of the customer's response from the previous promotion/response history. The influence diagram framework was used to integrate the predicted probability with the cost and benefit related to the possible actions. It was demonstrated that decision support by the Bayesian network learning model itself can be misleading. However, by linking the Bayesian network learning model with rigorous decision-making techniques such as influence diagrams, the decision support system developed in this paper was shown to provide an intelligent decision advice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 167\n",
      "Title: Managing risk in a new telecommunications service development process through a scenario planning approach\n",
      "Abstract: Managing risk in a new product and service development process is one of the major challenges for many business managers. A scenario planning approach was incorporated into a new telecommunications service development process in order to understand the uncertainties shaping the future economic, business and technological environments. Understanding the major drivers for uncertainties helped in gaining insight and thereby generated new strategies for reducing risks and taking advantage of opportunities from uncertainty. In order to demonstrate the process and value of the approach, it was applied to a new telecommunications service concept, the Phoneweb service, which allows Internet access through telephones rather than a computer interface.\n",
      "Content: Managing risk in a new telecommunications service development process through a scenario planning approach Managing risk in a new product and service development process is one of the major challenges for many business managers. A scenario planning approach was incorporated into a new telecommunications service development process in order to understand the uncertainties shaping the future economic, business and technological environments. Understanding the major drivers for uncertainties helped in gaining insight and thereby generated new strategies for reducing risks and taking advantage of opportunities from uncertainty. In order to demonstrate the process and value of the approach, it was applied to a new telecommunications service concept, the Phoneweb service, which allows Internet access through telephones rather than a computer interface.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 168\n",
      "Title: The impact of Web quality and playfulness on user acceptance of online retailing\n",
      "Abstract: We investigated the effect of playfulness on user acceptance of online retailing and tested the relationship between Web quality factors and user acceptance behavior. A survey of 942 users of Web-based online retailing was conducted to test our model. The results showed that playfulness plays an important role in enhancing user attitude and behavioral intention to use a site. We also found that Web quality, categorized into system, information, and service quality, had a significant impact on the perceived ease of use, playfulness, and usefulness, and consequently, that it encouraged website use in the context of online retailing. Our study thus provided a balanced and integrative framework for determining Web quality. It enhanced our knowledge of the effect of playfulness, which should help Web practitioners and researchers better understand user behavior in Web-based online retailing.\n",
      "Content: The impact of Web quality and playfulness on user acceptance of online retailing We investigated the effect of playfulness on user acceptance of online retailing and tested the relationship between Web quality factors and user acceptance behavior. A survey of 942 users of Web-based online retailing was conducted to test our model. The results showed that playfulness plays an important role in enhancing user attitude and behavioral intention to use a site. We also found that Web quality, categorized into system, information, and service quality, had a significant impact on the perceived ease of use, playfulness, and usefulness, and consequently, that it encouraged website use in the context of online retailing. Our study thus provided a balanced and integrative framework for determining Web quality. It enhanced our knowledge of the effect of playfulness, which should help Web practitioners and researchers better understand user behavior in Web-based online retailing.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 169\n",
      "Title: Tailoring Database Training for End Users\n",
      "Abstract: Lack of familiarity with database design methods could prevent many end users from effectively implementing their database management system packages. An inexpensive solution would be for end users to learn required database design skills from software tutors tailored to their needs. This research describes two tutors developed to teach these skills to end users. The tutors were based on a modified Entity-Relationship database design method. They improved an end user's natural learning process by incorporating design principles and facilitators. Empirical comparison of the tutors tested the teaching effectiveness of the facilitators. The results lead to recommendations for closing the gap between skills required and skills learned by end users in database design. Development of tutors that teach specific database design skills irrespective of the software package used in implementation has important implications for practitioners and researchers.\n",
      "Content: Tailoring Database Training for End Users Lack of familiarity with database design methods could prevent many end users from effectively implementing their database management system packages. An inexpensive solution would be for end users to learn required database design skills from software tutors tailored to their needs. This research describes two tutors developed to teach these skills to end users. The tutors were based on a modified Entity-Relationship database design method. They improved an end user's natural learning process by incorporating design principles and facilitators. Empirical comparison of the tutors tested the teaching effectiveness of the facilitators. The results lead to recommendations for closing the gap between skills required and skills learned by end users in database design. Development of tutors that teach specific database design skills irrespective of the software package used in implementation has important implications for practitioners and researchers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 170\n",
      "Title: When Algorithmic Predictions Use Human-Generated Data: A Bias-Aware Classification Algorithm for Breast Cancer Diagnosis\n",
      "Abstract: When algorithms use data generated by human beings, they inherit the errors stemming from human biases, which likely diminishes their performance. We examine the design and value of a bias-aware linear classification algorithm that accounts for bias in input data, using breast cancer diagnosis as our specific setting. In this context, a referring physician makes a follow-up recommendation to a patient based on two inputs: the patient’s clinical-risk information and the radiologist’s mammogram assessment. Critically, the radiologist’s assessment could be biased by the clinical-risk information, which in turn can negatively affect the referring physician’s performance. Thus, a bias-aware algorithm has the potential to be of significant value if integrated into a clinical decision support system used by the referring physician. We develop and show that a bias-aware algorithm can eliminate the adverse impact of bias if the error in the mammogram assessment due to radiologist’s bias has no variance. On the other hand, in the presence of error variance, the adverse impact of bias can be mitigated, but not eliminated, by the bias-aware algorithm. The bias-aware algorithm assigns less (more) weight to the clinical-risk information (radiologist’s mammogram assessment) when the mean error increases (decreases), but the reverse happens when the error variance increases. Using point estimates obtained from mammography practice and the medical literature, we show that the bias-aware algorithm can significantly improve the expected patient life years or the accuracy of decisions based on mammography.The online appendix is available at https://doi.org/10.1287/isre.2018.0789.\n",
      "Content: When Algorithmic Predictions Use Human-Generated Data: A Bias-Aware Classification Algorithm for Breast Cancer Diagnosis When algorithms use data generated by human beings, they inherit the errors stemming from human biases, which likely diminishes their performance. We examine the design and value of a bias-aware linear classification algorithm that accounts for bias in input data, using breast cancer diagnosis as our specific setting. In this context, a referring physician makes a follow-up recommendation to a patient based on two inputs: the patient’s clinical-risk information and the radiologist’s mammogram assessment. Critically, the radiologist’s assessment could be biased by the clinical-risk information, which in turn can negatively affect the referring physician’s performance. Thus, a bias-aware algorithm has the potential to be of significant value if integrated into a clinical decision support system used by the referring physician. We develop and show that a bias-aware algorithm can eliminate the adverse impact of bias if the error in the mammogram assessment due to radiologist’s bias has no variance. On the other hand, in the presence of error variance, the adverse impact of bias can be mitigated, but not eliminated, by the bias-aware algorithm. The bias-aware algorithm assigns less (more) weight to the clinical-risk information (radiologist’s mammogram assessment) when the mean error increases (decreases), but the reverse happens when the error variance increases. Using point estimates obtained from mammography practice and the medical literature, we show that the bias-aware algorithm can significantly improve the expected patient life years or the accuracy of decisions based on mammography.The online appendix is available at https://doi.org/10.1287/isre.2018.0789.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 171\n",
      "Title: It Road Warriors: Balancing Work--Family Conflict, Job Autonomy, and Work Overload to Mitigate Turnover Intentions\n",
      "Abstract: This study examines the antecedents of turnover intention among information technology road warriors. Road warriors are IT professionals who spend most of their workweek away from home at a client site. Building on Moore's (2000) work  on turnover intention, this article develops and tests a model that is context-specific to the road warrior situation. The model highlights the effects of work--family conflict and job autonomy, factors especially applicable to the road warrior's circumstances. Data were gathered from a company in the computer and software services industry. This study provides empirical evidence for the effects of work--family conflict,  perceived work overload, fairness of rewards, and job autonomy on organizational commitment and work exhaustion for road warriors. The results suggest that work--family conflict is a key source of stress among IT road warriors because they have to juggle family and job duties as they work at distant client sites during the week. These findings  suggest that the context of the IT worker matters to turnover intention, and that models that are adaptive to the work context will more effectively predict and explain turnover  intention.\n",
      "Content: It Road Warriors: Balancing Work--Family Conflict, Job Autonomy, and Work Overload to Mitigate Turnover Intentions This study examines the antecedents of turnover intention among information technology road warriors. Road warriors are IT professionals who spend most of their workweek away from home at a client site. Building on Moore's (2000) work  on turnover intention, this article develops and tests a model that is context-specific to the road warrior situation. The model highlights the effects of work--family conflict and job autonomy, factors especially applicable to the road warrior's circumstances. Data were gathered from a company in the computer and software services industry. This study provides empirical evidence for the effects of work--family conflict,  perceived work overload, fairness of rewards, and job autonomy on organizational commitment and work exhaustion for road warriors. The results suggest that work--family conflict is a key source of stress among IT road warriors because they have to juggle family and job duties as they work at distant client sites during the week. These findings  suggest that the context of the IT worker matters to turnover intention, and that models that are adaptive to the work context will more effectively predict and explain turnover  intention.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 172\n",
      "Title: Moving Beyond Intentions and Toward the Theory of Trying: Effects of Work Environment and Gender on Post-Adoption Information Technology Use\n",
      "Abstract: Grounded in the theory of trying, this study examines the influence of the work environment and gender on trying to innovate with information technology. The study extends the innovation diffusion literature by offering a theory-driven explanation for examining trying to innovate with IT and a parsimonious measure for this construct. Drawing on the theory of reasoned action, we argue that work environment impediments render intentions inadequate for examining post-adoption IT use. Instead of examining intentions, we introduce the goal-based construct of trying to innovate with IT as an appropriate dependent variable for examining post-adoption IT use. Statistical analysis supports the reliability and validity of a parsimonious measure of trying to innovate with IT. The study focuses on two research questions. First, do perceptions of the work environment such as overload and autonomy influence individuals' trying to innovate with IT? Second, does gender influence the relationship between perceptions of the environment and trying to innovate with IT? The model articulates how perceptions of the environment moderated by gender may influence trying to innovate with IT. Results provide evidence that overload and autonomy are antecedents to trying to innovate with information technology. Further, findings confirm that autonomy interacts with overload to determine trying to innovate with IT and that these relationships vary by gender. Implications for research and practice are offered.\n",
      "Content: Moving Beyond Intentions and Toward the Theory of Trying: Effects of Work Environment and Gender on Post-Adoption Information Technology Use Grounded in the theory of trying, this study examines the influence of the work environment and gender on trying to innovate with information technology. The study extends the innovation diffusion literature by offering a theory-driven explanation for examining trying to innovate with IT and a parsimonious measure for this construct. Drawing on the theory of reasoned action, we argue that work environment impediments render intentions inadequate for examining post-adoption IT use. Instead of examining intentions, we introduce the goal-based construct of trying to innovate with IT as an appropriate dependent variable for examining post-adoption IT use. Statistical analysis supports the reliability and validity of a parsimonious measure of trying to innovate with IT. The study focuses on two research questions. First, do perceptions of the work environment such as overload and autonomy influence individuals' trying to innovate with IT? Second, does gender influence the relationship between perceptions of the environment and trying to innovate with IT? The model articulates how perceptions of the environment moderated by gender may influence trying to innovate with IT. Results provide evidence that overload and autonomy are antecedents to trying to innovate with information technology. Further, findings confirm that autonomy interacts with overload to determine trying to innovate with IT and that these relationships vary by gender. Implications for research and practice are offered.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 173\n",
      "Title: Women in the information technology profession: a literature review, synthesis and research agenda\n",
      "Abstract: Gender differences in IT careers appear to be affecting the competitiveness of companies globally. It is posited that given the current labor shortage in the IT industry, it has become more important than ever to reduce sources of leakage in the IT career paths of women. A model of barriers faced by women in the field of information technology is presented. Three distinct career stages of career choices, persistence and advancement are analyzed. At each stage, the effects of social and structural factors which may act as barriers are identified and discussed. Social factors include social expectations, work–family conflict and informal networks, while the structural factors are occupational culture, lack of role models and mentors, demographic composition and institutional structures. A proposed research agenda is offered. It is suggested that these social and structural factors as well as their interactions will result in turnover of women in IT.\n",
      "Content: Women in the information technology profession: a literature review, synthesis and research agenda Gender differences in IT careers appear to be affecting the competitiveness of companies globally. It is posited that given the current labor shortage in the IT industry, it has become more important than ever to reduce sources of leakage in the IT career paths of women. A model of barriers faced by women in the field of information technology is presented. Three distinct career stages of career choices, persistence and advancement are analyzed. At each stage, the effects of social and structural factors which may act as barriers are identified and discussed. Social factors include social expectations, work–family conflict and informal networks, while the structural factors are occupational culture, lack of role models and mentors, demographic composition and institutional structures. A proposed research agenda is offered. It is suggested that these social and structural factors as well as their interactions will result in turnover of women in IT.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 174\n",
      "Title: Responsible innovation with digital platforms: Cases in India and Canada\n",
      "Abstract: Marginalized communities globally encounter grand challenges such as lack of access to education, healthcare, and sustained livelihoods. Several initiatives to address these complex, global problems have resulted in fragmented solutions. Recognizing this, there have been several calls for the study of responsible innovation (RI) to address grand challenges. Digital platforms such as AirBnB, Uber and so forth have now become commonplace and are known to generate economic value but also face criticism for being exploitative and exclusive. Only a handful of studies show how similar platforms can innovate responsibly to serve marginalized communities by generating simultaneous economic and social value. To address this gap, our study examines the cases of two platforms that orchestrated ecosystems consisting of individuals from marginalized communities, government agencies, and other entities to provide physical, digital and societal solutions based on principles of RI. We contribute to the RI and IS literatures to show how RI solutions can be fostered through digital platforms to address grand challenges. The article provides empirical evidence of all four dimensions of the RI framework—anticipation, reflexivity, inclusion, and responsiveness - and their operationalization through digital platforms. This research lays the foundation for future studies at the intersection of RI and digital platforms literature. The study also provides practice insights on developing digital platform solutions for marginalized communities to address grand challenges and is useful to policymakers to formulate appropriate interventions. It pushes the theoretical and practice boundaries of our understanding of RI and digital platforms.\n",
      "Content: Responsible innovation with digital platforms: Cases in India and Canada Marginalized communities globally encounter grand challenges such as lack of access to education, healthcare, and sustained livelihoods. Several initiatives to address these complex, global problems have resulted in fragmented solutions. Recognizing this, there have been several calls for the study of responsible innovation (RI) to address grand challenges. Digital platforms such as AirBnB, Uber and so forth have now become commonplace and are known to generate economic value but also face criticism for being exploitative and exclusive. Only a handful of studies show how similar platforms can innovate responsibly to serve marginalized communities by generating simultaneous economic and social value. To address this gap, our study examines the cases of two platforms that orchestrated ecosystems consisting of individuals from marginalized communities, government agencies, and other entities to provide physical, digital and societal solutions based on principles of RI. We contribute to the RI and IS literatures to show how RI solutions can be fostered through digital platforms to address grand challenges. The article provides empirical evidence of all four dimensions of the RI framework—anticipation, reflexivity, inclusion, and responsiveness - and their operationalization through digital platforms. This research lays the foundation for future studies at the intersection of RI and digital platforms literature. The study also provides practice insights on developing digital platform solutions for marginalized communities to address grand challenges and is useful to policymakers to formulate appropriate interventions. It pushes the theoretical and practice boundaries of our understanding of RI and digital platforms.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 175\n",
      "Title: Content Sampling, Household Informedness, and the Consumption of Digital Information Goods\n",
      "Abstract: Technology and media are delivering content that is transforming society. Providers must compete for consumer attention to sell their digital information goods effectively. This is challenging, since there is a high level of uncertainty associated with the consumption of such goods. Service providers often use free programming to share product information. We examine the effectiveness of content sampling strategy used for on-demand series dramas, a unique class of entertainment goods. The data were extracted from a large set of household video-on-demand (VoD) viewing records and combined with external data sources. We extended a propensity score matching (PSM) approach to handle censored data, which permitted us to explore the main causal relationships. Relevant theories in the marketing and information systems disciplines informed our research on consumer involvement and informedness for decision making under uncertainty, the consumption of information goods, and seller strategies for digital content. The results show that content sampling stimulates higher demand for series dramas, but in a more nuanced way than was expected. Samples of the series reveal quality information to consumers and allow them to assess preference fit directly. As a result, they become more informed about their purchase decisions. Also, households seem to be willing to pay more to be better informed, and informed households tend to purchase more. This suggests that content providers should invest in strategies that help consumers to understand the preference fit of information goods.\n",
      "Content: Content Sampling, Household Informedness, and the Consumption of Digital Information Goods Technology and media are delivering content that is transforming society. Providers must compete for consumer attention to sell their digital information goods effectively. This is challenging, since there is a high level of uncertainty associated with the consumption of such goods. Service providers often use free programming to share product information. We examine the effectiveness of content sampling strategy used for on-demand series dramas, a unique class of entertainment goods. The data were extracted from a large set of household video-on-demand (VoD) viewing records and combined with external data sources. We extended a propensity score matching (PSM) approach to handle censored data, which permitted us to explore the main causal relationships. Relevant theories in the marketing and information systems disciplines informed our research on consumer involvement and informedness for decision making under uncertainty, the consumption of information goods, and seller strategies for digital content. The results show that content sampling stimulates higher demand for series dramas, but in a more nuanced way than was expected. Samples of the series reveal quality information to consumers and allow them to assess preference fit directly. As a result, they become more informed about their purchase decisions. Also, households seem to be willing to pay more to be better informed, and informed households tend to purchase more. This suggests that content providers should invest in strategies that help consumers to understand the preference fit of information goods.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 176\n",
      "Title: A Korean group decision support system\n",
      "Abstract: A Group Decision Support System (GDSS) can be used to improve communication in many languages. Heretofore, the vast majority of research with these systems has been conducted on American groups using English. Here, we demonstrate how groups of Korean students used a GDSS developed at the University of Mississippi to exchange comments in Korean and English anonymously and simultaneously. The study found no significant differences between the English and Korean systems in terms of self-assessed ratings of evaluation apprehension, production blocking, and process satisfaction. Participants rated both systems favourably, supporting our hypothesis that Korean groups can benefit from the use of a GDSS.\n",
      "Content: A Korean group decision support system A Group Decision Support System (GDSS) can be used to improve communication in many languages. Heretofore, the vast majority of research with these systems has been conducted on American groups using English. Here, we demonstrate how groups of Korean students used a GDSS developed at the University of Mississippi to exchange comments in Korean and English anonymously and simultaneously. The study found no significant differences between the English and Korean systems in terms of self-assessed ratings of evaluation apprehension, production blocking, and process satisfaction. Participants rated both systems favourably, supporting our hypothesis that Korean groups can benefit from the use of a GDSS.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 177\n",
      "Title: Electronic brainstorming in small and large groups\n",
      "Abstract: An experiment was conducted with small groups averaging about eight people and large groups averaging about 48 people in size. We compared the group members' perceived production blocking, evaluation apprehension, and satisfaction with a meeting of each group size using electronic and verbal brainstorming. While there was no significant difference in perceived satisfaction, evaluation apprehension, or production blocking in either group size using electronic brainstorming, there was a significant difference between small verbal groups and large verbal groups. There was also a significant difference in the dependent measures between large verbal groups and large electronic brainstorming groups. Results suggest that the benefits of electronic brainstorming are more pronounced with larger groups than with smaller groups.\n",
      "Content: Electronic brainstorming in small and large groups An experiment was conducted with small groups averaging about eight people and large groups averaging about 48 people in size. We compared the group members' perceived production blocking, evaluation apprehension, and satisfaction with a meeting of each group size using electronic and verbal brainstorming. While there was no significant difference in perceived satisfaction, evaluation apprehension, or production blocking in either group size using electronic brainstorming, there was a significant difference between small verbal groups and large verbal groups. There was also a significant difference in the dependent measures between large verbal groups and large electronic brainstorming groups. Results suggest that the benefits of electronic brainstorming are more pronounced with larger groups than with smaller groups.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 178\n",
      "Title: A group decision support system for multilingual groups\n",
      "Abstract: Communication in multilingual groups is very difficult. Even if all participants in the group know a common language, it may be a first language for some of the group and a second language for others. Communication in such situations is not equal for all group members. A multilingual Group Decision Support System (GDSS) allows all members to communicate in their native languages, eliminating the group's linguistic problems. This paper describes a prototype multilingual GDSS that provides a high degree of translation accuracy while providing other benefits of Group Decision Support Systems, such as anonymity, parallel communication, and automated recording of the discussion.\n",
      "Content: A group decision support system for multilingual groups Communication in multilingual groups is very difficult. Even if all participants in the group know a common language, it may be a first language for some of the group and a second language for others. Communication in such situations is not equal for all group members. A multilingual Group Decision Support System (GDSS) allows all members to communicate in their native languages, eliminating the group's linguistic problems. This paper describes a prototype multilingual GDSS that provides a high degree of translation accuracy while providing other benefits of Group Decision Support Systems, such as anonymity, parallel communication, and automated recording of the discussion.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 179\n",
      "Title: A group decision support system for multicultural and multilingual communication\n",
      "Abstract: A Group Decision Support System (GDSS) can be used to lower or break barriers to group communication that are caused by differences in language and culture among meeting participants. This paper describes typical examples of communication barriers among group members with different cultural and lingual backgrounds and how a GDSS can help with these problems. The paper also describes a prototype GDSS developed at the University of Mississippi that translates among English, German, and Spanish.\n",
      "Content: A group decision support system for multicultural and multilingual communication A Group Decision Support System (GDSS) can be used to lower or break barriers to group communication that are caused by differences in language and culture among meeting participants. This paper describes typical examples of communication barriers among group members with different cultural and lingual backgrounds and how a GDSS can help with these problems. The paper also describes a prototype GDSS developed at the University of Mississippi that translates among English, German, and Spanish.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 180\n",
      "Title: An abductive model of group support systems\n",
      "Abstract: Few researchers have attempted to model group support system meeting behavior mathematically. Using Abductive Information Modeling (AIM), we show that group size and idea generation type are primary predictors of group process satisfaction. While similar to artificial neural networks, abduction frequently provides simpler models and yields weights for links among the model variables. Results show that the interrelationships among the model variables are non-linear.\n",
      "Content: An abductive model of group support systems Few researchers have attempted to model group support system meeting behavior mathematically. Using Abductive Information Modeling (AIM), we show that group size and idea generation type are primary predictors of group process satisfaction. While similar to artificial neural networks, abduction frequently provides simpler models and yields weights for links among the model variables. Results show that the interrelationships among the model variables are non-linear.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 181\n",
      "Title: A comparison of two electronic idea generation techniques\n",
      "Abstract: Much research has compared verbal with electronic brainstorming, but very few studies have investigated the effects of different electronic techniques. Most studies of electronic brainstorming have been based upon the individual poolwriting technique. However, gallery writing may be superior to it in some situations. Here, we report on an experiment involving 88 subjects in nine groups of approximately ten people each. The subjects used both electronic techniques to discuss different problems. Results show that the subjects were more satisfied with gallery writing and preferred it. Although more raw comments were generated using poolwriting, the number of quality comments and the number of unique, quality comments were not significantly different.\n",
      "Content: A comparison of two electronic idea generation techniques Much research has compared verbal with electronic brainstorming, but very few studies have investigated the effects of different electronic techniques. Most studies of electronic brainstorming have been based upon the individual poolwriting technique. However, gallery writing may be superior to it in some situations. Here, we report on an experiment involving 88 subjects in nine groups of approximately ten people each. The subjects used both electronic techniques to discuss different problems. Results show that the subjects were more satisfied with gallery writing and preferred it. Although more raw comments were generated using poolwriting, the number of quality comments and the number of unique, quality comments were not significantly different.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 182\n",
      "Title: A comparison of synchronous and virtual legislative session groups faced with an idea generation task\n",
      "Abstract: An experiment was conducted with groups of about eight people in face-to-face and geographically-distributed electronic meeting environments. While similar studies have focused on the behavior of group members working together in a single room or working individually in different rooms (a nominal group), this research looks at a hybrid environment in which part of a group is working in one room and another part is working simultaneously in a different room that is linked via a local area network, both parts forming a virtual group. Experimental results showed that such groups generated significantly more unique, quality comments than did face-to-face groups, and that participants were significantly more satisfied with that type of meeting. These and other results indicate that groups may be able to meet effectively when distributed geographically.\n",
      "Content: A comparison of synchronous and virtual legislative session groups faced with an idea generation task An experiment was conducted with groups of about eight people in face-to-face and geographically-distributed electronic meeting environments. While similar studies have focused on the behavior of group members working together in a single room or working individually in different rooms (a nominal group), this research looks at a hybrid environment in which part of a group is working in one room and another part is working simultaneously in a different room that is linked via a local area network, both parts forming a virtual group. Experimental results showed that such groups generated significantly more unique, quality comments than did face-to-face groups, and that participants were significantly more satisfied with that type of meeting. These and other results indicate that groups may be able to meet effectively when distributed geographically.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 183\n",
      "Title: Flaming among first-time group support system users\n",
      "Abstract: Numerous benefits, including increases in efficiency, effectiveness, and participant satisfaction, have been noted in the literature when electronic meetings are used in place of traditional, oral meetings. However, several costs, or process losses, have also been observed, including an increase in ‘flaming’ characterized by insults or even obscenities. This paper describes how flaming may be correlated with numerous task and group member characteristics. Results of a case study show that a large number of flames are unrelated to the topic and that a small minority of those writing comments are responsible for the majority of flames. No variables were found to be significant predictors, but its incidence was exclusively among males.\n",
      "Content: Flaming among first-time group support system users Numerous benefits, including increases in efficiency, effectiveness, and participant satisfaction, have been noted in the literature when electronic meetings are used in place of traditional, oral meetings. However, several costs, or process losses, have also been observed, including an increase in ‘flaming’ characterized by insults or even obscenities. This paper describes how flaming may be correlated with numerous task and group member characteristics. Results of a case study show that a large number of flames are unrelated to the topic and that a small minority of those writing comments are responsible for the majority of flames. No variables were found to be significant predictors, but its incidence was exclusively among males.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 184\n",
      "Title: Requirements-driven data engineering\n",
      "Abstract: In the early 1990s, the effectiveness and efficiency of the information systems (IS) supporting the US Department of Defense's non-combat operations was questioned. As had many organizations, the support had evolved into multiple, redundant, unintegrated, undocumented, stove-piped IS. These systems require unnecessarily large non-combat IS expenses, supporting war fighting efforts. Lack of integration hindered the Department from effectively providing mission support information. DOD's efforts to re-engineer the non-combat IS is one of the first attempts to apply requirements-driven data engineering to a large systems environment. Its application to DOD's non-combat IS data environment has provided tangible results: (1) from the top down, an enterprise model (EM) now specifies Department-wide requirements capable of guiding future integration and development efforts; (2) from the bottom up, non-combat IS are being significantly reduced, simplifying the overall problem; and (3) data quality engineering methods, guided by the EM, are being developed and applied to the remaining IS. This success has achieved a prerequisite necessary to increase the effectiveness and efficiency of the systems.\n",
      "Content: Requirements-driven data engineering In the early 1990s, the effectiveness and efficiency of the information systems (IS) supporting the US Department of Defense's non-combat operations was questioned. As had many organizations, the support had evolved into multiple, redundant, unintegrated, undocumented, stove-piped IS. These systems require unnecessarily large non-combat IS expenses, supporting war fighting efforts. Lack of integration hindered the Department from effectively providing mission support information. DOD's efforts to re-engineer the non-combat IS is one of the first attempts to apply requirements-driven data engineering to a large systems environment. Its application to DOD's non-combat IS data environment has provided tangible results: (1) from the top down, an enterprise model (EM) now specifies Department-wide requirements capable of guiding future integration and development efforts; (2) from the bottom up, non-combat IS are being significantly reduced, simplifying the overall problem; and (3) data quality engineering methods, guided by the EM, are being developed and applied to the remaining IS. This success has achieved a prerequisite necessary to increase the effectiveness and efficiency of the systems.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 185\n",
      "Title: Two decades of research on business intelligence system adoption, utilization and success – A systematic literature review\n",
      "Abstract: In the recent era of technological advances and hyper-competition, business intelligence (BI) systems have attracted significant attention from executives and decision makers due to their ability to provide complex and competitive information inputs for the decision process. Following the world of practice, research into the adoption, utilization and success of BI systems has grown substantially over the past two decades. The literature suggests that organizations have largely failed to capture the benefits of BI systems to their full extent and are seeking ways to leverage value from the implemented systems. However, prior studies do not have any comprehensive study that discusses the issues and challenges related to adoption, utilization and success of BI systems. In this study, using a systematic literature review, we present comprehensive knowledge about what has been found in the domain of BI system adoption, utilization and success. A total of 111 peer-reviewed studies, covering three categories – adoption, utilization and success – published between 2000 and 2019, were selected. The findings present the research methods, underpinning theories and key factors employed to study BI system adoption, utilization and success. In addition, the review identified the key issues related to BI adoption, utilization and success and highlighted the areas that have attracted more or less attention. This study also suggests future directions for researchers and practitioners in terms of unexplored themes that may help organizations to obtain value from BI systems.\n",
      "Content: Two decades of research on business intelligence system adoption, utilization and success – A systematic literature review In the recent era of technological advances and hyper-competition, business intelligence (BI) systems have attracted significant attention from executives and decision makers due to their ability to provide complex and competitive information inputs for the decision process. Following the world of practice, research into the adoption, utilization and success of BI systems has grown substantially over the past two decades. The literature suggests that organizations have largely failed to capture the benefits of BI systems to their full extent and are seeking ways to leverage value from the implemented systems. However, prior studies do not have any comprehensive study that discusses the issues and challenges related to adoption, utilization and success of BI systems. In this study, using a systematic literature review, we present comprehensive knowledge about what has been found in the domain of BI system adoption, utilization and success. A total of 111 peer-reviewed studies, covering three categories – adoption, utilization and success – published between 2000 and 2019, were selected. The findings present the research methods, underpinning theories and key factors employed to study BI system adoption, utilization and success. In addition, the review identified the key issues related to BI adoption, utilization and success and highlighted the areas that have attracted more or less attention. This study also suggests future directions for researchers and practitioners in terms of unexplored themes that may help organizations to obtain value from BI systems.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 186\n",
      "Title: Network sampling and classification: An investigation of network model representations\n",
      "Abstract: Methods for generating a random sample of networks with desired properties are important tools for the analysis of social, biological, and information networks. Algorithm-based approaches to sampling networks have received a great deal of attention in recent literature. Most of these algorithms are based on simple intuitions that associate the full features of connectivity patterns with specific values of only one or two network metrics. Substantive conclusions are crucially dependent on this association holding true. However, the extent to which this simple intuition holds true is not yet known. In this paper, we examine the association between the connectivity patterns that a network sampling algorithm aims to generate and the connectivity patterns of the generated networks, measured by an existing set of popular network metrics. We find that different network sampling algorithms can yield networks with similar connectivity patterns. We also find that the alternative algorithms for the same connectivity pattern can yield networks with different connectivity patterns. We argue that conclusions based on simulated network studies must focus on the full features of the connectivity patterns of a network instead of on the limited set of networkmetrics for a specific network type. This fact has important implications for network data analysis: for instance, implications related to the way significance is currently assessed.\n",
      "Content: Network sampling and classification: An investigation of network model representations Methods for generating a random sample of networks with desired properties are important tools for the analysis of social, biological, and information networks. Algorithm-based approaches to sampling networks have received a great deal of attention in recent literature. Most of these algorithms are based on simple intuitions that associate the full features of connectivity patterns with specific values of only one or two network metrics. Substantive conclusions are crucially dependent on this association holding true. However, the extent to which this simple intuition holds true is not yet known. In this paper, we examine the association between the connectivity patterns that a network sampling algorithm aims to generate and the connectivity patterns of the generated networks, measured by an existing set of popular network metrics. We find that different network sampling algorithms can yield networks with similar connectivity patterns. We also find that the alternative algorithms for the same connectivity pattern can yield networks with different connectivity patterns. We argue that conclusions based on simulated network studies must focus on the full features of the connectivity patterns of a network instead of on the limited set of networkmetrics for a specific network type. This fact has important implications for network data analysis: for instance, implications related to the way significance is currently assessed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 187\n",
      "Title: An entropy approach to disclosure risk assessment: Lessons from real applications and simulated domains\n",
      "Abstract: We live in an increasingly mobile world, which leads to the duplication of information across domains. Though organizations attempt to obscure the identities of their constituents when sharing information for worthwhile purposes, such as basic research, the uncoordinated nature of such environment can lead to privacy vulnerabilities. For instance, disparate healthcare providers can collect information on the same patient. Federal policy requires that such providers share “de-identified” sensitive data, such as biomedical (e.g., clinical and genomic) records. But at the same time, such providers can share identified information, devoid of sensitive biomedical data, for administrative functions. On a provider-by-provider basis, the biomedical and identified records appear unrelated, however, links can be established when multiple providers' databases are studied jointly. The problem, known as trail disclosure, is a generalized phenomenon and occurs because an individual's location access pattern can be matched across the shared databases. Due to technical and legal constraints, it is often difficult to coordinate between providers and thus it is critical to assess the disclosure risk in distributed environments, so that we can develop techniques to mitigate such risks. Research on privacy protection has so far focused on developing technologies to suppress or encrypt identifiers associated with sensitive information. There is a growing body of work on the formal assessment of the disclosure risk of database entries in publicly shared databases, but less attention has been paid to the distributed setting. In this research, we review the trail disclosure problem in several domains with known vulnerabilities and show that disclosure risk is influenced by the distribution of how people visit service providers. Based on empirical evidence, we propose an entropy metric for assessing such risk in shared databases prior to their release. This metric assesses risk by leveraging the statistical characteristics of a visit distribution, as opposed to person-level data. It is computationally efficient and superior to existing risk assessment methods, which rely on ad hoc assessment that are often computationally expensive and unreliable. We evaluate our approach on a range of location access patterns in simulated environments. Our results demonstrate that the approach is effective at estimating trail disclosure risks and the amount of self-information contained in a distributed system is one of the main driving factors.\n",
      "Content: An entropy approach to disclosure risk assessment: Lessons from real applications and simulated domains We live in an increasingly mobile world, which leads to the duplication of information across domains. Though organizations attempt to obscure the identities of their constituents when sharing information for worthwhile purposes, such as basic research, the uncoordinated nature of such environment can lead to privacy vulnerabilities. For instance, disparate healthcare providers can collect information on the same patient. Federal policy requires that such providers share “de-identified” sensitive data, such as biomedical (e.g., clinical and genomic) records. But at the same time, such providers can share identified information, devoid of sensitive biomedical data, for administrative functions. On a provider-by-provider basis, the biomedical and identified records appear unrelated, however, links can be established when multiple providers' databases are studied jointly. The problem, known as trail disclosure, is a generalized phenomenon and occurs because an individual's location access pattern can be matched across the shared databases. Due to technical and legal constraints, it is often difficult to coordinate between providers and thus it is critical to assess the disclosure risk in distributed environments, so that we can develop techniques to mitigate such risks. Research on privacy protection has so far focused on developing technologies to suppress or encrypt identifiers associated with sensitive information. There is a growing body of work on the formal assessment of the disclosure risk of database entries in publicly shared databases, but less attention has been paid to the distributed setting. In this research, we review the trail disclosure problem in several domains with known vulnerabilities and show that disclosure risk is influenced by the distribution of how people visit service providers. Based on empirical evidence, we propose an entropy metric for assessing such risk in shared databases prior to their release. This metric assesses risk by leveraging the statistical characteristics of a visit distribution, as opposed to person-level data. It is computationally efficient and superior to existing risk assessment methods, which rely on ad hoc assessment that are often computationally expensive and unreliable. We evaluate our approach on a range of location access patterns in simulated environments. Our results demonstrate that the approach is effective at estimating trail disclosure risks and the amount of self-information contained in a distributed system is one of the main driving factors.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 188\n",
      "Title: Enterprise architecture operationalization and institutional pluralism: The case of the Norwegian Hospital sector\n",
      "Abstract: Enterprise architecture (EA) is a systematic way of designing, planning, and implementing process and technology changes to address the complexity of information system (IS) landscapes. EA is operationalized when architecture visions move towards realization through concrete projects. We report a case study on the dynamics of operationalizing EA in the Norwegian hospital sector by exploring different EA project trajectories. Our empirical context is an institutionally pluralistic setting where multiple logics coexist. We show that the distinct logic of EA is added to the institutional context and we find that tensions among existing medical, technical, and managerial logics and EA principles and assumptions emerge. We contribute to the under-researched topic of EA operationalization by suggesting a model that demonstrates how the meeting of multiple institutional logics can lead to varying degrees of differentiation or even disassociation from EA visions during decision-taking in projects. Furthermore, we advance extant research on IS projects' implementation in institutionally pluralistic settings by providing an empirical account of actors' interactions and project leadership arrangements that contribute to the persistence of coexisting logics in a dynamic equilibrium.\n",
      "Content: Enterprise architecture operationalization and institutional pluralism: The case of the Norwegian Hospital sector Enterprise architecture (EA) is a systematic way of designing, planning, and implementing process and technology changes to address the complexity of information system (IS) landscapes. EA is operationalized when architecture visions move towards realization through concrete projects. We report a case study on the dynamics of operationalizing EA in the Norwegian hospital sector by exploring different EA project trajectories. Our empirical context is an institutionally pluralistic setting where multiple logics coexist. We show that the distinct logic of EA is added to the institutional context and we find that tensions among existing medical, technical, and managerial logics and EA principles and assumptions emerge. We contribute to the under-researched topic of EA operationalization by suggesting a model that demonstrates how the meeting of multiple institutional logics can lead to varying degrees of differentiation or even disassociation from EA visions during decision-taking in projects. Furthermore, we advance extant research on IS projects' implementation in institutionally pluralistic settings by providing an empirical account of actors' interactions and project leadership arrangements that contribute to the persistence of coexisting logics in a dynamic equilibrium.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 189\n",
      "Title: The re-regulation of working communities and relationships in the context of flexwork: A spacing identity approach\n",
      "Abstract: Existing studies on flexwork stress its individualizing inclination by showing how it gives au­ tonomy to employees, boosts individual productivity, or supports personal well-being at the expense of group cohesiveness, social ties and other characteristics of the “collective” in orga­ nizations. Obviously, flexwork both continues and contributes to an individualization process of working activities and relationships. But, how exactly does flexwork re-regulate working re­ lationships and communities? Is the “collective” irremediably damaged and doomed to disap­ pear? Building on a case study conducted in an insurance company having implemented flexwork, we observe invisibilized employees working from diverse premises (e.g., home, office, etc.) initiating alternative ways of staying united and close. This article shows the re-regulation of these working relationships and communities' through a collective identity process involving de/ re-spacing identity; i.e., the spatial and material aspects of flexible work in relation to identity.\n",
      "Content: The re-regulation of working communities and relationships in the context of flexwork: A spacing identity approach Existing studies on flexwork stress its individualizing inclination by showing how it gives au­ tonomy to employees, boosts individual productivity, or supports personal well-being at the expense of group cohesiveness, social ties and other characteristics of the “collective” in orga­ nizations. Obviously, flexwork both continues and contributes to an individualization process of working activities and relationships. But, how exactly does flexwork re-regulate working re­ lationships and communities? Is the “collective” irremediably damaged and doomed to disap­ pear? Building on a case study conducted in an insurance company having implemented flexwork, we observe invisibilized employees working from diverse premises (e.g., home, office, etc.) initiating alternative ways of staying united and close. This article shows the re-regulation of these working relationships and communities' through a collective identity process involving de/ re-spacing identity; i.e., the spatial and material aspects of flexible work in relation to identity.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 190\n",
      "Title: An adaptive learning to rank algorithm: Learning automata approach\n",
      "Abstract: The recent years have witnessed the birth and explosive growth of the web. It is obvious that the exponential growth of the web has made it into a huge interconnected source of information wherein finding a document without a searching tool is unimaginable. Today's search engines try to provide the most relevant suggestions to the user queries. To do this, different strategies are used to enhance the precision of the information retrieval process. In this paper, a learning method is proposed to rank the web documents in a search engine. The proposed method takes advantage of the user feedback to enhance the precision of the search results. To do so, it uses a learning automata-based approach to train the search engine. In this method, the user feedback is defined as its interest to review an item. Within the search results, the document that is visited by the user is more likely relevant to the user query. Therefore, its choice probability must be increased by the learning automaton. By this, the rank of the most relevant documents increases as that of the others decreases. To investigate the efficiency of the proposed method, extensive simulation experiment is conducted on well-known data collections. The obtained results show the superiority of the proposed approach over the existing methods in terms of mean average precision, precision at position n, and normalized discount cumulative gain.\n",
      "Content: An adaptive learning to rank algorithm: Learning automata approach The recent years have witnessed the birth and explosive growth of the web. It is obvious that the exponential growth of the web has made it into a huge interconnected source of information wherein finding a document without a searching tool is unimaginable. Today's search engines try to provide the most relevant suggestions to the user queries. To do this, different strategies are used to enhance the precision of the information retrieval process. In this paper, a learning method is proposed to rank the web documents in a search engine. The proposed method takes advantage of the user feedback to enhance the precision of the search results. To do so, it uses a learning automata-based approach to train the search engine. In this method, the user feedback is defined as its interest to review an item. Within the search results, the document that is visited by the user is more likely relevant to the user query. Therefore, its choice probability must be increased by the learning automaton. By this, the rank of the most relevant documents increases as that of the others decreases. To investigate the efficiency of the proposed method, extensive simulation experiment is conducted on well-known data collections. The obtained results show the superiority of the proposed approach over the existing methods in terms of mean average precision, precision at position n, and normalized discount cumulative gain.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 191\n",
      "Title: Knowledge networks in new product development projects: A transactive memory perspective\n",
      "Abstract: Even though an individual's knowledge network is known to contribute to the effectiveness and efficiency of his or her work in groups, the way that network building occurs has not been carefully investigated. In our study, activities of new product development teams were analyzed to determine the antecedents and consequences on the transactive memory systems, the moderating affect of task complexity was also considered. We examined 69 new product development projects and found that team stability, team member familiarity, and interpersonal trust had a positive impact on the transactive memory system and also had a positive influence on team learning, speed-to-market, and new product success. Further, we found that the impact of the transactive memory system on team learning, speed-to-market, and new product success was higher when there was a higher task complexity. Theoretical and managerial implications of the study findings are discussed.\n",
      "Content: Knowledge networks in new product development projects: A transactive memory perspective Even though an individual's knowledge network is known to contribute to the effectiveness and efficiency of his or her work in groups, the way that network building occurs has not been carefully investigated. In our study, activities of new product development teams were analyzed to determine the antecedents and consequences on the transactive memory systems, the moderating affect of task complexity was also considered. We examined 69 new product development projects and found that team stability, team member familiarity, and interpersonal trust had a positive impact on the transactive memory system and also had a positive influence on team learning, speed-to-market, and new product success. Further, we found that the impact of the transactive memory system on team learning, speed-to-market, and new product success was higher when there was a higher task complexity. Theoretical and managerial implications of the study findings are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 192\n",
      "Title: New product development team intelligence: Antecedents and consequences\n",
      "Abstract: Our study investigated the effect of team knowledge on new product development (NPD). By investigating 207 NPD projects, we found that the declarative and procedural knowledge of the team and their use of IT had a positive influence on the team's knowledge base; and that the higher the functional diversity of the project team, the greater their overall knowledge. We also found that team knowledge positively impacted new product creativity and success in the market place.\n",
      "Content: New product development team intelligence: Antecedents and consequences Our study investigated the effect of team knowledge on new product development (NPD). By investigating 207 NPD projects, we found that the declarative and procedural knowledge of the team and their use of IT had a positive influence on the team's knowledge base; and that the higher the functional diversity of the project team, the greater their overall knowledge. We also found that team knowledge positively impacted new product creativity and success in the market place.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 193\n",
      "Title: Antecedents and consequences of team potency in software development projects\n",
      "Abstract: Developing new software quickly, successfully, and at low cost is critical in organizations. Ways of assessing the effectiveness of development teams has highlighted measures of factors, such as teamwork, group cohesiveness, and team integration, but the use of group potency theory (the collective belief of a group that it can be effective) is rare. In our study, we investigated antecedents of and consequences to group potency in software development project teams. By examining 53 software development project teams collected from small and medium-sized software firms in Turkey, we found, that team potency positively affected speed-to-market, development cost, and market success of the product. We also found that trust among project team members, past experiences of the members, and team empowerment had a positive impact on the team potency during the project. Managerial and theoretical implications are discussed.\n",
      "Content: Antecedents and consequences of team potency in software development projects Developing new software quickly, successfully, and at low cost is critical in organizations. Ways of assessing the effectiveness of development teams has highlighted measures of factors, such as teamwork, group cohesiveness, and team integration, but the use of group potency theory (the collective belief of a group that it can be effective) is rare. In our study, we investigated antecedents of and consequences to group potency in software development project teams. By examining 53 software development project teams collected from small and medium-sized software firms in Turkey, we found, that team potency positively affected speed-to-market, development cost, and market success of the product. We also found that trust among project team members, past experiences of the members, and team empowerment had a positive impact on the team potency during the project. Managerial and theoretical implications are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 194\n",
      "Title: Antecedents and consequences of collective empathy in software development project teams\n",
      "Abstract: The term empathy has attracted many researchers from a variety of disciplines; however, a team's collective empathy, which is composed of cognitive, affective, and behavioral dimensions, has rarely been addressed in the literature. In this study, we empirically investigated the relationship between the collective empathy of a team and the effectiveness of its project process. Additionally, we tested the role of team intimacy-related factors, such as interpersonal trust, within-team communication, and team member familiarity, in collective empathy, as well as the moderating role of group norms on the collective empathy-process effectiveness link. By studying 122 software development projects, we found that cognitive-based trust, formal within-team communication, and team member familiarity influence the collective empathy of project teams. We also found that collective empathy affects team learning and product speed-to-market and results in lower project development costs. Furthermore, we determined that the existence of group norms moderates the relationships among collective empathy, speed-to-market, and lower development costs. The managerial and theoretical implications of the study have also been provided.\n",
      "Content: Antecedents and consequences of collective empathy in software development project teams The term empathy has attracted many researchers from a variety of disciplines; however, a team's collective empathy, which is composed of cognitive, affective, and behavioral dimensions, has rarely been addressed in the literature. In this study, we empirically investigated the relationship between the collective empathy of a team and the effectiveness of its project process. Additionally, we tested the role of team intimacy-related factors, such as interpersonal trust, within-team communication, and team member familiarity, in collective empathy, as well as the moderating role of group norms on the collective empathy-process effectiveness link. By studying 122 software development projects, we found that cognitive-based trust, formal within-team communication, and team member familiarity influence the collective empathy of project teams. We also found that collective empathy affects team learning and product speed-to-market and results in lower project development costs. Furthermore, we determined that the existence of group norms moderates the relationships among collective empathy, speed-to-market, and lower development costs. The managerial and theoretical implications of the study have also been provided.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 195\n",
      "Title: From Placebo to Panacea: Studying the Diffusion of IT Management Techniques with Ambiguous Efficiencies: The Case of Capability Maturity Model\n",
      "Abstract: In light of the inherent shortcomings of single-perspective approaches in IT diffusion research, in this paper, we develop a multi-perspective framework for studying the diffusion of IT management techniques. The framework is then applied to explain the diffusion of capability maturity model (CMM). This research contributes to information systems theory by (a) illustrating how several different theoretical perspectives (i.e., forced-selection, efficient choice, fashion, and fad) can be used to explain an IT management innovation diffusion; (b) identifying the specific limitations of each perspective; and (c) demonstrating how these perspectives can be reconciled and yield a holistic understanding of the diffusion trajectory. Building on 20+ years of CMM research, the propositions of this paper shed more light on the underlying dynamics driving the adoption decision among software vendors, and will inform IS scholars and practitioners about the types of actions that can foster the dissemination of emerging IT management techniques.\n",
      "Content: From Placebo to Panacea: Studying the Diffusion of IT Management Techniques with Ambiguous Efficiencies: The Case of Capability Maturity Model In light of the inherent shortcomings of single-perspective approaches in IT diffusion research, in this paper, we develop a multi-perspective framework for studying the diffusion of IT management techniques. The framework is then applied to explain the diffusion of capability maturity model (CMM). This research contributes to information systems theory by (a) illustrating how several different theoretical perspectives (i.e., forced-selection, efficient choice, fashion, and fad) can be used to explain an IT management innovation diffusion; (b) identifying the specific limitations of each perspective; and (c) demonstrating how these perspectives can be reconciled and yield a holistic understanding of the diffusion trajectory. Building on 20+ years of CMM research, the propositions of this paper shed more light on the underlying dynamics driving the adoption decision among software vendors, and will inform IS scholars and practitioners about the types of actions that can foster the dissemination of emerging IT management techniques.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 196\n",
      "Title: The ongoing quest for the IT artifact: Looking back, moving forward\n",
      "Abstract: More than 10 years ago, Orlikowski and Iacono (2001) examined the conceptualization of Information Technology (IT) in Information Systems Research (ISR) articles published in the 1990s. Their main conclusion was that the majority of these articles did not properly conceptualize the IT artifact. They recommended that IS researchers start to theorize about the IT artifact and employ rich conceptualizations of IT. The Orlikowski and Iacono paper provides a strong anchor point from which to analyze the evolution of the IS discipline. In order to obtain an up-to-date image of contemporary IS research, and to assess how the IS field has evolved since the 1990s, we carried out a similar analysis on a more recent and broader set of articles, that is, the full set (N=644) of papers published between 2006 and 2009 by six top North American (ISR, MISQ, JAIS) and European (JIT, ISJ, EJIS) journals. The statistics in our results reveal no drastic advance in terms of deeper engagement with the IT artifact; more than 39% of the articles in our set are virtually mute about the artifact, and less than 16% employ an ensemble view of IT. Moreover, we note differences among the North American and European journals. Implications of the findings for two perspectives central to the IS research legitimacy debate are discussed.\n",
      "Content: The ongoing quest for the IT artifact: Looking back, moving forward More than 10 years ago, Orlikowski and Iacono (2001) examined the conceptualization of Information Technology (IT) in Information Systems Research (ISR) articles published in the 1990s. Their main conclusion was that the majority of these articles did not properly conceptualize the IT artifact. They recommended that IS researchers start to theorize about the IT artifact and employ rich conceptualizations of IT. The Orlikowski and Iacono paper provides a strong anchor point from which to analyze the evolution of the IS discipline. In order to obtain an up-to-date image of contemporary IS research, and to assess how the IS field has evolved since the 1990s, we carried out a similar analysis on a more recent and broader set of articles, that is, the full set (N=644) of papers published between 2006 and 2009 by six top North American (ISR, MISQ, JAIS) and European (JIT, ISJ, EJIS) journals. The statistics in our results reveal no drastic advance in terms of deeper engagement with the IT artifact; more than 39% of the articles in our set are virtually mute about the artifact, and less than 16% employ an ensemble view of IT. Moreover, we note differences among the North American and European journals. Implications of the findings for two perspectives central to the IS research legitimacy debate are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 197\n",
      "Title: Vicious and virtuous cycles in ERP implementation: a case study of interrelations between critical success factors\n",
      "Abstract: ERP implementations are complex undertakings. Recent research has provided us with plausible critical success factors (CSFs) for such implementations. This article describes how one list of CSFs (Somers & Nelson, 2001) was used to analyse and explain project performance in one ERP implementation in the aviation industry. In this particular case, poor project performance led to a serious project crisis but this situation was turned around into a success. The list of CSFs employed was found to be helpful and appropriate in explaining both the initial failure and the eventual success of the implementation. CSFs in this case appeared to be highly correlated, ie changes in any one of them would influence most of the others as well. The reversal in performance after the project crisis was caused by substantial changes in attitudes with most of the stakeholders involved, such as top management, project management, project champion and software vendor.\n",
      "Content: Vicious and virtuous cycles in ERP implementation: a case study of interrelations between critical success factors ERP implementations are complex undertakings. Recent research has provided us with plausible critical success factors (CSFs) for such implementations. This article describes how one list of CSFs (Somers & Nelson, 2001) was used to analyse and explain project performance in one ERP implementation in the aviation industry. In this particular case, poor project performance led to a serious project crisis but this situation was turned around into a success. The list of CSFs employed was found to be helpful and appropriate in explaining both the initial failure and the eventual success of the implementation. CSFs in this case appeared to be highly correlated, ie changes in any one of them would influence most of the others as well. The reversal in performance after the project crisis was caused by substantial changes in attitudes with most of the stakeholders involved, such as top management, project management, project champion and software vendor.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 198\n",
      "Title: Reversing a relationship spiral: From vicious to virtuous cycles in IT outsourcing\n",
      "Abstract: IT outsourcing (ITO) remains a popular business practice, but many buyers and suppliers of IT services are caught in a vicious relationship spiral of low trust, bad collaboration and mediocre performance. This paper describes a novel process understanding of how vicious cycles work and suggests a new method for how they can be reversed into virtuous cycles. Based on the action research and complementary system dynamics simulation, this paper demonstrates how an ineffective ITO relationship between a European Harbour Authority and its main IT supplier ITCo was formed and, later, transformed. The method, involving collaborative redesign of service workflows, applied in this action research triggered the reversal of an otherwise downward relationship spiral. Both the empirical facts from the action research data and the system dynamics simulation data are provided as evidence. We conclude the paper with conceptual and methodological contributions as well as scope for future research.\n",
      "Content: Reversing a relationship spiral: From vicious to virtuous cycles in IT outsourcing IT outsourcing (ITO) remains a popular business practice, but many buyers and suppliers of IT services are caught in a vicious relationship spiral of low trust, bad collaboration and mediocre performance. This paper describes a novel process understanding of how vicious cycles work and suggests a new method for how they can be reversed into virtuous cycles. Based on the action research and complementary system dynamics simulation, this paper demonstrates how an ineffective ITO relationship between a European Harbour Authority and its main IT supplier ITCo was formed and, later, transformed. The method, involving collaborative redesign of service workflows, applied in this action research triggered the reversal of an otherwise downward relationship spiral. Both the empirical facts from the action research data and the system dynamics simulation data are provided as evidence. We conclude the paper with conceptual and methodological contributions as well as scope for future research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 199\n",
      "Title: Strategies for global information systems development\n",
      "Abstract: Developing global information systems is a formidable task. Multinational companies operate in regions that are thousands of miles, many time zones, and many cultures away from headquarters. Organizing the activities and aligning the tasks and mindsets of people that are so far apart and to change the way that business is conducted through the use of IS is a major challenge. This study discusses alternative global IS development strategies and the factors that impact their selection. Four systems from a large transportation company are presented as real life examples to demonstrate the viability of these strategies and the accompanying factors.\n",
      "Content: Strategies for global information systems development Developing global information systems is a formidable task. Multinational companies operate in regions that are thousands of miles, many time zones, and many cultures away from headquarters. Organizing the activities and aligning the tasks and mindsets of people that are so far apart and to change the way that business is conducted through the use of IS is a major challenge. This study discusses alternative global IS development strategies and the factors that impact their selection. Four systems from a large transportation company are presented as real life examples to demonstrate the viability of these strategies and the accompanying factors.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 200\n",
      "Title: Knowledge contributions in design science research: Paths of knowledge types\n",
      "Abstract: Design science research addresses important, complex real-world problems. Although well-accepted as part of research in information systems, initiating or progressing a design science research project still requires effort to describe how knowledge creation emerges and its underlying dynamics. Given the existing body of knowledge on design science research, it should be possible to learn from that knowledge to progress future work. This paper analyzes design science research projects to identify and make explicit their knowledge contributions while recognizing the plurality of a project's knowledge contributions with respect to a project's knowledge scope and knowledge goals. The construct of a path of knowledge types is introduced that represents how knowledge con­ tributions are dynamically created throughout a project. These paths form the basis for the derivation of seven design science research strategies, which lead to guidelines for initiating or progressing a project. This effort is compared to other research that analyzes the growing body of work in design science with respect to knowledge contributions and project classifications.\n",
      "Content: Knowledge contributions in design science research: Paths of knowledge types Design science research addresses important, complex real-world problems. Although well-accepted as part of research in information systems, initiating or progressing a design science research project still requires effort to describe how knowledge creation emerges and its underlying dynamics. Given the existing body of knowledge on design science research, it should be possible to learn from that knowledge to progress future work. This paper analyzes design science research projects to identify and make explicit their knowledge contributions while recognizing the plurality of a project's knowledge contributions with respect to a project's knowledge scope and knowledge goals. The construct of a path of knowledge types is introduced that represents how knowledge con­ tributions are dynamically created throughout a project. These paths form the basis for the derivation of seven design science research strategies, which lead to guidelines for initiating or progressing a project. This effort is compared to other research that analyzes the growing body of work in design science with respect to knowledge contributions and project classifications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 201\n",
      "Title: Experimental evaluation of user performance on two-dimensional and three-dimensional perspective displays in discrete-event simulation\n",
      "Abstract: Several experiments were carried out to compare the impacts of using a two dimensional (2D) plan view or a three dimensional (3D) perspective view in discrete event simulation visual displays. The experiments measured the performance of participants in spotting errors, describing the model, and suggesting improvements to the system. The participants using the 3D perspective display performed much better in spotting errors, taking on average about one third of the time of participants observing the 2D display. They also did much better in describing the model. There was no significant difference in suggesting improvements although this may have been because this task was easy. Most participants preferred the 3D perspective view when asked to compare the displays. The experiments indicate that the detailed design of the visual display may have a considerable effect on some of the tasks in a simulation project and hence on whether the overall project is successful.\n",
      "Content: Experimental evaluation of user performance on two-dimensional and three-dimensional perspective displays in discrete-event simulation Several experiments were carried out to compare the impacts of using a two dimensional (2D) plan view or a three dimensional (3D) perspective view in discrete event simulation visual displays. The experiments measured the performance of participants in spotting errors, describing the model, and suggesting improvements to the system. The participants using the 3D perspective display performed much better in spotting errors, taking on average about one third of the time of participants observing the 2D display. They also did much better in describing the model. There was no significant difference in suggesting improvements although this may have been because this task was easy. Most participants preferred the 3D perspective view when asked to compare the displays. The experiments indicate that the detailed design of the visual display may have a considerable effect on some of the tasks in a simulation project and hence on whether the overall project is successful.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 202\n",
      "Title: Trust and commitment within a virtual brand community: The mediating role of brand relationship quality\n",
      "Abstract: This study seeks to clarify the antecedents and consequences of trust and commitment within the brand fan page context on Facebook, examining a sample of 210 respondents using structural equation modeling. The results highlight the positive effect of economic and hedonic benefits on trust and commitment within the brand fan page. Mediation analysis reveals that trust and commitment developed within the brand fan page will be transformed into positive “word of mouth” for the respective brand if fans have a strong relationship quality with the brand. Further, we found that young and female fans with a high level of engagement, having a strong relationship with the brand, spread positive WOM. Our findings broaden ways for developing relational governance in a firm-initiated virtual brand community by providing new levers and guidance for marketers to build strong customer relationships.\n",
      "Content: Trust and commitment within a virtual brand community: The mediating role of brand relationship quality This study seeks to clarify the antecedents and consequences of trust and commitment within the brand fan page context on Facebook, examining a sample of 210 respondents using structural equation modeling. The results highlight the positive effect of economic and hedonic benefits on trust and commitment within the brand fan page. Mediation analysis reveals that trust and commitment developed within the brand fan page will be transformed into positive “word of mouth” for the respective brand if fans have a strong relationship quality with the brand. Further, we found that young and female fans with a high level of engagement, having a strong relationship with the brand, spread positive WOM. Our findings broaden ways for developing relational governance in a firm-initiated virtual brand community by providing new levers and guidance for marketers to build strong customer relationships.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 203\n",
      "Title: A Comprehensive Review and Synthesis of Open Source Research\n",
      "Abstract: The open source movement has grown steadily and matured in recent years, and this growth has been mirrored by a rise in open source related research. The objective of this paper is to pause and reflect on the state of the field. We start by conducting a comprehensive literature review of open source research, and organize the resulting 618 peer-reviewed articles into a taxonomy. Elements of this taxonomy are defined and described. We then draw on a number of existing categorization schemes to develop a framework to situate open source research within a wider nomological network. Building on concepts from systems theory, we propose a holistic framework of open source research. This framework incorporates current research, as represented by the taxonomy, identifies gaps and areas of overlap, and charts a path for future work.\n",
      "Content: A Comprehensive Review and Synthesis of Open Source Research The open source movement has grown steadily and matured in recent years, and this growth has been mirrored by a rise in open source related research. The objective of this paper is to pause and reflect on the state of the field. We start by conducting a comprehensive literature review of open source research, and organize the resulting 618 peer-reviewed articles into a taxonomy. Elements of this taxonomy are defined and described. We then draw on a number of existing categorization schemes to develop a framework to situate open source research within a wider nomological network. Building on concepts from systems theory, we propose a holistic framework of open source research. This framework incorporates current research, as represented by the taxonomy, identifies gaps and areas of overlap, and charts a path for future work.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 204\n",
      "Title: Development and validation of an instrument to measure user perceived service quality of mHealth\n",
      "Abstract: The role of service quality in fostering the growth of mHealth services has gained much attention in the academic and practitioner communities. However, empirical research in this area has been beset by inadequate conceptualization and the lack of a validated scale. This study addresses these limitations by theoretically conceptualizing and empirically validating a multidimensional service quality scale in the mHealth context. The findings show that mHealth service quality is a hierarchical, multidimensional, and reflective construct, which consists of three primary dimensions and eight subdimensions. The results also confirm that the mHealth service quality scale is more effective at predicting satisfaction and continuance in a nomological network.\n",
      "Content: Development and validation of an instrument to measure user perceived service quality of mHealth The role of service quality in fostering the growth of mHealth services has gained much attention in the academic and practitioner communities. However, empirical research in this area has been beset by inadequate conceptualization and the lack of a validated scale. This study addresses these limitations by theoretically conceptualizing and empirically validating a multidimensional service quality scale in the mHealth context. The findings show that mHealth service quality is a hierarchical, multidimensional, and reflective construct, which consists of three primary dimensions and eight subdimensions. The results also confirm that the mHealth service quality scale is more effective at predicting satisfaction and continuance in a nomological network.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 205\n",
      "Title: Drug prescription behavior and decision support systems\n",
      "Abstract: Adverse drug events plague the outcomes of health care services. In this research, we propose a clinical learning model that incorporates the use of a decision support system (DSS) in drug prescriptions to improve physicians' decisions about the initial drug selection and administration. The model allows for both the analytical investigation of the effects of different DSS features on clinical learning and the estimation of the physician learning behavior given a panel data set. The analytical results suggest that using a DSS to improve physicians' prescription decisions would positively influence their clinical learning. Conversely, without improvements in successful drug selection, the use of a DSS would negatively affect clinical learning. The empirical results provide further evidence on the factors that drive physicians' responses to information sources and the extent to which they rely on clinical experience in prescribing drugs.\n",
      "Content: Drug prescription behavior and decision support systems Adverse drug events plague the outcomes of health care services. In this research, we propose a clinical learning model that incorporates the use of a decision support system (DSS) in drug prescriptions to improve physicians' decisions about the initial drug selection and administration. The model allows for both the analytical investigation of the effects of different DSS features on clinical learning and the estimation of the physician learning behavior given a panel data set. The analytical results suggest that using a DSS to improve physicians' prescription decisions would positively influence their clinical learning. Conversely, without improvements in successful drug selection, the use of a DSS would negatively affect clinical learning. The empirical results provide further evidence on the factors that drive physicians' responses to information sources and the extent to which they rely on clinical experience in prescribing drugs.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 206\n",
      "Title: A Strategic Analysis of Multi-Channel Expert Services\n",
      "Abstract: Using stylized models, we investigate when and how expert service providers should offer their services online, and whether they should charge separate prices for face-to-face and online services or provide the online service as a free supplement. Interestingly, consumer surplus can rise when a monopolist charges different prices for face-to-face and online services, and it may drop when the monopolist starts offering the online service as a free add-on to its face-to-face service. We find that a market-wide adoption of the online channel by competing experts in a duopoly setting intensifies price competition and thereby reduces overall profits. Furthermore, the rate of adoption is highest when the online service is moderately effective, whereas one of the experts refuses to offer the service when it is highly effective. These results provide theoretical support for the viability of online expert services as well as practical guidance on pricing strategies.\n",
      "Content: A Strategic Analysis of Multi-Channel Expert Services Using stylized models, we investigate when and how expert service providers should offer their services online, and whether they should charge separate prices for face-to-face and online services or provide the online service as a free supplement. Interestingly, consumer surplus can rise when a monopolist charges different prices for face-to-face and online services, and it may drop when the monopolist starts offering the online service as a free add-on to its face-to-face service. We find that a market-wide adoption of the online channel by competing experts in a duopoly setting intensifies price competition and thereby reduces overall profits. Furthermore, the rate of adoption is highest when the online service is moderately effective, whereas one of the experts refuses to offer the service when it is highly effective. These results provide theoretical support for the viability of online expert services as well as practical guidance on pricing strategies.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 207\n",
      "Title: Brand Crisis and Customer Relationship Management on Social Media: Evidence from a Natural Experiment from the Airline Industry\n",
      "Abstract: In this study, we investigate the effect of a brand crisis on the customer relationship management (CRM) efforts of brands on social media. Despite the opportunities social media offers to brands to connect and engage with customers, it is still unclear how a brand crisis can change a brand’s social CRM efforts. Social media platforms can amplify the negative consequences of a brand crisis. However, the unique and public nature of social media platforms can offer new means for brands to handle a brand crisis and publicly manage their customer relationships. Compared with traditional CRM, social media enables brands to communicate with their customers publicly, which can strengthen their relationships with customers and maintain higher levels of customer engagement. Leveraging a natural experiment setting, we investigate the impact of the United Airlines crisis on three dimensions of social CRM efforts: informativeness, timeliness, and attentiveness. Contrary to traditional CRM efforts and recommendations, we find that the brand crisis increases informativeness efforts but reduces timeliness and attentiveness efforts.History: Yong Tan, Senior Editor; Beibei Li, Associate Editor.Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1159.\n",
      "Content: Brand Crisis and Customer Relationship Management on Social Media: Evidence from a Natural Experiment from the Airline Industry In this study, we investigate the effect of a brand crisis on the customer relationship management (CRM) efforts of brands on social media. Despite the opportunities social media offers to brands to connect and engage with customers, it is still unclear how a brand crisis can change a brand’s social CRM efforts. Social media platforms can amplify the negative consequences of a brand crisis. However, the unique and public nature of social media platforms can offer new means for brands to handle a brand crisis and publicly manage their customer relationships. Compared with traditional CRM, social media enables brands to communicate with their customers publicly, which can strengthen their relationships with customers and maintain higher levels of customer engagement. Leveraging a natural experiment setting, we investigate the impact of the United Airlines crisis on three dimensions of social CRM efforts: informativeness, timeliness, and attentiveness. Contrary to traditional CRM efforts and recommendations, we find that the brand crisis increases informativeness efforts but reduces timeliness and attentiveness efforts.History: Yong Tan, Senior Editor; Beibei Li, Associate Editor.Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1159.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 208\n",
      "Title: Why people keep coming back to Facebook: Explaining and predicting continuance participation from an extended theory of planned behaviour perspective\n",
      "Abstract: This study examines the continuance participation intentions and behaviour on Facebook, as a representative of Social Networking Sites (SNSs), from a social and behavioural perspective. The study extends the Theory of Planned Behaviour (TPB) through the inclusion of perceived value construct and utilizes the extended theory to explain users' continuance participation intentions and behaviour on Facebook. Despite the recent massive uptake of Facebook, our review of the related-literature revealed that very few studies tackled such technologies from the context of post-adoption as in this research. Using data from surveys of undergraduate and postgraduate students in Jordan (n=403), the extended theory was tested using statistical analysis methods. The results show that attitude, subjective norm, perceived behavioural control, and perceived value have significant effect on the continuance participation intention of post-adopters. Further, the results show that continuance participation intention and perceived value have significant effect on continuance participation behaviour. However, the results show that perceived behavioural control has no significant effect on continuance participation behaviour of post-adopters. When comparing the extended theory developed in this study with the standard TPB, it was found that the inclusion of the perceived value construct in the extended theory is fruitful; as such an extension explained an additional 11.6% of the variance in continuance participation intention and 4.5% of the variance in continuance participation behaviour over the standard TPB constructs. Consistent with the research on value-driven post-adoption behaviour, these findings suggest that continuance intentions and behaviour of users of Facebook are likely to be greater when they perceive the behaviour to be associated with significant added-value (i.e. benefits outperform sacrifices).\n",
      "Content: Why people keep coming back to Facebook: Explaining and predicting continuance participation from an extended theory of planned behaviour perspective This study examines the continuance participation intentions and behaviour on Facebook, as a representative of Social Networking Sites (SNSs), from a social and behavioural perspective. The study extends the Theory of Planned Behaviour (TPB) through the inclusion of perceived value construct and utilizes the extended theory to explain users' continuance participation intentions and behaviour on Facebook. Despite the recent massive uptake of Facebook, our review of the related-literature revealed that very few studies tackled such technologies from the context of post-adoption as in this research. Using data from surveys of undergraduate and postgraduate students in Jordan (n=403), the extended theory was tested using statistical analysis methods. The results show that attitude, subjective norm, perceived behavioural control, and perceived value have significant effect on the continuance participation intention of post-adopters. Further, the results show that continuance participation intention and perceived value have significant effect on continuance participation behaviour. However, the results show that perceived behavioural control has no significant effect on continuance participation behaviour of post-adopters. When comparing the extended theory developed in this study with the standard TPB, it was found that the inclusion of the perceived value construct in the extended theory is fruitful; as such an extension explained an additional 11.6% of the variance in continuance participation intention and 4.5% of the variance in continuance participation behaviour over the standard TPB constructs. Consistent with the research on value-driven post-adoption behaviour, these findings suggest that continuance intentions and behaviour of users of Facebook are likely to be greater when they perceive the behaviour to be associated with significant added-value (i.e. benefits outperform sacrifices).\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 209\n",
      "Title: Developing a unified framework of the business model concept\n",
      "Abstract: Recent rapid advances in Information and Communication Technologies (ICTs) have highlighted the rising importance of the Business Model (BM) concept in the field of Information Systems (IS). Despite agreement on its importance to an organization's success, the concept is still fuzzy and vague, and there is little consensus regarding its compositional facets. Identifying the fundamental concepts, modeling principles, practical functions, and reach of the BM relevant to IS and other business concepts is by no means complete. This paper, following a comprehensive review of the literature, principally employs the content analysis method and utilizes a deductive reasoning approach to provide a hierarchical taxonomy of the BM concepts from which to develop a more comprehensive framework. This framework comprises four fundamental aspects. First, it identifies four primary BM dimensions along with their constituent elements forming a complete ontological structure of the concept. Second, it cohesively organizes the BM modeling principles, that is, guidelines and features. Third, it explains the reach of the concept showing its interactions and intersections with strategy, business processes, and IS so as to place the BM within the world of digital business. Finally, the framework explores three major functions of BMs within digital organizations to shed light on the practical significance of the concept. Hence, this paper links the BM facets in a novel manner offering an intact definition. In doing so, this paper provides a unified conceptual framework for the BM concept that we argue is comprehensive and appropriate to the complex nature of businesses today. This leads to fruitful implications for theory and practice and also enables us to suggest a research agenda using our conceptual framework.\n",
      "Content: Developing a unified framework of the business model concept Recent rapid advances in Information and Communication Technologies (ICTs) have highlighted the rising importance of the Business Model (BM) concept in the field of Information Systems (IS). Despite agreement on its importance to an organization's success, the concept is still fuzzy and vague, and there is little consensus regarding its compositional facets. Identifying the fundamental concepts, modeling principles, practical functions, and reach of the BM relevant to IS and other business concepts is by no means complete. This paper, following a comprehensive review of the literature, principally employs the content analysis method and utilizes a deductive reasoning approach to provide a hierarchical taxonomy of the BM concepts from which to develop a more comprehensive framework. This framework comprises four fundamental aspects. First, it identifies four primary BM dimensions along with their constituent elements forming a complete ontological structure of the concept. Second, it cohesively organizes the BM modeling principles, that is, guidelines and features. Third, it explains the reach of the concept showing its interactions and intersections with strategy, business processes, and IS so as to place the BM within the world of digital business. Finally, the framework explores three major functions of BMs within digital organizations to shed light on the practical significance of the concept. Hence, this paper links the BM facets in a novel manner offering an intact definition. In doing so, this paper provides a unified conceptual framework for the BM concept that we argue is comprehensive and appropriate to the complex nature of businesses today. This leads to fruitful implications for theory and practice and also enables us to suggest a research agenda using our conceptual framework.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 210\n",
      "Title: Information technology (IT) in Saudi Arabia: Culture and the acceptance and use of IT\n",
      "Abstract: The unified theory of acceptance and use of technology (UTAUT), a model of the user acceptance of IT, synthesizes elements from several prevailing user acceptance models. It has been credited with explaining a larger proportion of the variance of ‘intention to use’ and ‘usage behavior’ than do preceding models. However, it has not been validated in non-Western cultures. Using a survey sample collected from 722 knowledge workers using desktop computer applications on a voluntary basis in Saudi Arabia, we examined the relative power of a modified version of UTAUT in determining ‘intention to use’ and ‘usage behavior’. We found that the model explained 39.1% of intention to use variance, and 42.1% of usage variance. In addition, drawing on the theory of cultural dimensions, we hypothesized and tested the similarities and differences between the North American and Saudi validations of UTAUT in terms of cultural differences that affected the organizational acceptance of IT in the two societies.\n",
      "Content: Information technology (IT) in Saudi Arabia: Culture and the acceptance and use of IT The unified theory of acceptance and use of technology (UTAUT), a model of the user acceptance of IT, synthesizes elements from several prevailing user acceptance models. It has been credited with explaining a larger proportion of the variance of ‘intention to use’ and ‘usage behavior’ than do preceding models. However, it has not been validated in non-Western cultures. Using a survey sample collected from 722 knowledge workers using desktop computer applications on a voluntary basis in Saudi Arabia, we examined the relative power of a modified version of UTAUT in determining ‘intention to use’ and ‘usage behavior’. We found that the model explained 39.1% of intention to use variance, and 42.1% of usage variance. In addition, drawing on the theory of cultural dimensions, we hypothesized and tested the similarities and differences between the North American and Saudi validations of UTAUT in terms of cultural differences that affected the organizational acceptance of IT in the two societies.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 211\n",
      "Title: A research case study: Difficulties and recommendations when using a textual data mining tool\n",
      "Abstract: Although many interesting results have been reported by researchers using numeric data mining methods, there are still questions that need answering before textual data mining tools will be considered generally useful due to the effort needed to learn and use them. In 2011, we generated a dataset from the legal statements (mainly privacy policy and terms of use) on the websites of 475 of the US Fortune 500 Companies and used it as input to see what we could detect about the organizational relationships between the companies by using a textual data mining tool. We hoped to find that the tool would cluster similar corporations into the same industrial sector, as validated by the company's self-reported North American Industry Classification System code (NAICS). Unfortunately, this proved only marginally successful, leading us to ask why and to pose our research question: What problems occur when a data-mining tool is used to analyze large textual datasets that are unstructured, complex, duplicative, and contain many homonyms and synonyms? In analyzing our large dataset we learned a great deal about the problem and fortunately, after significant effort, determined how to “massage” the raw dataset to improve the process and learn how the tool can be better used in research situations. We also found that NAICS, as self-reported by companies, are of dubious value to a researcher—a matter briefly discussed.\n",
      "Content: A research case study: Difficulties and recommendations when using a textual data mining tool Although many interesting results have been reported by researchers using numeric data mining methods, there are still questions that need answering before textual data mining tools will be considered generally useful due to the effort needed to learn and use them. In 2011, we generated a dataset from the legal statements (mainly privacy policy and terms of use) on the websites of 475 of the US Fortune 500 Companies and used it as input to see what we could detect about the organizational relationships between the companies by using a textual data mining tool. We hoped to find that the tool would cluster similar corporations into the same industrial sector, as validated by the company's self-reported North American Industry Classification System code (NAICS). Unfortunately, this proved only marginally successful, leading us to ask why and to pose our research question: What problems occur when a data-mining tool is used to analyze large textual datasets that are unstructured, complex, duplicative, and contain many homonyms and synonyms? In analyzing our large dataset we learned a great deal about the problem and fortunately, after significant effort, determined how to “massage” the raw dataset to improve the process and learn how the tool can be better used in research situations. We also found that NAICS, as self-reported by companies, are of dubious value to a researcher—a matter briefly discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 212\n",
      "Title: A semantic enhanced hybrid recommendation approach: A case study of e-Government tourism service recommendation system\n",
      "Abstract: Recommender systems are effectively used as a personalized information filtering technology to automatically predict and identify a set of interesting items on behalf of users according to their personal needs and preferences. Collaborative Filtering (CF) approach is commonly used in the context of recommender systems; however, obtaining better prediction accuracy and overcoming the main limitations of the standard CF recommendation algorithms, such as sparsity and cold-start item problems, remain a significant challenge. Recent developments in personalization and recommendation techniques support the use of semantic enhanced hybrid recommender systems, which incorporate ontology-based semantic similarity measure with other recommendation approaches to improve the quality of recommendations. Consequently, this paper presents the effectiveness of utilizing semantic knowledge of items to enhance the recommendation quality. It proposes a new Inferential Ontology-based Semantic Similarity (IOBSS) measure to evaluate semantic similarity between items in a specific domain of interest by taking into account their explicit hierarchical relationships, shared attributes and implicit relationships. The paper further proposes a hybrid semantic enhanced recommendation approach by combining the new IOBSS measure and the standard item-based CF approach. A set of experiments with promising results validates the effectiveness of the proposed hybrid approach, using a case study of the Australian e-Government tourism services.\n",
      "Content: A semantic enhanced hybrid recommendation approach: A case study of e-Government tourism service recommendation system Recommender systems are effectively used as a personalized information filtering technology to automatically predict and identify a set of interesting items on behalf of users according to their personal needs and preferences. Collaborative Filtering (CF) approach is commonly used in the context of recommender systems; however, obtaining better prediction accuracy and overcoming the main limitations of the standard CF recommendation algorithms, such as sparsity and cold-start item problems, remain a significant challenge. Recent developments in personalization and recommendation techniques support the use of semantic enhanced hybrid recommender systems, which incorporate ontology-based semantic similarity measure with other recommendation approaches to improve the quality of recommendations. Consequently, this paper presents the effectiveness of utilizing semantic knowledge of items to enhance the recommendation quality. It proposes a new Inferential Ontology-based Semantic Similarity (IOBSS) measure to evaluate semantic similarity between items in a specific domain of interest by taking into account their explicit hierarchical relationships, shared attributes and implicit relationships. The paper further proposes a hybrid semantic enhanced recommendation approach by combining the new IOBSS measure and the standard item-based CF approach. A set of experiments with promising results validates the effectiveness of the proposed hybrid approach, using a case study of the Australian e-Government tourism services.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 213\n",
      "Title: The influence of attitudes on personal computer utilization among knowledge workers: the case of Saudi Arabia\n",
      "Abstract: Since the introduction of personal computers (PCs) in the early 1980s, Saudi Arabia has made major investments in PCs to match its rapidly growing economy. As a result, the PC business has become one of the fastest growing sectors in the Kingdom of Saudi Arabia. Our paper reports on the results of a study which investigates the relationships between end-users' attitudes and PC utilization among knowledge workers in the context of Saudi Arabia. To gain a better understanding of the factors that influence the use of PCs, we adopted Triandis' theory which suggests that behavior is determined by attitudes, social norms, habits and expected consequences of behavior. Our study is based on previous efforts to test the theory's validity in Saudi Arabia. Our results suggest that PC utilization is determined by individual attitudes, personal characteristics, such as PC experience, facilitating conditions, such as PC access and social factors. We also observed that respondents to our questionnaire differ in the level of importance they attribute to the factors hypothesized as influencing PC utilization compared to Canadian respondents in a previous study.\n",
      "Content: The influence of attitudes on personal computer utilization among knowledge workers: the case of Saudi Arabia Since the introduction of personal computers (PCs) in the early 1980s, Saudi Arabia has made major investments in PCs to match its rapidly growing economy. As a result, the PC business has become one of the fastest growing sectors in the Kingdom of Saudi Arabia. Our paper reports on the results of a study which investigates the relationships between end-users' attitudes and PC utilization among knowledge workers in the context of Saudi Arabia. To gain a better understanding of the factors that influence the use of PCs, we adopted Triandis' theory which suggests that behavior is determined by attitudes, social norms, habits and expected consequences of behavior. Our study is based on previous efforts to test the theory's validity in Saudi Arabia. Our results suggest that PC utilization is determined by individual attitudes, personal characteristics, such as PC experience, facilitating conditions, such as PC access and social factors. We also observed that respondents to our questionnaire differ in the level of importance they attribute to the factors hypothesized as influencing PC utilization compared to Canadian respondents in a previous study.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 214\n",
      "Title: ERP software implementation: an integrative framework\n",
      "Abstract: ERP implementation is a socio-technical challenge that requires a fundamentally different outlook from technologically-driven innovation, and will depend on a balanced perspective where the organisation as a total system is considered. ERP implementation is considered to rely on behavioural processes and actions. It is a process that involves macro-implementation at the strategic level, and micro-implementation at the operational level. This therefore means that implementation in the context of ERP systems is not possible through an ON/OFF approach whereby deployment of the new systems will necessarily yield the desired and expected results. Understanding the implementation process through a balanced perspective will therefore prevent any unpleasant surprises, and will ensure and guide the change process to be embedded in a painless fashion. The balanced perspective means that socio-technical considerations must be borne in mind; the strategic, tactical and operational steps clearly defined; and the expected benefits evaluated and tracked through creating seamless and solid integration. This paper proposes an integrative framework for ERP implementation based on an extensive review of the factors and the essential elements that contribute to success in the context of ERP implementation.\n",
      "Content: ERP software implementation: an integrative framework ERP implementation is a socio-technical challenge that requires a fundamentally different outlook from technologically-driven innovation, and will depend on a balanced perspective where the organisation as a total system is considered. ERP implementation is considered to rely on behavioural processes and actions. It is a process that involves macro-implementation at the strategic level, and micro-implementation at the operational level. This therefore means that implementation in the context of ERP systems is not possible through an ON/OFF approach whereby deployment of the new systems will necessarily yield the desired and expected results. Understanding the implementation process through a balanced perspective will therefore prevent any unpleasant surprises, and will ensure and guide the change process to be embedded in a painless fashion. The balanced perspective means that socio-technical considerations must be borne in mind; the strategic, tactical and operational steps clearly defined; and the expected benefits evaluated and tracked through creating seamless and solid integration. This paper proposes an integrative framework for ERP implementation based on an extensive review of the factors and the essential elements that contribute to success in the context of ERP implementation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 215\n",
      "Title: The Role of Design Characteristics in Shaping Perceptions of Similarity: The Case of Online Shopping Assistants\n",
      "Abstract: This research proposes that technological artifacts are perceived as social actors, and that users can attribute personality and behavioral traits to them. These formed perceptions interact with the user's own characteristics to construct an evaluation of the similarity between the user and the technological artifact. Such perceptions of similarity are important because individuals tend to more positively evaluate others, in this case technological artifacts, to whom they are more similar. Using an automated shopping assistant as one type of technological artifact, we investigate two types of perceived similarity between the customer and the artifact: perceived personality similarity and perceived behavioral similarity. We then investigate how design characteristics drive a customer's perceptions of these similarities and, importantly, the bases for those design characteristics. Decisional guidance and speech act theory provide the basis for personality manifestation, while normative versus heuristic-based decision rules provide the basis for behavioral manifestation. We apply these design bases in an experiment. The results demonstrate that IT design characteristics can be used to manifest desired personalities and behaviors in a technological artifact. Moreover, these manifestations of personality and behavior interact with the customer's own personality and behaviors to create matching perceptions of personality and behavioral similarity between the customer and the artifact. This study emphasizes the need to consider technological artifacts as social actors and describes the specific ways in which technology design can manifest social attributes. In doing so, we show that it is possible to match the social attributes of a technological artifact with those of the user.\n",
      "Content: The Role of Design Characteristics in Shaping Perceptions of Similarity: The Case of Online Shopping Assistants This research proposes that technological artifacts are perceived as social actors, and that users can attribute personality and behavioral traits to them. These formed perceptions interact with the user's own characteristics to construct an evaluation of the similarity between the user and the technological artifact. Such perceptions of similarity are important because individuals tend to more positively evaluate others, in this case technological artifacts, to whom they are more similar. Using an automated shopping assistant as one type of technological artifact, we investigate two types of perceived similarity between the customer and the artifact: perceived personality similarity and perceived behavioral similarity. We then investigate how design characteristics drive a customer's perceptions of these similarities and, importantly, the bases for those design characteristics. Decisional guidance and speech act theory provide the basis for personality manifestation, while normative versus heuristic-based decision rules provide the basis for behavioral manifestation. We apply these design bases in an experiment. The results demonstrate that IT design characteristics can be used to manifest desired personalities and behaviors in a technological artifact. Moreover, these manifestations of personality and behavior interact with the customer's own personality and behaviors to create matching perceptions of personality and behavioral similarity between the customer and the artifact. This study emphasizes the need to consider technological artifacts as social actors and describes the specific ways in which technology design can manifest social attributes. In doing so, we show that it is possible to match the social attributes of a technological artifact with those of the user.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 216\n",
      "Title: The Adoption of Online Shopping Assistants: Perceived Similarity as an Antecedent to Evaluative Beliefs\n",
      "Abstract: In recent work, researchers have supplemented traditional IS adoption models with new constructs that capture users' relational, social, and emotional beliefs. These beliefs have given rise to questions regarding their antecedents and the nature of the user-artifact relationship. This paper sheds light on these questions by asserting that users perceive and respond to information technology (IT) artifacts as social partners and form perceptions about their social characteristics. Subsequently, users' perceptions of the similarity of these characteristics to their own affect evaluations of these artifacts. Within the context of online shopping and using an automated shopping assistant, our paper draws upon social psychology and human-computer interaction research in developing hypotheses regarding the effects of perceived personality similarity (PPS) and perceived decision process similarity (PDPS) on a number of beliefs (enjoyment, social presence, trust, ease of use, and usefulness). The results indicate that PDPS acts as an antecedent to these beliefs, while the effects of PPS are largely mediated by PDPS. Furthermore, the results reveal that the effects of perceived similarity, in general, exceed those of the effects of the individual assessments of the user's and the assistant's personalities and decision processes. These results have important implications for IS design. They highlight the importance of designing artifacts that can be matched to users' characteristics. They also underscore the importance of considering similarity perceptions rather than solely focusing on perceptions of the IT artifact's characteristics; a common approach in IS adoption research.\n",
      "Content: The Adoption of Online Shopping Assistants: Perceived Similarity as an Antecedent to Evaluative Beliefs In recent work, researchers have supplemented traditional IS adoption models with new constructs that capture users' relational, social, and emotional beliefs. These beliefs have given rise to questions regarding their antecedents and the nature of the user-artifact relationship. This paper sheds light on these questions by asserting that users perceive and respond to information technology (IT) artifacts as social partners and form perceptions about their social characteristics. Subsequently, users' perceptions of the similarity of these characteristics to their own affect evaluations of these artifacts. Within the context of online shopping and using an automated shopping assistant, our paper draws upon social psychology and human-computer interaction research in developing hypotheses regarding the effects of perceived personality similarity (PPS) and perceived decision process similarity (PDPS) on a number of beliefs (enjoyment, social presence, trust, ease of use, and usefulness). The results indicate that PDPS acts as an antecedent to these beliefs, while the effects of PPS are largely mediated by PDPS. Furthermore, the results reveal that the effects of perceived similarity, in general, exceed those of the effects of the individual assessments of the user's and the assistant's personalities and decision processes. These results have important implications for IS design. They highlight the importance of designing artifacts that can be matched to users' characteristics. They also underscore the importance of considering similarity perceptions rather than solely focusing on perceptions of the IT artifact's characteristics; a common approach in IS adoption research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 217\n",
      "Title: Designing Online Virtual Advisors to Encourage Customer Self-disclosure: A Theoretical Model and an Empirical Test\n",
      "Abstract: Virtual advisors (VA) are tools that assist users in making decisions. Using VAs necessitates the disclosure of personal information, especially when they are employed in personalized contexts such as healthcare, where disclosure is vital to providing valid and accurate advice. Yet, extant research has largely overlooked the factors that encourage or inhibit users' from disclosing to VAs. In contrast, this study investigates the determinants of users' intentions to self-disclose, and examines how VAs can be designed to enhance these intentions. The results of a study in the context of skin care advice reveal that the intention to disclose to a VA is not only the product of a rational process, but that perceptions of the VA and the relationship with it are important. The results further show that a parsimonious set of design elements can be used to endow a VA with desired characteristics that enhance the willingness to disclose. The study contributes to our understanding of the factors influencing users' intentions to provide personal information to a VA, which extend beyond the expected benefits and costs. The study further demonstrates that social exchange theory can be applied in contexts in which humans are interacting with automated VAs.\n",
      "Content: Designing Online Virtual Advisors to Encourage Customer Self-disclosure: A Theoretical Model and an Empirical Test Virtual advisors (VA) are tools that assist users in making decisions. Using VAs necessitates the disclosure of personal information, especially when they are employed in personalized contexts such as healthcare, where disclosure is vital to providing valid and accurate advice. Yet, extant research has largely overlooked the factors that encourage or inhibit users' from disclosing to VAs. In contrast, this study investigates the determinants of users' intentions to self-disclose, and examines how VAs can be designed to enhance these intentions. The results of a study in the context of skin care advice reveal that the intention to disclose to a VA is not only the product of a rational process, but that perceptions of the VA and the relationship with it are important. The results further show that a parsimonious set of design elements can be used to endow a VA with desired characteristics that enhance the willingness to disclose. The study contributes to our understanding of the factors influencing users' intentions to provide personal information to a VA, which extend beyond the expected benefits and costs. The study further demonstrates that social exchange theory can be applied in contexts in which humans are interacting with automated VAs.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 218\n",
      "Title: The Adoption and Use of IT Artifacts: A New Interaction-Centric Model for the Study of User-Artifact Relationships\n",
      "Abstract: The question of why a user adopts an information technology (IT) artifact has received ample research attention in the past few decades. Although recent adoption research has focused on investigating some of the relational and experiential aspects associated with adopting and using IT artifacts, the theories utilized have been static in nature. Furthermore, many have been based on traditional models like TAM and TPB, which focus on the utilitarian benefits that users accrue from their interactions with IT artifacts. Independently, recent research has paid much-needed attention to factors surrounding the use of IT artifacts. In this paper, we offer an overview of a theoretical model that connects these two interrelated processes. Starting with a survey of concepts related to social interactions, we present an argument in support of viewing IT artifacts as social actors, whose characteristics are manifested within the context of interactions. The proposed interaction-centric model highlights how the characteristics of an IT artifact, together with the user's internal system and other structuring factors, affect users' choices in terms of how to utilize the artifact. The nature of that utilization, subsequently, affects the beliefs users form about the artifact and the outcomes from using it. Furthermore, the model proposes that users will also form beliefs about their bond or relationship with the IT artifact. These beliefs do not refer to observations made in a single interaction, but rather concern users' mental representations of past interactions and outcomes. To facilitate the study of the relationship that develops from user-artifact interactions over time, the model describes how past interactions affect future ones. Specifically, it proposes that deciding how to utilize an IT artifact in subsequent interaction, consistent with theories of relationship development, is influenced by already held beliefs about the artifact and the relationship with it.\n",
      "Content: The Adoption and Use of IT Artifacts: A New Interaction-Centric Model for the Study of User-Artifact Relationships The question of why a user adopts an information technology (IT) artifact has received ample research attention in the past few decades. Although recent adoption research has focused on investigating some of the relational and experiential aspects associated with adopting and using IT artifacts, the theories utilized have been static in nature. Furthermore, many have been based on traditional models like TAM and TPB, which focus on the utilitarian benefits that users accrue from their interactions with IT artifacts. Independently, recent research has paid much-needed attention to factors surrounding the use of IT artifacts. In this paper, we offer an overview of a theoretical model that connects these two interrelated processes. Starting with a survey of concepts related to social interactions, we present an argument in support of viewing IT artifacts as social actors, whose characteristics are manifested within the context of interactions. The proposed interaction-centric model highlights how the characteristics of an IT artifact, together with the user's internal system and other structuring factors, affect users' choices in terms of how to utilize the artifact. The nature of that utilization, subsequently, affects the beliefs users form about the artifact and the outcomes from using it. Furthermore, the model proposes that users will also form beliefs about their bond or relationship with the IT artifact. These beliefs do not refer to observations made in a single interaction, but rather concern users' mental representations of past interactions and outcomes. To facilitate the study of the relationship that develops from user-artifact interactions over time, the model describes how past interactions affect future ones. Specifically, it proposes that deciding how to utilize an IT artifact in subsequent interaction, consistent with theories of relationship development, is influenced by already held beliefs about the artifact and the relationship with it.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 219\n",
      "Title: An Empirical Investigation of the Antecedents and Consequences of Privacy Uncertainty in the Context of Mobile Apps\n",
      "Abstract: When using digital goods that extensively collect user information, privacy uncertainty, which is consumers’ difficulty in assessing the privacy of the data they entrust to others, is a major concern. We extend the existing literature on uncertainty in online marketplaces by incorporating privacy uncertainty, and we distinguish among three subdimensions of privacy uncertainty—namely collection, use, and protection. We subsequently theorize and empirically test the antecedents and consequences of this new construct in the context of mobile apps. Consistent with economic theory, we argue that because consumers possess less information than the app seller as a result of the presence of hidden characteristics and hidden action, privacy uncertainty is evoked. Using the factorial survey method, we test our theoretical model in the context of buying a mobile app. The results show that privacy uncertainty significantly influences potential users’ intention to use an app above and beyond their uncertainty about the seller and the product. Privacy uncertainty also affects the perceived risk associated with using an app and the price consumers are willing to pay for it. In addition, we find that privacy uncertainty is driven by the uncertainty about privacy practices in regard to the collection, use, and protection of data collected at the time of downloading the app—and more important, the data collected while the app is being used. The results of another factorial survey study suggest that privacy uncertainty is distinct from seller and product uncertainties and has unique drivers. The results of a survey of current users of existing mobile apps indicate that the effects of privacy uncertainty extend to the postadoption stage, where it remains a strong influencer of continued use intentions and perceived risk.\n",
      "Content: An Empirical Investigation of the Antecedents and Consequences of Privacy Uncertainty in the Context of Mobile Apps When using digital goods that extensively collect user information, privacy uncertainty, which is consumers’ difficulty in assessing the privacy of the data they entrust to others, is a major concern. We extend the existing literature on uncertainty in online marketplaces by incorporating privacy uncertainty, and we distinguish among three subdimensions of privacy uncertainty—namely collection, use, and protection. We subsequently theorize and empirically test the antecedents and consequences of this new construct in the context of mobile apps. Consistent with economic theory, we argue that because consumers possess less information than the app seller as a result of the presence of hidden characteristics and hidden action, privacy uncertainty is evoked. Using the factorial survey method, we test our theoretical model in the context of buying a mobile app. The results show that privacy uncertainty significantly influences potential users’ intention to use an app above and beyond their uncertainty about the seller and the product. Privacy uncertainty also affects the perceived risk associated with using an app and the price consumers are willing to pay for it. In addition, we find that privacy uncertainty is driven by the uncertainty about privacy practices in regard to the collection, use, and protection of data collected at the time of downloading the app—and more important, the data collected while the app is being used. The results of another factorial survey study suggest that privacy uncertainty is distinct from seller and product uncertainties and has unique drivers. The results of a survey of current users of existing mobile apps indicate that the effects of privacy uncertainty extend to the postadoption stage, where it remains a strong influencer of continued use intentions and perceived risk.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 220\n",
      "Title: BRM: A methodology for improving the practical relevance of belief-based information technology usage theories\n",
      "Abstract: There has been extensive research in Information Systems (IS) regarding the individual-level beliefs that are germane to technology usage. Theories like the technology acceptance model, user satisfaction, service quality, the diffusion of innovations theory, and others identify a wide variety of influential beliefs, such as those that target properties of the system itself (e.g., its features), the use of it (e.g., its ease of use), or other contextual aspects (e.g., the pressure from peers to use it). These theories have been invaluable for understanding one of the most fundamental phenomena of interest to IS researchers and practitioners: individuals’ adoption and use of information technology (IT). However, as valuable as these theories are, there has been growing criticism about the fact that several of their constituting belief constructs (e.g., usefulness or quality) do not lend themselves to prescriptions for actionable interventions, in particular those geared toward IT design. This is especially prob­ lematic when it comes to applying or extending them to the contexts of emerging technologies, where best design practices are still to be discovered and tested. We address this concern by developing a Broadness Reduction Methodology (BRM) that relies on Fishbein and Ajzen’s (1975) foundational work on the nature and formation of beliefs to help researchers develop belief-based theoretical models that are more relevant to IT practitioners. In order to illustrate the application of our methodology and demonstrate its validity, we apply it to the context of ebusiness. In doing so, we decompose the broad construct of supporting-service functionality, which has been shown to be a strong predictor of satisfaction and continued usage, into more specific constructs that are more amenable to actionable design recommendations.\n",
      "Content: BRM: A methodology for improving the practical relevance of belief-based information technology usage theories There has been extensive research in Information Systems (IS) regarding the individual-level beliefs that are germane to technology usage. Theories like the technology acceptance model, user satisfaction, service quality, the diffusion of innovations theory, and others identify a wide variety of influential beliefs, such as those that target properties of the system itself (e.g., its features), the use of it (e.g., its ease of use), or other contextual aspects (e.g., the pressure from peers to use it). These theories have been invaluable for understanding one of the most fundamental phenomena of interest to IS researchers and practitioners: individuals’ adoption and use of information technology (IT). However, as valuable as these theories are, there has been growing criticism about the fact that several of their constituting belief constructs (e.g., usefulness or quality) do not lend themselves to prescriptions for actionable interventions, in particular those geared toward IT design. This is especially prob­ lematic when it comes to applying or extending them to the contexts of emerging technologies, where best design practices are still to be discovered and tested. We address this concern by developing a Broadness Reduction Methodology (BRM) that relies on Fishbein and Ajzen’s (1975) foundational work on the nature and formation of beliefs to help researchers develop belief-based theoretical models that are more relevant to IT practitioners. In order to illustrate the application of our methodology and demonstrate its validity, we apply it to the context of ebusiness. In doing so, we decompose the broad construct of supporting-service functionality, which has been shown to be a strong predictor of satisfaction and continued usage, into more specific constructs that are more amenable to actionable design recommendations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 221\n",
      "Title: Taxing the development structure of open source communities: An information processing view\n",
      "Abstract: Committers in Free/Libre and Open Source Software (FLOSS) projects shoulder responsibility for evaluating contributions and coordinating the broader community development effort. Given committers' central role in development processes, we examine whether how they are organized influences FLOSS community performance. Specifically, drawing on the lens of Organizational Information Processing Theory (OIPT), we develop a model that explains how committal a structure's ability to manage information impacts FLOSS community performance. Based on archival data drawn from 237 active FLOSS communities, we found that the performance of centralized and decentralized FLOSS communities varied with three conditions tied to information flows: task routineness, uncertainty and task interdependence. Our empirical results support the idea that FLOSS communities performing development tasks that are generally routine, highly interdependent, and generate little contributor uncertainty will perform better under a centralized committal structure. On the other hand, decentralized committal structures thrive under the conditions of task non-routineness, low task interdependence, and high contributor uncertainty. We conclude with a discussion of results, limitations, and directions for future research.\n",
      "Content: Taxing the development structure of open source communities: An information processing view Committers in Free/Libre and Open Source Software (FLOSS) projects shoulder responsibility for evaluating contributions and coordinating the broader community development effort. Given committers' central role in development processes, we examine whether how they are organized influences FLOSS community performance. Specifically, drawing on the lens of Organizational Information Processing Theory (OIPT), we develop a model that explains how committal a structure's ability to manage information impacts FLOSS community performance. Based on archival data drawn from 237 active FLOSS communities, we found that the performance of centralized and decentralized FLOSS communities varied with three conditions tied to information flows: task routineness, uncertainty and task interdependence. Our empirical results support the idea that FLOSS communities performing development tasks that are generally routine, highly interdependent, and generate little contributor uncertainty will perform better under a centralized committal structure. On the other hand, decentralized committal structures thrive under the conditions of task non-routineness, low task interdependence, and high contributor uncertainty. We conclude with a discussion of results, limitations, and directions for future research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 222\n",
      "Title: Developing and validating an instrument for measuring user-perceived web quality\n",
      "Abstract: Many of the instruments to measure information and system quality were developed in the context of mainframe and PC-based technologies of yesteryears. With the proliferation of the Internet and World Wide Web applications, users are increasingly interfacing and interacting with web-based applications. It is, therefore, important to develop new instruments and scales, which are directly targeted to these new interfaces and applications. In this article, we report on the development of an instrument that captures key characteristics of web site quality from the user’s perspective. The 25-item instrument measures four dimensions of web quality: specific content, content quality, appearance and technical adequacy. While improvements are possible, the instrument exhibits excellent psychometric properties. The instrument would be useful to organizations and web designers as it provides an aggregate measure of web quality, and to researchers in related web research.\n",
      "Content: Developing and validating an instrument for measuring user-perceived web quality Many of the instruments to measure information and system quality were developed in the context of mainframe and PC-based technologies of yesteryears. With the proliferation of the Internet and World Wide Web applications, users are increasingly interfacing and interacting with web-based applications. It is, therefore, important to develop new instruments and scales, which are directly targeted to these new interfaces and applications. In this article, we report on the development of an instrument that captures key characteristics of web site quality from the user’s perspective. The 25-item instrument measures four dimensions of web quality: specific content, content quality, appearance and technical adequacy. While improvements are possible, the instrument exhibits excellent psychometric properties. The instrument would be useful to organizations and web designers as it provides an aggregate measure of web quality, and to researchers in related web research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 223\n",
      "Title: An Integrated Performance Model of Information Systems Projects\n",
      "Abstract: This study makes an initial attempt to validate an integrated, theoretically driven performance model of information systems (IS) projects. IS project performance is defined in terms of task, psychological, and organizational outcomes. We draw upon different theoretical perspectives including IS, organizational teams, and project management to link six categories of variables to IS project performance: technology characteristics, project characteristics, task characteristics, people characteristics, organizational characteristics, and work processes. Data collected via a field survey of IS project leaders in 84 manufacturing organizations were used to test the proposed model. Support is found for three conclusions: (1) IS project performance is a multidimensional construct, (2) certain preconditions falling into the above categories have to exist to achieve a high performing IS project, and (3) there is a possible cross-relationship among the variables studied by IS research, organizational teams research, and project management research. We discuss the implications of this study for future research and managerial practice.\n",
      "Content: An Integrated Performance Model of Information Systems Projects This study makes an initial attempt to validate an integrated, theoretically driven performance model of information systems (IS) projects. IS project performance is defined in terms of task, psychological, and organizational outcomes. We draw upon different theoretical perspectives including IS, organizational teams, and project management to link six categories of variables to IS project performance: technology characteristics, project characteristics, task characteristics, people characteristics, organizational characteristics, and work processes. Data collected via a field survey of IS project leaders in 84 manufacturing organizations were used to test the proposed model. Support is found for three conclusions: (1) IS project performance is a multidimensional construct, (2) certain preconditions falling into the above categories have to exist to achieve a high performing IS project, and (3) there is a possible cross-relationship among the variables studied by IS research, organizational teams research, and project management research. We discuss the implications of this study for future research and managerial practice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 224\n",
      "Title: An empirical examination of the role of social integration in system development projects\n",
      "Abstract: Abstract. In spite of the apparent importance of social integration for work collectives within organizations, information systems researchers have so far paid little, if any, attention to evaluating its role in system development projects. The present study tries to contribute to the literature by proposing and testing a model that examines some of the antecedents and consequences of social integration in system development projects. Data collected from system development project leaders working in 84 US organizations were used to test the model. The findings suggest that higher social integration and, consequently, higher system development project performance is best attained when management provides basic support for the work of the project. The results also reveal that the nature of the relationship between social integration and project performance may be contingent upon some other factors. The implications of the findings of this research are discussed.\n",
      "Content: An empirical examination of the role of social integration in system development projects Abstract. In spite of the apparent importance of social integration for work collectives within organizations, information systems researchers have so far paid little, if any, attention to evaluating its role in system development projects. The present study tries to contribute to the literature by proposing and testing a model that examines some of the antecedents and consequences of social integration in system development projects. Data collected from system development project leaders working in 84 US organizations were used to test the model. The findings suggest that higher social integration and, consequently, higher system development project performance is best attained when management provides basic support for the work of the project. The results also reveal that the nature of the relationship between social integration and project performance may be contingent upon some other factors. The implications of the findings of this research are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 225\n",
      "Title: The development of two tools for measuring the easiness and usefulness of transactional Web sites\n",
      "Abstract: The worldwide diffusion of Electronic Commerce shifts the exchange relationship between buyers and sellers from the face-to-face model to the face-to-screen model; and it is, therefore, important for organisations to consider the Web attributes that attract users. Yet, conceptualising, empirically testing, and refining tools for measuring the easiness and usefulness of transactional Web sites are lacking. The present study tries to fill this gap in the literature by describing the development of two tools for measuring perceived easiness and usefulness in a Web context. The findings of exploratory and confirmatory factor analyses reveal that the two instruments demonstrate sound measurement properties and would be useful to organisations interested in setting up an electronic business, and to scholars interested in Web research.\n",
      "Content: The development of two tools for measuring the easiness and usefulness of transactional Web sites The worldwide diffusion of Electronic Commerce shifts the exchange relationship between buyers and sellers from the face-to-face model to the face-to-screen model; and it is, therefore, important for organisations to consider the Web attributes that attract users. Yet, conceptualising, empirically testing, and refining tools for measuring the easiness and usefulness of transactional Web sites are lacking. The present study tries to fill this gap in the literature by describing the development of two tools for measuring perceived easiness and usefulness in a Web context. The findings of exploratory and confirmatory factor analyses reveal that the two instruments demonstrate sound measurement properties and would be useful to organisations interested in setting up an electronic business, and to scholars interested in Web research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 226\n",
      "Title: An assessment of the use of Transaction Cost Theory in information technology outsourcing\n",
      "Abstract: Transaction Cost Theory (TCT) has been widely used in information technology outsourcing (ITO) research to explain and predict outsourcing decisions and outsourcing-related outcomes. This research, however, has led to mixed and unexpected results in terms of the effects of transaction attributes on outsourcing decisions and outcomes. This study assesses the empirical literature employing TCT-based ITO models in terms of its faithfulness to the precepts of TCT, and argues that one possible explanation for the mixed results is that the extant models do not capture all the essential elements of TCT. First, there are core TCT constructs that the extant models do not take into account; second, the linkages among constructs that the IT outsourcing models have hypothesized are not always in line with TCT precepts; and third, the normative nature of the theory is not always captured by the extant models. This paper, therefore, aims to provide one possible answer to the question: “Why have the appropriations made of TCT to study IT outsourcing produced mixed results?”\n",
      "Content: An assessment of the use of Transaction Cost Theory in information technology outsourcing Transaction Cost Theory (TCT) has been widely used in information technology outsourcing (ITO) research to explain and predict outsourcing decisions and outsourcing-related outcomes. This research, however, has led to mixed and unexpected results in terms of the effects of transaction attributes on outsourcing decisions and outcomes. This study assesses the empirical literature employing TCT-based ITO models in terms of its faithfulness to the precepts of TCT, and argues that one possible explanation for the mixed results is that the extant models do not capture all the essential elements of TCT. First, there are core TCT constructs that the extant models do not take into account; second, the linkages among constructs that the IT outsourcing models have hypothesized are not always in line with TCT precepts; and third, the normative nature of the theory is not always captured by the extant models. This paper, therefore, aims to provide one possible answer to the question: “Why have the appropriations made of TCT to study IT outsourcing produced mixed results?”\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 227\n",
      "Title: Platforms as service ecosystems: Lessons from social media\n",
      "Abstract: The growing business expansion of social media platforms is changing their identity and transforming the practices of networking, data and content sharing with which social media have been commonly associated. We empirically investigate these shifts in the context of TripAdvisor and its evolution since its very establishment. We trace the mutations of the platform along three stages we identify as search engine, social media platform and end-to-end service ecosystem. Our findings reveal the underlying patterns of data types, technological functionalities and actor configurations that punctuate the business expansion of TripAdvisor and lead to the formation of its service ecosystem. We contribute to the understanding of the current trajectory in which social media find themselves as well as to the literature on platforms and ecosystems. We point out the importance of services that develop as commercially viable and constantly updatable data bundles out of diverse and dynamic data types. Such services are essential to the making of the complementarities that are claimed to underlie ecosystem formation.\n",
      "Content: Platforms as service ecosystems: Lessons from social media The growing business expansion of social media platforms is changing their identity and transforming the practices of networking, data and content sharing with which social media have been commonly associated. We empirically investigate these shifts in the context of TripAdvisor and its evolution since its very establishment. We trace the mutations of the platform along three stages we identify as search engine, social media platform and end-to-end service ecosystem. Our findings reveal the underlying patterns of data types, technological functionalities and actor configurations that punctuate the business expansion of TripAdvisor and lead to the formation of its service ecosystem. We contribute to the understanding of the current trajectory in which social media find themselves as well as to the literature on platforms and ecosystems. We point out the importance of services that develop as commercially viable and constantly updatable data bundles out of diverse and dynamic data types. Such services are essential to the making of the complementarities that are claimed to underlie ecosystem formation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 228\n",
      "Title: Customer relationships and the small software firm: A framework for understanding challenges faced in marketing\n",
      "Abstract: This paper identifies the major marketing challenges small software firms face during their growth and internationalization processes. It starts with an analysis of small software company activities along a continuum from ‘project business’ to ‘product business.’ This is followed by a brief analysis of the two major schools of thought in marketing, in which there is a paradigm shift from the traditional notion of marketing-mix management towards ‘relationship marketing’ is discernible. Finally, the discussion is summarized in a framework for identifying the major marketing challenges facing software company managers at the beginning of the next millenium.\n",
      "Content: Customer relationships and the small software firm: A framework for understanding challenges faced in marketing This paper identifies the major marketing challenges small software firms face during their growth and internationalization processes. It starts with an analysis of small software company activities along a continuum from ‘project business’ to ‘product business.’ This is followed by a brief analysis of the two major schools of thought in marketing, in which there is a paradigm shift from the traditional notion of marketing-mix management towards ‘relationship marketing’ is discernible. Finally, the discussion is summarized in a framework for identifying the major marketing challenges facing software company managers at the beginning of the next millenium.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 229\n",
      "Title: Decision support capabilities of enterprise content management systems: An empirical investigation\n",
      "Abstract: Enterprise content management (ECM) systems help organizations cope with the increasing complexity and volume of data and information. Despite the growing popularity of ECM, published literature indicates that organizations primarily use ECM for operational benefits, while the strategic decision making capabilities are rarely considered. Thus, the most significant rewards of ECM implementation may be largely forgone. This study investigates the potential of ECM technology for decision support. A research model is proposed and validated via an empirical investigation. The results show that ECM positively influences problem identification and definition, decision making speed and analysis, decision quality, and decision makers' satisfaction.\n",
      "Content: Decision support capabilities of enterprise content management systems: An empirical investigation Enterprise content management (ECM) systems help organizations cope with the increasing complexity and volume of data and information. Despite the growing popularity of ECM, published literature indicates that organizations primarily use ECM for operational benefits, while the strategic decision making capabilities are rarely considered. Thus, the most significant rewards of ECM implementation may be largely forgone. This study investigates the potential of ECM technology for decision support. A research model is proposed and validated via an empirical investigation. The results show that ECM positively influences problem identification and definition, decision making speed and analysis, decision quality, and decision makers' satisfaction.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 230\n",
      "Title: Temporal Motivations of Volunteers to Participate in Cultural Crowdsourcing Work\n",
      "Abstract: Crowdsourcing (CS) by cultural and heritage institutions engage volunteers in online projects without monetary compensation. Uncertainty concerning online volunteer motivation has led to a growing body of academic research. This study contributes to that debate, by extending focus to CS volunteer work in nonprofit cultural institutions where no monetary benefit is offered to volunteers. This study examines motivations of high performing volunteers in a newspaper digitisation CS project, initiated by the National Library of Australia. Volunteers are motivated by personal, collective, and external factors, and these motivations change over time. Volunteers initially show intrinsic motivations, though both intrinsic and extrinsic motivations play a critical role in their continued participation. Volunteer contributions range from data shaping (e.g., correcting digitised optical character recognition data) to knowledge shaping (e.g., shaping historical data through tagging and commenting, but also through development of norms and social roles). The locus of motivation (intrinsic or extrinsic) also changes with different kinds of contributions. The distinction between data and knowledge shaping contributions, and the locus and focus of motivation behind these activities, has implications for the design of CS systems. Design for improved usability through cognitive and physical system affordances and development of social mechanisms for ongoing participation is discussed.\n",
      "Content: Temporal Motivations of Volunteers to Participate in Cultural Crowdsourcing Work Crowdsourcing (CS) by cultural and heritage institutions engage volunteers in online projects without monetary compensation. Uncertainty concerning online volunteer motivation has led to a growing body of academic research. This study contributes to that debate, by extending focus to CS volunteer work in nonprofit cultural institutions where no monetary benefit is offered to volunteers. This study examines motivations of high performing volunteers in a newspaper digitisation CS project, initiated by the National Library of Australia. Volunteers are motivated by personal, collective, and external factors, and these motivations change over time. Volunteers initially show intrinsic motivations, though both intrinsic and extrinsic motivations play a critical role in their continued participation. Volunteer contributions range from data shaping (e.g., correcting digitised optical character recognition data) to knowledge shaping (e.g., shaping historical data through tagging and commenting, but also through development of norms and social roles). The locus of motivation (intrinsic or extrinsic) also changes with different kinds of contributions. The distinction between data and knowledge shaping contributions, and the locus and focus of motivation behind these activities, has implications for the design of CS systems. Design for improved usability through cognitive and physical system affordances and development of social mechanisms for ongoing participation is discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 231\n",
      "Title: The role of system-use practices for sustaining motivation in crowdsourcing: A technology-in-practice perspective\n",
      "Abstract: The success of crowdsourcing (CS) systems depends on sustained participation, which is an ongoing challenge for the majority of CS providers. Unfortunately, participants are frequently demotivated by technical difficulties and the incorrect use of CS systems, which can result in CS failure. Although the literature generally assumes that sustained participation in CS is determined by a shift between intrinsic and extrinsic motivation, the role of system-use practices in facilitating such a shift remains unknown. We explore how CS system-use practices influence participants' sustained motivation, evolving from initiation to progression to sustention. Using the notion of technology-in-practice as a lens, we develop and examine a process model using an in-depth case study of a large-scale ongoing CS project, the Australian Newspaper Digitisation Program. The findings suggest that CS participants' motivation is shaped by an evolving combination of three basic components (i.e., contextual condition, outcome and action intensity) and mediated by two types of system-use practice (i.e., passive and active). Passive-use practices facilitate sustaining motivation from initiation to progression, whereas active-use practices have a key role in sustention. Our study contributes to the emerging literature on the substantial role of system-use practices in sustaining motivation, resulting in sustained participation. The findings also offer actionable insights into improving the viability of CS systems in retaining and motivating continuous and increased contributions from participants.\n",
      "Content: The role of system-use practices for sustaining motivation in crowdsourcing: A technology-in-practice perspective The success of crowdsourcing (CS) systems depends on sustained participation, which is an ongoing challenge for the majority of CS providers. Unfortunately, participants are frequently demotivated by technical difficulties and the incorrect use of CS systems, which can result in CS failure. Although the literature generally assumes that sustained participation in CS is determined by a shift between intrinsic and extrinsic motivation, the role of system-use practices in facilitating such a shift remains unknown. We explore how CS system-use practices influence participants' sustained motivation, evolving from initiation to progression to sustention. Using the notion of technology-in-practice as a lens, we develop and examine a process model using an in-depth case study of a large-scale ongoing CS project, the Australian Newspaper Digitisation Program. The findings suggest that CS participants' motivation is shaped by an evolving combination of three basic components (i.e., contextual condition, outcome and action intensity) and mediated by two types of system-use practice (i.e., passive and active). Passive-use practices facilitate sustaining motivation from initiation to progression, whereas active-use practices have a key role in sustention. Our study contributes to the emerging literature on the substantial role of system-use practices in sustaining motivation, resulting in sustained participation. The findings also offer actionable insights into improving the viability of CS systems in retaining and motivating continuous and increased contributions from participants.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 232\n",
      "Title: Too Tired and in Too Good of a Mood to Worry About Privacy: Explaining the Privacy Paradox Through the Lens of Effort Level in Information Processing\n",
      "Abstract: The confluence of digital transactions, growing cybersecurity threats, and the internet of the future (e.g., web 3.0 and the metaverse) have made information privacy increasingly important to consumers and companies that rely on consumers willingly sharing their personal information. Although information privacy has been of interest to researchers for decades and much has been learned, one thing that perplexes scholars is the privacy paradox, which we define as a mismatch between stated privacy concerns and actual disclosure behaviors. In this paper, we shed light on this phenomenon and show that low-effort information processing triggered by cognitive depletion (Experiment 1), positive mood (Experiment 2), or both (Experiment 3) significantly attenuates the association between stated privacy concerns and disclosure behaviors. These findings do not indicate that individuals do not care about privacy because we find consistent evidence in the three experiments for a significant negative association between stated privacy concerns and disclosure behaviors when individuals have sufficient cognitive capacity (Experiment 1), experience a negative (or neutral) mood (Experiment 2), or have sufficient cognitive capacity coupled with a negative mood state (Experiment 3). Our findings reveal that the paradox is neither an absolute phenomenon nor a myth, but its existence is conditional on contextual factors, including psychological factors related to information processing. We discuss our contribution to privacy theory and provide implications for consumers, companies, and policymakers. History: Alessandro Acquisti, Senior Editor; Idris Adjerid, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1182.\n",
      "Content: Too Tired and in Too Good of a Mood to Worry About Privacy: Explaining the Privacy Paradox Through the Lens of Effort Level in Information Processing The confluence of digital transactions, growing cybersecurity threats, and the internet of the future (e.g., web 3.0 and the metaverse) have made information privacy increasingly important to consumers and companies that rely on consumers willingly sharing their personal information. Although information privacy has been of interest to researchers for decades and much has been learned, one thing that perplexes scholars is the privacy paradox, which we define as a mismatch between stated privacy concerns and actual disclosure behaviors. In this paper, we shed light on this phenomenon and show that low-effort information processing triggered by cognitive depletion (Experiment 1), positive mood (Experiment 2), or both (Experiment 3) significantly attenuates the association between stated privacy concerns and disclosure behaviors. These findings do not indicate that individuals do not care about privacy because we find consistent evidence in the three experiments for a significant negative association between stated privacy concerns and disclosure behaviors when individuals have sufficient cognitive capacity (Experiment 1), experience a negative (or neutral) mood (Experiment 2), or have sufficient cognitive capacity coupled with a negative mood state (Experiment 3). Our findings reveal that the paradox is neither an absolute phenomenon nor a myth, but its existence is conditional on contextual factors, including psychological factors related to information processing. We discuss our contribution to privacy theory and provide implications for consumers, companies, and policymakers. History: Alessandro Acquisti, Senior Editor; Idris Adjerid, Associate Editor. Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1182.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 233\n",
      "Title: Revisiting DSS Implementation Research: A Meta-Analysis of the Literature and Suggestions for Researchers\n",
      "Abstract:  Information systems are becoming increasingly critical to the daily operations and success of many firms. This, combined with the rising investments in design and development of these system.s, make implementation a high priority research topic. Although information systems implementation has been a topic of interest to researchers over the past two decades, the extent to which the existing body of research reflects substantial and cumulative development is not entirely clear.The objective of this study is to conduct a rigorous and quantitative review of the empirical DSS implementation fiterature as a basis for providing guidelines for implementation management and conduct of future research. Metaanalysis of 144 findings from 33 studies indicates that user-situational variables (involvement, training and experience) are more important than psychological factors to DSS implementation success and that user-situational variables can improve the implementation success by as much as 30 percent. Furthermore, the meta-analytic findings regarding the methodological characteristics of studies provide useful insights for the design of future research studies .of implementation. The findings also allow us to put into perspective the incremental contribution of additional substantive and empirical studies in this area. Additionally, several specific domains (e.g., construct validation research on user involvement and casual modeling) might profit most from future research efforts. Research Framework and Variables,~. large body of DSS implementation studies have investigated the relationship between userrelated factors and implementation success. The framework in Figure 1 encompasses different perspectives and common themes from the previous work in the DSS implementation genre. The core of the framework consists of four sets of user-related factors believed to influence DSS implementation success: cognitive style, personality, demographics, and user-situational variables. The relationships between these factors and DSS implementation are believed to be irffluenced by a number of contextual variables consisting of decision-making tasks, (e.g., task type or task complexity), organizational factors (e.g., top management support), and external factors (e.g., competitive considerations). Although the potential moderating effects of the contextual variables are recognized at a conceptual level, the empirical studies of user factors and DSS implementation are rarely inclusive of all the variable sets displayed in Figure 1. Therefore, our meta-analytic study was organized around the core relationships between user factors and DSS implementation success. \n",
      "Content: Revisiting DSS Implementation Research: A Meta-Analysis of the Literature and Suggestions for Researchers  Information systems are becoming increasingly critical to the daily operations and success of many firms. This, combined with the rising investments in design and development of these system.s, make implementation a high priority research topic. Although information systems implementation has been a topic of interest to researchers over the past two decades, the extent to which the existing body of research reflects substantial and cumulative development is not entirely clear.The objective of this study is to conduct a rigorous and quantitative review of the empirical DSS implementation fiterature as a basis for providing guidelines for implementation management and conduct of future research. Metaanalysis of 144 findings from 33 studies indicates that user-situational variables (involvement, training and experience) are more important than psychological factors to DSS implementation success and that user-situational variables can improve the implementation success by as much as 30 percent. Furthermore, the meta-analytic findings regarding the methodological characteristics of studies provide useful insights for the design of future research studies .of implementation. The findings also allow us to put into perspective the incremental contribution of additional substantive and empirical studies in this area. Additionally, several specific domains (e.g., construct validation research on user involvement and casual modeling) might profit most from future research efforts. Research Framework and Variables,~. large body of DSS implementation studies have investigated the relationship between userrelated factors and implementation success. The framework in Figure 1 encompasses different perspectives and common themes from the previous work in the DSS implementation genre. The core of the framework consists of four sets of user-related factors believed to influence DSS implementation success: cognitive style, personality, demographics, and user-situational variables. The relationships between these factors and DSS implementation are believed to be irffluenced by a number of contextual variables consisting of decision-making tasks, (e.g., task type or task complexity), organizational factors (e.g., top management support), and external factors (e.g., competitive considerations). Although the potential moderating effects of the contextual variables are recognized at a conceptual level, the empirical studies of user factors and DSS implementation are rarely inclusive of all the variable sets displayed in Figure 1. Therefore, our meta-analytic study was organized around the core relationships between user factors and DSS implementation success. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 234\n",
      "Title: An Empirical Examination of the Influence of Organizational Culture on Knowledge Management Practices\n",
      "Abstract: Knowledge management to facilitate the creation, storage, transfer, and application of knowledge in organizations has received wide attention in practice and research in the past several years. Often cited as a significant challenge in knowledge management practices is the issue of organizational culture. Although many studies raise the issue of organizational culture's influence on knowledge management success, few investigate the way in which this influence manifests itself. This paper aims to explore how organizational culture influences knowledge management practices. Using a case study method, we examine the cultural values and knowledge management approaches within a large global information services company and one of its knowledge communities. The findings highlight the influence of culture on the use of knowledge management technologies and the outcomes of such use.\n",
      "Content: An Empirical Examination of the Influence of Organizational Culture on Knowledge Management Practices Knowledge management to facilitate the creation, storage, transfer, and application of knowledge in organizations has received wide attention in practice and research in the past several years. Often cited as a significant challenge in knowledge management practices is the issue of organizational culture. Although many studies raise the issue of organizational culture's influence on knowledge management success, few investigate the way in which this influence manifests itself. This paper aims to explore how organizational culture influences knowledge management practices. Using a case study method, we examine the cultural values and knowledge management approaches within a large global information services company and one of its knowledge communities. The findings highlight the influence of culture on the use of knowledge management technologies and the outcomes of such use.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 235\n",
      "Title: Research Commentary: Technology-Mediated Learning—A Call for Greater Depth and Breadth of Research\n",
      "Abstract: Universities and corporate training facilities have been investing in information technologies to improve education and training at an increasing rate during the past decade. Many new companies are emerging to provide tools and services to enable the effective design of IT-based learning solutions. Although research on technology-mediated learning has increased in recent years, it still lags behind developments in practice. This essay suggests potential research avenues in the area of technology-mediated learning. It seeks to motivate greater depth of research into the question of how technology enhances learning. This question requires an explicit consideration of relationships among technology capabilities, instructional strategy, psychological processes, and contextual factors involved in learning. The essay also recommends attention to a greater breadth of research questions, including issues of how technology-mediated learning affects program design and what structures and processes universities can employ to facilitate innovation.\n",
      "Content: Research Commentary: Technology-Mediated Learning—A Call for Greater Depth and Breadth of Research Universities and corporate training facilities have been investing in information technologies to improve education and training at an increasing rate during the past decade. Many new companies are emerging to provide tools and services to enable the effective design of IT-based learning solutions. Although research on technology-mediated learning has increased in recent years, it still lags behind developments in practice. This essay suggests potential research avenues in the area of technology-mediated learning. It seeks to motivate greater depth of research into the question of how technology enhances learning. This question requires an explicit consideration of relationships among technology capabilities, instructional strategy, psychological processes, and contextual factors involved in learning. The essay also recommends attention to a greater breadth of research questions, including issues of how technology-mediated learning affects program design and what structures and processes universities can employ to facilitate innovation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 236\n",
      "Title: Review:  Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues\n",
      "Abstract:  Knowledge is a broad and abstract notion that has defined epistemological debate in western philosophy since the classical Greek era. In the past Alavi & Leidner/Knowledge Management \n",
      "Content: Review:  Knowledge Management and Knowledge Management Systems: Conceptual Foundations and Research Issues  Knowledge is a broad and abstract notion that has defined epistemological debate in western philosophy since the classical Greek era. In the past Alavi & Leidner/Knowledge Management \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 237\n",
      "Title: A Comparative Study of Distributed Learning Environments on Learning Outcomes\n",
      "Abstract: Advances in information and communication technologies have fueled rapid growth in the popularity of technology-supported distributed learning (DL). Many educational institutions, both academic and corporate, have undertaken initiatives that leverage the myriad of available DL technologies. Despite their rapid growth in popularity, however, alternative technologies for DL are seldom systematically evaluated for learning efficacy. Considering the increasing range of information and communication technologies available for the development of DL environments, we believe it is paramount for studies to compare the relative learning outcomes of various technologies. In this research, we employed a quasi-experimental field study approach to investigate the relative learning effectiveness of two collaborative DL environments in the context of an executive development program. We also adopted a framework of hierarchical characteristics of group support system (GSS) technologies, outlined by DeSanctis and Gallupe (1987), as the basis for characterizing the two DL environments. One DL environment employed a simple e-mail and listserv capability while the other used a sophisticated GSS (herein referred to as Beta system).Interestingly, the learning outcome of the e-mail environment was higher than the learning outcome of the more sophisticated GSS environment. The post-hoc analysis of the electronic messages indicated that the students in groups using the e-mail system exchanged a higher percentage of messages related to the learning task. The Beta system users exchanged a higher level of technology sense-making messages. No significant difference was observed in the students' satisfaction with the learning process under the two DL environments.\n",
      "Content: A Comparative Study of Distributed Learning Environments on Learning Outcomes Advances in information and communication technologies have fueled rapid growth in the popularity of technology-supported distributed learning (DL). Many educational institutions, both academic and corporate, have undertaken initiatives that leverage the myriad of available DL technologies. Despite their rapid growth in popularity, however, alternative technologies for DL are seldom systematically evaluated for learning efficacy. Considering the increasing range of information and communication technologies available for the development of DL environments, we believe it is paramount for studies to compare the relative learning outcomes of various technologies. In this research, we employed a quasi-experimental field study approach to investigate the relative learning effectiveness of two collaborative DL environments in the context of an executive development program. We also adopted a framework of hierarchical characteristics of group support system (GSS) technologies, outlined by DeSanctis and Gallupe (1987), as the basis for characterizing the two DL environments. One DL environment employed a simple e-mail and listserv capability while the other used a sophisticated GSS (herein referred to as Beta system).Interestingly, the learning outcome of the e-mail environment was higher than the learning outcome of the more sophisticated GSS environment. The post-hoc analysis of the electronic messages indicated that the students in groups using the e-mail system exchanged a higher percentage of messages related to the learning task. The Beta system users exchanged a higher level of technology sense-making messages. No significant difference was observed in the students' satisfaction with the learning process under the two DL environments.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 238\n",
      "Title: Computer-Mediate Collaborative Learning: An Empirical Evaluation\n",
      "Abstract:  National commissions and scholarly reports on the status of contemporary higher education have frequently been critical of the college, experience; the emphasis on transmitting fixed bodies of information and a failure to develop problem solving and critical thinking skills have been cited as serious weaknesses in higher education systems. Colleges and universities have additional reasons to redevelop central pedagogies for students. Individuals need to learn at higher rates of effectiveness and efficiency than ever before because of rapidly growing bodies of relevant information and the escalation of knowledge and skill requirements for most jobs.Recent developments in computer hardware, software, and communication technologies create exciting new opportunities for the educational use of these technologies. The objective of this study is to go I~eyond the traditional classroom instructional modes (e.g., lectures and class discussions) to develop and evaluate computer-supported pedagogical approaches. More specifically, this study investigates whether the use of a group decision support system (GDSS) in a collaborative learning process enhances student learning and evaluation of classroom experiences.The findings of a study involving 127 MBA students indicate that GDSS-supported collaborative learning leads to higher levels of perceived skill development, self.reported learning, and evaluation of classroom experience in comparison with non-GDSS supported collaborative learning. Furthermore, the final test grades of the group of students who were exposed to GDSS-supported collaborative learning were significantly higher than those of the other group of students who participated in the experimenL \n",
      "Content: Computer-Mediate Collaborative Learning: An Empirical Evaluation  National commissions and scholarly reports on the status of contemporary higher education have frequently been critical of the college, experience; the emphasis on transmitting fixed bodies of information and a failure to develop problem solving and critical thinking skills have been cited as serious weaknesses in higher education systems. Colleges and universities have additional reasons to redevelop central pedagogies for students. Individuals need to learn at higher rates of effectiveness and efficiency than ever before because of rapidly growing bodies of relevant information and the escalation of knowledge and skill requirements for most jobs.Recent developments in computer hardware, software, and communication technologies create exciting new opportunities for the educational use of these technologies. The objective of this study is to go I~eyond the traditional classroom instructional modes (e.g., lectures and class discussions) to develop and evaluate computer-supported pedagogical approaches. More specifically, this study investigates whether the use of a group decision support system (GDSS) in a collaborative learning process enhances student learning and evaluation of classroom experiences.The findings of a study involving 127 MBA students indicate that GDSS-supported collaborative learning leads to higher levels of perceived skill development, self.reported learning, and evaluation of classroom experience in comparison with non-GDSS supported collaborative learning. Furthermore, the final test grades of the group of students who were exposed to GDSS-supported collaborative learning were significantly higher than those of the other group of students who participated in the experimenL \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 239\n",
      "Title: Gist: A Model for Design and Management of Content and Interactivity of Customer-Centric Web Sites\n",
      "Abstract: Customer-centric Web-based systems, such as e-commerce Web sites, or sites that support customer relationship management (CRM) activities, are themselves information systems, but their design and maintenance need to follow vastly different approaches from the traditional systems lifecycle approach. Based on marketing frame-  works that are applicable to the online world, and following design science principles, we develop a model to guide the design and the continuous management of such sites. The model makes extensive use of current technologies for tracking the customers and their behaviors, and combines elements of data mining and statistical analyses. A case study based on a financial services Web site is used to provide a preliminary validation and design evaluation of our approach. The case study showed considerable measured improvement in the effectiveness of the company's Web site. In addition, it also highlighted an important benefit of the our approach: the identification of previously unknown or unexpected segments of visitors. This finding can lead to promising new  business opportunities.\n",
      "Content: Gist: A Model for Design and Management of Content and Interactivity of Customer-Centric Web Sites Customer-centric Web-based systems, such as e-commerce Web sites, or sites that support customer relationship management (CRM) activities, are themselves information systems, but their design and maintenance need to follow vastly different approaches from the traditional systems lifecycle approach. Based on marketing frame-  works that are applicable to the online world, and following design science principles, we develop a model to guide the design and the continuous management of such sites. The model makes extensive use of current technologies for tracking the customers and their behaviors, and combines elements of data mining and statistical analyses. A case study based on a financial services Web site is used to provide a preliminary validation and design evaluation of our approach. The case study showed considerable measured improvement in the effectiveness of the company's Web site. In addition, it also highlighted an important benefit of the our approach: the identification of previously unknown or unexpected segments of visitors. This finding can lead to promising new  business opportunities.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 240\n",
      "Title: Marketplace and technology standards for B2B e-commerce: progress, challenges, and the state of the art\n",
      "Abstract: We have examined standards required for successful e-commerce (EC) architectures and evaluated the strengths and limitations of current systems that have been developed to support EC. We find that there is an unfilled need for systems that can reliably locate buyers and sellers in electronic marketplaces and also facilitate automated transactions. The notion of a ubiquitous network where loosely coupled buyers and sellers can reliably find each other in real time, evaluate products, negotiate prices, and conduct transactions is not adequately supported by current systems. These findings were based on an analysis of mainline EC architectures: EDI, company Websites, B2B hubs, e-Procurement systems, and Web Services. Limitations of each architecture were identified. Particular attention was given to the strengths and weaknesses of the Web Services architecture, since it may overcome some limitations of the other approaches.\n",
      "Content: Marketplace and technology standards for B2B e-commerce: progress, challenges, and the state of the art We have examined standards required for successful e-commerce (EC) architectures and evaluated the strengths and limitations of current systems that have been developed to support EC. We find that there is an unfilled need for systems that can reliably locate buyers and sellers in electronic marketplaces and also facilitate automated transactions. The notion of a ubiquitous network where loosely coupled buyers and sellers can reliably find each other in real time, evaluate products, negotiate prices, and conduct transactions is not adequately supported by current systems. These findings were based on an analysis of mainline EC architectures: EDI, company Websites, B2B hubs, e-Procurement systems, and Web Services. Limitations of each architecture were identified. Particular attention was given to the strengths and weaknesses of the Web Services architecture, since it may overcome some limitations of the other approaches.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 241\n",
      "Title: eHR software, multinational corporations and emerging China: Exploring the role of information through a postcolonial lens\n",
      "Abstract: This paper seeks to offer an alternative account of Human Resources Information software (eHR) informed by a critical/postcolonial view on information systems. In so doing, it aims to explore the possibilities for managing people that information brings when Human Resources Management practices are transferred from “developed” to “developing” countries. The paper relies on several qualitative in-depth interviews with renowned Chinese Human Resources experts in Shanghai, and the examination of diverse eHR software-related documentation and functionalities. Critical discourse analysis was used to examine these sources. The findings show that eHR information systems bring new governance possibilities that support and expand the discipline of Human Resources Management. The use of eHR software in people management gives a new momentum and increased dominance to key Western-originated practices, such as HR-based performance management. Information brings new ordering options that facilitate the transferability, mobility and standardization of HR values, discourse and practices and, ultimately, the construction of a global “generified employee”. The paper offers a first critical analysis of eHR software, showing the need to understand the relevancy of the informating power of these systems for a postcolonial critique of ICT. It offers a view of the “micro-processes” that facilitate organizational transfer from the multinational corporation headquarters to the subsidiaries and across countries. In so doing, it challenges mainstream deterministic assumptions and apolitical approaches to this technology.\n",
      "Content: eHR software, multinational corporations and emerging China: Exploring the role of information through a postcolonial lens This paper seeks to offer an alternative account of Human Resources Information software (eHR) informed by a critical/postcolonial view on information systems. In so doing, it aims to explore the possibilities for managing people that information brings when Human Resources Management practices are transferred from “developed” to “developing” countries. The paper relies on several qualitative in-depth interviews with renowned Chinese Human Resources experts in Shanghai, and the examination of diverse eHR software-related documentation and functionalities. Critical discourse analysis was used to examine these sources. The findings show that eHR information systems bring new governance possibilities that support and expand the discipline of Human Resources Management. The use of eHR software in people management gives a new momentum and increased dominance to key Western-originated practices, such as HR-based performance management. Information brings new ordering options that facilitate the transferability, mobility and standardization of HR values, discourse and practices and, ultimately, the construction of a global “generified employee”. The paper offers a first critical analysis of eHR software, showing the need to understand the relevancy of the informating power of these systems for a postcolonial critique of ICT. It offers a view of the “micro-processes” that facilitate organizational transfer from the multinational corporation headquarters to the subsidiaries and across countries. In so doing, it challenges mainstream deterministic assumptions and apolitical approaches to this technology.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 242\n",
      "Title: Clarifying the effects of Internet monitoring on job attitudes: The mediating role of employee trust\n",
      "Abstract: The Internet is a fast growing mechanism for providing workplace monitoring. We examined how its implementation affects employees’ trust in the organization. We hypothesized that giving employees advance notice of monitoring and providing them a justification for it would enhance their trust. We investigated how employees’ perceptions of organizational support prior to monitoring moderated these relationships by conducting a longitudinal field experiment. We found that advance notice and perceived organizational support exerted significant main and interactive effects on post-implementation trust. In turn, trust significantly affected employees’ job satisfaction, organizational commitment, and turnover intentions.\n",
      "Content: Clarifying the effects of Internet monitoring on job attitudes: The mediating role of employee trust The Internet is a fast growing mechanism for providing workplace monitoring. We examined how its implementation affects employees’ trust in the organization. We hypothesized that giving employees advance notice of monitoring and providing them a justification for it would enhance their trust. We investigated how employees’ perceptions of organizational support prior to monitoring moderated these relationships by conducting a longitudinal field experiment. We found that advance notice and perceived organizational support exerted significant main and interactive effects on post-implementation trust. In turn, trust significantly affected employees’ job satisfaction, organizational commitment, and turnover intentions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 243\n",
      "Title: Information systems control alignment: Complementary and conflicting systems development controls\n",
      "Abstract: This study presents a new concept called information systems control alignment, which examines the degree that the underlying characteristics of four main information systems (IS) control dimensions are mutually complementary. Using three case studies, our research uncovers two high-functioning control patterns – one with traditional characteristics and one with agile characteristics – that demonstrate positive alignment among the control environment, control mechanisms, socio-emotional behaviors, and execution of controls. By better understanding the circumstances that contribute to control conflicts, organizations can be increasingly mindful of cultivating a complementary relationship among the control dimensions when designing, implementing, monitoring and adjusting controls within IS processes.\n",
      "Content: Information systems control alignment: Complementary and conflicting systems development controls This study presents a new concept called information systems control alignment, which examines the degree that the underlying characteristics of four main information systems (IS) control dimensions are mutually complementary. Using three case studies, our research uncovers two high-functioning control patterns – one with traditional characteristics and one with agile characteristics – that demonstrate positive alignment among the control environment, control mechanisms, socio-emotional behaviors, and execution of controls. By better understanding the circumstances that contribute to control conflicts, organizations can be increasingly mindful of cultivating a complementary relationship among the control dimensions when designing, implementing, monitoring and adjusting controls within IS processes.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 244\n",
      "Title: Utopia in the solution of the Bucket Order Problem\n",
      "Abstract: This paper deals with group decision making and, in particular, with rank aggregation, which is the problem of aggregating individual preferences (rankings) in order to obtain a consensus ranking. Although this consensus ranking is usually a permutation of all the ranked items, in this paper we tackle the situation in which some items can be tied, that is, the consensus shows that there is no preference among them. This problem has arisen recently and is known as the Optimal Bucket Order Problem (OBOP). In this paper we propose two improvements to the standard greedy algorithm usually considered to approach the bucket order problem: the Bucket Pivot Algorithm (BPA). The first improvement is based on the introduction of the Utopian Matrix, a matrix associated to a pair order matrix that represents the precedences in a collection of rankings. This idealization constitutes a superoptimal solution to the OBOP, which can be used as an extreme (sometimes feasible) best value. The second improvement is based on the use of several items as pivots to generate the bucket order, in contrast to BPA that only uses a single pivot. The set of items playing the role of decision-maker is dynamically created. We analyze separately the contribution of each improvement and also their joint effect. The statistical analysis of the experiments carried out shows that the combined use of both techniques is the best choice, showing a significant improvement in accuracy (17%) with respect to the original BPA and providing an important reduction in the variance of the output. Moreover, we provide decision rules to help the decision maker to select the right algorithm according to the problem instance.\n",
      "Content: Utopia in the solution of the Bucket Order Problem This paper deals with group decision making and, in particular, with rank aggregation, which is the problem of aggregating individual preferences (rankings) in order to obtain a consensus ranking. Although this consensus ranking is usually a permutation of all the ranked items, in this paper we tackle the situation in which some items can be tied, that is, the consensus shows that there is no preference among them. This problem has arisen recently and is known as the Optimal Bucket Order Problem (OBOP). In this paper we propose two improvements to the standard greedy algorithm usually considered to approach the bucket order problem: the Bucket Pivot Algorithm (BPA). The first improvement is based on the introduction of the Utopian Matrix, a matrix associated to a pair order matrix that represents the precedences in a collection of rankings. This idealization constitutes a superoptimal solution to the OBOP, which can be used as an extreme (sometimes feasible) best value. The second improvement is based on the use of several items as pivots to generate the bucket order, in contrast to BPA that only uses a single pivot. The set of items playing the role of decision-maker is dynamically created. We analyze separately the contribution of each improvement and also their joint effect. The statistical analysis of the experiments carried out shows that the combined use of both techniques is the best choice, showing a significant improvement in accuracy (17%) with respect to the original BPA and providing an important reduction in the variance of the output. Moreover, we provide decision rules to help the decision maker to select the right algorithm according to the problem instance.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 245\n",
      "Title: Advances in intelligent information technology: re-branding or progress towards conscious machines?\n",
      "Abstract: Is artificial intelligence (AI) just something that is done in laboratories disconnected from the development of the pragmatic computing, which constitutes current information technology or does it contribute to progress in computing and information technology? It has even been suggested that advances in AI are merely a re-branding exercise for promises that are rarely kept. This paper is a personal view of the forces that have driven the development of AI in the past and what might be a serious paradigm shift in the future. The latter points to what appears to be the most abstruse corner of the subject: the modelling of the human brain and the possibility of designing systems with the brain's ability to create conscious thought. There have been accusations that AI is always ahead on promise and behind on delivery. This is an inaccurate view. In broad terms, the argument presented here suggests that as AI developed, progress was achieved by overcoming unforeseen difficulties in the pursuit of very ambitious targets, not just a re-branding of promises. This process not only advanced AI but also fed into the mainstream of computing that underpins the information technology of the present time. While the outcome of the paradigm shift towards conscious machines, which is examined at the end of this paper is still unclear, it is possible to speculate how information technology might be affected in the future.\n",
      "Content: Advances in intelligent information technology: re-branding or progress towards conscious machines? Is artificial intelligence (AI) just something that is done in laboratories disconnected from the development of the pragmatic computing, which constitutes current information technology or does it contribute to progress in computing and information technology? It has even been suggested that advances in AI are merely a re-branding exercise for promises that are rarely kept. This paper is a personal view of the forces that have driven the development of AI in the past and what might be a serious paradigm shift in the future. The latter points to what appears to be the most abstruse corner of the subject: the modelling of the human brain and the possibility of designing systems with the brain's ability to create conscious thought. There have been accusations that AI is always ahead on promise and behind on delivery. This is an inaccurate view. In broad terms, the argument presented here suggests that as AI developed, progress was achieved by overcoming unforeseen difficulties in the pursuit of very ambitious targets, not just a re-branding of promises. This process not only advanced AI but also fed into the mainstream of computing that underpins the information technology of the present time. While the outcome of the paradigm shift towards conscious machines, which is examined at the end of this paper is still unclear, it is possible to speculate how information technology might be affected in the future.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 246\n",
      "Title: Partners of humans: a realistic assessment of the role of robots in the foreseeable future\n",
      "Abstract: As robots are generally thought to perform human-like tasks, they depend on the successes of information technology in the area of artificial intelligence to succeed in such pursuits. But robots, through their anthropomorphic character and their weighty presence in science fiction, attract the attention of the press and the media in a way that, at times, blurs the distinction between the actual state of the art and exaggerated claims. This makes it hard to assess the true functional positioning of robots, how this is likely to move forward and whether the outcome of progress could be detrimental to human society. The aim of this paper is to review the actual level of competence that is being achieved in robotics research laboratories and a plausible impact that this is likely to have on human control over life and jobs. The key thesis here is that cognition in machines and even an artificial form of consciousness lead to operations in a set of tasks (the ‘algorithmic’ category) which is different from that available to truly cognitive and conscious human beings (the ‘life-need’ category): that is, in the paper it is argued that a major category error (Ryle in The concept of mind, University of Chicago Press, Chicago, 1949) looms in predictions of serious threats to humanity. As far as a threat to jobs goes, it is argued that early attention to education and re-skilling of humans in the workplace can lead to an effective symbiosis between people and robots.\n",
      "Content: Partners of humans: a realistic assessment of the role of robots in the foreseeable future As robots are generally thought to perform human-like tasks, they depend on the successes of information technology in the area of artificial intelligence to succeed in such pursuits. But robots, through their anthropomorphic character and their weighty presence in science fiction, attract the attention of the press and the media in a way that, at times, blurs the distinction between the actual state of the art and exaggerated claims. This makes it hard to assess the true functional positioning of robots, how this is likely to move forward and whether the outcome of progress could be detrimental to human society. The aim of this paper is to review the actual level of competence that is being achieved in robotics research laboratories and a plausible impact that this is likely to have on human control over life and jobs. The key thesis here is that cognition in machines and even an artificial form of consciousness lead to operations in a set of tasks (the ‘algorithmic’ category) which is different from that available to truly cognitive and conscious human beings (the ‘life-need’ category): that is, in the paper it is argued that a major category error (Ryle in The concept of mind, University of Chicago Press, Chicago, 1949) looms in predictions of serious threats to humanity. As far as a threat to jobs goes, it is argued that early attention to education and re-skilling of humans in the workplace can lead to an effective symbiosis between people and robots.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 247\n",
      "Title: Letting living intelligence put the artificial version in its place\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 248\n",
      "Title: The Soft Edge: A Natural History and Future of the Information Revolution\n",
      "Abstract:  Organizational memory and organizational learning are two approaches towards improving organizational processes. Organi zational memory is intended to augment the knowledge of groups by providing some record of the organization's know-how. Organizational learning on the other hand aims at turning organizations into systems able to think, reason, and learn as opposed to learning on the level of the individual. The business processes of modern organizations are highly supported by information technology, such as workflow systems, collaboration supporting tools, or special purpose applications. Recently, the intersecting area between the two fields, organizational memory and learning on the one hand, and information technology on the other hand, has gained a lot of attention in information systems research. Robert Neilson's book represents an interesting contribution to this area.To be more precise, the book presents the results of a case study investigating the effect of collaborative technology, namely Lotus Notes, on organizational learning. Two main issues are examined: \n",
      "Content: The Soft Edge: A Natural History and Future of the Information Revolution  Organizational memory and organizational learning are two approaches towards improving organizational processes. Organi zational memory is intended to augment the knowledge of groups by providing some record of the organization's know-how. Organizational learning on the other hand aims at turning organizations into systems able to think, reason, and learn as opposed to learning on the level of the individual. The business processes of modern organizations are highly supported by information technology, such as workflow systems, collaboration supporting tools, or special purpose applications. Recently, the intersecting area between the two fields, organizational memory and learning on the one hand, and information technology on the other hand, has gained a lot of attention in information systems research. Robert Neilson's book represents an interesting contribution to this area.To be more precise, the book presents the results of a case study investigating the effect of collaborative technology, namely Lotus Notes, on organizational learning. Two main issues are examined: \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 249\n",
      "Title: An introduction to qualitative research\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 250\n",
      "Title: Software process improvement: Concepts and practices\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 251\n",
      "Title: Handbook of Action Research Participative Inquiry and Practice\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 252\n",
      "Title: Managing Industrial Knowledge; Creation, Transfer and Utilization\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 253\n",
      "Title: Designing Collaborative Systems. A Practical Guide to Ethnography\n",
      "Abstract: The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "Content: Designing Collaborative Systems. A Practical Guide to Ethnography The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 254\n",
      "Title: Knowledge Management Systems\n",
      "Abstract: The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "Content: Knowledge Management Systems The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 255\n",
      "Title: Visualizing Argumentation: Software Tools for Collaborative and Educational Sense-making\n",
      "Abstract: The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "Content: Visualizing Argumentation: Software Tools for Collaborative and Educational Sense-making The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 256\n",
      "Title: The Business of Systems Integration\n",
      "Abstract: The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "Content: The Business of Systems Integration The European Journal of Information Systems provides a distinctive European perspective on the theory and practice of information systems for a global audience. We encourage first rate research articles by academics, but also case studies and reflective articles by practitioners. We provide a critical view on technology, development, implementation, strategy, management and policy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 257\n",
      "Title: A user-friendly marketing decision support system for the product line design using evolutionary algorithms\n",
      "Abstract: A marketing decision support system (MDSS) is presented. It has a user-friendly and easy to learn menu driven interface. Its purpose is to assist a marketing manager in designing a line of substitute products. Optimal product line design is a very important marketing decision. The MDSS uses three different optimization criteria. It examines different scenarios using the “What if analysis”. Also, it finds optimal solutions only for small sized problems using the complete enumeration method and near optimal solutions for real sized problems using evolutionary algorithms. The user is not forced to be familiar with the underlying models.\n",
      "Content: A user-friendly marketing decision support system for the product line design using evolutionary algorithms A marketing decision support system (MDSS) is presented. It has a user-friendly and easy to learn menu driven interface. Its purpose is to assist a marketing manager in designing a line of substitute products. Optimal product line design is a very important marketing decision. The MDSS uses three different optimization criteria. It examines different scenarios using the “What if analysis”. Also, it finds optimal solutions only for small sized problems using the complete enumeration method and near optimal solutions for real sized problems using evolutionary algorithms. The user is not forced to be familiar with the underlying models.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 258\n",
      "Title: Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks\n",
      "Abstract: The goal of this study is to show an alternative method to corporate failure prediction. In the last decades Artificial Neural Networks have been widely used for this task. These models have the advantage of being able to detect non-linear relationships and show a good performance in presence of noisy information, as it usually happens, in corporate failure prediction problems. AdaBoost is a novel ensemble learning algorithm that constructs its base classifiers in sequence using different versions of the training data set. In this paper, we compare the prediction accuracy of both techniques on a set of European firms, considering the usual predicting variables such as financial ratios, as well as qualitative variables, such as firm size, activity and legal structure. We show that our approach decreases the generalization error by about thirty percent with respect to the error produced with a neural network.\n",
      "Content: Bankruptcy forecasting: An empirical comparison of AdaBoost and neural networks The goal of this study is to show an alternative method to corporate failure prediction. In the last decades Artificial Neural Networks have been widely used for this task. These models have the advantage of being able to detect non-linear relationships and show a good performance in presence of noisy information, as it usually happens, in corporate failure prediction problems. AdaBoost is a novel ensemble learning algorithm that constructs its base classifiers in sequence using different versions of the training data set. In this paper, we compare the prediction accuracy of both techniques on a set of European firms, considering the usual predicting variables such as financial ratios, as well as qualitative variables, such as firm size, activity and legal structure. We show that our approach decreases the generalization error by about thirty percent with respect to the error produced with a neural network.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 259\n",
      "Title: Customer relationship management in call centers: The uneasy process of re(form)ing the subject through the ‘people-by-numbers’ approach\n",
      "Abstract: Real-time technology has the capability of symbolising both customers and call center representatives (and the moment of interaction), purely by/as numbers, or forms. The pinnacle of this data processing is customer relationship management (CRM), where the digitised data is assembled so as to reproduce a mimetic model of the customer. This could be seen as a metamyth (Adams & Ingersoll, 1990) that, in its concealed appearance within corporate databases, seems to cuts loose from any critical inquiry. In this paper, we offer an embryonic form of such a critique through the analysis of a number of original call center case studies. It seeks to analyze the nature of abstraction at the heart of IT-based CRM practices, and the contradictions that such abstraction can foster.\n",
      "Content: Customer relationship management in call centers: The uneasy process of re(form)ing the subject through the ‘people-by-numbers’ approach Real-time technology has the capability of symbolising both customers and call center representatives (and the moment of interaction), purely by/as numbers, or forms. The pinnacle of this data processing is customer relationship management (CRM), where the digitised data is assembled so as to reproduce a mimetic model of the customer. This could be seen as a metamyth (Adams & Ingersoll, 1990) that, in its concealed appearance within corporate databases, seems to cuts loose from any critical inquiry. In this paper, we offer an embryonic form of such a critique through the analysis of a number of original call center case studies. It seeks to analyze the nature of abstraction at the heart of IT-based CRM practices, and the contradictions that such abstraction can foster.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 260\n",
      "Title: An empirical study on the susceptibility to social engineering in social networking sites: The case of Facebook\n",
      "Abstract: Research suggests that social engineering attacks pose a significant security risk, with social networking sites (SNSs) being the most common source of these attacks. Recent studies showed that social engineers could succeed even among those organizations that identify themselves as being aware of social engineering techniques. Although organizations recognize the serious risks of social engineering, there is little understanding and control of such threats. This may be partly due to the complexity of human behaviors in failing to recognize attackers in SNSs. Due to the vital role that impersonation plays in influencing users to fall victim to social engineering deception, this paper aims to investigate the impact of source characteristics on users' susceptibility to social engineering victimization on Facebook. In doing so, we identify source credibility dimensions in terms of social engineering on Facebook, Facebook-based source characteristics that influence users to judge an attacker as per these dimensions, and mediation effects that these dimensions play between Facebook-based source characteristics and susceptibility to social engineering victimization. © 2017 The OR Society.\n",
      "Content: An empirical study on the susceptibility to social engineering in social networking sites: The case of Facebook Research suggests that social engineering attacks pose a significant security risk, with social networking sites (SNSs) being the most common source of these attacks. Recent studies showed that social engineers could succeed even among those organizations that identify themselves as being aware of social engineering techniques. Although organizations recognize the serious risks of social engineering, there is little understanding and control of such threats. This may be partly due to the complexity of human behaviors in failing to recognize attackers in SNSs. Due to the vital role that impersonation plays in influencing users to fall victim to social engineering deception, this paper aims to investigate the impact of source characteristics on users' susceptibility to social engineering victimization on Facebook. In doing so, we identify source credibility dimensions in terms of social engineering on Facebook, Facebook-based source characteristics that influence users to judge an attacker as per these dimensions, and mediation effects that these dimensions play between Facebook-based source characteristics and susceptibility to social engineering victimization. © 2017 The OR Society.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 261\n",
      "Title: Adverse Selection in B2B Secondary Market Online Auctions for IT Equipment: An Empirical Analysis\n",
      "Abstract: Online business-to-business auctions for used IT products have emerged as a viable market for finding a second life for these products, rather than having them end up in landfills as e-waste. As part of the growing “secondary market” landscape, these online B2B auctions are significantly affected by adverse selection since uncertainty about product quality from their first life remains in place. We study how these adverse selection costs may be identified and reduced in online B2B auctions for mobile phones using a proprietary data set for pallets of iPhone devices. We focus on the differences between carrier-locked and unlocked iPhones, and the degree to which the jailbreaking of devices may lead to adverse selection costs. We first show that uncertainty with respect to the possibility of jailbreaking-to-unlock induces significant adverse selection costs in this market. We identify a clear method that these adverse selection costs may be reduced through policies implemented in the primary market. We find that adverse selection costs exist with respect to jailbreaking-to-unlock, by comparing prices obtained for locked and unlocked devices, as well as pallets where this information is not disclosed. However, when some of the uncertainty surrounding jailbreaking-to-unlock is removed by virtue of an exogenous policy change implemented by Verizon in the primary market, i.e., to sell all iPhones as factory unlocked, adverse selection costs are significantly reduced. Our work has significant implications for enhancing the efficiency of secondary markets for IT products, by virtue of highlighting the connections between primary and secondary markets. Managerial and theoretical implications that emerge from this work are discussed in the paper.\n",
      "Content: Adverse Selection in B2B Secondary Market Online Auctions for IT Equipment: An Empirical Analysis Online business-to-business auctions for used IT products have emerged as a viable market for finding a second life for these products, rather than having them end up in landfills as e-waste. As part of the growing “secondary market” landscape, these online B2B auctions are significantly affected by adverse selection since uncertainty about product quality from their first life remains in place. We study how these adverse selection costs may be identified and reduced in online B2B auctions for mobile phones using a proprietary data set for pallets of iPhone devices. We focus on the differences between carrier-locked and unlocked iPhones, and the degree to which the jailbreaking of devices may lead to adverse selection costs. We first show that uncertainty with respect to the possibility of jailbreaking-to-unlock induces significant adverse selection costs in this market. We identify a clear method that these adverse selection costs may be reduced through policies implemented in the primary market. We find that adverse selection costs exist with respect to jailbreaking-to-unlock, by comparing prices obtained for locked and unlocked devices, as well as pallets where this information is not disclosed. However, when some of the uncertainty surrounding jailbreaking-to-unlock is removed by virtue of an exogenous policy change implemented by Verizon in the primary market, i.e., to sell all iPhones as factory unlocked, adverse selection costs are significantly reduced. Our work has significant implications for enhancing the efficiency of secondary markets for IT products, by virtue of highlighting the connections between primary and secondary markets. Managerial and theoretical implications that emerge from this work are discussed in the paper.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 262\n",
      "Title: Impressionable or Immune? Examining the Influence of Marquee Sellers in B2B Secondary Market Platforms for IT Products\n",
      "Abstract: Although digital platforms have become mainstream, there still remain some unanswered questions pertaining to managing platform ecosystems. One such unexplored question pertains to the effect of marquee sellers on the platform—marquee sellers arguably attract other sellers and buyers to the platform, thereby enhancing the platform’s value. In this paper, we study how adding a marquee seller to a business-to-business (B2B) secondary-market platform for IT products affects other sellers, in terms of the prices they obtain for comparable products. Using proprietary data on B2B secondary-market auctions obtained from a platform provider, we show that the entry of a marquee seller has a positive effect on the prices obtained by other sellers on the platform, reflecting a reference-price effect. We further show that this positive effect on final prices is moderated by the extent to which bidders are active on multiple seller sites on the same platform and the extent to which bidders participate in the marquee seller site. We explain these effects using theory in multihoming and involvement, in terms of how reference prices are set. Our work extends the platforms literature by considering the specific influence of a marquee seller on other sellers, thereby informing platform owners on the implications of the advice to add more marquee sellers to platforms. We also contribute to the literature on secondary markets for durable, used IT products, which are instrumental in reducing electronic waste and mitigating the environmental damage done by electronic products in landfills.History: Raghu Santanam, Senior Editor; Pallab Sanyal, Associate Editor.Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1137.\n",
      "Content: Impressionable or Immune? Examining the Influence of Marquee Sellers in B2B Secondary Market Platforms for IT Products Although digital platforms have become mainstream, there still remain some unanswered questions pertaining to managing platform ecosystems. One such unexplored question pertains to the effect of marquee sellers on the platform—marquee sellers arguably attract other sellers and buyers to the platform, thereby enhancing the platform’s value. In this paper, we study how adding a marquee seller to a business-to-business (B2B) secondary-market platform for IT products affects other sellers, in terms of the prices they obtain for comparable products. Using proprietary data on B2B secondary-market auctions obtained from a platform provider, we show that the entry of a marquee seller has a positive effect on the prices obtained by other sellers on the platform, reflecting a reference-price effect. We further show that this positive effect on final prices is moderated by the extent to which bidders are active on multiple seller sites on the same platform and the extent to which bidders participate in the marquee seller site. We explain these effects using theory in multihoming and involvement, in terms of how reference prices are set. Our work extends the platforms literature by considering the specific influence of a marquee seller on other sellers, thereby informing platform owners on the implications of the advice to add more marquee sellers to platforms. We also contribute to the literature on secondary markets for durable, used IT products, which are instrumental in reducing electronic waste and mitigating the environmental damage done by electronic products in landfills.History: Raghu Santanam, Senior Editor; Pallab Sanyal, Associate Editor.Supplemental Material: The online appendix is available at https://doi.org/10.1287/isre.2022.1137.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 263\n",
      "Title: Linking dimensions of social media use to job performance: The role of social capital\n",
      "Abstract: Organizations are increasingly adopting new technologies, such as social media, that afford employees a repertoire of uses not simply focused on work, but also on socialization and entertainment. Knowledge regarding the impact of such diverse technologies on job performance, however, is currently limited. This study adopts a technology use lens to study the effect of three categories of social media use – social, hedonic, and cognitive – on job performance, as mediated by three dimensions of social capital. The research was conducted via a large-scale survey within a multinational Information Technology company. Social and cognitive uses of technology were empirically shown to have a positive, albeit indirect, effect on employees’ routine and innovative job performance. Hedonic use of the technology, while having a direct negative impact on routine performance was shown to positively contribute to the development of social ties, leading to a mitigating positive influence on innovative performance. This interesting positive side of hedonic use, along with all findings from our study, are discussed and used to offer insights to future research and practice.\n",
      "Content: Linking dimensions of social media use to job performance: The role of social capital Organizations are increasingly adopting new technologies, such as social media, that afford employees a repertoire of uses not simply focused on work, but also on socialization and entertainment. Knowledge regarding the impact of such diverse technologies on job performance, however, is currently limited. This study adopts a technology use lens to study the effect of three categories of social media use – social, hedonic, and cognitive – on job performance, as mediated by three dimensions of social capital. The research was conducted via a large-scale survey within a multinational Information Technology company. Social and cognitive uses of technology were empirically shown to have a positive, albeit indirect, effect on employees’ routine and innovative job performance. Hedonic use of the technology, while having a direct negative impact on routine performance was shown to positively contribute to the development of social ties, leading to a mitigating positive influence on innovative performance. This interesting positive side of hedonic use, along with all findings from our study, are discussed and used to offer insights to future research and practice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 264\n",
      "Title: Intelligent Web proxy caching approaches based on machine learning techniques\n",
      "Abstract: In this paper, machine learning techniques are used to enhance the performances of conventional Web proxy caching policies such as Least-Recently-Used (LRU), Greedy-Dual-Size (GDS) and Greedy-Dual-Size-Frequency (GDSF). A support vector machine (SVM) and a decision tree (C4.5) are intelligently incorporated with conventional Web proxy caching techniques to form intelligent caching approaches known as SVM–LRU, SVM–GDSF and C4.5–GDS. The proposed intelligent approaches are evaluated by trace-driven simulation and compared with the most relevant Web proxy caching polices. Experimental results have revealed that the proposed SVM–LRU, SVM–GDSF and C4.5–GDS significantly improve the performances of LRU, GDSF and GDS respectively.\n",
      "Content: Intelligent Web proxy caching approaches based on machine learning techniques In this paper, machine learning techniques are used to enhance the performances of conventional Web proxy caching policies such as Least-Recently-Used (LRU), Greedy-Dual-Size (GDS) and Greedy-Dual-Size-Frequency (GDSF). A support vector machine (SVM) and a decision tree (C4.5) are intelligently incorporated with conventional Web proxy caching techniques to form intelligent caching approaches known as SVM–LRU, SVM–GDSF and C4.5–GDS. The proposed intelligent approaches are evaluated by trace-driven simulation and compared with the most relevant Web proxy caching polices. Experimental results have revealed that the proposed SVM–LRU, SVM–GDSF and C4.5–GDS significantly improve the performances of LRU, GDSF and GDS respectively.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 265\n",
      "Title: Post-Story: Influence of Introducing Story Feature on Social Media Posts\n",
      "Abstract: Driven by the need to enhance user traffic on social media (SM) platforms for increasing their advertising revenues, SM platforms are experimenting with new content creation features. However, it is unclear if such initiatives are also beneficial for SM profile owners such as influencers, who are the prime content creators on the SM platforms who use SM posts to build their influence within their network of followers. Our study investigates the effect of introducing one such new SM feature: the \"story\" on the creation and consumption of SM posts. Leveraging social penetration theory, we hypothesize the influence of introducing story feature on (1) the frequency of SM post creation by profile owners and (2) the extent of follower engagement with SM posts. Employing a quasi-experimental design, we find that the introduction of the story feature reduces the frequency of SM post creation, but the enhanced self-disclosure through the story feature increases follower engagement with the SM posts. However, these effects are moderated by the situating culture of the SM communities: while low-power-distance cultures value profile owners' self-disclosure, high-power-distance cultures exhibit a mixed influence. Advancing literature on social penetration theory and SM user engagement, our study demonstrates that new self-disclosive SM content creation features do not necessarily benefit all the concerned stakeholders and that the effectiveness of such features might vary from one community to another. Hence, the intended impact of introducing new SM features needs to be carefully evaluated by SM platforms in a holistic manner.\n",
      "Content: Post-Story: Influence of Introducing Story Feature on Social Media Posts Driven by the need to enhance user traffic on social media (SM) platforms for increasing their advertising revenues, SM platforms are experimenting with new content creation features. However, it is unclear if such initiatives are also beneficial for SM profile owners such as influencers, who are the prime content creators on the SM platforms who use SM posts to build their influence within their network of followers. Our study investigates the effect of introducing one such new SM feature: the \"story\" on the creation and consumption of SM posts. Leveraging social penetration theory, we hypothesize the influence of introducing story feature on (1) the frequency of SM post creation by profile owners and (2) the extent of follower engagement with SM posts. Employing a quasi-experimental design, we find that the introduction of the story feature reduces the frequency of SM post creation, but the enhanced self-disclosure through the story feature increases follower engagement with the SM posts. However, these effects are moderated by the situating culture of the SM communities: while low-power-distance cultures value profile owners' self-disclosure, high-power-distance cultures exhibit a mixed influence. Advancing literature on social penetration theory and SM user engagement, our study demonstrates that new self-disclosive SM content creation features do not necessarily benefit all the concerned stakeholders and that the effectiveness of such features might vary from one community to another. Hence, the intended impact of introducing new SM features needs to be carefully evaluated by SM platforms in a holistic manner.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 266\n",
      "Title: A collaborative filtering approach for recommending OLAP sessions\n",
      "Abstract: While OLAP has a key role in supporting effective exploration of multidimensional cubes, the huge number of aggregations and selections that can be operated on data may make the user experience disorientating. To address this issue, in the paper we propose a recommendation approach stemming from collaborative filtering. We claim that the whole sequence of queries belonging to an OLAP session is valuable because it gives the user a compound and synergic view of data; for this reason, our goal is not to recommend single OLAP queries but OLAP sessions. Like other collaborative approaches, ours features three phases: (i) search the log for sessions that bear some similarity with the one currently being issued by the user; (ii) extract the most relevant subsessions; and (iii) adapt the top-ranked subsession to the current user's session. However, it is the first that treats sessions as first-class citizens, using new techniques for comparing sessions, finding meaningful recommendation candidates, and adapting them to the current session. After describing our approach, we discuss the results of a large set of effectiveness and efficiency tests based on different measures of recommendation quality.\n",
      "Content: A collaborative filtering approach for recommending OLAP sessions While OLAP has a key role in supporting effective exploration of multidimensional cubes, the huge number of aggregations and selections that can be operated on data may make the user experience disorientating. To address this issue, in the paper we propose a recommendation approach stemming from collaborative filtering. We claim that the whole sequence of queries belonging to an OLAP session is valuable because it gives the user a compound and synergic view of data; for this reason, our goal is not to recommend single OLAP queries but OLAP sessions. Like other collaborative approaches, ours features three phases: (i) search the log for sessions that bear some similarity with the one currently being issued by the user; (ii) extract the most relevant subsessions; and (iii) adapt the top-ranked subsession to the current user's session. However, it is the first that treats sessions as first-class citizens, using new techniques for comparing sessions, finding meaningful recommendation candidates, and adapting them to the current session. After describing our approach, we discuss the results of a large set of effectiveness and efficiency tests based on different measures of recommendation quality.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 267\n",
      "Title: Identity matching and information acquisition: Estimation of optimal threshold parameters\n",
      "Abstract: With the growing volume of collected and stored data from customer interactions that have recently shifted towards online channels, an important challenge faced by today's businesses is appropriately dealing with data quality problems. A key step in the data cleaning process is the matching and merging of customer records to assess the identity of individuals. The practical importance of this research is exemplified by a large client firm that deals with private label credit cards. They needed to know whether there existed histories of new customers within the company, in order to decide on the appropriate parameters of possible card offerings. The company incurs substantial costs if they incorrectly “match” an incoming application with an existing customer (Type I error), and also if they falsely assume that there is no match (Type II error). While there is a good deal of generic identity matching software available, that will provide a “strength” score for each potential match, the question of how to use the scores for new applications is of great interest and is addressed in this work. The academic significance lies in the analysis of the score thresholds that are typically used in decision making. That is, upper and lower thresholds are set, where matches are accepted above the former, rejected below the latter, and more information is gathered between the two. We show, for the first time, that the optimal thresholds can be considered to be parameters of a matching distribution, and a number of estimators of these parameters are developed and analyzed. Then extensive computations show the effects of various factors on the convergence rates of the estimates.\n",
      "Content: Identity matching and information acquisition: Estimation of optimal threshold parameters With the growing volume of collected and stored data from customer interactions that have recently shifted towards online channels, an important challenge faced by today's businesses is appropriately dealing with data quality problems. A key step in the data cleaning process is the matching and merging of customer records to assess the identity of individuals. The practical importance of this research is exemplified by a large client firm that deals with private label credit cards. They needed to know whether there existed histories of new customers within the company, in order to decide on the appropriate parameters of possible card offerings. The company incurs substantial costs if they incorrectly “match” an incoming application with an existing customer (Type I error), and also if they falsely assume that there is no match (Type II error). While there is a good deal of generic identity matching software available, that will provide a “strength” score for each potential match, the question of how to use the scores for new applications is of great interest and is addressed in this work. The academic significance lies in the analysis of the score thresholds that are typically used in decision making. That is, upper and lower thresholds are set, where matches are accepted above the former, rejected below the latter, and more information is gathered between the two. We show, for the first time, that the optimal thresholds can be considered to be parameters of a matching distribution, and a number of estimators of these parameters are developed and analyzed. Then extensive computations show the effects of various factors on the convergence rates of the estimates.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 268\n",
      "Title: Is more always better? Investigating the task-technology fit theory in an online user context\n",
      "Abstract: We used Task-Technology Fit (TTF) theory to examine the drivers and consequences of successful task completion by a user in an online context. The theory suggests that the fit between characteristics of the task and those of the website predicts user performance and behavioral intentions. Our hypotheses were tested using the input of two large scale studies performed in twelve industries and involving 13,135 participants. Results, which were replicated in a proximate culture, lend support to the predictions of Task-Technology Fit theory. The site information quality and ease of use were the only technology factors that significantly drove the users to a successful completion of their information tasks, rather than the site's graphical attractiveness, interactivity, security and privacy factors. The findings further suggested that focusing on the enhancement of site characteristics that have low fit with the task is not effective as it resulted in slowing the successful completion of the online task.\n",
      "Content: Is more always better? Investigating the task-technology fit theory in an online user context We used Task-Technology Fit (TTF) theory to examine the drivers and consequences of successful task completion by a user in an online context. The theory suggests that the fit between characteristics of the task and those of the website predicts user performance and behavioral intentions. Our hypotheses were tested using the input of two large scale studies performed in twelve industries and involving 13,135 participants. Results, which were replicated in a proximate culture, lend support to the predictions of Task-Technology Fit theory. The site information quality and ease of use were the only technology factors that significantly drove the users to a successful completion of their information tasks, rather than the site's graphical attractiveness, interactivity, security and privacy factors. The findings further suggested that focusing on the enhancement of site characteristics that have low fit with the task is not effective as it resulted in slowing the successful completion of the online task.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 269\n",
      "Title: Some systems implications of EU Data Protection Directive\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 270\n",
      "Title: Information Architecture: In Search of Efficient Flexibility\n",
      "Abstract:  This article addresses how information systems architecture can be used to support organizations in the 1990s--organizations that face the dual challenge of \"speed and flexibility\" and \"low cost and efficiency. \"At the heart of this challenge is the basic notion that information systems have been anything but flexible in the past and .that, for many firms, information systems are more disablers of flexibility than enablers. The article discusses two architectural solutions to this problem: \"the high road and the low road, \"and the benefits and pitfalls of each. We conclude that neither solution will succeed on its own and that firms need to combine elements of both to meet the challenges of the 1990s. This article is based on some of the things we have learned through research, case writing, and consulting while working with a variety of organizations over the past three years. These experiences have illustrated the importance of and the struggle with IS architecture for today's global competitors. The content is intended to help guide, provoke, stimulate, and entertain others who believe that the integration of information technology with organizational strategy and structure is of paramount concern to senior managers. \n",
      "Content: Information Architecture: In Search of Efficient Flexibility  This article addresses how information systems architecture can be used to support organizations in the 1990s--organizations that face the dual challenge of \"speed and flexibility\" and \"low cost and efficiency. \"At the heart of this challenge is the basic notion that information systems have been anything but flexible in the past and .that, for many firms, information systems are more disablers of flexibility than enablers. The article discusses two architectural solutions to this problem: \"the high road and the low road, \"and the benefits and pitfalls of each. We conclude that neither solution will succeed on its own and that firms need to combine elements of both to meet the challenges of the 1990s. This article is based on some of the things we have learned through research, case writing, and consulting while working with a variety of organizations over the past three years. These experiences have illustrated the importance of and the struggle with IS architecture for today's global competitors. The content is intended to help guide, provoke, stimulate, and entertain others who believe that the integration of information technology with organizational strategy and structure is of paramount concern to senior managers. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 271\n",
      "Title: How Should Technology-Mediated Organizational Change Be Explained? A Comparison of the Contributions of Critical Realism and Activity Theory\n",
      "Abstract: In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within \"activity systems\" and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the \"inscription\" of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.\n",
      "Content: How Should Technology-Mediated Organizational Change Be Explained? A Comparison of the Contributions of Critical Realism and Activity Theory In this paper, critical realism and activity theory are compared within the context of theorizing technology-mediated organizational change. An activity theoretic analysis of the implementation of large-scale disruptive information systems in a public sector setting (in particular concerning paramedic treatment of heart attack patients and ambulance dispatch work activity) is used to illustrate how activity theory makes a significant contribution to critical realism, by (1) locating technology within \"activity systems\" and theorizing change through contradictions and congruencies within those systems; (2) developing recent critical realism-inspired theorization of the \"inscription\" of cultural and social relations within technology; and (3) developing recent insights of critical realist researchers regarding the way in which the performance management agenda is mediated through IS.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 272\n",
      "Title: Trust, power and interorganizational information systems: the case of the electronic trading community TransLease\n",
      "Abstract: Abstract. This paper focuses on Cap Gemini’s electronic commerce system, TransLease. TransLease is an interorganizational information system (IOS), which facilitates electronic commerce between motor vehicle leasing and repair companies. During our investigation, the system was used by approximately 1000 repair agents working for seven of the UK’s leading vehicle leasing and contract hire companies. This system was originally developed by AT&T and acquired by Cap Gemini in July 1998. At the time of acquisition, the system was seen as being of high strategic value, although it was also seen as underperforming. This paper reports the results of an action research project, which formed one element of the process by which Cap Gemini investigated the former problem. In the paper, TransLease is described as a complex electronic community, dependent upon the existence of symbiotic relationships. As such, the problems that the system users and developers experienced can be attributed to factors that impeded the mutual benefit accruing from participation in the system. The efficacy of the terms of exchange and the degree to which participants mutually benefit through electronic interaction is determined by the complex interplay of a number of relational and organizational factors. The research therefore illustrates the importance of the ‘soft’ organizational issues in IOS management and development, and suggests a conceptual model of the factors relevant in this case. At the time of this study, TransLease was still in the early stages of its life cycle, having only been available in the marketplace for approximately 18 months. During this time, through recognizing the complex problems and issues detailed in this paper, Cap Gemini accordingly redressed the way in which the system was managed and maintained. TransLease is now seen as having matured into a highly successful example of an IOS – a view reflected by its position as market...\n",
      "Content: Trust, power and interorganizational information systems: the case of the electronic trading community TransLease Abstract. This paper focuses on Cap Gemini’s electronic commerce system, TransLease. TransLease is an interorganizational information system (IOS), which facilitates electronic commerce between motor vehicle leasing and repair companies. During our investigation, the system was used by approximately 1000 repair agents working for seven of the UK’s leading vehicle leasing and contract hire companies. This system was originally developed by AT&T and acquired by Cap Gemini in July 1998. At the time of acquisition, the system was seen as being of high strategic value, although it was also seen as underperforming. This paper reports the results of an action research project, which formed one element of the process by which Cap Gemini investigated the former problem. In the paper, TransLease is described as a complex electronic community, dependent upon the existence of symbiotic relationships. As such, the problems that the system users and developers experienced can be attributed to factors that impeded the mutual benefit accruing from participation in the system. The efficacy of the terms of exchange and the degree to which participants mutually benefit through electronic interaction is determined by the complex interplay of a number of relational and organizational factors. The research therefore illustrates the importance of the ‘soft’ organizational issues in IOS management and development, and suggests a conceptual model of the factors relevant in this case. At the time of this study, TransLease was still in the early stages of its life cycle, having only been available in the marketplace for approximately 18 months. During this time, through recognizing the complex problems and issues detailed in this paper, Cap Gemini accordingly redressed the way in which the system was managed and maintained. TransLease is now seen as having matured into a highly successful example of an IOS – a view reflected by its position as market...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 273\n",
      "Title: Information sharing and interoperability: the case of major incident management\n",
      "Abstract: Public sector inter-organisational information sharing and interoperability is an area of increasing concern and intense investment for practice and an area of increasing scholarship. This paper focuses on one particular set of public sector organisations (emergency services) and illuminates the key technological and organisational issues they face concerning information sharing and interoperability. The particular contexts in which these are studied are ones where decisions are non-trivial and made in high-velocity environments. In these conditions the problems and significance of inter-organisational information sharing and interoperability are accentuated. We analyse data gathered from two studies: the first focused on ‘first responders’ (police, fire and ambulance services) in the United Kingdom. The second, a follow on study, with emergency service managers and interoperability project managers in the United Kingdom and the European Union. Using activity theory as a conceptual framework we describe the informational problems critical emergency responders face in their initial response to, and management of, an incident. We argue that rather than focusing on interoperability as a primarily technological issue it should be managed as an organisational and informational issue. Second, we argue that rather than designing for anomalous situations we should design systems, which will function during both anomalous and routine situations. Third, we argue for focus on harmonisation of policies, procedures and working practices.\n",
      "Content: Information sharing and interoperability: the case of major incident management Public sector inter-organisational information sharing and interoperability is an area of increasing concern and intense investment for practice and an area of increasing scholarship. This paper focuses on one particular set of public sector organisations (emergency services) and illuminates the key technological and organisational issues they face concerning information sharing and interoperability. The particular contexts in which these are studied are ones where decisions are non-trivial and made in high-velocity environments. In these conditions the problems and significance of inter-organisational information sharing and interoperability are accentuated. We analyse data gathered from two studies: the first focused on ‘first responders’ (police, fire and ambulance services) in the United Kingdom. The second, a follow on study, with emergency service managers and interoperability project managers in the United Kingdom and the European Union. Using activity theory as a conceptual framework we describe the informational problems critical emergency responders face in their initial response to, and management of, an incident. We argue that rather than focusing on interoperability as a primarily technological issue it should be managed as an organisational and informational issue. Second, we argue that rather than designing for anomalous situations we should design systems, which will function during both anomalous and routine situations. Third, we argue for focus on harmonisation of policies, procedures and working practices.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 274\n",
      "Title: Culture, power and politics in ICT outsourcing in higher education institutions\n",
      "Abstract: UK public sector policy changes have driven Higher Education Institutions (HEI) towards a competitive and often turbulent market-focused environment. To respond to these dramatic institutional changes, many institutions began to strategically re-focus their management efforts on adapting and surviving in this environment. As part of their efforts, HEIs identified their Information and Communication Technology (ICT) function as an essential function to survive in this environment. This implied that HEIs had to address the institutional role of ICT by defining detailed strategies that aligned the ICT function to the HEIs educational goals. On the other hand, HEIs had to make sure their ICT could support their technology and service requirements, for which they considered pursing a more radical approach, that of outsourcing their ICT to a third party supplier. This research paper reports on embryonic attempts by three British Universities to outsource their ICT, highlighting in particular the ‘cultural, power and political’ issues that arose when public sector institutions follow the example of private sector organisations—by outsourcing to a third party service supplier.\n",
      "Content: Culture, power and politics in ICT outsourcing in higher education institutions UK public sector policy changes have driven Higher Education Institutions (HEI) towards a competitive and often turbulent market-focused environment. To respond to these dramatic institutional changes, many institutions began to strategically re-focus their management efforts on adapting and surviving in this environment. As part of their efforts, HEIs identified their Information and Communication Technology (ICT) function as an essential function to survive in this environment. This implied that HEIs had to address the institutional role of ICT by defining detailed strategies that aligned the ICT function to the HEIs educational goals. On the other hand, HEIs had to make sure their ICT could support their technology and service requirements, for which they considered pursing a more radical approach, that of outsourcing their ICT to a third party supplier. This research paper reports on embryonic attempts by three British Universities to outsource their ICT, highlighting in particular the ‘cultural, power and political’ issues that arose when public sector institutions follow the example of private sector organisations—by outsourcing to a third party service supplier.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 275\n",
      "Title: Information Systems Research Behaviors: What Are the Normative Standards?\n",
      "Abstract: Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers' careers influence their judgments of these behaviors. Membership in the Association for Information Systems (AIS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers' judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the AIS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.\n",
      "Content: Information Systems Research Behaviors: What Are the Normative Standards? Information systems researchers frequently face quandaries in their professional lives. We present the results of a study of academic IS researchers that assesses their judgments and the prevalence of 29 questionable research-related behaviors. We find that the focus and stages of researchers' careers influence their judgments of these behaviors. Membership in the Association for Information Systems (AIS) and adherence to the AIS Code of Research Conduct are also associated with IS researchers' judgments. There is strong evidence to suggest that IS researchers expect to engage in questionable behaviors more in the future than they report having done in the past. As a result of the study, we recommend that the IS community revisit the AIS Code of Research Conduct on a regular basis and take active steps to both educate its members on professional normative standards and to uphold the standards of our community.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 276\n",
      "Title: Academic Data Collection in Electronic Environments: Defining Acceptable Use of Internet Resources\n",
      "Abstract: Academic researchers access commercial web sites to collect research data. This research practice is likely to increase. Is this appropriate? Is this legal? Such commercial web sites are maintained to achieve business objectives; research access uses site resources for other purposes. Web site administrators may, therefore, deem academic data collection inappropriate. Is there a process to make research access more open and acceptable to web site owners and administrators? These are significant issues. This article clarifies the problems and suggests possible approaches to handle the issues with sensitivity and openness. Research access to commercial web sites may be manual (using a standard web browser) or automated (using automated data collection agents). These approaches have different effects on web sites. Researchers using manual access tend to make a limited number of page requests because manual access is costly to perform. Researchers using automated access methods can request large numbers of pages at a low cost. Therefore, web site administrators tend to view manual access and automated access very differently. Because of the number of accesses and the nonbusiness purpose, automated research requests for data are sometimes blocked by site administration using a variety of means (both technological and legal). This paper details the pertinent legal issues including trespass, copyright violation, and breech of contract. It also explains the nature of express and implied consent by site administration for research access. Based on the issues presented, guidelines for researchers are proposed to reduce objections to research activities, to facilitate communication with web site administration, and to achieve express or implied consent. These include notification to web site administration of intended automated research activity, description of the research project posted as a web page, and clear identification of automated requests for web pages. In order to encourage good research practices with respect to automated data collection, suggestions are made with respect to disclosing methods used in research papers and for self regulation by academic associations\n",
      "Content: Academic Data Collection in Electronic Environments: Defining Acceptable Use of Internet Resources Academic researchers access commercial web sites to collect research data. This research practice is likely to increase. Is this appropriate? Is this legal? Such commercial web sites are maintained to achieve business objectives; research access uses site resources for other purposes. Web site administrators may, therefore, deem academic data collection inappropriate. Is there a process to make research access more open and acceptable to web site owners and administrators? These are significant issues. This article clarifies the problems and suggests possible approaches to handle the issues with sensitivity and openness. Research access to commercial web sites may be manual (using a standard web browser) or automated (using automated data collection agents). These approaches have different effects on web sites. Researchers using manual access tend to make a limited number of page requests because manual access is costly to perform. Researchers using automated access methods can request large numbers of pages at a low cost. Therefore, web site administrators tend to view manual access and automated access very differently. Because of the number of accesses and the nonbusiness purpose, automated research requests for data are sometimes blocked by site administration using a variety of means (both technological and legal). This paper details the pertinent legal issues including trespass, copyright violation, and breech of contract. It also explains the nature of express and implied consent by site administration for research access. Based on the issues presented, guidelines for researchers are proposed to reduce objections to research activities, to facilitate communication with web site administration, and to achieve express or implied consent. These include notification to web site administration of intended automated research activity, description of the research project posted as a web page, and clear identification of automated requests for web pages. In order to encourage good research practices with respect to automated data collection, suggestions are made with respect to disclosing methods used in research papers and for self regulation by academic associations\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 277\n",
      "Title: The Effects of State-Based and Event-Based Data Representation on User Performance in Query Formulation Tasks\n",
      "Abstract: Ad hoc query formulation is an important task in effectively utilizing organizational data resources. To facilitate this task, managers and casual end-users are commonly presented with database views expressly constructed for their use. Differences in the way in which things, states, and events are represented in such views can affect a user's ability to understand the database, potentially leading to different levels of performance (i.e., accuracy, confidence, and prediction of the accuracy of their queries). An experiment was conducted over the Internet involving 342 subjects from 6 universities in North America and Europe to investigate these effects. When presented with an event-based view, subjects expressing low or very low comfort levels in reading entity-relationship diagrams expressed confidence that better predicted query accuracy although there were no significant differences in actual query accuracy or level of confidence expressed.\n",
      "Content: The Effects of State-Based and Event-Based Data Representation on User Performance in Query Formulation Tasks Ad hoc query formulation is an important task in effectively utilizing organizational data resources. To facilitate this task, managers and casual end-users are commonly presented with database views expressly constructed for their use. Differences in the way in which things, states, and events are represented in such views can affect a user's ability to understand the database, potentially leading to different levels of performance (i.e., accuracy, confidence, and prediction of the accuracy of their queries). An experiment was conducted over the Internet involving 342 subjects from 6 universities in North America and Europe to investigate these effects. When presented with an event-based view, subjects expressing low or very low comfort levels in reading entity-relationship diagrams expressed confidence that better predicted query accuracy although there were no significant differences in actual query accuracy or level of confidence expressed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 278\n",
      "Title: A Research Note on Representing Part-Whole Relations in Conceptual Modeling\n",
      "Abstract: Empirical research is an important methodology for the study of conceptual modeling practices. The recently published article \"Representing Part-Whole Relations in Conceptual Modeling: An Empirical Evaluation\" (Shanks et al. 2008) uses the lens of ontology to study a relatively sophisticated aspect of conceptual modeling practice, the representation of aggregation and composition. It contends that some analysts argue that a composite should be represented as a relationship while others argue that a composite should be represented as an entity. We find no evidence of such a dispute in the data modeling literature. We observe that composites are objects. By definition, all object-types should be represented as entities. Therefore, using the relationship construct to represent composites should not be seen as a viable alternative. Additionally, we found significant conceptual and methodological issues within the study that call its conclusions into question. As a way to offer insight into the requisite methodological procedures for research in this area, we conducted two experiments that both explicate and address the issues raised. Our results call into question the utility of using ontology as a foundation for conceptual modeling practice. Furthermore, they suggest a contrary but at least equally plausible explanation for the results reported by Shanks et al. In conducting this work we hope to encourage dialogue that will be beneficial for future endeavors aimed at identifying, developing, and evaluating appropriate foundations for the discipline of conceptual modeling.\n",
      "Content: A Research Note on Representing Part-Whole Relations in Conceptual Modeling Empirical research is an important methodology for the study of conceptual modeling practices. The recently published article \"Representing Part-Whole Relations in Conceptual Modeling: An Empirical Evaluation\" (Shanks et al. 2008) uses the lens of ontology to study a relatively sophisticated aspect of conceptual modeling practice, the representation of aggregation and composition. It contends that some analysts argue that a composite should be represented as a relationship while others argue that a composite should be represented as an entity. We find no evidence of such a dispute in the data modeling literature. We observe that composites are objects. By definition, all object-types should be represented as entities. Therefore, using the relationship construct to represent composites should not be seen as a viable alternative. Additionally, we found significant conceptual and methodological issues within the study that call its conclusions into question. As a way to offer insight into the requisite methodological procedures for research in this area, we conducted two experiments that both explicate and address the issues raised. Our results call into question the utility of using ontology as a foundation for conceptual modeling practice. Furthermore, they suggest a contrary but at least equally plausible explanation for the results reported by Shanks et al. In conducting this work we hope to encourage dialogue that will be beneficial for future endeavors aimed at identifying, developing, and evaluating appropriate foundations for the discipline of conceptual modeling.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 279\n",
      "Title: Is Query Reuse Potentially Harmful? Anchoring and Adjustment in Adapting Existing Database Queries\n",
      "Abstract: Reusing database queries by adapting them to satisfy new information requests is an attractive strategy for extracting information from databases without involving database specialists. However, the reuse of information systems artifacts has been shown to be susceptible to the phenomenon of anchoring and adjustment. Anchoring often leads to a systematic adjustment bias in which people fail to make sufficient changes to an anchor in response to the needs of a new task. In a study involving 157 novice query writers from six universities, we examined the effect of this phenomenon on the reuse of Structured Query Language (SQL) queries under varying levels of domain familiarity and for different types of anchors. Participants developed SQL queries to respond to four information requests in a familiar domain and four information requests in an unfamiliar domain. For two information requests in each domain, participants were also provided with sample queries (anchors) that answered similar information requests. We found evidence that the opportunity to reuse sample queries resulted in an adjustment bias leading to poorer quality query results and greater overconfidence in the correctness of results. The results also indicate that the strength of the adjustment bias depends on a combination of domain familiarity and type of anchor. This study demonstrates that anchoring and adjustment during query reuse can lead to queries that are less accurate than those written from scratch. We also extend the concept of anchoring and adjustment by distinguishing between surface-structure and deep-structure anchors and by considering the impact of domain familiarity on the adjustment bias.\n",
      "Content: Is Query Reuse Potentially Harmful? Anchoring and Adjustment in Adapting Existing Database Queries Reusing database queries by adapting them to satisfy new information requests is an attractive strategy for extracting information from databases without involving database specialists. However, the reuse of information systems artifacts has been shown to be susceptible to the phenomenon of anchoring and adjustment. Anchoring often leads to a systematic adjustment bias in which people fail to make sufficient changes to an anchor in response to the needs of a new task. In a study involving 157 novice query writers from six universities, we examined the effect of this phenomenon on the reuse of Structured Query Language (SQL) queries under varying levels of domain familiarity and for different types of anchors. Participants developed SQL queries to respond to four information requests in a familiar domain and four information requests in an unfamiliar domain. For two information requests in each domain, participants were also provided with sample queries (anchors) that answered similar information requests. We found evidence that the opportunity to reuse sample queries resulted in an adjustment bias leading to poorer quality query results and greater overconfidence in the correctness of results. The results also indicate that the strength of the adjustment bias depends on a combination of domain familiarity and type of anchor. This study demonstrates that anchoring and adjustment during query reuse can lead to queries that are less accurate than those written from scratch. We also extend the concept of anchoring and adjustment by distinguishing between surface-structure and deep-structure anchors and by considering the impact of domain familiarity on the adjustment bias.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 280\n",
      "Title: How well do shopbots represent online markets? A study of shopbots’ vendor coverage strategy\n",
      "Abstract: Consumers often use shopbots to search for information when making purchase decisions in Internet markets. Although they have varying sensitivity to shopbot bias, consumers generally prefer accurate market representation. However, in choosing the accuracy of market representation, shopbots must balance the desires of consumers with the costs of providing their services and with the desires of the vendors, who are often the largest source of their revenue. In this paper, we study how accurately shopbots represent a market and analyze the strategies shopbots adopt to achieve market representativeness. We theoretically identify two important drivers in shopbot vendor coverage strategy – how many vendors it covers (shopbot size) and which vendors it covers (shopbot affiliation) – and analytically show how the drivers affect shopbot market representativeness. We report the results of a large-scale study in which we collected 2.2 million vendor price listings from eight shopbots and develop metrics for measuring shopbot size, shopbot affiliation, and shopbot market representativeness. We found that (1) shopbots do not represent markets equally well; (2) size drives a shopbot's market representativeness positively whereas affiliation drives a shopbot's market representativeness negatively; (3) shopbots follow differnet vendor representative strategies to pursue market representativeness.\n",
      "Content: How well do shopbots represent online markets? A study of shopbots’ vendor coverage strategy Consumers often use shopbots to search for information when making purchase decisions in Internet markets. Although they have varying sensitivity to shopbot bias, consumers generally prefer accurate market representation. However, in choosing the accuracy of market representation, shopbots must balance the desires of consumers with the costs of providing their services and with the desires of the vendors, who are often the largest source of their revenue. In this paper, we study how accurately shopbots represent a market and analyze the strategies shopbots adopt to achieve market representativeness. We theoretically identify two important drivers in shopbot vendor coverage strategy – how many vendors it covers (shopbot size) and which vendors it covers (shopbot affiliation) – and analytically show how the drivers affect shopbot market representativeness. We report the results of a large-scale study in which we collected 2.2 million vendor price listings from eight shopbots and develop metrics for measuring shopbot size, shopbot affiliation, and shopbot market representativeness. We found that (1) shopbots do not represent markets equally well; (2) size drives a shopbot's market representativeness positively whereas affiliation drives a shopbot's market representativeness negatively; (3) shopbots follow differnet vendor representative strategies to pursue market representativeness.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 281\n",
      "Title: IT and the video game industry: tensions and mutual shaping\n",
      "Abstract: This paper examines the influence of information technology (IT) on a distinct but closely related industry, the video game industry. We conceptualize the effects of IT as a process of translating three related dimensions of a technological frame – technology performance, industry practices, and use vision – from one industry to another. Through historical examples, we argue that the impact of IT on the video game industry is shaped and limited by this translation process, particularly when tensions between the two industries lead to the development of new complementary or replacement technologies, practices, or visions. Although heavily dependent on IT, the video game industry has had to ignore, postpone, or substantially modify important IT software tools, processors, storage media, graphics, and networking technologies because of these industry contradictions.\n",
      "Content: IT and the video game industry: tensions and mutual shaping This paper examines the influence of information technology (IT) on a distinct but closely related industry, the video game industry. We conceptualize the effects of IT as a process of translating three related dimensions of a technological frame – technology performance, industry practices, and use vision – from one industry to another. Through historical examples, we argue that the impact of IT on the video game industry is shaped and limited by this translation process, particularly when tensions between the two industries lead to the development of new complementary or replacement technologies, practices, or visions. Although heavily dependent on IT, the video game industry has had to ignore, postpone, or substantially modify important IT software tools, processors, storage media, graphics, and networking technologies because of these industry contradictions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 282\n",
      "Title: Factors impacting the perceived organizational support of IT employees\n",
      "Abstract: Organizations today face shortages of IT personnel. We investigated workplace factors in one state government in hope of identifying factors that influence perceived organizational support (POS) within an IT work environment. A combination of job characteristics (challenging job and perceived workload), job stressors (work exhaustion, role conflict, and role ambiguity), and the organization’s discretionary actions (pay-for-performance and mentoring opportunities) were measured and hierarchical regression was used to determine the relationships. Four control variables were also included (age, gender, organizational tenure, and professional versus administrator status). Role ambiguity, role conflict, work exhaustion, career mentoring, and pay-for-performance together explained 62% of the variance in the IT employees’ POS. Career mentoring and role ambiguity explained most of the variance.\n",
      "Content: Factors impacting the perceived organizational support of IT employees Organizations today face shortages of IT personnel. We investigated workplace factors in one state government in hope of identifying factors that influence perceived organizational support (POS) within an IT work environment. A combination of job characteristics (challenging job and perceived workload), job stressors (work exhaustion, role conflict, and role ambiguity), and the organization’s discretionary actions (pay-for-performance and mentoring opportunities) were measured and hierarchical regression was used to determine the relationships. Four control variables were also included (age, gender, organizational tenure, and professional versus administrator status). Role ambiguity, role conflict, work exhaustion, career mentoring, and pay-for-performance together explained 62% of the variance in the IT employees’ POS. Career mentoring and role ambiguity explained most of the variance.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 283\n",
      "Title: A co-evolutionary complex systems perspective on information systems\n",
      "Abstract: The co-evolution of information systems (IS) and the processes that underpin the construction and development of IT systems are explained from a complex systems perspective. Evolution operates at the microscopic level; in organizations, this is the individual or agent. Each agent has an idiosyncratic view of the organization, using to some extent personal constructs in dealing with the reality of organizational life. These objects or constructs can be described and measured by most agents; they are well defined. Many of these objects are represented in electronic, IT systems. Each agent also has their own view as to how they know what they know, that is, their epistemology, which we argue is their IS, and is wider than the IT systems they use. The IS of each agent co-evolves, by interaction with other agents, based on the agent's view of reality. The interaction of all agents constitutes the organization. Even more importantly, different values and interests motivate each agent. This is their axiology and it is what motivates them to learn and to develop their IS. An agent-based axiological framework is essential to understanding the evolution of organizations. It is the interaction of agents that builds consensus as to the shared reality of the organization, and this affects each agent's ability and motivation to evolve IS further. In addition, we propose that it is time that IT systems included modelling capabilities, based on multi-agent representations of the organization and its context, to explore and support strategic thinking and decision making.\n",
      "Content: A co-evolutionary complex systems perspective on information systems The co-evolution of information systems (IS) and the processes that underpin the construction and development of IT systems are explained from a complex systems perspective. Evolution operates at the microscopic level; in organizations, this is the individual or agent. Each agent has an idiosyncratic view of the organization, using to some extent personal constructs in dealing with the reality of organizational life. These objects or constructs can be described and measured by most agents; they are well defined. Many of these objects are represented in electronic, IT systems. Each agent also has their own view as to how they know what they know, that is, their epistemology, which we argue is their IS, and is wider than the IT systems they use. The IS of each agent co-evolves, by interaction with other agents, based on the agent's view of reality. The interaction of all agents constitutes the organization. Even more importantly, different values and interests motivate each agent. This is their axiology and it is what motivates them to learn and to develop their IS. An agent-based axiological framework is essential to understanding the evolution of organizations. It is the interaction of agents that builds consensus as to the shared reality of the organization, and this affects each agent's ability and motivation to evolve IS further. In addition, we propose that it is time that IT systems included modelling capabilities, based on multi-agent representations of the organization and its context, to explore and support strategic thinking and decision making.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 284\n",
      "Title: User Managers' Systems Needs\n",
      "Abstract:  A bstractBased on responses from over five hundred user managers, this article investigates managers' demand for new application systems. To begin, the current situation is assessed from two aspects. First, how many systems by type do user managers now have and how appropriate are the systems. Second, for important managerial tasks, what support (by systems type) users have and how appropriate are those systems. Then the two components of user managers' demand for new systems, the number of systems and the types of systems, are examined. The results reveal an overwhelming level of managerial demand for new systems and major shifts in demand mix by systems type. The implications of this current and future demand for IS management are presented. \n",
      "Content: User Managers' Systems Needs  A bstractBased on responses from over five hundred user managers, this article investigates managers' demand for new application systems. To begin, the current situation is assessed from two aspects. First, how many systems by type do user managers now have and how appropriate are the systems. Second, for important managerial tasks, what support (by systems type) users have and how appropriate are those systems. Then the two components of user managers' demand for new systems, the number of systems and the types of systems, are examined. The results reveal an overwhelming level of managerial demand for new systems and major shifts in demand mix by systems type. The implications of this current and future demand for IS management are presented. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 285\n",
      "Title: A research note regarding the development of the consensus on appropriation scale\n",
      "Abstract: Measurement is perhaps the most difficult aspect of behavioral research. In a recent edition of ISR, a scale for consensus on appropriation was developed. Consensus on appropriation is one of [...]\n",
      "Content: A research note regarding the development of the consensus on appropriation scale Measurement is perhaps the most difficult aspect of behavioral research. In a recent edition of ISR, a scale for consensus on appropriation was developed. Consensus on appropriation is one of [...]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 286\n",
      "Title: Tun-OCM: A model-driven approach to support database tuning decision making\n",
      "Abstract: Database tuning is a task executed by Database Administrators (DBAs) based on their practical experience and on tuning systems, which support DBA actions towards improving the performance of a database system. It is notoriously a complex task that requires precise domain knowledge about possible database configurations. Ideally, a DBA should keep track of several Database Management Systems (DBMS) parameters, configure data structures, and must be aware about possible interferences among several database (DB) configurations. We claim that an automatic tuning system is a decision support system and DB tuning may also be seen as a configuration management task. Therefore, we may characterize it by means of a formal domain conceptuali­ zation, benefiting from existing control practices and computational support in the configuration management domain. This work presents Tun-OCM, a conceptual model represented as a well-founded ontology, that en­ compasses a novel characterization of the database tuning domain as a configuration management conceptu­ alization to support decision making. We develop and represent Tun-OCM using the CM-OPL methodology and its underlying language. The benefits of Tun-OCM are discussed by instantiating it in a real scenario.\n",
      "Content: Tun-OCM: A model-driven approach to support database tuning decision making Database tuning is a task executed by Database Administrators (DBAs) based on their practical experience and on tuning systems, which support DBA actions towards improving the performance of a database system. It is notoriously a complex task that requires precise domain knowledge about possible database configurations. Ideally, a DBA should keep track of several Database Management Systems (DBMS) parameters, configure data structures, and must be aware about possible interferences among several database (DB) configurations. We claim that an automatic tuning system is a decision support system and DB tuning may also be seen as a configuration management task. Therefore, we may characterize it by means of a formal domain conceptuali­ zation, benefiting from existing control practices and computational support in the configuration management domain. This work presents Tun-OCM, a conceptual model represented as a well-founded ontology, that en­ compasses a novel characterization of the database tuning domain as a configuration management conceptu­ alization to support decision making. We develop and represent Tun-OCM using the CM-OPL methodology and its underlying language. The benefits of Tun-OCM are discussed by instantiating it in a real scenario.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 287\n",
      "Title: Multivariate data quality assessment based on rotated factor scores and confidence ellipsoids\n",
      "Abstract: This study explores the nature of the correlation in data to estimate the data quality to be used in decision-making processes. The main contribution of this research is the introduction of a new multivariate method based on rotated factor scores by varimax strategy for the repeatability and reproducibility study to effectively identify possible data of poor quality leading to measurement errors. In addition, a new confidence ellipsoid-based decision support method is developed. The efficiency of the proposed method was demonstrated using the metallographic measurements of the geometric characteristics of the resistance spot welding process. To prove the efficiency of the proposed method, it was compared with other consolidated techniques such as the analysis of variance, weighted principal components method, and factor analysis without rotation. Thus, we verified that the proposed method performed better interpretation of the latent information, minimizing the dimensionality of the data, and separating the quality attributes analyzed by clusters. One response group was classified as acceptable, and the other as marginal. These results were verified by the confidence ellipsoids, in which the proposed method obeyed the Bonferroni bilateral limits, outlining the factors which demonstrated superior discriminatory power with non-overlapping ellipsoids avoiding the confounding and favoring the better data quality analysis for multicriteria decision-making. When compared with the other approaches, the proposed method demonstrated more reliable and robust results without such deficiencies as inversion of the groupings, neglection of the variance-covariance structure, and the variability attributed to the data within the measurement system.\n",
      "Content: Multivariate data quality assessment based on rotated factor scores and confidence ellipsoids This study explores the nature of the correlation in data to estimate the data quality to be used in decision-making processes. The main contribution of this research is the introduction of a new multivariate method based on rotated factor scores by varimax strategy for the repeatability and reproducibility study to effectively identify possible data of poor quality leading to measurement errors. In addition, a new confidence ellipsoid-based decision support method is developed. The efficiency of the proposed method was demonstrated using the metallographic measurements of the geometric characteristics of the resistance spot welding process. To prove the efficiency of the proposed method, it was compared with other consolidated techniques such as the analysis of variance, weighted principal components method, and factor analysis without rotation. Thus, we verified that the proposed method performed better interpretation of the latent information, minimizing the dimensionality of the data, and separating the quality attributes analyzed by clusters. One response group was classified as acceptable, and the other as marginal. These results were verified by the confidence ellipsoids, in which the proposed method obeyed the Bonferroni bilateral limits, outlining the factors which demonstrated superior discriminatory power with non-overlapping ellipsoids avoiding the confounding and favoring the better data quality analysis for multicriteria decision-making. When compared with the other approaches, the proposed method demonstrated more reliable and robust results without such deficiencies as inversion of the groupings, neglection of the variance-covariance structure, and the variability attributed to the data within the measurement system.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 288\n",
      "Title: Situated with Infrastructures: Interactivity and Entanglement in Sensor Data Interpretation\n",
      "Abstract: This paper elaborates on situatedness as an empirical phenomenon in computer-mediated settings. It is based on studies of petroleum engineers and how they work with digital sensor data. We show how their work practices are born out of a history of constitutive entanglement with specific types of sensors, the data they produce, and the information systems that process them. This entanglement arises from interaction between humans, technology, and the oil reservoir and is a fundamental aspect of the situations in which interpretative work occurs. We empirically show how different sensors in the petroleum production systems produce data in interaction with their surroundings, and that these data are creatively \"stretched\" to represent subsurface phenomena. When groups of engineers collaborate remotely with colleagues to make sense of problematic data, entanglement with specific II's is an important aspect of situatedness. The situationally particular in these settings is not as much a matter of locations as of histories of interaction with specific technologies. The notion of situatedness has been pivotal in stressing the importance of the particular circumstances in which work is performed. It has throughout its history been a counterweight to rationalistic accounts of work and the focus on design of standardized work processes. Here we show that patterns of interaction with specific information infrastructures make up a crucial part of situated work and that these may have non-local dimensions.\n",
      "Content: Situated with Infrastructures: Interactivity and Entanglement in Sensor Data Interpretation This paper elaborates on situatedness as an empirical phenomenon in computer-mediated settings. It is based on studies of petroleum engineers and how they work with digital sensor data. We show how their work practices are born out of a history of constitutive entanglement with specific types of sensors, the data they produce, and the information systems that process them. This entanglement arises from interaction between humans, technology, and the oil reservoir and is a fundamental aspect of the situations in which interpretative work occurs. We empirically show how different sensors in the petroleum production systems produce data in interaction with their surroundings, and that these data are creatively \"stretched\" to represent subsurface phenomena. When groups of engineers collaborate remotely with colleagues to make sense of problematic data, entanglement with specific II's is an important aspect of situatedness. The situationally particular in these settings is not as much a matter of locations as of histories of interaction with specific technologies. The notion of situatedness has been pivotal in stressing the importance of the particular circumstances in which work is performed. It has throughout its history been a counterweight to rationalistic accounts of work and the focus on design of standardized work processes. Here we show that patterns of interaction with specific information infrastructures make up a crucial part of situated work and that these may have non-local dimensions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 289\n",
      "Title: Team Size, Dispersion, and Social Loafing in Technology-Supported Teams: A Perspective on the Theory of Moral Disengagement\n",
      "Abstract: The article reports the results of a study which investigated social loafing in a team setting. The study involved 32 teams of students assigned to brainstorming tasks using group systems software. Cognitive mechanisms derived from moral disengagement theory were tested as possible drivers of social loafing behavior. These included attribution of blame, diffusion of responsibility, and dehumanization. All of these correlated with the effect of team size on social loafing, but only dehumanization was found to mediate the effect of dispersion.\n",
      "Content: Team Size, Dispersion, and Social Loafing in Technology-Supported Teams: A Perspective on the Theory of Moral Disengagement The article reports the results of a study which investigated social loafing in a team setting. The study involved 32 teams of students assigned to brainstorming tasks using group systems software. Cognitive mechanisms derived from moral disengagement theory were tested as possible drivers of social loafing behavior. These included attribution of blame, diffusion of responsibility, and dehumanization. All of these correlated with the effect of team size on social loafing, but only dehumanization was found to mediate the effect of dispersion.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 290\n",
      "Title: Risk management in ERP project introduction: Review of the literature\n",
      "Abstract: In recent years ERP systems have received much attention. However, ERP projects have often been found to be complex and risky to implement in business enterprises. The organizational relevance and risk of ERP projects make it important for organizations to focus on ways to make ERP implementation successful. We collected and analyzed a number of key articles discussing and analyzing ERP implementation. The different approaches taken in the literature were compared from a risk management point of view to highlight the key risk factors and their impact on project success. Literature was further classified in order to address and analyze each risk factor and its relevance during the stages of the ERP project life cycle.\n",
      "Content: Risk management in ERP project introduction: Review of the literature In recent years ERP systems have received much attention. However, ERP projects have often been found to be complex and risky to implement in business enterprises. The organizational relevance and risk of ERP projects make it important for organizations to focus on ways to make ERP implementation successful. We collected and analyzed a number of key articles discussing and analyzing ERP implementation. The different approaches taken in the literature were compared from a risk management point of view to highlight the key risk factors and their impact on project success. Literature was further classified in order to address and analyze each risk factor and its relevance during the stages of the ERP project life cycle.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 291\n",
      "Title: A comparative axiomatic characterization of the Banzhaf–Owen coalitional value\n",
      "Abstract: A compact axiomatic characterization of the modified Banzhaf value for games with a coalition structure (Banzhaf–Owen value, for short) is provided. The axiomatic system used here can be compared with parallel axiomatizations of other coalitional values such as the Owen value or the Alonso–Fiestras value, thus giving arguments to defend the use of one of them that will depend on the context where they are to be applied.\n",
      "Content: A comparative axiomatic characterization of the Banzhaf–Owen coalitional value A compact axiomatic characterization of the modified Banzhaf value for games with a coalition structure (Banzhaf–Owen value, for short) is provided. The axiomatic system used here can be compared with parallel axiomatizations of other coalitional values such as the Owen value or the Alonso–Fiestras value, thus giving arguments to defend the use of one of them that will depend on the context where they are to be applied.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 292\n",
      "Title: A new power index based on minimal winning coalitions without any surplus\n",
      "Abstract: In this paper we propose a new power index useful for the evaluation of each member in a committee, or democratic institution, and the degree of influence over the voting decision making system. The proposed solution is based on the observation that democratic organizations not only tend to form coalitions which can by themselves guarantee the control of the organization, but that they also do it in an extremely efficient way that avoids the inclusion of powerful members if they can be replaced by weaker ones. The mathematical foundation of the new measure is based on two different axiomatizations. A comparison with other well-known measures is also done.\n",
      "Content: A new power index based on minimal winning coalitions without any surplus In this paper we propose a new power index useful for the evaluation of each member in a committee, or democratic institution, and the degree of influence over the voting decision making system. The proposed solution is based on the observation that democratic organizations not only tend to form coalitions which can by themselves guarantee the control of the organization, but that they also do it in an extremely efficient way that avoids the inclusion of powerful members if they can be replaced by weaker ones. The mathematical foundation of the new measure is based on two different axiomatizations. A comparison with other well-known measures is also done.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 293\n",
      "Title: Understanding web site redesigns in small- and medium-sized enterprises (SMEs): a U.K.-based study on the applicability of e-commerce Stage Models\n",
      "Abstract: Despite the efforts of governments and the various support programmes, achievement of advanced stages of e-commerce by small- and medium-sized enterprises (SMEs) is still very low. There have been some attempts to study the dynamic nature of websites, but there is still little research evidence to explain why and how SMEs evolve their web presence. This paper aims to develop a comprehensive classification of drivers for web site redesign based on interviews with various members of staff from SMEs in the U.K. that have recently redesigned their web sites. A sequential mixed-methodological analysis, involving the use of qualitative and quantitative data analysis, was used to develop the classification. This enabled the development of a framework that classified seven main categories of drivers for web site redesign. The drivers identified were: changing business requirements, evolving internet strategies, addressing user needs, maintenance, changing technology, pressure from peers/competitors, and the influence of developers. However, only the first four were found to be significant in the study. The categorisation and the findings suggest a number of key determinants not explicitly addressed by other work. In addition, the findings provide little support for the staged approach to e-commerce progression as few companies reported the implementation of sophisticated internet technology features as a main reason for their web site redesigns. The contributions of this paper are firstly, to provide an instrument to the academic and practitioner communities interested in the topic of web site evolution. Secondly, the categorisation of drivers for redesign and the individual reasons found in this study are expected to provide assistance to SME managers to justify, plan and strategise internet investments realistically and effectively.\n",
      "Content: Understanding web site redesigns in small- and medium-sized enterprises (SMEs): a U.K.-based study on the applicability of e-commerce Stage Models Despite the efforts of governments and the various support programmes, achievement of advanced stages of e-commerce by small- and medium-sized enterprises (SMEs) is still very low. There have been some attempts to study the dynamic nature of websites, but there is still little research evidence to explain why and how SMEs evolve their web presence. This paper aims to develop a comprehensive classification of drivers for web site redesign based on interviews with various members of staff from SMEs in the U.K. that have recently redesigned their web sites. A sequential mixed-methodological analysis, involving the use of qualitative and quantitative data analysis, was used to develop the classification. This enabled the development of a framework that classified seven main categories of drivers for web site redesign. The drivers identified were: changing business requirements, evolving internet strategies, addressing user needs, maintenance, changing technology, pressure from peers/competitors, and the influence of developers. However, only the first four were found to be significant in the study. The categorisation and the findings suggest a number of key determinants not explicitly addressed by other work. In addition, the findings provide little support for the staged approach to e-commerce progression as few companies reported the implementation of sophisticated internet technology features as a main reason for their web site redesigns. The contributions of this paper are firstly, to provide an instrument to the academic and practitioner communities interested in the topic of web site evolution. Secondly, the categorisation of drivers for redesign and the individual reasons found in this study are expected to provide assistance to SME managers to justify, plan and strategise internet investments realistically and effectively.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 294\n",
      "Title: Flaming in electronic communication\n",
      "Abstract: Communication through computer networks, electronic salons, and virtual communities has its price. Often relatively anonymous and socially detached, electronic communication allows people to write things online that they would seldom consider saying face-to-face, sometimes generating flames. In a study of the motives to flame based upon Uses and Gratifications Theory (UGT), 160 subjects generated comments anonymously in parallel with a group support system (GSS) idea generation program. Results showed that high levels of assertiveness and sensation seeking predicted flaming, and males tended to participate more in the activity than did females.\n",
      "Content: Flaming in electronic communication Communication through computer networks, electronic salons, and virtual communities has its price. Often relatively anonymous and socially detached, electronic communication allows people to write things online that they would seldom consider saying face-to-face, sometimes generating flames. In a study of the motives to flame based upon Uses and Gratifications Theory (UGT), 160 subjects generated comments anonymously in parallel with a group support system (GSS) idea generation program. Results showed that high levels of assertiveness and sensation seeking predicted flaming, and males tended to participate more in the activity than did females.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 295\n",
      "Title: Sequential Pricing of Multiple Products: Leveraging Revealed Preferences of Retail Customers Online and with Auto-ID Technologies\n",
      "Abstract: Technological advances enable sellers to price discriminate based on a customer's revealed purchasing intentions. E-tailers can track items in online shopping carts and radio frequency identification tags enable retailers to do the same in brick-and-mortar stores. To leverage this information, it is important to understand how this new visibility impacts pricing and market outcomes. We propose a model in which a seller sets prices for goods A and B, allowing for the possibility of sequentially revising the price for good B if the buyer reveals a preference for good A by making an initial purchase decision. We derive comparative statics results for the prices of products that have superadditive or subadditive values, and also for the associated profits. We also run simulations for a range of distributions of buyer values, to compare sequential pricing with mixed bundling. The results indicate that information technology-enabled sequential pricing can increase profits relative to mixed bundling or pure components pricing for substitute goods due to a reduction of intraseller competition. We also consider the case of goods with positively or negatively correlated values and find that when sellers can condition the second good's price on the buyer's decision to purchase the first good, sequential pricing increases profits when customer's values for the goods are highly positively correlated.\n",
      "Content: Sequential Pricing of Multiple Products: Leveraging Revealed Preferences of Retail Customers Online and with Auto-ID Technologies Technological advances enable sellers to price discriminate based on a customer's revealed purchasing intentions. E-tailers can track items in online shopping carts and radio frequency identification tags enable retailers to do the same in brick-and-mortar stores. To leverage this information, it is important to understand how this new visibility impacts pricing and market outcomes. We propose a model in which a seller sets prices for goods A and B, allowing for the possibility of sequentially revising the price for good B if the buyer reveals a preference for good A by making an initial purchase decision. We derive comparative statics results for the prices of products that have superadditive or subadditive values, and also for the associated profits. We also run simulations for a range of distributions of buyer values, to compare sequential pricing with mixed bundling. The results indicate that information technology-enabled sequential pricing can increase profits relative to mixed bundling or pure components pricing for substitute goods due to a reduction of intraseller competition. We also consider the case of goods with positively or negatively correlated values and find that when sellers can condition the second good's price on the buyer's decision to purchase the first good, sequential pricing increases profits when customer's values for the goods are highly positively correlated.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 296\n",
      "Title: Market share analysis and prognosis using qualitative reasoning\n",
      "Abstract: Today, extensive quantitative modelling of the performance of marketing activities is possible due to the availability of rich data sets on product sales and related marketing actions. However, there are a number of product categories for which only inaccurate sales figures or data about marketing efforts exist (e.g., because a significant part of the sales is realized in stores without scanners or which do not make the data available to firm outsiders). In such cases, reliable quantitative findings about the impact of marketing-mix variables on sales may not be achievable but qualitative reasoning may, at least, indicate likely implications of past or planned marketing activities in terms of directions of change. Even in markets with good data qualitative reasoning may be a worthwhile first modelling approach when market conditions change so significantly that a previously used model is deemed unsatisfactory and not enough data exist yet for a specification of a revised quantitative model. We show specifically how qualitative reasoning can be used to diagnose and predict market share changes. The diagnostic part represents a combination of qualitative reasoning and extensions of a previously published rule-based expert system for that task while the prognostic part is solely based on qualitative process theory and order of magnitude reasoning.\n",
      "Content: Market share analysis and prognosis using qualitative reasoning Today, extensive quantitative modelling of the performance of marketing activities is possible due to the availability of rich data sets on product sales and related marketing actions. However, there are a number of product categories for which only inaccurate sales figures or data about marketing efforts exist (e.g., because a significant part of the sales is realized in stores without scanners or which do not make the data available to firm outsiders). In such cases, reliable quantitative findings about the impact of marketing-mix variables on sales may not be achievable but qualitative reasoning may, at least, indicate likely implications of past or planned marketing activities in terms of directions of change. Even in markets with good data qualitative reasoning may be a worthwhile first modelling approach when market conditions change so significantly that a previously used model is deemed unsatisfactory and not enough data exist yet for a specification of a revised quantitative model. We show specifically how qualitative reasoning can be used to diagnose and predict market share changes. The diagnostic part represents a combination of qualitative reasoning and extensions of a previously published rule-based expert system for that task while the prognostic part is solely based on qualitative process theory and order of magnitude reasoning.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 297\n",
      "Title: Understanding the global diffusion of B2B E-commerce (B2B EC): An integrated model\n",
      "Abstract: Using institutional theory, this study offers an integrated framework that describes the diffusion of business-to-business e-commerce within a country. The model specifies the role of national institutional frameworks, international institutional pressures, and market complexity in business-to-business e-commerce diffusion. We test this model using archival, cross-sectional data from 146 countries for the period from 2013 to 2016. The study also compares the roles of these factors in business-to-business e-commerce diffusion across developed and developing countries. The results suggest that national institutional frameworks, international institutional pressures, and market complexity contribute positively to business-to-business e-commerce diffusion and that the influence of these variables varies according to the degree of a country’s development. Theoretical, research, and managerial implications of the study are also discussed.\n",
      "Content: Understanding the global diffusion of B2B E-commerce (B2B EC): An integrated model Using institutional theory, this study offers an integrated framework that describes the diffusion of business-to-business e-commerce within a country. The model specifies the role of national institutional frameworks, international institutional pressures, and market complexity in business-to-business e-commerce diffusion. We test this model using archival, cross-sectional data from 146 countries for the period from 2013 to 2016. The study also compares the roles of these factors in business-to-business e-commerce diffusion across developed and developing countries. The results suggest that national institutional frameworks, international institutional pressures, and market complexity contribute positively to business-to-business e-commerce diffusion and that the influence of these variables varies according to the degree of a country’s development. Theoretical, research, and managerial implications of the study are also discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 298\n",
      "Title: Virtual team effectiveness: The role of knowledge sharing and trust\n",
      "Abstract: Organizations utilize virtual teams to gather experts who collaborate online to accomplish organizational tasks. The virtual nature of these teams creates challenges to effective collaboration and team outcomes. This research addresses the social effects of knowledge sharing on virtual teams. We propose a conceptual model which hypothesizes a relationship between knowledge sharing, trust, collaboration, and team effectiveness in virtual team settings. The findings suggest that knowledge sharing positively influences trust and collaboration among virtual team members. The findings also suggest that while trust positively influences virtual team collaboration, it does not have a significant direct effect on team effectiveness.\n",
      "Content: Virtual team effectiveness: The role of knowledge sharing and trust Organizations utilize virtual teams to gather experts who collaborate online to accomplish organizational tasks. The virtual nature of these teams creates challenges to effective collaboration and team outcomes. This research addresses the social effects of knowledge sharing on virtual teams. We propose a conceptual model which hypothesizes a relationship between knowledge sharing, trust, collaboration, and team effectiveness in virtual team settings. The findings suggest that knowledge sharing positively influences trust and collaboration among virtual team members. The findings also suggest that while trust positively influences virtual team collaboration, it does not have a significant direct effect on team effectiveness.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 299\n",
      "Title: Oracle8i Data Warehousing\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 300\n",
      "Title: Software Licenses in Context: The Challenge of Heterogeneously-Licensed Systems\n",
      "Abstract: The prevailing approach to free/open source software and licenses has been that each system is developed, distributed, and used under the terms of a single license. But it is increasingly common for information systems and other software to be composed with components from a variety of sources, and with a diversity of licenses. This may result in possible license conflicts and organizational liability for failure to fulfill license obligations. Research and practice to date have not kept up with this sea-change in software licensing arising from free/open source software development. System consumers and users consequently rely on ad hoc heuristics (or costly legal advice) to determine which license rights and obligations are in effect, often with less than optimal results; consulting services are offered to identify unknowing unauthorized use of licensed software in information systems; and researchers have shown how the choice of a (single) specific license for a product affects project success and system adoption. Legal scholars have examined how pairs of software licenses conflict but only in simple contexts. We present an approach for understanding and modeling software licenses, as well as for analyzing conflicts among groups of licenses in realistic system contexts, and for guiding the acquisition, integration, or development of systems with free/open source components in such an environment. This work is based on an empirical analysis of representative software licenses and of heterogeneously-licensed systems. Our approach provides guidance for achieving a \"best-of-breed\" component strategy while obtaining desired license rights in exchange for acceptable obligations.\n",
      "Content: Software Licenses in Context: The Challenge of Heterogeneously-Licensed Systems The prevailing approach to free/open source software and licenses has been that each system is developed, distributed, and used under the terms of a single license. But it is increasingly common for information systems and other software to be composed with components from a variety of sources, and with a diversity of licenses. This may result in possible license conflicts and organizational liability for failure to fulfill license obligations. Research and practice to date have not kept up with this sea-change in software licensing arising from free/open source software development. System consumers and users consequently rely on ad hoc heuristics (or costly legal advice) to determine which license rights and obligations are in effect, often with less than optimal results; consulting services are offered to identify unknowing unauthorized use of licensed software in information systems; and researchers have shown how the choice of a (single) specific license for a product affects project success and system adoption. Legal scholars have examined how pairs of software licenses conflict but only in simple contexts. We present an approach for understanding and modeling software licenses, as well as for analyzing conflicts among groups of licenses in realistic system contexts, and for guiding the acquisition, integration, or development of systems with free/open source components in such an environment. This work is based on an empirical analysis of representative software licenses and of heterogeneously-licensed systems. Our approach provides guidance for achieving a \"best-of-breed\" component strategy while obtaining desired license rights in exchange for acceptable obligations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 301\n",
      "Title: Incorrect data in the widely used Inside Airbnb dataset\n",
      "Abstract: Several recently published papers in Decision Support Systems discussed issues related to data quality in In­ formation Systems research. In this short research note, I build on the work introduced in these papers and document two data quality issues discovered in a large open dataset commonly used in research. Inside Airbnb (IA) collects data from places and reviews as posted by users of Airbnb.com. Visitors can effortlessly download data collected by IA for several locations around the globe. While the dataset is widely used in academic research, no thorough investigation of the dataset and its validity has been conducted. This note examines the dataset and explains an issue of incorrect data added to the dataset. Findings suggest that this issue can be attributed to systemic errors in the data collection process. The results suggest that the use of unverified open datasets can be problematic, although the discoveries presented in this work may not be significant enough to challenge all published research that used the IA dataset. Additionally, findings indicate that the incorrect data happens because of a new feature implemented by Airbnb. Thus, unless changes are made, it is likely that the consequences of this issue will only become more severe. Finally, this note explores why reproducibility is a problem when two different releases of the dataset are compared.\n",
      "Content: Incorrect data in the widely used Inside Airbnb dataset Several recently published papers in Decision Support Systems discussed issues related to data quality in In­ formation Systems research. In this short research note, I build on the work introduced in these papers and document two data quality issues discovered in a large open dataset commonly used in research. Inside Airbnb (IA) collects data from places and reviews as posted by users of Airbnb.com. Visitors can effortlessly download data collected by IA for several locations around the globe. While the dataset is widely used in academic research, no thorough investigation of the dataset and its validity has been conducted. This note examines the dataset and explains an issue of incorrect data added to the dataset. Findings suggest that this issue can be attributed to systemic errors in the data collection process. The results suggest that the use of unverified open datasets can be problematic, although the discoveries presented in this work may not be significant enough to challenge all published research that used the IA dataset. Additionally, findings indicate that the incorrect data happens because of a new feature implemented by Airbnb. Thus, unless changes are made, it is likely that the consequences of this issue will only become more severe. Finally, this note explores why reproducibility is a problem when two different releases of the dataset are compared.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 302\n",
      "Title: Development Patterns for Decision Support Systems\n",
      "Abstract:  Examples from a case-oriented survey of fifty-six decision support systems are cited to illustrate each of four implementation patterns that were observed in the sample. These four system development patterns are defined in terms of high or low degrees of (I) initiation by the user and (2) participation the development process. A surprising finding is that systems initiated by users and implemented with their active participation account for less than one fourth of all cases. A qualitative analysis of the cases explains this finding by interpreting the four patterns in terms of six frequently observed system development situations, each of which has its own purpose and dynamics. Categorizing these situations under the headings \"built for, \" \"sold to, \"'  or \"forced upon, \"' key questions and concerns for various stakeholders in system development efforts are cited. A concluding quandary for management involves the choice of a project portfolio that is neither too risky nor too safe. \n",
      "Content: Development Patterns for Decision Support Systems  Examples from a case-oriented survey of fifty-six decision support systems are cited to illustrate each of four implementation patterns that were observed in the sample. These four system development patterns are defined in terms of high or low degrees of (I) initiation by the user and (2) participation the development process. A surprising finding is that systems initiated by users and implemented with their active participation account for less than one fourth of all cases. A qualitative analysis of the cases explains this finding by interpreting the four patterns in terms of six frequently observed system development situations, each of which has its own purpose and dynamics. Categorizing these situations under the headings \"built for, \" \"sold to, \"'  or \"forced upon, \"' key questions and concerns for various stakeholders in system development efforts are cited. A concluding quandary for management involves the choice of a project portfolio that is neither too risky nor too safe. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 303\n",
      "Title: A work system view of DSS in its fourth decade\n",
      "Abstract: The initially revolutionary DSS agenda is now ancient history. This paper argues that “decision support” provides a richer basis than “DSS” in both practice and research. Using a loan-processing example involving two banks, it shows how work system concepts might be applied to understand decision support in real world settings, and how decision support can come from many sources other than technical artifacts such as DSS. Shifting the focus from “DSS as artifact” to “decision support within a work system” reduces the chances of being misled by techno-hype, vendor sales pitches, and incomplete understanding of determinants of success in organizations.\n",
      "Content: A work system view of DSS in its fourth decade The initially revolutionary DSS agenda is now ancient history. This paper argues that “decision support” provides a richer basis than “DSS” in both practice and research. Using a loan-processing example involving two banks, it shows how work system concepts might be applied to understand decision support in real world settings, and how decision support can come from many sources other than technical artifacts such as DSS. Shifting the focus from “DSS as artifact” to “decision support within a work system” reduces the chances of being misled by techno-hype, vendor sales pitches, and incomplete understanding of determinants of success in organizations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 304\n",
      "Title: Possibilities for cross-fertilization between interpretive approaches and other methods for analyzing information systems\n",
      "Abstract: This paper explores possibilities for cross-fertilization between interpretive approaches and other approaches for performing the initial analysis of an information system as part of an effort to redesign and improve it. The paper presents a hypothetical situation concerning the analysis of a loan approval system in a large bank. It assumes that ethnographers observed three systems analysis projects that applied different approaches in three identical banks. It uses hypothetical accounts of the three analysis efforts to propose likely differences in the process and in the results. These differences illustrate possible opportunities for cross-fertilization that might make each approach more powerful and reliable. The paper concludes that the most likely direction for cross-fertilization is from interpretive approaches to the other approaches. An earlier version of this paper was presented at the First International Workshop on Interpretive Approaches to Information Systems and Computing Research, SIG-IAM, Brunel University, July 25–27, 2002, to motivate discussion about the applications, strengths, and limitations of interpretive approaches and to help in the further development of systems analysis methods.\n",
      "Content: Possibilities for cross-fertilization between interpretive approaches and other methods for analyzing information systems This paper explores possibilities for cross-fertilization between interpretive approaches and other approaches for performing the initial analysis of an information system as part of an effort to redesign and improve it. The paper presents a hypothetical situation concerning the analysis of a loan approval system in a large bank. It assumes that ethnographers observed three systems analysis projects that applied different approaches in three identical banks. It uses hypothetical accounts of the three analysis efforts to propose likely differences in the process and in the results. These differences illustrate possible opportunities for cross-fertilization that might make each approach more powerful and reliable. The paper concludes that the most likely direction for cross-fertilization is from interpretive approaches to the other approaches. An earlier version of this paper was presented at the First International Workshop on Interpretive Approaches to Information Systems and Computing Research, SIG-IAM, Brunel University, July 25–27, 2002, to motivate discussion about the applications, strengths, and limitations of interpretive approaches and to help in the further development of systems analysis methods.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 305\n",
      "Title: Defining information systems as work systems: implications for the IS field\n",
      "Abstract: The lack of an agreed upon definition of information system (IS) is one of many obstacles troubling the academic IS discipline. After listing a number of definitions of IS, this paper defines IS as a special case of work system as defined in Alter (1999a). This definition has many desirable characteristics: it is easy to understand; differentiates IS from information technology (IT); covers totally manual, partially automated, and totally automated ISs; links to a life cycle model that generates many insights about development and implementation problems; provides a simple guideline that helps in interpreting common IS/IT jargon; and has other useful implications related to IS concepts, IS terminology, and the analysis and design of ISs. The paper presents the proposed IS definition and evaluates the definition in terms of simplicity, clarity, scope, systematic power, explanatory power, validity, reliability, and fruitfulness. An app1Appendix summarizes previously published concepts and two frameworks that flow from the proposed definition and are useful for appreciating many points in the evaluation section.\n",
      "Content: Defining information systems as work systems: implications for the IS field The lack of an agreed upon definition of information system (IS) is one of many obstacles troubling the academic IS discipline. After listing a number of definitions of IS, this paper defines IS as a special case of work system as defined in Alter (1999a). This definition has many desirable characteristics: it is easy to understand; differentiates IS from information technology (IT); covers totally manual, partially automated, and totally automated ISs; links to a life cycle model that generates many insights about development and implementation problems; provides a simple guideline that helps in interpreting common IS/IT jargon; and has other useful implications related to IS concepts, IS terminology, and the analysis and design of ISs. The paper presents the proposed IS definition and evaluates the definition in terms of simplicity, clarity, scope, systematic power, explanatory power, validity, reliability, and fruitfulness. An app1Appendix summarizes previously published concepts and two frameworks that flow from the proposed definition and are useful for appreciating many points in the evaluation section.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 306\n",
      "Title: Work System Theory: Overview of Core Concepts, Extensions, and Challenges for the Future\n",
      "Abstract: This paper presents a current, accessible, and overarching view of work system theory. WST is the core of an integrated body of theory that emerged from a long-term research project to develop a systems analysis and design method for business professionals called the work system method (WSM). After discussing WST's basic premises and its two central frameworks, this paper summarizes the relationship between WST and WSM. It shows how experience with early versions of WSM led to three extensions of WST that addressed limitations-in-use in one of the central frameworks in WST. After comparisons with related theories, this paper closes with an evaluation of progress to date, new directions for research related to WST, and implications for the IS discipline. The two appendices summarize the long term research from which WST emerged and use a positioning map to show how WST is related to other topics in the IS discipline.\n",
      "Content: Work System Theory: Overview of Core Concepts, Extensions, and Challenges for the Future This paper presents a current, accessible, and overarching view of work system theory. WST is the core of an integrated body of theory that emerged from a long-term research project to develop a systems analysis and design method for business professionals called the work system method (WSM). After discussing WST's basic premises and its two central frameworks, this paper summarizes the relationship between WST and WSM. It shows how experience with early versions of WSM led to three extensions of WST that addressed limitations-in-use in one of the central frameworks in WST. After comparisons with related theories, this paper closes with an evaluation of progress to date, new directions for research related to WST, and implications for the IS discipline. The two appendices summarize the long term research from which WST emerged and use a positioning map to show how WST is related to other topics in the IS discipline.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 307\n",
      "Title: The concept of ‘IT artifact’ has outlived its usefulness and should be retired now\n",
      "Abstract: Vastly inconsistent definitions of the term “the IT artifact” in leading journals and conferences demonstrate why it no longer means anything in particular and should be retired from the active IS lexicon. Examples from the literature show why artifact-cousins, such as the IS artifact, sociotechnical artifact, social artifact, and ensemble artifact should be used with great care, if not retired as well. Any void created by these retirements could be filled through the following approaches: (i) relabeling with simple terms that are immediately understandable; (ii) adopting guidelines for making sense of the whole X-artifact family; and (iii) sidestepping the IT artifact and focusing directly on IT-enabled work systems in organizations.\n",
      "Content: The concept of ‘IT artifact’ has outlived its usefulness and should be retired now Vastly inconsistent definitions of the term “the IT artifact” in leading journals and conferences demonstrate why it no longer means anything in particular and should be retired from the active IS lexicon. Examples from the literature show why artifact-cousins, such as the IS artifact, sociotechnical artifact, social artifact, and ensemble artifact should be used with great care, if not retired as well. Any void created by these retirements could be filled through the following approaches: (i) relabeling with simple terms that are immediately understandable; (ii) adopting guidelines for making sense of the whole X-artifact family; and (iii) sidestepping the IT artifact and focusing directly on IT-enabled work systems in organizations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 308\n",
      "Title: Work System Theory as a Platform: Response to a Research Perspective Article by Niederman and March\n",
      "Abstract: In this paper, I respond to \"Moving the Work System Theory Forward\" (Niederman & March, 2014), a JAIS research perspective paper about another paper on work system theory (Alter, 2013e). The research perspective paper recognizes value in the work system approach, suggests that WST is not a proper theory, and suggests areas for related theory development. After summarizing the main ideas in WST, I explain disagreements between Niederman and March (2014) and Alter (2013e)-(hereafter called N&M and the WST paper) about what WST is and what WST should become. I note that N&M interprets basic ideas in WST differently than the WST paper defines them. I note that N&M's critique of WST is anchored in issues about the nature of theory, especially a preference for Gregor's type 4 theory. I explain that WST is a special case of general system theory and, as such, should not and cannot take the form of a theory that expresses relationships between independent variables, moderating variables, and dependent variables. I also explain why the WST paper called WST a theory when it might have been called something else, and also why the WST paper does not treat the development of the work system method (WSM) as a design science research project. Lastly, I respond directly to N&M's title, \"Moving the Work System Theory Forward\" by explaining that WST is becoming a platform for applications and extensions in IS and other disciplines, which I illustrate with examples under five categories.\n",
      "Content: Work System Theory as a Platform: Response to a Research Perspective Article by Niederman and March In this paper, I respond to \"Moving the Work System Theory Forward\" (Niederman & March, 2014), a JAIS research perspective paper about another paper on work system theory (Alter, 2013e). The research perspective paper recognizes value in the work system approach, suggests that WST is not a proper theory, and suggests areas for related theory development. After summarizing the main ideas in WST, I explain disagreements between Niederman and March (2014) and Alter (2013e)-(hereafter called N&M and the WST paper) about what WST is and what WST should become. I note that N&M interprets basic ideas in WST differently than the WST paper defines them. I note that N&M's critique of WST is anchored in issues about the nature of theory, especially a preference for Gregor's type 4 theory. I explain that WST is a special case of general system theory and, as such, should not and cannot take the form of a theory that expresses relationships between independent variables, moderating variables, and dependent variables. I also explain why the WST paper called WST a theory when it might have been called something else, and also why the WST paper does not treat the development of the work system method (WSM) as a design science research project. Lastly, I respond directly to N&M's title, \"Moving the Work System Theory Forward\" by explaining that WST is becoming a platform for applications and extensions in IS and other disciplines, which I illustrate with examples under five categories.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 309\n",
      "Title: Nothing is more practical than a good conceptual artifact… which may be a theory, framework, model, metaphor, paradigm or perhaps some other abstraction\n",
      "Abstract: This research commentary proposes a way to make progress in the IS discipline's inconclusive discussion about the nature and role of theory. In some ways, the creation and testing of theory seems to be the primary goal of IS research. Despite that, there are persistent questions whether theory has become a fetish in the IS discipline and whether the routinized production and testing of mid-range theories is little more than an uninspired script that reduces the value and interest of IS research. This paper reframes the discussion around the idea of ‘conceptual artifact’ that has been discussed widely in educational psychology for over a decade. Conceptual artifacts are abstract knowledge objects that can be produced, tested and improved. This paper recognizes the value of both abstract knowledge (conceptual artifacts) and non-abstract knowledge. It explains that theorizing produces, evaluates or improves useful conceptual artifacts that may or may not be theories. It validates four premises related to conceptual artifacts by showing that theorizing related to work system theory created or used many different types of conceptual artifacts. It identifies nine criteria for evaluating conceptual artifacts and shows that some of them differ from typical criteria for evaluating Gregor Type IV theories. As a whole, it argues that that privileging theory over other types of conceptual artifacts may not be beneficial in pursuing the research questions that the IS discipline needs to study.\n",
      "Content: Nothing is more practical than a good conceptual artifact… which may be a theory, framework, model, metaphor, paradigm or perhaps some other abstraction This research commentary proposes a way to make progress in the IS discipline's inconclusive discussion about the nature and role of theory. In some ways, the creation and testing of theory seems to be the primary goal of IS research. Despite that, there are persistent questions whether theory has become a fetish in the IS discipline and whether the routinized production and testing of mid-range theories is little more than an uninspired script that reduces the value and interest of IS research. This paper reframes the discussion around the idea of ‘conceptual artifact’ that has been discussed widely in educational psychology for over a decade. Conceptual artifacts are abstract knowledge objects that can be produced, tested and improved. This paper recognizes the value of both abstract knowledge (conceptual artifacts) and non-abstract knowledge. It explains that theorizing produces, evaluates or improves useful conceptual artifacts that may or may not be theories. It validates four premises related to conceptual artifacts by showing that theorizing related to work system theory created or used many different types of conceptual artifacts. It identifies nine criteria for evaluating conceptual artifacts and shows that some of them differ from typical criteria for evaluating Gregor Type IV theories. As a whole, it argues that that privileging theory over other types of conceptual artifacts may not be beneficial in pursuing the research questions that the IS discipline needs to study.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 310\n",
      "Title: Help that is not recognized: Harmful neglect of decision support systems\n",
      "Abstract: Decision support systems (DSSs) aim to enhance the performance of decision makers, but to do so DSSs have to be adopted and used. Technology acceptance research shows that user evaluations (i.e., beliefs, perceptions, and attitudes) are key drivers of adoption and use. This article first presents evidence from the literature suggesting that the link between user evaluations of DSSs and actual performance may be weak, or sometimes even negative. The authors then present two empirical studies in which they found a serious disconnect between user evaluations and actual performance. If user evaluations do not accurately reflect performance, then this may lead to harmful neglect of performance-enhancing DSSs. The article concludes with a discussion of interventions that may alleviate this problem.\n",
      "Content: Help that is not recognized: Harmful neglect of decision support systems Decision support systems (DSSs) aim to enhance the performance of decision makers, but to do so DSSs have to be adopted and used. Technology acceptance research shows that user evaluations (i.e., beliefs, perceptions, and attitudes) are key drivers of adoption and use. This article first presents evidence from the literature suggesting that the link between user evaluations of DSSs and actual performance may be weak, or sometimes even negative. The authors then present two empirical studies in which they found a serious disconnect between user evaluations and actual performance. If user evaluations do not accurately reflect performance, then this may lead to harmful neglect of performance-enhancing DSSs. The article concludes with a discussion of interventions that may alleviate this problem.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 311\n",
      "Title: The Effects of IT-Enabled Cognitive Stimulation Tools on Creative Problem Solving: A Dual Pathway to Creativity\n",
      "Abstract: We investigate the effectiveness of three types of IT-enabled cognitive stimulation tools for enhancing creative problem solving: mind mappers, process guides, and stimuli providers. Based on the dual pathway to creativity models, the authors examine the extent to which these tools are capable of stimulating individuals to explore their knowledge base more deeply (i.e., the persistence pathway) and more broadly (i.e., the flexibility pathway) and, hence, help to produce more novel ideas. In a laboratory study with business students, they find that, as compared to unaided individuals, IT-enabled stimuli providers enhance individual creativity more than process guides and mind mappers. As for the underlying creative process, stimuli providers push individuals to explore their knowledge base more deeply and more broadly, leading to more novel but, unexpectedly, also more useful ideas. The reported findings may facilitate the development of creativity support systems and their assignment to individuals and tasks.\n",
      "Content: The Effects of IT-Enabled Cognitive Stimulation Tools on Creative Problem Solving: A Dual Pathway to Creativity We investigate the effectiveness of three types of IT-enabled cognitive stimulation tools for enhancing creative problem solving: mind mappers, process guides, and stimuli providers. Based on the dual pathway to creativity models, the authors examine the extent to which these tools are capable of stimulating individuals to explore their knowledge base more deeply (i.e., the persistence pathway) and more broadly (i.e., the flexibility pathway) and, hence, help to produce more novel ideas. In a laboratory study with business students, they find that, as compared to unaided individuals, IT-enabled stimuli providers enhance individual creativity more than process guides and mind mappers. As for the underlying creative process, stimuli providers push individuals to explore their knowledge base more deeply and more broadly, leading to more novel but, unexpectedly, also more useful ideas. The reported findings may facilitate the development of creativity support systems and their assignment to individuals and tasks.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 312\n",
      "Title: Supporting Creative Problem Solving with a Case-Based Reasoning System\n",
      "Abstract: Attention for the division of work between computers and humans is growing due to ever-increasing computer capabilities. Over the past decades, creativity support systems (CSSs) have gained ground as a means to enhance individual, group, and organizational creativity. Whereas prior research has focused primarily on the main effects of CSSs, we explore the interaction effects with the creative ability of the individual. In this paper, we investigate the use of the case-based reasoning (CBR) technology, which is based on the principle of analogical reasoning, to aid individuals in solving business problems creatively. The expectations as to why the CBR technology should enhance individual creativity, and under what conditions (i.e., the type and number of cases that are made available), are derived from creative cognition theory, and are tested empirically. In a series of studies, a CBR system loaded with a diverse set of cases was found to enhance the performance of individuals with lower creative ability, but it did not help the most creative individuals. Although the literature suggests that cases from remote problem domains should lead to more novel solutions, loading the CBR system only with cases closely related to the problem domain proved more effective than remote cases only. Finally, loading the CBR system with a larger set of diverse cases was found to positively influence the creativity of the solutions. These findings have the following implications for CSSs and creative cognition theory: (1) when considering the effectiveness of CSSs it is important to take into account the creative ability of the individual (i.e., \"one size does not fit all\"), (2) making a sufficiently large and diverse set of cases available is better for stimulating creativity, and (3) providing cases that are too remote may be counterproductive. On a practical note, organizations seeking to redesign their division of labor between individuals and machines can easily follow the CBR approach presented here using their own set of cases.\n",
      "Content: Supporting Creative Problem Solving with a Case-Based Reasoning System Attention for the division of work between computers and humans is growing due to ever-increasing computer capabilities. Over the past decades, creativity support systems (CSSs) have gained ground as a means to enhance individual, group, and organizational creativity. Whereas prior research has focused primarily on the main effects of CSSs, we explore the interaction effects with the creative ability of the individual. In this paper, we investigate the use of the case-based reasoning (CBR) technology, which is based on the principle of analogical reasoning, to aid individuals in solving business problems creatively. The expectations as to why the CBR technology should enhance individual creativity, and under what conditions (i.e., the type and number of cases that are made available), are derived from creative cognition theory, and are tested empirically. In a series of studies, a CBR system loaded with a diverse set of cases was found to enhance the performance of individuals with lower creative ability, but it did not help the most creative individuals. Although the literature suggests that cases from remote problem domains should lead to more novel solutions, loading the CBR system only with cases closely related to the problem domain proved more effective than remote cases only. Finally, loading the CBR system with a larger set of diverse cases was found to positively influence the creativity of the solutions. These findings have the following implications for CSSs and creative cognition theory: (1) when considering the effectiveness of CSSs it is important to take into account the creative ability of the individual (i.e., \"one size does not fit all\"), (2) making a sufficiently large and diverse set of cases available is better for stimulating creativity, and (3) providing cases that are too remote may be counterproductive. On a practical note, organizations seeking to redesign their division of labor between individuals and machines can easily follow the CBR approach presented here using their own set of cases.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 313\n",
      "Title: Using structural technology acceptance models to segment intended users of a new technology: Propositions and an empirical illustration\n",
      "Abstract: This article aims to offer an alternative method to analyse technology acceptance models, namely a segment-wise analysis. The empirical illustration of this method involves data that were collected during a company-wide implementation of a Sales Force Automation technology in Europe. The data comprise a variety of commonly used technology-related, context-related, and person-related variables. The segmentation procedure, which involved a finite mixture partial least squares estimation, provides more insight into the different ways in which people come to accept new technologies. Unlike other segmentation studies published in IS journals, the resulting segments are based on similarities and differences in the structure of the underlying theoretical models rather than (a collection of) individual variables. Further research or a re-analysis of existing data should help establish robust “technology acceptance model”-based segments as well as comprehensive profiles of the individuals in each segment.\n",
      "Content: Using structural technology acceptance models to segment intended users of a new technology: Propositions and an empirical illustration This article aims to offer an alternative method to analyse technology acceptance models, namely a segment-wise analysis. The empirical illustration of this method involves data that were collected during a company-wide implementation of a Sales Force Automation technology in Europe. The data comprise a variety of commonly used technology-related, context-related, and person-related variables. The segmentation procedure, which involved a finite mixture partial least squares estimation, provides more insight into the different ways in which people come to accept new technologies. Unlike other segmentation studies published in IS journals, the resulting segments are based on similarities and differences in the structure of the underlying theoretical models rather than (a collection of) individual variables. Further research or a re-analysis of existing data should help establish robust “technology acceptance model”-based segments as well as comprehensive profiles of the individuals in each segment.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 314\n",
      "Title: Productivity and Performance Effects of Business Process Reengineering: A Firm-Level Analysis\n",
      "Abstract: We empirically investigate whether business process reengineering (BPR), which requires substantial investment in information technology to integrate separate tasks into complete cross-functional processes, is associated with enhanced firm productivity and performance. We analyze firm-level panel data covering the period 1987–2008 using fixed effects and first differencing, standard methods that account for unobservable firm-level effects. We find that return on assets drops significantly during the project initiation year. According to fixed effects results, the performance and productivity measures improve in a decreasing manner after project initiation, suggesting that BPR indeed positively affects firm performance on average. We also find that enterprise-wide BPR projects are associated with more negative returns during project initiation than functionally focused projects. However, there is no clear evidence regarding their superiority over functionally focused BPR projects in terms of performance improvements after project initiation, perhaps because grand projects are risky and sometimes lead to grand failures.\n",
      "Content: Productivity and Performance Effects of Business Process Reengineering: A Firm-Level Analysis We empirically investigate whether business process reengineering (BPR), which requires substantial investment in information technology to integrate separate tasks into complete cross-functional processes, is associated with enhanced firm productivity and performance. We analyze firm-level panel data covering the period 1987–2008 using fixed effects and first differencing, standard methods that account for unobservable firm-level effects. We find that return on assets drops significantly during the project initiation year. According to fixed effects results, the performance and productivity measures improve in a decreasing manner after project initiation, suggesting that BPR indeed positively affects firm performance on average. We also find that enterprise-wide BPR projects are associated with more negative returns during project initiation than functionally focused projects. However, there is no clear evidence regarding their superiority over functionally focused BPR projects in terms of performance improvements after project initiation, perhaps because grand projects are risky and sometimes lead to grand failures.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 315\n",
      "Title: Cost and benefit analysis of authentication systems\n",
      "Abstract: This study investigates the key elements an online service or product provider needs to consider when adopting another single-factor or two-factor authentication system. We also uncover the conditions that make the new one-factor or two-factor authentication system more preferable. By using the probability of system failure, this study generalizes all possible combination of authentication systems into four different cases. This generalization allows us to compare different systems and to determine the key factors managers need to consider when adopting a new authentication system. The key factors are (1) additional implementation costs, (2) customer switching which is determined by the market share and customers' preferences, and (3) expected losses when the new system fails. This study also suggests that if the provider chooses an expensive new system, the provider needs to have a larger market share to justify the spending. Also, regulators can encourage the adoption of a more secure authentication system by changing the penalty a firm faces when the system fails. Finally, it could also be preferable to have both one-factor and two-factor authentication systems depending on the customers' characteristics.\n",
      "Content: Cost and benefit analysis of authentication systems This study investigates the key elements an online service or product provider needs to consider when adopting another single-factor or two-factor authentication system. We also uncover the conditions that make the new one-factor or two-factor authentication system more preferable. By using the probability of system failure, this study generalizes all possible combination of authentication systems into four different cases. This generalization allows us to compare different systems and to determine the key factors managers need to consider when adopting a new authentication system. The key factors are (1) additional implementation costs, (2) customer switching which is determined by the market share and customers' preferences, and (3) expected losses when the new system fails. This study also suggests that if the provider chooses an expensive new system, the provider needs to have a larger market share to justify the spending. Also, regulators can encourage the adoption of a more secure authentication system by changing the penalty a firm faces when the system fails. Finally, it could also be preferable to have both one-factor and two-factor authentication systems depending on the customers' characteristics.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 316\n",
      "Title: A Multigeneration Diffusion Model for IT-Intensive Game Consoles\n",
      "Abstract: The video game industry has attracted more and more attention not only from technology giants such as Microsoft but also from software developers and private investors. Information technology dictates how game console producers compete in the marketplace. Intensive IT competition in each console generation has shifted the market balance. Competitors jockey to position themselves as the first-mover within a generation or to wait and enter the market with cheaper and more advanced technologies. To capture the characteristics of IT-intensive products, we propose a multigeneration diffusion model that captures both cannibalization and competition effects. We apply the model to analyze game console diffusion with real shipment data for three game consoles from two companies: Sony and Microsoft. We analyze two scenarios: one with only Sony's products, and one with both companies' products. We find that the cannibalization between Sony's products is minimal, and Microsoft maintains a strong competitive edge that has challenged Sony's market position. The results also explain how Sony has maintained its position as the market leader over the last two generations. This research sheds light on the nature of an IT-intensive game console competition between companies and generations.\n",
      "Content: A Multigeneration Diffusion Model for IT-Intensive Game Consoles The video game industry has attracted more and more attention not only from technology giants such as Microsoft but also from software developers and private investors. Information technology dictates how game console producers compete in the marketplace. Intensive IT competition in each console generation has shifted the market balance. Competitors jockey to position themselves as the first-mover within a generation or to wait and enter the market with cheaper and more advanced technologies. To capture the characteristics of IT-intensive products, we propose a multigeneration diffusion model that captures both cannibalization and competition effects. We apply the model to analyze game console diffusion with real shipment data for three game consoles from two companies: Sony and Microsoft. We analyze two scenarios: one with only Sony's products, and one with both companies' products. We find that the cannibalization between Sony's products is minimal, and Microsoft maintains a strong competitive edge that has challenged Sony's market position. The results also explain how Sony has maintained its position as the market leader over the last two generations. This research sheds light on the nature of an IT-intensive game console competition between companies and generations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 317\n",
      "Title: The pursuit of trust in ad hoc virtual teams: how much electronic portrayal is too much?\n",
      "Abstract: This study develops and tests the concept of electronic portrayal in synchronous computer-mediated communication of ad hoc virtual teams. Electronic portrayal is the extent to which a communication system portrays the true identity of its users. A theoretical model is developed based upon which it is hypothesized that increased information available due to electronic portrayal will impact trust in ad hoc virtual teams. An experiment is conducted to test the model by manipulating the graphical identification of users of a system as well as the rehearsability of the system. Rehearsability is the extent to which users can reread and edit their messages before submitting them to the synchronous communication system. The results show that the combination of both factors – identification and rehearsability – impacts trust among team members. Specifically, partial electronic portrayal (only one form of true-to-life representation) has the most positive impact on trust. This effect is moderated by communication-related variables such as self-disclosure, impressions and virtual co-presence. The implication of these results is that too much true identity information negatively impacts trust. This research provides theoretical and practical contributions for understanding the importance of identification and rehearsability in synchronous group communication.\n",
      "Content: The pursuit of trust in ad hoc virtual teams: how much electronic portrayal is too much? This study develops and tests the concept of electronic portrayal in synchronous computer-mediated communication of ad hoc virtual teams. Electronic portrayal is the extent to which a communication system portrays the true identity of its users. A theoretical model is developed based upon which it is hypothesized that increased information available due to electronic portrayal will impact trust in ad hoc virtual teams. An experiment is conducted to test the model by manipulating the graphical identification of users of a system as well as the rehearsability of the system. Rehearsability is the extent to which users can reread and edit their messages before submitting them to the synchronous communication system. The results show that the combination of both factors – identification and rehearsability – impacts trust among team members. Specifically, partial electronic portrayal (only one form of true-to-life representation) has the most positive impact on trust. This effect is moderated by communication-related variables such as self-disclosure, impressions and virtual co-presence. The implication of these results is that too much true identity information negatively impacts trust. This research provides theoretical and practical contributions for understanding the importance of identification and rehearsability in synchronous group communication.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 318\n",
      "Title: The stability of electric energy markets\n",
      "Abstract: Power system markets represented by dynamic equations provide insights into the market behavior which are not available from static models. In particular: (1) markets that are required to balance supply and demand precisely at all times may be unstable if one supplier exhibits economies of scale and will be unstable if two suppliers exhibit this behavior. The instability is characterized by one or more positive eigenvalues. (2) Markets where some energy imbalance is allowed to accumulate can exhibit an instability, depending on the exact values of time constants and delays in the system. (3) Congestion can be helpful from the perspective of stability: a market can become unstable in the eigenvalue sense if congestion is removed. (4) A power system (with stable electromechanical dynamic behavior when considered by itself) and market (by itself stable) can, when analyzed jointly, exhibit unstable behavior. Some of the instabilities alluded here are nothing more than fluctuations in demands and prices. However, fluctuations are likely to require larger security margins, thus greater costs to operate the system.\n",
      "Content: The stability of electric energy markets Power system markets represented by dynamic equations provide insights into the market behavior which are not available from static models. In particular: (1) markets that are required to balance supply and demand precisely at all times may be unstable if one supplier exhibits economies of scale and will be unstable if two suppliers exhibit this behavior. The instability is characterized by one or more positive eigenvalues. (2) Markets where some energy imbalance is allowed to accumulate can exhibit an instability, depending on the exact values of time constants and delays in the system. (3) Congestion can be helpful from the perspective of stability: a market can become unstable in the eigenvalue sense if congestion is removed. (4) A power system (with stable electromechanical dynamic behavior when considered by itself) and market (by itself stable) can, when analyzed jointly, exhibit unstable behavior. Some of the instabilities alluded here are nothing more than fluctuations in demands and prices. However, fluctuations are likely to require larger security margins, thus greater costs to operate the system.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 319\n",
      "Title: Solving power flow problems with a Matlab implementation of the Power System Applications Data Dictionary\n",
      "Abstract: This paper implements a power flow application and variations using the IEEE Power System Application Data Dictionary (PSADD) within a Matlab environment. It describes a number of useful data and implementation techniques for a variety of applications. The techniques include the use of very compact and efficient code for the computation of Power Transfer Distribution Factors (PTDF). PTDF are an important element of present and proposed congestion management strategies for power systems, particularly when these systems must operate in a deregulated environment.\n",
      "Content: Solving power flow problems with a Matlab implementation of the Power System Applications Data Dictionary This paper implements a power flow application and variations using the IEEE Power System Application Data Dictionary (PSADD) within a Matlab environment. It describes a number of useful data and implementation techniques for a variety of applications. The techniques include the use of very compact and efficient code for the computation of Power Transfer Distribution Factors (PTDF). PTDF are an important element of present and proposed congestion management strategies for power systems, particularly when these systems must operate in a deregulated environment.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 320\n",
      "Title: Controlling power systems with price signals\n",
      "Abstract: This paper revisits the possibility of controlling the power system entirely by means of price signals. It expands on notions introduced in an earlier paper and addresses several unresolved issues: problems with linear cost structures, response delays, varying costs, market power and stability problems caused by market/system interactions. The results suggest that control by price can, in fact, be made to work with some caveats.\n",
      "Content: Controlling power systems with price signals This paper revisits the possibility of controlling the power system entirely by means of price signals. It expands on notions introduced in an earlier paper and addresses several unresolved issues: problems with linear cost structures, response delays, varying costs, market power and stability problems caused by market/system interactions. The results suggest that control by price can, in fact, be made to work with some caveats.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 321\n",
      "Title: Confessions of an information worker: a critical analysis of information requirements discourse\n",
      "Abstract: This paper seeks to demonstrate the benefit of critical discourse analysis as a research approach for examining information systems development. Research has shown that eliciting user requirements is a critical activity of information systems development. However, the requirements phase is not only a key activity, it is highly problematic. Requirements determination is considered a process fraught with conflicting, inconsistent and competing viewpoints in which users and analysts do not share a “consensual domain,” thus barring them from reaching agreements about requirements. Therefore, analytic tools that recognize and examine requirements analysis as a polyphonic interaction hold much promise for improving requirements elicitation and analysis. Critical discourse analysis offers the tools to examine systematically the fundamental substance of requirements elicitation — interactional talk. This analysis employs sociolinguistic methods for specifying the linguistic features of different types of discourse units and the way they are tied together to create meaning, but also concerns itself with critically examining social context. In this paper requirements elicitation takes on the form of a “confessional” act where the individual verbalizes thoughts, intentions and consciousness. Findings show that during this ritual, discourse is revealed as a dialectic between two different domains of meaning, that of analyst and client. Analysts, in their official roles, propose a “frame” which conflicts with that proposed by clients during interviews. Changes in frames and deft face-saving work during interviews function to discursively produce and challenge client identities. The paper explores the tension between the frames proposed by the analyst and client during interviews which explains some of the frustrations and “gaps” which characterize this type of encounter. Issues of power inequality, identity formation, and symbolic control are presented as explanations of why competing frames are proposed and sustained while resisted by clients.\n",
      "Content: Confessions of an information worker: a critical analysis of information requirements discourse This paper seeks to demonstrate the benefit of critical discourse analysis as a research approach for examining information systems development. Research has shown that eliciting user requirements is a critical activity of information systems development. However, the requirements phase is not only a key activity, it is highly problematic. Requirements determination is considered a process fraught with conflicting, inconsistent and competing viewpoints in which users and analysts do not share a “consensual domain,” thus barring them from reaching agreements about requirements. Therefore, analytic tools that recognize and examine requirements analysis as a polyphonic interaction hold much promise for improving requirements elicitation and analysis. Critical discourse analysis offers the tools to examine systematically the fundamental substance of requirements elicitation — interactional talk. This analysis employs sociolinguistic methods for specifying the linguistic features of different types of discourse units and the way they are tied together to create meaning, but also concerns itself with critically examining social context. In this paper requirements elicitation takes on the form of a “confessional” act where the individual verbalizes thoughts, intentions and consciousness. Findings show that during this ritual, discourse is revealed as a dialectic between two different domains of meaning, that of analyst and client. Analysts, in their official roles, propose a “frame” which conflicts with that proposed by clients during interviews. Changes in frames and deft face-saving work during interviews function to discursively produce and challenge client identities. The paper explores the tension between the frames proposed by the analyst and client during interviews which explains some of the frustrations and “gaps” which characterize this type of encounter. Issues of power inequality, identity formation, and symbolic control are presented as explanations of why competing frames are proposed and sustained while resisted by clients.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 322\n",
      "Title: Examining technology, structure and identity during an Enterprise System implementation\n",
      "Abstract: Abstract.  This paper presents a longitudinal study of an Enterprise System (ES) implementation by critically examining the discursive context in which an ES implementation unfolds. The findings show that users strongly supported the ES in the earlier stage of implementation when the technology was an imaginary phenomenon. However, in later stages, when the technology is in use, user support was not consistent. In this phase, the ES produces loss of control and an inability to function as an arbiter of fairness (in allocating resources associated with the system), thereby directly challenging existing professional identities and roles. These outcomes, in turn, generate acts of resistance on the part of workers. Users reach inside the technology and reshape it by devising creative workarounds that produce a sense of reskilling to counter the deskilling produced by the loss of control and power. The analysis also shows that an ES is a complex social phenomenon that is intricately linked to and complicit in shaping organizational structure and identity. In particular, this study shows how technology, structure and identity are in a mutually constitutive relationship.\n",
      "Content: Examining technology, structure and identity during an Enterprise System implementation Abstract.  This paper presents a longitudinal study of an Enterprise System (ES) implementation by critically examining the discursive context in which an ES implementation unfolds. The findings show that users strongly supported the ES in the earlier stage of implementation when the technology was an imaginary phenomenon. However, in later stages, when the technology is in use, user support was not consistent. In this phase, the ES produces loss of control and an inability to function as an arbiter of fairness (in allocating resources associated with the system), thereby directly challenging existing professional identities and roles. These outcomes, in turn, generate acts of resistance on the part of workers. Users reach inside the technology and reshape it by devising creative workarounds that produce a sense of reskilling to counter the deskilling produced by the loss of control and power. The analysis also shows that an ES is a complex social phenomenon that is intricately linked to and complicit in shaping organizational structure and identity. In particular, this study shows how technology, structure and identity are in a mutually constitutive relationship.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 323\n",
      "Title: Algorithmic Processes of Social Alertness and Social Transmission:  How Bots Disseminate Information on Twitter\n",
      "Abstract: Despite increased empirical attention, theory on bots and how they act to disseminate information on social media remains poorly understood. Our study leverages the conduit brokerage perspective and the findings of a multiple case study to develop a novel framework of algorithmic conduit brokerage for understanding information dissemination by bots and the design choices that may influence their actions. Algorithmic conduit brokerage encompasses two intertwined processes. The first process, algorithmic social alertness, relies on bot activity to curate and reconfigure information. Algorithmic social alertness is significant because it involves action triggers that dictate the kinds of information being searched, discovered, and retrieved by bots. The second process, algorithmic social transmission, relies on bot activity to embellish and distribute the information curated. Algorithmic social transmission is important because it can broaden the reach of information disseminated by bots through increased discoverability and directed targeting. The two algorithmic conduit brokerage processes we offer are unique to bots and distinct from the original conceptualization of conduit brokerage, which is rooted in human activity. First, since bots lack the human ability of sensemaking and are instead fueled by automation and action triggers rather than by emotions, algorithmic conduit brokerage is more invariant and reliable than human conduit brokerage. Second, automation increases the speed and scale of information curation and transfer, making algorithmic conduit brokerage not only more consistent but also faster and more extensive. Third, algorithmic conduit brokerage includes a set of new concepts (e.g., action triggers and rapid scaling) that are specific to bots and therefore not applicable to human conduit brokerage.\n",
      "Content: Algorithmic Processes of Social Alertness and Social Transmission:  How Bots Disseminate Information on Twitter Despite increased empirical attention, theory on bots and how they act to disseminate information on social media remains poorly understood. Our study leverages the conduit brokerage perspective and the findings of a multiple case study to develop a novel framework of algorithmic conduit brokerage for understanding information dissemination by bots and the design choices that may influence their actions. Algorithmic conduit brokerage encompasses two intertwined processes. The first process, algorithmic social alertness, relies on bot activity to curate and reconfigure information. Algorithmic social alertness is significant because it involves action triggers that dictate the kinds of information being searched, discovered, and retrieved by bots. The second process, algorithmic social transmission, relies on bot activity to embellish and distribute the information curated. Algorithmic social transmission is important because it can broaden the reach of information disseminated by bots through increased discoverability and directed targeting. The two algorithmic conduit brokerage processes we offer are unique to bots and distinct from the original conceptualization of conduit brokerage, which is rooted in human activity. First, since bots lack the human ability of sensemaking and are instead fueled by automation and action triggers rather than by emotions, algorithmic conduit brokerage is more invariant and reliable than human conduit brokerage. Second, automation increases the speed and scale of information curation and transfer, making algorithmic conduit brokerage not only more consistent but also faster and more extensive. Third, algorithmic conduit brokerage includes a set of new concepts (e.g., action triggers and rapid scaling) that are specific to bots and therefore not applicable to human conduit brokerage.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 324\n",
      "Title: Trading team composition for the intraday multistock market\n",
      "Abstract: Automated traders operate market shares without human intervention. We propose a Trading Team based on atomic traders with opportunity detectors and simple effectors. The detectors signalize trading opportunities. For each trading signal, the effectors follow deterministic rules on when and what to trade in the market. The detectors are based on Partial Least Squares. We perform some trading experiments with twelve BM&FBovespa stocks. The empirical findings indicate that the proposed trading strategy reaches a 77.26% annualized profit, outperforming by 380.07% the chosen baseline strategy with a 16.07% profit. We also investigate Multistock Resolution Strategy (MSR) performance subject to brokerage commissions and income tax. Whenever the initial investment is at least US$ 50,000, the MSR strategy provides a profit of at least 38.63%.\n",
      "Content: Trading team composition for the intraday multistock market Automated traders operate market shares without human intervention. We propose a Trading Team based on atomic traders with opportunity detectors and simple effectors. The detectors signalize trading opportunities. For each trading signal, the effectors follow deterministic rules on when and what to trade in the market. The detectors are based on Partial Least Squares. We perform some trading experiments with twelve BM&FBovespa stocks. The empirical findings indicate that the proposed trading strategy reaches a 77.26% annualized profit, outperforming by 380.07% the chosen baseline strategy with a 16.07% profit. We also investigate Multistock Resolution Strategy (MSR) performance subject to brokerage commissions and income tax. Whenever the initial investment is at least US$ 50,000, the MSR strategy provides a profit of at least 38.63%.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 325\n",
      "Title: Where You Live Matters: Local Bank Competition, Online Marketplace Lending, and Disparity in Borrower Benefits\n",
      "Abstract: In the past decade, the proliferation of online marketplace lending has been disrupting the consumer credit market, especially personal loans used for debt consolidation. These lenders, for example, Lending Club, transcend the geographic boundaries within which local banks operate and offer homogeneous access and terms to borrowers. However, the ultimate benefits borrowers derive from marketplace lending can differ significantly because local alternatives may be used to replace marketplace loans when they are available and favorable. Correspondingly, if local bank competition drives the substitution of an existing marketplace loan with a traditional bank loan, the promise of equal benefits to all borrowers from marketplace lending is unlikely to fully materialize. This competitive dynamic also has implications for policy making, particularly in judging the ramifications of bank mergers and acquisitions. We utilize data from Lending Club, a major peer-to-peer (P2P) lending platform, to study whether local market structure drives heterogeneous incentives of traditional banks to convert borrowers of marketplace lending to conventional bank loans. The results indicate that a borrower who resides in a more competitive market is more likely to pay off a P2P loan early by making a large, one-time payment compared with a borrower from a less competitive market. Thus, borrowers from different markets do not benefit equally from online marketplace lending disrupting the consumer credit market. Our study has implications for online marketplace lending, other FinTech-based markets, and the consumer credit market in general.\n",
      "Content: Where You Live Matters: Local Bank Competition, Online Marketplace Lending, and Disparity in Borrower Benefits In the past decade, the proliferation of online marketplace lending has been disrupting the consumer credit market, especially personal loans used for debt consolidation. These lenders, for example, Lending Club, transcend the geographic boundaries within which local banks operate and offer homogeneous access and terms to borrowers. However, the ultimate benefits borrowers derive from marketplace lending can differ significantly because local alternatives may be used to replace marketplace loans when they are available and favorable. Correspondingly, if local bank competition drives the substitution of an existing marketplace loan with a traditional bank loan, the promise of equal benefits to all borrowers from marketplace lending is unlikely to fully materialize. This competitive dynamic also has implications for policy making, particularly in judging the ramifications of bank mergers and acquisitions. We utilize data from Lending Club, a major peer-to-peer (P2P) lending platform, to study whether local market structure drives heterogeneous incentives of traditional banks to convert borrowers of marketplace lending to conventional bank loans. The results indicate that a borrower who resides in a more competitive market is more likely to pay off a P2P loan early by making a large, one-time payment compared with a borrower from a less competitive market. Thus, borrowers from different markets do not benefit equally from online marketplace lending disrupting the consumer credit market. Our study has implications for online marketplace lending, other FinTech-based markets, and the consumer credit market in general.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 326\n",
      "Title: Shared Prosperity (or Lack Thereof) in the Sharing Economy\n",
      "Abstract: This paper examines the potential economic spillover effects of a home-sharing platform—Airbnb—on the growth of a complimentary local service—restaurants. By circumventing traditional land-use regulations and providing access to underutilized inventory, Airbnb attracts visitors to locales that are not traditional tourist destinations. Although visitors generally bring significant spending power, it is unclear whether visitors use Airbnb primarily for lodging and thus do not contribute to the adjacent economy. To evaluate this, we focus on the impact of Airbnb on the restaurant employment growth across locales in New York City (NYC). Specifically, we focus on areas in NYC that did not attract a significant tourist volume prior to the emergence of a home-sharing service. Our results indicate a salient and economically significant positive spillover effect on restaurant job growth in an average NYC locality. A one-percentage-point increase in the intensity of Airbnb activity (Airbnb reviews per household) leads to approximately 1.7% restaurant employment growth. Since home-sharing visitors are lodging in areas that are not accustomed to tourists, we also investigate the demographic and market-structure-related heterogeneity of our results. Notably, restaurants in areas with a relatively high number of White residents disproportionately benefit from the economic spillover of Airbnb activity, whereas the impact in majority Black areas is not statistically significant. We validate the underlying mechanism behind the results by evaluating the impact of Airbnb on Yelp visitor reviews, revealing that areas with increased Airbnb activity experience a surge in their share of NYC visitor reviews. This result is further validated by evaluating the impact of a unique Airbnb neighborhood-level exogenous policy recently implemented in New Orleans.\n",
      "Content: Shared Prosperity (or Lack Thereof) in the Sharing Economy This paper examines the potential economic spillover effects of a home-sharing platform—Airbnb—on the growth of a complimentary local service—restaurants. By circumventing traditional land-use regulations and providing access to underutilized inventory, Airbnb attracts visitors to locales that are not traditional tourist destinations. Although visitors generally bring significant spending power, it is unclear whether visitors use Airbnb primarily for lodging and thus do not contribute to the adjacent economy. To evaluate this, we focus on the impact of Airbnb on the restaurant employment growth across locales in New York City (NYC). Specifically, we focus on areas in NYC that did not attract a significant tourist volume prior to the emergence of a home-sharing service. Our results indicate a salient and economically significant positive spillover effect on restaurant job growth in an average NYC locality. A one-percentage-point increase in the intensity of Airbnb activity (Airbnb reviews per household) leads to approximately 1.7% restaurant employment growth. Since home-sharing visitors are lodging in areas that are not accustomed to tourists, we also investigate the demographic and market-structure-related heterogeneity of our results. Notably, restaurants in areas with a relatively high number of White residents disproportionately benefit from the economic spillover of Airbnb activity, whereas the impact in majority Black areas is not statistically significant. We validate the underlying mechanism behind the results by evaluating the impact of Airbnb on Yelp visitor reviews, revealing that areas with increased Airbnb activity experience a surge in their share of NYC visitor reviews. This result is further validated by evaluating the impact of a unique Airbnb neighborhood-level exogenous policy recently implemented in New Orleans.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 327\n",
      "Title: Empirical studies of geographically distributed agile development communication challenges: A systematic review\n",
      "Abstract: There is increasing interest in studying and applying geographically distributed agile development (GDAD). Much has been published on GDAD communication. There is a need to systematically review and synthesize the literature on GDAD communication challenges. Using the SLR approach and applying customized search criteria derived from the research questions, 21 relevant empirical studies were identified and reviewed in this paper. The data from these papers were extracted to identify communication challenges and the techniques used to overcome these challenges. The findings of this research serve as a resource for GDAD practitioners and researchers when setting future research priorities and directions.\n",
      "Content: Empirical studies of geographically distributed agile development communication challenges: A systematic review There is increasing interest in studying and applying geographically distributed agile development (GDAD). Much has been published on GDAD communication. There is a need to systematically review and synthesize the literature on GDAD communication challenges. Using the SLR approach and applying customized search criteria derived from the research questions, 21 relevant empirical studies were identified and reviewed in this paper. The data from these papers were extracted to identify communication challenges and the techniques used to overcome these challenges. The findings of this research serve as a resource for GDAD practitioners and researchers when setting future research priorities and directions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 328\n",
      "Title: Ontology-supported case-based reasoning approach for intelligent m-Government emergency response services\n",
      "Abstract: There is a critical need to develop a mobile-based emergency response system (MERS) to help reduce risks in emergency situations. Existing systems only provide short message service (SMS) notifications, and the decision support is weak, especially in man-made disaster situations. This paper presents a MERS ontology-supported case-based reasoning (OS-CBR) method, with implementation, to support emergency decision makers to effectively respond to emergencies. The advantages of the OS-CBR approach is that it builds a case retrieving process, which provides a more convenient system for decision support based on knowledge from, and solutions provided for past disaster events. The OS-CBR approach includes a set of algorithms that have been successfully implemented in four components: data acquisition; ontology; knowledge base; and reasoning; as a sub-system of the MERS framework. A set of experiments and case studies validated the OS-CBR approach and application, and demonstrate its efficiency.\n",
      "Content: Ontology-supported case-based reasoning approach for intelligent m-Government emergency response services There is a critical need to develop a mobile-based emergency response system (MERS) to help reduce risks in emergency situations. Existing systems only provide short message service (SMS) notifications, and the decision support is weak, especially in man-made disaster situations. This paper presents a MERS ontology-supported case-based reasoning (OS-CBR) method, with implementation, to support emergency decision makers to effectively respond to emergencies. The advantages of the OS-CBR approach is that it builds a case retrieving process, which provides a more convenient system for decision support based on knowledge from, and solutions provided for past disaster events. The OS-CBR approach includes a set of algorithms that have been successfully implemented in four components: data acquisition; ontology; knowledge base; and reasoning; as a sub-system of the MERS framework. A set of experiments and case studies validated the OS-CBR approach and application, and demonstrate its efficiency.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 329\n",
      "Title: The world and business computing in 2051\n",
      "Abstract: This paper projects the future of information technology within the context of the business and social environment in the mid 21st century. Some time in the next two decades we will enter our fourth industrial era, fueled by evolution in power, genetics, space exploration and information technologies. Unlike other industrial revolutions, this will be attended by widespread consolidations in industries as diverse as utility, retail, pharmaceuticals as well as the government. The trade wing of the UN, the United Nations Trade Organization (UNTO) is established to deal with global issues. The UNTO passes a number of resolutions including # 1/10 which deals with a uniform system of personal identification for all individuals. Due to free trade facilitated by electronic bureaucracies and global transportation grids, GNP growth rates triple. However, the consolidations will lead to the disappearance of clerical, administrative and management employees, leaving only the top management and highly skilled professionals. These changes will result in companies outsourcing all their routine processing to transaction and data centres. Employees will work from well equipped IT centres. The internet is in its third incarnation and is supported by a comprehensive web intelligence to answer all possible questions. Other technological changes include Terahertz computers, Personal identification Devices (PID), AI chips and hierarchical databases with relational interfaces. The cornucopia resulting from progress will lead to a more enlightened society. However, the stress of change will result in an increased degree of philosophical inquiry.\n",
      "Content: The world and business computing in 2051 This paper projects the future of information technology within the context of the business and social environment in the mid 21st century. Some time in the next two decades we will enter our fourth industrial era, fueled by evolution in power, genetics, space exploration and information technologies. Unlike other industrial revolutions, this will be attended by widespread consolidations in industries as diverse as utility, retail, pharmaceuticals as well as the government. The trade wing of the UN, the United Nations Trade Organization (UNTO) is established to deal with global issues. The UNTO passes a number of resolutions including # 1/10 which deals with a uniform system of personal identification for all individuals. Due to free trade facilitated by electronic bureaucracies and global transportation grids, GNP growth rates triple. However, the consolidations will lead to the disappearance of clerical, administrative and management employees, leaving only the top management and highly skilled professionals. These changes will result in companies outsourcing all their routine processing to transaction and data centres. Employees will work from well equipped IT centres. The internet is in its third incarnation and is supported by a comprehensive web intelligence to answer all possible questions. Other technological changes include Terahertz computers, Personal identification Devices (PID), AI chips and hierarchical databases with relational interfaces. The cornucopia resulting from progress will lead to a more enlightened society. However, the stress of change will result in an increased degree of philosophical inquiry.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 330\n",
      "Title: Business-IT alignment as a coevolution process: An empirical study\n",
      "Abstract: In this paper, we provide a detailed insight into the complex coevolution dynamics that shape the alignment process by analyzing how different mechanisms and factors are mutually related in complex networks of feedback loops. We combine insights from the literature on alignment as a (coevolution) process with literature on alignment as a state to identify the different components of the organization’s socio-technical system that influence alignment, the relationships between these components, and the role that different factors play. In our empirical analysis (based multiple case studies) we then focus on the actual interplay between relevant factors. Using a causal loop diagramming approach - based on system dynamics – we analyze how these factors mutually influence each other through various feedback loops and thus shape the alignment process. We extend previous literature on the alignment process by identifying the way that the complex interplay between different factors shapes this process. By identifying the feedback loops between relevant factors, we also provide more insight into the complex bottom-up and top-down dynamics that shape the process, and that provide explanations for why this process is charac­ terized by transitions between alignment and misalignment. For practice, our paper provides a deeper understanding of the alignment process, which is a precondition for improving alignment practices in organisations.\n",
      "Content: Business-IT alignment as a coevolution process: An empirical study In this paper, we provide a detailed insight into the complex coevolution dynamics that shape the alignment process by analyzing how different mechanisms and factors are mutually related in complex networks of feedback loops. We combine insights from the literature on alignment as a (coevolution) process with literature on alignment as a state to identify the different components of the organization’s socio-technical system that influence alignment, the relationships between these components, and the role that different factors play. In our empirical analysis (based multiple case studies) we then focus on the actual interplay between relevant factors. Using a causal loop diagramming approach - based on system dynamics – we analyze how these factors mutually influence each other through various feedback loops and thus shape the alignment process. We extend previous literature on the alignment process by identifying the way that the complex interplay between different factors shapes this process. By identifying the feedback loops between relevant factors, we also provide more insight into the complex bottom-up and top-down dynamics that shape the process, and that provide explanations for why this process is charac­ terized by transitions between alignment and misalignment. For practice, our paper provides a deeper understanding of the alignment process, which is a precondition for improving alignment practices in organisations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 331\n",
      "Title: Capabilities and metrics in DevOps: A design science study\n",
      "Abstract: Customer demands, competition, regulatory environments, and sophisticated external threats have all increased the importance of DevOps in IT organizations. However, DevOps adoption is still uneven, emphasizing the need to provide management with relevant IS data and insights. Regrettably, there is a measurement inefficiency between these capabilities.\n",
      "Content: Capabilities and metrics in DevOps: A design science study Customer demands, competition, regulatory environments, and sophisticated external threats have all increased the importance of DevOps in IT organizations. However, DevOps adoption is still uneven, emphasizing the need to provide management with relevant IS data and insights. Regrettably, there is a measurement inefficiency between these capabilities.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 332\n",
      "Title: A socio-cognitive interpretation of the potential effects of downsizing on software quality performance\n",
      "Abstract: Organizational downsizing research indicates that downsizing does not always realize its strategic intent and may, in fact, have a detrimental impact on organizational performance. In this paper, we extend the notion that downsizing negatively impacts performance and argue that organizational downsizing can potentially be detrimental to software quality performance. Using social cognitive theory (SCT), we primarily interpret the negative impacts of downsizing on software quality performance by arguing that downsizing results in a realignment of social networks (environmental factors), thereby affecting the self-efficacy and outcome expectations of a software professional (personal factors), which, in turn, affect software quality performance (outcome of behaviour undertaken). We synthesize relevant literature from the software quality, SCT and downsizing research streams and develop a conceptual model. Two major impacts of downsizing are hypothesized in the conceptual model. First, downsizing destroys formal and informal social networks in organizations, which, in turn, negatively impacts software developers' self-efficacy and outcome expectations through their antecedents, with consequent negative impacts on software development process efficiency and software product quality, the two major components of software quality performance. Second, downsizing negatively affects antecedents of software development process efficiency, namely top management leadership, management infrastructure sophistication, process management efficacy and stakeholder participation with consequent negative impacts on software quality performance. This theoretically grounded discourse can help demonstrate how organizational downsizing can potentially impact software quality performance through key intervening constructs. We also discuss how downsizing and other intervening constructs can be managed to mitigate the negative impacts of downsizing on software quality performance.\n",
      "Content: A socio-cognitive interpretation of the potential effects of downsizing on software quality performance Organizational downsizing research indicates that downsizing does not always realize its strategic intent and may, in fact, have a detrimental impact on organizational performance. In this paper, we extend the notion that downsizing negatively impacts performance and argue that organizational downsizing can potentially be detrimental to software quality performance. Using social cognitive theory (SCT), we primarily interpret the negative impacts of downsizing on software quality performance by arguing that downsizing results in a realignment of social networks (environmental factors), thereby affecting the self-efficacy and outcome expectations of a software professional (personal factors), which, in turn, affect software quality performance (outcome of behaviour undertaken). We synthesize relevant literature from the software quality, SCT and downsizing research streams and develop a conceptual model. Two major impacts of downsizing are hypothesized in the conceptual model. First, downsizing destroys formal and informal social networks in organizations, which, in turn, negatively impacts software developers' self-efficacy and outcome expectations through their antecedents, with consequent negative impacts on software development process efficiency and software product quality, the two major components of software quality performance. Second, downsizing negatively affects antecedents of software development process efficiency, namely top management leadership, management infrastructure sophistication, process management efficacy and stakeholder participation with consequent negative impacts on software quality performance. This theoretically grounded discourse can help demonstrate how organizational downsizing can potentially impact software quality performance through key intervening constructs. We also discuss how downsizing and other intervening constructs can be managed to mitigate the negative impacts of downsizing on software quality performance.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 333\n",
      "Title: Conviviality of Internet social networks: An exploratory study of Internet campaigns in Iran\n",
      "Abstract: In this study, we focus on the relationship between Internet social networks and societal change by examining case studies of the impact of Internet-based campaigns in Iran. Ivan Illich's theory of ‘Conviviality of Tools’ enables an analysis of the conviviality of the Internet. Subsequently, this conceptual lens is used to examine empirical data from two Internet-based campaigns. The paper contributes theoretical and practical implications regarding conviviality of Internet social networks and the accomplishment of conviviality in society. Our findings show that Internet conviviality cannot be treated as an independent variable with deterministic outcomes on society, but as a technology that is shaped by ongoing economic and political forces. The Iranian Internet social networks are not universally accessible, frequently induce fragmented, nonsensical, and enraged discussion and its potential as a tool of liberation is tempered by the Iranian government adaption of systems of surveillance and censorship. We argue that the findings of this study have some general implications of value to researchers studying computerisation movements and Internet social networks in other countries.\n",
      "Content: Conviviality of Internet social networks: An exploratory study of Internet campaigns in Iran In this study, we focus on the relationship between Internet social networks and societal change by examining case studies of the impact of Internet-based campaigns in Iran. Ivan Illich's theory of ‘Conviviality of Tools’ enables an analysis of the conviviality of the Internet. Subsequently, this conceptual lens is used to examine empirical data from two Internet-based campaigns. The paper contributes theoretical and practical implications regarding conviviality of Internet social networks and the accomplishment of conviviality in society. Our findings show that Internet conviviality cannot be treated as an independent variable with deterministic outcomes on society, but as a technology that is shaped by ongoing economic and political forces. The Iranian Internet social networks are not universally accessible, frequently induce fragmented, nonsensical, and enraged discussion and its potential as a tool of liberation is tempered by the Iranian government adaption of systems of surveillance and censorship. We argue that the findings of this study have some general implications of value to researchers studying computerisation movements and Internet social networks in other countries.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 334\n",
      "Title: Searching for information in a time-pressured setting: experiences with a Text-based and an Image-based decision support system\n",
      "Abstract: Searching for the right information and making quick, accurate decisions within time-pressured settings is often non-trivial. We contrast the relative efficacies of written English (Text) and a more concise, compact communication mode (Image) for information search and decision making by using a financial incentive scheme to apply implicit time pressure on subjects. We found that, while Image users earned as much as Text users, they achieved this earnings parity by following speedier but less accurate strategies. We conclude with thoughts on possible refinements to our work that could steer subjects in the ideal direction of fast, accurate, lucrative decisions with languages like Image.\n",
      "Content: Searching for information in a time-pressured setting: experiences with a Text-based and an Image-based decision support system Searching for the right information and making quick, accurate decisions within time-pressured settings is often non-trivial. We contrast the relative efficacies of written English (Text) and a more concise, compact communication mode (Image) for information search and decision making by using a financial incentive scheme to apply implicit time pressure on subjects. We found that, while Image users earned as much as Text users, they achieved this earnings parity by following speedier but less accurate strategies. We conclude with thoughts on possible refinements to our work that could steer subjects in the ideal direction of fast, accurate, lucrative decisions with languages like Image.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 335\n",
      "Title: A Cost–Benefit framework for online management of a metacomputing system\n",
      "Abstract: Managing a large collection of networked machines, with a series of incoming jobs, requires that the jobs be assigned to machines wisely. A new approach to this problem is presented, inspired by economic principles: the Cost–Benefit framework. This framework simplifies complex assignment and admission control decisions, and performs well in practice. We demonstrate this framework in the context of an Internet-wide market for computational services and verify its utility for a classic network of workstations.\n",
      "Content: A Cost–Benefit framework for online management of a metacomputing system Managing a large collection of networked machines, with a series of incoming jobs, requires that the jobs be assigned to machines wisely. A new approach to this problem is presented, inspired by economic principles: the Cost–Benefit framework. This framework simplifies complex assignment and admission control decisions, and performs well in practice. We demonstrate this framework in the context of an Internet-wide market for computational services and verify its utility for a classic network of workstations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 336\n",
      "Title: Customer-oriented catalog segmentation: Effective solution approaches\n",
      "Abstract: We consider in this paper the customer-oriented catalog segmentation problem that consists of designing K catalogs, each of size r products that maximize the number of covered customers. A customer is covered if he/she has interest in at least the specified minimum number of products in one of the catalogs. The problem addresses the crucial issue of the design of the actual contents of the catalogs that serves as a back-end to catalog production for the purpose of more focused design of catalogs as a targeted marketing tool. We developed two algorithms to solve the problem. Results of an extensive computational study using real and synthetic data sets show that one of the proposed algorithms outperforms the state-of-the-art algorithm found in the literature in terms of customer coverage, resulting potentially in significant increase in organization profit. In the spirit of the guidance role that a Decision Support System (DSS) should play by recommending alternative, satisfactory solutions to the decision maker, the prototype of a DSS integrating all three algorithms is presented to provide the decision maker with an easy-to-use, yet powerful tool to examine various catalog design options and their implications on the contents of the catalogs and the clusters of covered customers.\n",
      "Content: Customer-oriented catalog segmentation: Effective solution approaches We consider in this paper the customer-oriented catalog segmentation problem that consists of designing K catalogs, each of size r products that maximize the number of covered customers. A customer is covered if he/she has interest in at least the specified minimum number of products in one of the catalogs. The problem addresses the crucial issue of the design of the actual contents of the catalogs that serves as a back-end to catalog production for the purpose of more focused design of catalogs as a targeted marketing tool. We developed two algorithms to solve the problem. Results of an extensive computational study using real and synthetic data sets show that one of the proposed algorithms outperforms the state-of-the-art algorithm found in the literature in terms of customer coverage, resulting potentially in significant increase in organization profit. In the spirit of the guidance role that a Decision Support System (DSS) should play by recommending alternative, satisfactory solutions to the decision maker, the prototype of a DSS integrating all three algorithms is presented to provide the decision maker with an easy-to-use, yet powerful tool to examine various catalog design options and their implications on the contents of the catalogs and the clusters of covered customers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 337\n",
      "Title: Dare to share: Protecting sensitive knowledge with data sanitization\n",
      "Abstract: Data sanitization is a process that is used to promote sharing of transactional databases among organizations while alleviating concerns of individual organizations by preserving confidentiality of their sensitive knowledge in the form of sensitive association rules. It hides the frequent itemsets corresponding to the sensitive association rules that contain sensitive knowledge by modifying the sensitive transactions that contain those itemsets. This process is guided by the need to minimize the impact on the data utility of the sanitized database by allowing mining as much as possible of the non-sensitive knowledge in the form non-sensitive association rules from the sanitized database. We propose three heuristic approaches for the sanitization problem. Results from extensive tests conducted on publicly available real datasets indicate that the approaches are effective and outperform a previous approach in terms of data utility at the expense of computational speed. The proposed approaches sanitize also the databases with great data accuracy, thus resulting in little distortion of the released databases. We recommend that the database owner sanitize the database using the third proposed hybrid approach.\n",
      "Content: Dare to share: Protecting sensitive knowledge with data sanitization Data sanitization is a process that is used to promote sharing of transactional databases among organizations while alleviating concerns of individual organizations by preserving confidentiality of their sensitive knowledge in the form of sensitive association rules. It hides the frequent itemsets corresponding to the sensitive association rules that contain sensitive knowledge by modifying the sensitive transactions that contain those itemsets. This process is guided by the need to minimize the impact on the data utility of the sanitized database by allowing mining as much as possible of the non-sensitive knowledge in the form non-sensitive association rules from the sanitized database. We propose three heuristic approaches for the sanitization problem. Results from extensive tests conducted on publicly available real datasets indicate that the approaches are effective and outperform a previous approach in terms of data utility at the expense of computational speed. The proposed approaches sanitize also the databases with great data accuracy, thus resulting in little distortion of the released databases. We recommend that the database owner sanitize the database using the third proposed hybrid approach.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 338\n",
      "Title: Optimal input/output reduction in production processes\n",
      "Abstract: While conventional Data Envelopment Analysis (DEA) models set targets for each operational unit, this paper considers the problem of input/output reduction in a centralized decision making environment. The purpose of this paper is to develop an approach to input/output reduction problem that typically occurs in organizations with a centralized decision-making environment. This paper shows that DEA can make an important contribution to this problem and discusses how DEA-based model can be used to determine an optimal input/output reduction plan. An application in banking sector with limitation in IT investment shows the usefulness of the proposed method.\n",
      "Content: Optimal input/output reduction in production processes While conventional Data Envelopment Analysis (DEA) models set targets for each operational unit, this paper considers the problem of input/output reduction in a centralized decision making environment. The purpose of this paper is to develop an approach to input/output reduction problem that typically occurs in organizations with a centralized decision-making environment. This paper shows that DEA can make an important contribution to this problem and discusses how DEA-based model can be used to determine an optimal input/output reduction plan. An application in banking sector with limitation in IT investment shows the usefulness of the proposed method.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 339\n",
      "Title: Technological Entitlement: It’s My Technology and I’ll (Ab)Use It How I Want To\n",
      "Abstract: Entitlement has been identified as a potentially valuable employee characteristic in the prediction of computer abuse but has not been studied systematically in the IS domain. We introduce the construct of technological entitlement as the persistent sense of being more deserving of technological resources, uses, and privileges compared to other employees. Adapting a model of general entitlement to the work technology context, we theorize that technological entitlement predicts computer abuse and that this relationship is amplified by perceptions of technology restriction. After developing and validating a scale to measure technological entitlement, we conduct three studies with working adult samples to test our hypotheses. In Study 1 (n = 187), using a behavioral design, we find that technological entitlement predicts computer abuse behavior (beyond general entitlement) and that this relationship is stronger when employees perceive organizational restrictions on technology usage. We replicate these findings in Study 2 (n = 339) with an experiment. In Study 3 (n = 156), we manipulate the context of restrictiveness within our experimental vignette to establish the generalizability of our moderator. We discuss how technological entitlement helps explain existing inconsistencies in the effectiveness of deterrence measures as well as other theoretical and practical implications of our work.\n",
      "Content: Technological Entitlement: It’s My Technology and I’ll (Ab)Use It How I Want To Entitlement has been identified as a potentially valuable employee characteristic in the prediction of computer abuse but has not been studied systematically in the IS domain. We introduce the construct of technological entitlement as the persistent sense of being more deserving of technological resources, uses, and privileges compared to other employees. Adapting a model of general entitlement to the work technology context, we theorize that technological entitlement predicts computer abuse and that this relationship is amplified by perceptions of technology restriction. After developing and validating a scale to measure technological entitlement, we conduct three studies with working adult samples to test our hypotheses. In Study 1 (n = 187), using a behavioral design, we find that technological entitlement predicts computer abuse behavior (beyond general entitlement) and that this relationship is stronger when employees perceive organizational restrictions on technology usage. We replicate these findings in Study 2 (n = 339) with an experiment. In Study 3 (n = 156), we manipulate the context of restrictiveness within our experimental vignette to establish the generalizability of our moderator. We discuss how technological entitlement helps explain existing inconsistencies in the effectiveness of deterrence measures as well as other theoretical and practical implications of our work.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 340\n",
      "Title: Effects of structural and trait competitiveness stimulated by points and leaderboards on user engagement and performance growth: A natural experiment with gamification in an informal learning environment\n",
      "Abstract: Rooted in theories of competitiveness and social comparison, we model the effects of users’ structural and trait competitiveness on their engagement and performance growth in an informal learning environment. We hypothesise that game elements of points and leaderboards stimulate users’ structural competitiveness, which affects users’ engagement and has an inverted-U effect on performance growth. We further hypothesise that these effects are stronger among individuals with higher trait competitiveness. We tested our hypotheses using data from a natural experiment conducted over 300 days on 88,310 unique users who made 215,920 game interactions within the Cyber Detectives exhibit at the Tech Interactive museum in California. Our results are based on two objective measures of trait-competitiveness as both behaviour and outcome (percentile ranking on total time spent and number of badges earned, respectively), multiple objective measures of user engagement (time spent per attempt, number of reattempts, and daily user attempts), and an objective measure of performance growth (points). Results provide overall support to our hypotheses. We contribute to the gamification literature by providing strong causal evidence of points and leaderboards triggering structural and trait competitiveness, which interact to affect both engagement and performance growth in informal learning contexts.\n",
      "Content: Effects of structural and trait competitiveness stimulated by points and leaderboards on user engagement and performance growth: A natural experiment with gamification in an informal learning environment Rooted in theories of competitiveness and social comparison, we model the effects of users’ structural and trait competitiveness on their engagement and performance growth in an informal learning environment. We hypothesise that game elements of points and leaderboards stimulate users’ structural competitiveness, which affects users’ engagement and has an inverted-U effect on performance growth. We further hypothesise that these effects are stronger among individuals with higher trait competitiveness. We tested our hypotheses using data from a natural experiment conducted over 300 days on 88,310 unique users who made 215,920 game interactions within the Cyber Detectives exhibit at the Tech Interactive museum in California. Our results are based on two objective measures of trait-competitiveness as both behaviour and outcome (percentile ranking on total time spent and number of badges earned, respectively), multiple objective measures of user engagement (time spent per attempt, number of reattempts, and daily user attempts), and an objective measure of performance growth (points). Results provide overall support to our hypotheses. We contribute to the gamification literature by providing strong causal evidence of points and leaderboards triggering structural and trait competitiveness, which interact to affect both engagement and performance growth in informal learning contexts.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 341\n",
      "Title: An extension of the technology acceptance model in an ERP implementation environment\n",
      "Abstract: This paper presents an extension to the technology acceptance model (TAM) and empirically examines it in an enterprise resource planning (ERP) implementation environment. The study evaluated the impact of one belief construct (shared beliefs in the benefits of a technology) and two widely recognized technology implementation success factors (training and communication) on the perceived usefulness and perceived ease of use during technology implementation. Shared beliefs refer to the beliefs that organizational participants share with their peers and superiors on the benefits of the ERP system. Using data gathered from the implementation of an ERP system, we showed that both training and project communication influence the shared beliefs that users form about the benefits of the technology and that the shared beliefs influence the perceived usefulness and ease of use of the technology. Thus, we provided empirical and theoretical support for the use of managerial interventions, such as training and communication, to influence the acceptance of technology, since perceived usefulness and ease of use contribute to behavioral intention to use the technology.\n",
      "Content: An extension of the technology acceptance model in an ERP implementation environment This paper presents an extension to the technology acceptance model (TAM) and empirically examines it in an enterprise resource planning (ERP) implementation environment. The study evaluated the impact of one belief construct (shared beliefs in the benefits of a technology) and two widely recognized technology implementation success factors (training and communication) on the perceived usefulness and perceived ease of use during technology implementation. Shared beliefs refer to the beliefs that organizational participants share with their peers and superiors on the benefits of the ERP system. Using data gathered from the implementation of an ERP system, we showed that both training and project communication influence the shared beliefs that users form about the benefits of the technology and that the shared beliefs influence the perceived usefulness and ease of use of the technology. Thus, we provided empirical and theoretical support for the use of managerial interventions, such as training and communication, to influence the acceptance of technology, since perceived usefulness and ease of use contribute to behavioral intention to use the technology.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 342\n",
      "Title: Discuss practical importance of results based on interval estimates and p-value functions, not only on point estimates and null p-values\n",
      "Abstract:  It has long been argued that we need to consider much more than an observed point estimate and a p-value to understand statistical results. One of the most persistent misconceptions about p-values is that they are necessarily calculated assuming a null hypothesis of no effect is true. Instead, p-values can and should be calculated for multiple hypothesized values for the effect size. For example, a p-value function allows us to visualize results continuously by examining how the p-value varies as we move across possible effect sizes. For more focused discussions, a 95% confidence interval shows the subset of possible effect sizes that have p-values larger than 0.05 as calculated from the same data and the same background statistical assumptions. In this sense a confidence interval can be taken as showing the effect sizes that are most compatible with the data, given the assumptions, and thus may be better termed a compatibility interval. The question that should then be asked is whether any or all of the effect sizes within the interval are substantial enough to be of practical importance. et al. (2022)  advise looking at the practical importance of an effect estimate, keeping in mind its uncertainty, rather than only describing results as 'statistically significant' or 'nonsignificant'. We applaud this and many of their other recommendations, for example that single p-values should be given with sensible precision and not be degraded to stars, letters, or binary inequalities ('p < 0.05') and that we should avoid using the phrase 'statistically significant' entirely. All of this is consistent with over 70 years of calls by many statistical writers to emphasize interval estimates over statistical tests (e.g. Altman et al. Sen \n",
      "Content: Discuss practical importance of results based on interval estimates and p-value functions, not only on point estimates and null p-values  It has long been argued that we need to consider much more than an observed point estimate and a p-value to understand statistical results. One of the most persistent misconceptions about p-values is that they are necessarily calculated assuming a null hypothesis of no effect is true. Instead, p-values can and should be calculated for multiple hypothesized values for the effect size. For example, a p-value function allows us to visualize results continuously by examining how the p-value varies as we move across possible effect sizes. For more focused discussions, a 95% confidence interval shows the subset of possible effect sizes that have p-values larger than 0.05 as calculated from the same data and the same background statistical assumptions. In this sense a confidence interval can be taken as showing the effect sizes that are most compatible with the data, given the assumptions, and thus may be better termed a compatibility interval. The question that should then be asked is whether any or all of the effect sizes within the interval are substantial enough to be of practical importance. et al. (2022)  advise looking at the practical importance of an effect estimate, keeping in mind its uncertainty, rather than only describing results as 'statistically significant' or 'nonsignificant'. We applaud this and many of their other recommendations, for example that single p-values should be given with sensible precision and not be degraded to stars, letters, or binary inequalities ('p < 0.05') and that we should avoid using the phrase 'statistically significant' entirely. All of this is consistent with over 70 years of calls by many statistical writers to emphasize interval estimates over statistical tests (e.g. Altman et al. Sen \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 343\n",
      "Title: Exploring the impact of socio-technical core-periphery structures in open source software development\n",
      "Abstract: In this paper we apply the social network concept of core-periphery structure to the socio-technical structure of a software development team. We propose a socio-technical pattern that can be used to locate emerging coordination problems in Open Source projects. With the help of our tool and method called TESNA, we demonstrate a method to monitor the socio-technical core-periphery movement in Open Source projects. We then study the impact of different core-periphery movements on Open Source projects. We conclude that a steady core-periphery shift towards the core is beneficial to the project, whereas shifts away from the core are clearly not good. Furthermore, oscillatory shifts towards and away from the core can be considered as an indication of the instability of the project. Such an analysis can provide developers with a good insight into the health of an Open Source project. Researchers can gain from the pattern theory, and from the method we use to study the core-periphery movements.\n",
      "Content: Exploring the impact of socio-technical core-periphery structures in open source software development In this paper we apply the social network concept of core-periphery structure to the socio-technical structure of a software development team. We propose a socio-technical pattern that can be used to locate emerging coordination problems in Open Source projects. With the help of our tool and method called TESNA, we demonstrate a method to monitor the socio-technical core-periphery movement in Open Source projects. We then study the impact of different core-periphery movements on Open Source projects. We conclude that a steady core-periphery shift towards the core is beneficial to the project, whereas shifts away from the core are clearly not good. Furthermore, oscillatory shifts towards and away from the core can be considered as an indication of the instability of the project. Such an analysis can provide developers with a good insight into the health of an Open Source project. Researchers can gain from the pattern theory, and from the method we use to study the core-periphery movements.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 344\n",
      "Title: OSPM: A design methodology for open strategic planning\n",
      "Abstract: This study employs a design science perspective to propose a methodology for open strategic planning (OSP). Habermas’ discourse theory and Bryson’s strategy change cycle are used as informing kernel theories. A methodology is proposed to satisfy the requirements retrieved from the kernel theories. The proposed methodology contains modules for a planning system and a planning process. Design principles are explained through a blueprint of the system and process. The proposed methodology is applied and evaluated in two cases. Contributions to the literature involve extending the literature on OSP to an applicable methodology with guidelines on how to implement open strategy.\n",
      "Content: OSPM: A design methodology for open strategic planning This study employs a design science perspective to propose a methodology for open strategic planning (OSP). Habermas’ discourse theory and Bryson’s strategy change cycle are used as informing kernel theories. A methodology is proposed to satisfy the requirements retrieved from the kernel theories. The proposed methodology contains modules for a planning system and a planning process. Design principles are explained through a blueprint of the system and process. The proposed methodology is applied and evaluated in two cases. Contributions to the literature involve extending the literature on OSP to an applicable methodology with guidelines on how to implement open strategy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 345\n",
      "Title: Winner's curse and parallel sales channels—Online auctions linked within e-tail websites\n",
      "Abstract: A sample of 416 online auctions was examined to determine the extent of overpayment (winner's curse) where online auctions and e-tail websites were linked together to form a parallel sales channel. The results indicated that 8.7% of the highest winning online auction bidders exceeded e-tail posted reference prices of identical retail merchandise found at the same website. Significantly, such bids exceeded the reference prices by a mean percentage dollar amount of 14.1% thus suggesting the existence of a winner's curse. The results also indicated that (1) there was a significant negative association between reference price and mean percentage dollar amount overbid; (2) there was a significant negative association between auction lot size and mean percentage dollar amount overbid; and (3) there was no significant association between overtime auctions and mean percentage dollar amount overbid. While manipulation of reference price and auction lot size might minimize winner's curse, erratic or irrational behavior (by online auction and/or e-tail websites) may lead to disinformation.\n",
      "Content: Winner's curse and parallel sales channels—Online auctions linked within e-tail websites A sample of 416 online auctions was examined to determine the extent of overpayment (winner's curse) where online auctions and e-tail websites were linked together to form a parallel sales channel. The results indicated that 8.7% of the highest winning online auction bidders exceeded e-tail posted reference prices of identical retail merchandise found at the same website. Significantly, such bids exceeded the reference prices by a mean percentage dollar amount of 14.1% thus suggesting the existence of a winner's curse. The results also indicated that (1) there was a significant negative association between reference price and mean percentage dollar amount overbid; (2) there was a significant negative association between auction lot size and mean percentage dollar amount overbid; and (3) there was no significant association between overtime auctions and mean percentage dollar amount overbid. While manipulation of reference price and auction lot size might minimize winner's curse, erratic or irrational behavior (by online auction and/or e-tail websites) may lead to disinformation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 346\n",
      "Title: The Effects of Operational and Financial Performance Failure on BI&A-Enabled Search Behaviors: A Theory of Performance-Driven Search\n",
      "Abstract: Business intelligence and analytics (BI&A) systems enable firms to analyze data and search for insights that could potentially lead to improved organizational performance. While there is evidence that BI&A systems can improve performance through search, our theoretical understanding of how and under what conditions firms leverage BI&A systems to conduct search is rather limited. In particular, while problemistic search theory posits that performance failures motivate search, how firms respond to different types of performance failures remains unclear. We draw on and extend problemistic search theory by theorizing that BI&A-enabled search is influenced by complex interactions between failures in financial and operational performance and performance-related aspirations. We refer to this notion as the Theory of Performance-driven Search (TPS) and test it using longitudinal data gathered for a four-year period from seven U.S. hospitals. We find evidence that firms employ BI&A systems to search in a narrow set of circumstances. We find that performance failures are an important antecedent of BI&A-enabled search. In particular, failures in financial performance, failures in operational performance, and their joint failures are important conditions that trigger BI&A-enabled search. We find that historical and social aspiration levels of financial and operational performance influence BI&A-enabled search during failures in operational performance. We also find that only in organizations experiencing a sustained failure in financial performance do operational performance failures trigger BI&A-enabled search and that the latency of search response is dependent on the speed of failure in financial performance. Through our findings, we make two important contributions: we extend the business value of IT literature by identifying the contextual conditions that trigger BI&A use for search and we extend problemistic search theory by theorizing for the differential effects of operational and financial performance failures.\n",
      "Content: The Effects of Operational and Financial Performance Failure on BI&A-Enabled Search Behaviors: A Theory of Performance-Driven Search Business intelligence and analytics (BI&A) systems enable firms to analyze data and search for insights that could potentially lead to improved organizational performance. While there is evidence that BI&A systems can improve performance through search, our theoretical understanding of how and under what conditions firms leverage BI&A systems to conduct search is rather limited. In particular, while problemistic search theory posits that performance failures motivate search, how firms respond to different types of performance failures remains unclear. We draw on and extend problemistic search theory by theorizing that BI&A-enabled search is influenced by complex interactions between failures in financial and operational performance and performance-related aspirations. We refer to this notion as the Theory of Performance-driven Search (TPS) and test it using longitudinal data gathered for a four-year period from seven U.S. hospitals. We find evidence that firms employ BI&A systems to search in a narrow set of circumstances. We find that performance failures are an important antecedent of BI&A-enabled search. In particular, failures in financial performance, failures in operational performance, and their joint failures are important conditions that trigger BI&A-enabled search. We find that historical and social aspiration levels of financial and operational performance influence BI&A-enabled search during failures in operational performance. We also find that only in organizations experiencing a sustained failure in financial performance do operational performance failures trigger BI&A-enabled search and that the latency of search response is dependent on the speed of failure in financial performance. Through our findings, we make two important contributions: we extend the business value of IT literature by identifying the contextual conditions that trigger BI&A use for search and we extend problemistic search theory by theorizing for the differential effects of operational and financial performance failures.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 347\n",
      "Title: An economic view of information systems\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 348\n",
      "Title: Matching client/server processing architectures with information processing requirements: A contingency study\n",
      "Abstract: The 1990s are witnessing the rapid growth of client/server (C/S) computing, but for an organization to benefit from a C/S model, it should ensure that the processing architecture matches its information needs. Researchers have suggested that organizations moving to this model should identify their information requirements, and then determine the appropriate architectures to support them. This study utilizes information processing theory to examine the match between an organization's information processing requirements and its C/S architectures. The independent variables in this study are task characteristics, and the processing architectures. The dependent variable is effectiveness. The data for this study was obtained from C/S managers and users in a variety of industries, through a combination of archival data, telephone interviews, and a mailed survey. It was analyzed using hierarchical regression. The results indicate that an appropriate match between task characteristics and C/S processing architectures is an important determinant of system effectiveness.\n",
      "Content: Matching client/server processing architectures with information processing requirements: A contingency study The 1990s are witnessing the rapid growth of client/server (C/S) computing, but for an organization to benefit from a C/S model, it should ensure that the processing architecture matches its information needs. Researchers have suggested that organizations moving to this model should identify their information requirements, and then determine the appropriate architectures to support them. This study utilizes information processing theory to examine the match between an organization's information processing requirements and its C/S architectures. The independent variables in this study are task characteristics, and the processing architectures. The dependent variable is effectiveness. The data for this study was obtained from C/S managers and users in a variety of industries, through a combination of archival data, telephone interviews, and a mailed survey. It was analyzed using hierarchical regression. The results indicate that an appropriate match between task characteristics and C/S processing architectures is an important determinant of system effectiveness.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 349\n",
      "Title: Profiling Web Usage in the Workplace: A Behavior-Based Artificial Intelligence Approach\n",
      "Abstract: Employees' nonwork-related Web surfing behavior results in millions of dollars of expenditure for organizations. This paper proposes the use of a behavior-based artificial intelligence system to profile employee Web usage behavior. Two artificial neural networks (ANN) incorporating genetic algorithm techniques were developed for this purpose. The system was validated with two different data sets. The classification performance of the neural network models was compared to that of a statistical method. The results indicate that one of the ANN models, namely the simple recurrent network, was a superior classifier for this behavior-based problem. In addition, the uncertainty inherent in such classification decisions was examined with a loss matrix, and the holdout samples were reclassified using a loss matrix. The output of this intelligent system can be highly beneficial to managers in designing effective Web management policies.\n",
      "Content: Profiling Web Usage in the Workplace: A Behavior-Based Artificial Intelligence Approach Employees' nonwork-related Web surfing behavior results in millions of dollars of expenditure for organizations. This paper proposes the use of a behavior-based artificial intelligence system to profile employee Web usage behavior. Two artificial neural networks (ANN) incorporating genetic algorithm techniques were developed for this purpose. The system was validated with two different data sets. The classification performance of the neural network models was compared to that of a statistical method. The results indicate that one of the ANN models, namely the simple recurrent network, was a superior classifier for this behavior-based problem. In addition, the uncertainty inherent in such classification decisions was examined with a loss matrix, and the holdout samples were reclassified using a loss matrix. The output of this intelligent system can be highly beneficial to managers in designing effective Web management policies.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 350\n",
      "Title: Cognitive Conflict and Consensus Generation in Virtual Teams During Knowledge Capture: Comparative Effectiveness of Techniques\n",
      "Abstract: Effective knowledge management has been increasingly cited as critical for businesses to compete successfully. Knowledge acquisition/capture, the first step in knowledge management, continues to be a bottleneck and is exacerbated when experts are geographically distributed. Furthermore, knowledge from multiple experts is likely to generate inconsistent knowledge for a given problem domain. There is thus a compelling need to generate consensus by resolving inconsistencies and conflicts that may occur among experts during the process of knowledge acquisition. This process is more challenging when dealing with virtual teams of experts. This study addresses task-based or cognitive conflicts among experts. A key objective of this study is to examine the effectiveness of two cognitive techniques-the repertory grid (or RepGrid) and Delphi-in generating consensus among experts during the knowledge capture process. A field experiment with geographically distributed real-world network experts involving multiple rounds of interaction over an extended period of time was conducted. Findings from this research indicate that, in the short run, Delphi works better than the RepGrid in reducing conflict and generating consensus. However, the RepGrid technique appears to perform better in the long run. We find similar results for satisfaction with the process and outcome. Our findings also indicate that experts using the RepGrid technique elicited more knowledge as well as higher-quality knowledge than experts using the Delphi technique. To sum up, our study indicates that RepGrid is superior to Delphi, and therefore managers should seriously consider the use of RepGrid in capturing knowledge from multiple and distributed experts when dealing with complex real-world issues.\n",
      "Content: Cognitive Conflict and Consensus Generation in Virtual Teams During Knowledge Capture: Comparative Effectiveness of Techniques Effective knowledge management has been increasingly cited as critical for businesses to compete successfully. Knowledge acquisition/capture, the first step in knowledge management, continues to be a bottleneck and is exacerbated when experts are geographically distributed. Furthermore, knowledge from multiple experts is likely to generate inconsistent knowledge for a given problem domain. There is thus a compelling need to generate consensus by resolving inconsistencies and conflicts that may occur among experts during the process of knowledge acquisition. This process is more challenging when dealing with virtual teams of experts. This study addresses task-based or cognitive conflicts among experts. A key objective of this study is to examine the effectiveness of two cognitive techniques-the repertory grid (or RepGrid) and Delphi-in generating consensus among experts during the knowledge capture process. A field experiment with geographically distributed real-world network experts involving multiple rounds of interaction over an extended period of time was conducted. Findings from this research indicate that, in the short run, Delphi works better than the RepGrid in reducing conflict and generating consensus. However, the RepGrid technique appears to perform better in the long run. We find similar results for satisfaction with the process and outcome. Our findings also indicate that experts using the RepGrid technique elicited more knowledge as well as higher-quality knowledge than experts using the Delphi technique. To sum up, our study indicates that RepGrid is superior to Delphi, and therefore managers should seriously consider the use of RepGrid in capturing knowledge from multiple and distributed experts when dealing with complex real-world issues.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 351\n",
      "Title: A Tangled Web: Should Online Review Portals Display Fraudulent Reviews?\n",
      "Abstract: The growing interest in online product reviews for legitimate promotion has been accompanied by an increase in fraudulent reviews. However, beyond algorithms for initial fraud detection, little is known about what review portals should do with fraudulent reviews after detecting them. In this paper, we address this question by studying how consumers respond to potentially fraudulent reviews and how review portals can leverage this knowledge to design better fraud management policies. To do this, we combine theoretical development from the trust literature with randomized experiments and statistical analysis using large-scale data from Yelp. We find that consumers tend to increase their trust in the information provided by review portals when the portal displays fraudulent reviews along with non-fraudulent reviews, as opposed to the common practice of censoring suspected fraudulent reviews. The impact of fraudulent reviews on consumers’ decision-making process increases with the uncertainty in the initial evaluation of product quality. We also find that consumers do not effectively process the content of fraudulent reviews (negative or positive). This result furthers the case for a decision heuristic that will incorporate the motivational differences between positive fraudulent reviews and negative fraudulent reviews to effectively aid consumers’ decision making.\n",
      "Content: A Tangled Web: Should Online Review Portals Display Fraudulent Reviews? The growing interest in online product reviews for legitimate promotion has been accompanied by an increase in fraudulent reviews. However, beyond algorithms for initial fraud detection, little is known about what review portals should do with fraudulent reviews after detecting them. In this paper, we address this question by studying how consumers respond to potentially fraudulent reviews and how review portals can leverage this knowledge to design better fraud management policies. To do this, we combine theoretical development from the trust literature with randomized experiments and statistical analysis using large-scale data from Yelp. We find that consumers tend to increase their trust in the information provided by review portals when the portal displays fraudulent reviews along with non-fraudulent reviews, as opposed to the common practice of censoring suspected fraudulent reviews. The impact of fraudulent reviews on consumers’ decision-making process increases with the uncertainty in the initial evaluation of product quality. We also find that consumers do not effectively process the content of fraudulent reviews (negative or positive). This result furthers the case for a decision heuristic that will incorporate the motivational differences between positive fraudulent reviews and negative fraudulent reviews to effectively aid consumers’ decision making.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 352\n",
      "Title: Self-Organizing in Blockchain Infrastructures: Generativity Through Shifting Objectives and Forking\n",
      "Abstract:  Given the ubiquity of digital technologies, and increased use of autonomous algorithms, it is likely that many of today's social and organizational processes will one day include autonomous elements. The Bitcoin blockchain is likely the first case of an increasingly generative and autonomous way of organizing, and the specific properties of blockchain infrastructuresdistribution of control, openness to manipulation, and generativity of the underlying source codemake it an ideal case to study patterns of self-organizing. This paper investigates the phenomenon of self-organizing through a study of forking in the Bitcoin blockchain infrastructure between 2010 and 2016. It adds to the emerging body of research on digital infrastructures, and particularly blockchain infrastructures, by conceptualizing forking as a pattern of self-organizing in blockchain infrastructures that specifically involves the underlying infrastructure, the scale of code changes, individual objectives, and collective adoption, whether specific or general. Thus, this paper demonstrates how forking in blockchain infrastructures mediates between divergent organizing objectives and existing capabilities, on the one hand, and generates self-organizing on the other hand. In this paper, we further contextualize our findings in extant work on digital infrastructures, offer a guide for designers of blockchain infrastructures, and propose the concept of \"generative mirroring\" as a pattern through which blockchain infrastructures and organizing adaptively coevolve. Abstract Self-Organizing in Blockchain Infrastructures \n",
      "Content: Self-Organizing in Blockchain Infrastructures: Generativity Through Shifting Objectives and Forking  Given the ubiquity of digital technologies, and increased use of autonomous algorithms, it is likely that many of today's social and organizational processes will one day include autonomous elements. The Bitcoin blockchain is likely the first case of an increasingly generative and autonomous way of organizing, and the specific properties of blockchain infrastructuresdistribution of control, openness to manipulation, and generativity of the underlying source codemake it an ideal case to study patterns of self-organizing. This paper investigates the phenomenon of self-organizing through a study of forking in the Bitcoin blockchain infrastructure between 2010 and 2016. It adds to the emerging body of research on digital infrastructures, and particularly blockchain infrastructures, by conceptualizing forking as a pattern of self-organizing in blockchain infrastructures that specifically involves the underlying infrastructure, the scale of code changes, individual objectives, and collective adoption, whether specific or general. Thus, this paper demonstrates how forking in blockchain infrastructures mediates between divergent organizing objectives and existing capabilities, on the one hand, and generates self-organizing on the other hand. In this paper, we further contextualize our findings in extant work on digital infrastructures, offer a guide for designers of blockchain infrastructures, and propose the concept of \"generative mirroring\" as a pattern through which blockchain infrastructures and organizing adaptively coevolve. Abstract Self-Organizing in Blockchain Infrastructures \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 353\n",
      "Title: Bayesian logic\n",
      "Abstract: We combine probabilistic logic and Bayesian networks to obtain the advantages of each in what we call Bayesian logic. Like probabilistic logic, it is a theoretically grounded way of representing and reasoning with uncertainty that uses only as much probabilistic information as one has, since it permits one to specify probabilities as intervals rather than precise values. Like Bayesian networks, it can capture conditional independence relations, which are probably our richest source of probabilistic knowledge. The inference problem in Bayesian logic can be solved as a nonlinear program (which becomes a linear program in ordinary probabilistic logic). We show that Benders decomposition, applied to the nonlinear program, allows one to use the same column generation methods in Bayesian logic that are now being used to solve inference problems in probabilistic logic. We also show that if the independence conditions are properly represented, the number of nonlinear constraints grows only linearly with the number of nodes in a large class of networks (rather than exponentially, as in the general case).\n",
      "Content: Bayesian logic We combine probabilistic logic and Bayesian networks to obtain the advantages of each in what we call Bayesian logic. Like probabilistic logic, it is a theoretically grounded way of representing and reasoning with uncertainty that uses only as much probabilistic information as one has, since it permits one to specify probabilities as intervals rather than precise values. Like Bayesian networks, it can capture conditional independence relations, which are probably our richest source of probabilistic knowledge. The inference problem in Bayesian logic can be solved as a nonlinear program (which becomes a linear program in ordinary probabilistic logic). We show that Benders decomposition, applied to the nonlinear program, allows one to use the same column generation methods in Bayesian logic that are now being used to solve inference problems in probabilistic logic. We also show that if the independence conditions are properly represented, the number of nonlinear constraints grows only linearly with the number of nodes in a large class of networks (rather than exponentially, as in the general case).\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 354\n",
      "Title: A linear programming framework for logics of uncertainty\n",
      "Abstract: Several logics for reasoning under uncertainty distribute “probability mass” over sets in some sense. These include probabilistic logic, Dempster-Shafer theory, other logics based on belief functions, and second-order probabilistic logic. We show that these logics are instances of a certain type of linear programming model, typically with exponentially many variables. We also show how a single linear programming package can implement these logics computationally if one “plugs in” a different column generation subroutine for each logic, although the practicality of this approach has been demonstrated so far only for probabilistic logic.\n",
      "Content: A linear programming framework for logics of uncertainty Several logics for reasoning under uncertainty distribute “probability mass” over sets in some sense. These include probabilistic logic, Dempster-Shafer theory, other logics based on belief functions, and second-order probabilistic logic. We show that these logics are instances of a certain type of linear programming model, typically with exponentially many variables. We also show how a single linear programming package can implement these logics computationally if one “plugs in” a different column generation subroutine for each logic, although the practicality of this approach has been demonstrated so far only for probabilistic logic.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 355\n",
      "Title: Information technology and transitions in the public service: a comparison of Scandinavia and the United States\n",
      "Abstract:  New information technologies have the potential to transform the ways governments are organized, the activities they perform, the manner in which such activities are performed and even the nature of the work itself. Governments in the US and Scandinavia have followed fundamentally different approaches to the introduction of computing and to dealing with its effects. In the US, automation has been individualistic -each individual unit of government has introduced the technology according to its own needs. For the most part, the implemented systems were small scale, have followed functional lines, have merely automated existing operations, were implemented incrementally and have evolved slowly over time. In contrast, automation in Scandinavia has been communal -systems have been designed, developed and implemented by communal data processing agencies serving an entire level of government, national or local. The systems introduced were relatively large scale, have crossed functional lines, have involved the reorganization of work, have integrated both data and work processes, and were implemented more or less simultaneously for all units or agencies of government. These differences in approach to automation have influenced each country's view of the role of government in anticipating and dealing with the effects of changes in computer technology on the public workforce. \n",
      "Content: Information technology and transitions in the public service: a comparison of Scandinavia and the United States  New information technologies have the potential to transform the ways governments are organized, the activities they perform, the manner in which such activities are performed and even the nature of the work itself. Governments in the US and Scandinavia have followed fundamentally different approaches to the introduction of computing and to dealing with its effects. In the US, automation has been individualistic -each individual unit of government has introduced the technology according to its own needs. For the most part, the implemented systems were small scale, have followed functional lines, have merely automated existing operations, were implemented incrementally and have evolved slowly over time. In contrast, automation in Scandinavia has been communal -systems have been designed, developed and implemented by communal data processing agencies serving an entire level of government, national or local. The systems introduced were relatively large scale, have crossed functional lines, have involved the reorganization of work, have integrated both data and work processes, and were implemented more or less simultaneously for all units or agencies of government. These differences in approach to automation have influenced each country's view of the role of government in anticipating and dealing with the effects of changes in computer technology on the public workforce. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 356\n",
      "Title: Activity-based design\n",
      "Abstract: In many types of activities, communicative and material activities are so intertwined that the one cannot be understood without taking the other into account. This is true of maritime and hospital work that are used as examples in the paper. The spatial context of the activity is also important: what you can do depends upon where you are. Finally, human and automatic machinery alternate in filling certain roles in the activity: sometime the officer maintains the course, sometimes the autopilot. Such activities require us to rethink the traditional oppositions between communication and instrumental actions, between human and non-human participants, and between an activity and its spatio-temporal context. The advent of pervasive technologies, where active or passive systems become embedded in our working and living spaces, from where they offer their services to us, puts the need to reconsider these basic oppositions high on the research agenda. This paper presents a consistent framework called habitats for understanding communicative and material activities and their interplay, for understanding how activities can be associated to physical surroundings, and for understanding how humans and automatic machinery can replace one another in an activity. It also gives an example of how to use the framework for design.\n",
      "Content: Activity-based design In many types of activities, communicative and material activities are so intertwined that the one cannot be understood without taking the other into account. This is true of maritime and hospital work that are used as examples in the paper. The spatial context of the activity is also important: what you can do depends upon where you are. Finally, human and automatic machinery alternate in filling certain roles in the activity: sometime the officer maintains the course, sometimes the autopilot. Such activities require us to rethink the traditional oppositions between communication and instrumental actions, between human and non-human participants, and between an activity and its spatio-temporal context. The advent of pervasive technologies, where active or passive systems become embedded in our working and living spaces, from where they offer their services to us, puts the need to reconsider these basic oppositions high on the research agenda. This paper presents a consistent framework called habitats for understanding communicative and material activities and their interplay, for understanding how activities can be associated to physical surroundings, and for understanding how humans and automatic machinery can replace one another in an activity. It also gives an example of how to use the framework for design.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 357\n",
      "Title: The impact of IT on decision structure and firm performance: evidence from the textile and apparel industry\n",
      "Abstract: Clarifying the relationships between information technology (IT), organizational performance, and decision structure remains an important area of inquiry in IS research. Through an empirical analysis and complementary case examples, our study examines these associations among firms operating in the US-based apparel and textile industry from 1992 to 1997. Based on data gathered from 50 public firms located across the USA, the study finds that IT used to enhance internal communication supports a decentralized decision structure, which in turn is associated with higher financial performance. Hence, IT exhibits an indirect performance effect. However, use of IT to enhance communication is also found to have a direct performance effect in large organizations. This paper proposes that use of communication enhancing IT can support organizational learning processes by facilitating flexible exchange of skills and knowledge across functional areas. Case examples are used to illustrate how these learning effects can materialize.\n",
      "Content: The impact of IT on decision structure and firm performance: evidence from the textile and apparel industry Clarifying the relationships between information technology (IT), organizational performance, and decision structure remains an important area of inquiry in IS research. Through an empirical analysis and complementary case examples, our study examines these associations among firms operating in the US-based apparel and textile industry from 1992 to 1997. Based on data gathered from 50 public firms located across the USA, the study finds that IT used to enhance internal communication supports a decentralized decision structure, which in turn is associated with higher financial performance. Hence, IT exhibits an indirect performance effect. However, use of IT to enhance communication is also found to have a direct performance effect in large organizations. This paper proposes that use of communication enhancing IT can support organizational learning processes by facilitating flexible exchange of skills and knowledge across functional areas. Case examples are used to illustrate how these learning effects can materialize.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 358\n",
      "Title: Information technology, strategic decision making approaches and organizational performance in different industrial settings\n",
      "Abstract: The present study considers potential performance effects associated with the communication enhancing capacity of information technology. Enhancement of an organization's communication capabilities may influence performance through improved strategic decision making, better coordination of strategic actions and by facilitating learning from strategic initiatives. Accordingly, the paper investigates the effects of internal communication through use of computer networks, Intranet, and external communication via the Internet in association with autonomous and participatory strategic decision making approaches and strategic planning. These relationships are tested in two different industrial settings characterized by low and high levels of dynamism and complexity to assess possible environmental contingencies. In less dynamic and complex industries, the results show a positive association between Intranet use and innovation, while Internet use has a positive association to profitability and to innovation in organizations adhering to a participatory decision approach. In more dynamic and complex industries, Intranet use combined with an autonomous decision approach is associated with high profitability and sales growth, while Internet use combined with participatory decision making is associated with higher innovation. Hence, the study finds evidence that innovation relates to use of the Internet and participation across industries, and that economic efficiency relates to use of Intranet and autonomy in dynamic and complex industries.\n",
      "Content: Information technology, strategic decision making approaches and organizational performance in different industrial settings The present study considers potential performance effects associated with the communication enhancing capacity of information technology. Enhancement of an organization's communication capabilities may influence performance through improved strategic decision making, better coordination of strategic actions and by facilitating learning from strategic initiatives. Accordingly, the paper investigates the effects of internal communication through use of computer networks, Intranet, and external communication via the Internet in association with autonomous and participatory strategic decision making approaches and strategic planning. These relationships are tested in two different industrial settings characterized by low and high levels of dynamism and complexity to assess possible environmental contingencies. In less dynamic and complex industries, the results show a positive association between Intranet use and innovation, while Internet use has a positive association to profitability and to innovation in organizations adhering to a participatory decision approach. In more dynamic and complex industries, Intranet use combined with an autonomous decision approach is associated with high profitability and sales growth, while Internet use combined with participatory decision making is associated with higher innovation. Hence, the study finds evidence that innovation relates to use of the Internet and participation across industries, and that economic efficiency relates to use of Intranet and autonomy in dynamic and complex industries.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 359\n",
      "Title: Strategic Investments for Platform Launch and Ecosystem Growth: A Dynamic Analysis\n",
      "Abstract: Multi-sided platforms must make decisions on both pricing and engineering investment and must continually adjust them as the platform scales over its lifecycle. Engineering investments can be allocated to features that improve a platform's standalone value, social features to take advantage of same-side network effects, or integration tools and boundary resources to facilitate third-party content creation. Guidance in the academic or practitioner literature is not granular. Moreover, relevant normative economic models that consider externalities are rarely dynamic. Hence, there is a gap in knowledge about how to best balance tradeoffs between different strategic decisions throughout the entire platform lifecycle. To begin to address this gap, we explore normative strategies for coordinating pricing and engineering investment decisions on a continuous basis under different ecosystem conditions. We build a simulation model informed by economics and marketing theory and perform extensive sensitivity analyses on key parameters in different ecosystem scenarios over a multi-period lifecycle. We find that pricing and investment strategies must continuously change to perform optimally. In particular, strategies that are most effective at launch often differ from those that are most effective during scaling as well as those most effective at maturity. We also find that the optimal strategy depends strongly on the monetization model and market aversion to price changes. Lastly, we specifically examine four different industry segments: mobile platforms, social media, the sharing economy, and business-to-business. The results provide evidence that the trajectory of platform pricing and investment strategies should greatly differ depending on industrial context.\n",
      "Content: Strategic Investments for Platform Launch and Ecosystem Growth: A Dynamic Analysis Multi-sided platforms must make decisions on both pricing and engineering investment and must continually adjust them as the platform scales over its lifecycle. Engineering investments can be allocated to features that improve a platform's standalone value, social features to take advantage of same-side network effects, or integration tools and boundary resources to facilitate third-party content creation. Guidance in the academic or practitioner literature is not granular. Moreover, relevant normative economic models that consider externalities are rarely dynamic. Hence, there is a gap in knowledge about how to best balance tradeoffs between different strategic decisions throughout the entire platform lifecycle. To begin to address this gap, we explore normative strategies for coordinating pricing and engineering investment decisions on a continuous basis under different ecosystem conditions. We build a simulation model informed by economics and marketing theory and perform extensive sensitivity analyses on key parameters in different ecosystem scenarios over a multi-period lifecycle. We find that pricing and investment strategies must continuously change to perform optimally. In particular, strategies that are most effective at launch often differ from those that are most effective during scaling as well as those most effective at maturity. We also find that the optimal strategy depends strongly on the monetization model and market aversion to price changes. Lastly, we specifically examine four different industry segments: mobile platforms, social media, the sharing economy, and business-to-business. The results provide evidence that the trajectory of platform pricing and investment strategies should greatly differ depending on industrial context.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 360\n",
      "Title: Model checking for design and assurance of e-Business processes\n",
      "Abstract: Use of the Internet for electronic business has the potential to revolutionize the way many businesses are conducted. Yet, several businesses have fallen victim to problems in information systems that facilitate e-Business. These problems are characterized by uncertainties due to system complexity, rapid development, interconnectivity, and a lack of familiarity with the new technologically based economy. This paper demonstrates how model checking can aid in the design and assurance of e-Business processes in environments characterized by distributed processing, parallelism, concurrency, communication uncertainties, and continuous operations.\n",
      "Content: Model checking for design and assurance of e-Business processes Use of the Internet for electronic business has the potential to revolutionize the way many businesses are conducted. Yet, several businesses have fallen victim to problems in information systems that facilitate e-Business. These problems are characterized by uncertainties due to system complexity, rapid development, interconnectivity, and a lack of familiarity with the new technologically based economy. This paper demonstrates how model checking can aid in the design and assurance of e-Business processes in environments characterized by distributed processing, parallelism, concurrency, communication uncertainties, and continuous operations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 361\n",
      "Title: Your memory is working against you: How eye tracking and memory explain habituation to security warnings\n",
      "Abstract: Security warnings are critical to the security of end users and their organizations, often representing the final defense against an attack. Because warnings require users to make a contextual judgment, it is critical that they pay close attention to warnings. However, research shows that users routinely disregard them. A major factor contributing to the ineffectiveness of warnings is habituation, the decreased response to a repeated warning. Although previous research has identified the problem of habituation, the phenomenon has only been observed indirectly through behavioral measures. Therefore, it is unclear how habituation develops in the brain in response to security warnings, and how this in turn influences users' perceptions of these warnings. This paper contributes by using eye tracking to measure the eye movement-based memory (EMM) effect, a neurophysiological manifestation of habituation in which people unconsciously scrutinize previously seen stimuli less than novel stimuli. We show that habituation sets in after only a few exposures to a warning and progresses rapidly with further repetitions. Using guidelines from the warning science literature, we design a polymorphic warning artifact which repeatedly changes its appearance. We demonstrate that our polymorphic warning artifact is substantially more resistant to habituation than conventional security warnings, offering an effective solution for practice. Finally, our results highlight the value of applying neuroscience to the domain of information security behavior.\n",
      "Content: Your memory is working against you: How eye tracking and memory explain habituation to security warnings Security warnings are critical to the security of end users and their organizations, often representing the final defense against an attack. Because warnings require users to make a contextual judgment, it is critical that they pay close attention to warnings. However, research shows that users routinely disregard them. A major factor contributing to the ineffectiveness of warnings is habituation, the decreased response to a repeated warning. Although previous research has identified the problem of habituation, the phenomenon has only been observed indirectly through behavioral measures. Therefore, it is unclear how habituation develops in the brain in response to security warnings, and how this in turn influences users' perceptions of these warnings. This paper contributes by using eye tracking to measure the eye movement-based memory (EMM) effect, a neurophysiological manifestation of habituation in which people unconsciously scrutinize previously seen stimuli less than novel stimuli. We show that habituation sets in after only a few exposures to a warning and progresses rapidly with further repetitions. Using guidelines from the warning science literature, we design a polymorphic warning artifact which repeatedly changes its appearance. We demonstrate that our polymorphic warning artifact is substantially more resistant to habituation than conventional security warnings, offering an effective solution for practice. Finally, our results highlight the value of applying neuroscience to the domain of information security behavior.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 362\n",
      "Title: How users perceive and respond to security messages: a NeuroIS research agenda and empirical study\n",
      "Abstract: Users are vital to the information security of organizations. In spite of technical safeguards, users make many critical security decisions. An example is users’ responses to security messages – discrete communication designed to persuade users to either impair or improve their security status. Research shows that although users are highly susceptible to malicious messages (e.g., phishing attacks), they are highly resistant to protective messages such as security warnings. Research is therefore needed to better understand how users perceive and respond to security messages. In this article, we argue for the potential of NeuroIS – cognitive neuroscience applied to Information Systems – to shed new light on users’ reception of security messages in the areas of (1) habituation, (2) stress, (3) fear, and (4) dual-task interference. We present an illustrative study that shows the value of using NeuroIS to investigate one of our research questions. This example uses eye tracking to gain unique insight into how habituation occurs when people repeatedly view security messages, allowing us to design more effective security messages. Our results indicate that the eye movement-based memory (EMM) effect is a cause of habituation to security messages – a phenomenon in which people unconsciously scrutinize stimuli that they have previously seen less than other stimuli. We show that after only a few exposures to a warning, this neural aspect of habituation sets in rapidly, and continues with further repetitions. We also created a polymorphic warning that continually updates its appearance and found that it is effective in substantially reducing the rate of habituation as measured by the EMM effect. Our research agenda and empirical example demonstrate the promise of using NeuroIS to gain novel insight into users’ responses to security messages that will encourage more secure user behaviors and facilitate more effective security message designs.\n",
      "Content: How users perceive and respond to security messages: a NeuroIS research agenda and empirical study Users are vital to the information security of organizations. In spite of technical safeguards, users make many critical security decisions. An example is users’ responses to security messages – discrete communication designed to persuade users to either impair or improve their security status. Research shows that although users are highly susceptible to malicious messages (e.g., phishing attacks), they are highly resistant to protective messages such as security warnings. Research is therefore needed to better understand how users perceive and respond to security messages. In this article, we argue for the potential of NeuroIS – cognitive neuroscience applied to Information Systems – to shed new light on users’ reception of security messages in the areas of (1) habituation, (2) stress, (3) fear, and (4) dual-task interference. We present an illustrative study that shows the value of using NeuroIS to investigate one of our research questions. This example uses eye tracking to gain unique insight into how habituation occurs when people repeatedly view security messages, allowing us to design more effective security messages. Our results indicate that the eye movement-based memory (EMM) effect is a cause of habituation to security messages – a phenomenon in which people unconsciously scrutinize stimuli that they have previously seen less than other stimuli. We show that after only a few exposures to a warning, this neural aspect of habituation sets in rapidly, and continues with further repetitions. We also created a polymorphic warning that continually updates its appearance and found that it is effective in substantially reducing the rate of habituation as measured by the EMM effect. Our research agenda and empirical example demonstrate the promise of using NeuroIS to gain novel insight into users’ responses to security messages that will encourage more secure user behaviors and facilitate more effective security message designs.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 363\n",
      "Title: From Warning to Wallpaper: Why the Brain Habituates to Security Warnings and What Can Be Done About It\n",
      "Abstract: Warning messages are fundamental to users’ security interactions. Unfortunately, they are largely ineffective, as shown by prior research. A key contributor to this failure is habituation: decreased response to a repeated warning. Previous research has only inferred the occurrence of habituation to warnings, or measured it indirectly, such as through the proxy of a related behavior. Therefore, there is a gap in our understanding of how habituation to security warnings develops in the brain. Without direct measures of habituation, we are limited in designing warnings that can mitigate its effects. In this study, we use neurophysiological measures to directly observe habituation as it occurs in the brain and behaviorally. We also design a polymorphic warning artifact that repeatedly changes its appearance in order to resist the effects of habituation. In an experiment using functional magnetic resonance imaging (fMRI; n = 25), we found that our polymorphic warning was significantly more resistant to habituation than were conventional warnings in regions of the brain related to attention. In a second experiment (n = 80), we implemented the four most resistant polymorphic warnings in a realistic setting. Using mouse cursor tracking as a surrogate for attention to unobtrusively measure habituation on participants’ personal computers, we found that polymorphic warnings reduced habituation compared to conventional warnings. Together, our findings reveal the substantial influence of neurobiology on users’ habituation to security warnings and security behavior in general, and we offer our polymorphic warning design as an effective solution to practice\n",
      "Content: From Warning to Wallpaper: Why the Brain Habituates to Security Warnings and What Can Be Done About It Warning messages are fundamental to users’ security interactions. Unfortunately, they are largely ineffective, as shown by prior research. A key contributor to this failure is habituation: decreased response to a repeated warning. Previous research has only inferred the occurrence of habituation to warnings, or measured it indirectly, such as through the proxy of a related behavior. Therefore, there is a gap in our understanding of how habituation to security warnings develops in the brain. Without direct measures of habituation, we are limited in designing warnings that can mitigate its effects. In this study, we use neurophysiological measures to directly observe habituation as it occurs in the brain and behaviorally. We also design a polymorphic warning artifact that repeatedly changes its appearance in order to resist the effects of habituation. In an experiment using functional magnetic resonance imaging (fMRI; n = 25), we found that our polymorphic warning was significantly more resistant to habituation than were conventional warnings in regions of the brain related to attention. In a second experiment (n = 80), we implemented the four most resistant polymorphic warnings in a realistic setting. Using mouse cursor tracking as a surrogate for attention to unobtrusively measure habituation on participants’ personal computers, we found that polymorphic warnings reduced habituation compared to conventional warnings. Together, our findings reveal the substantial influence of neurobiology on users’ habituation to security warnings and security behavior in general, and we offer our polymorphic warning design as an effective solution to practice\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 364\n",
      "Title: Practicing Safe Computing: A Multimethod Empirical Examination of Home Computer User Security Behavioral Intentions\n",
      "Abstract: Although firms are expending substantial resources to develop technology and processes that can help safeguard the security of their computing assets, increased attention is being focused on the role people play in maintaining a safe computing environment. Unlike employees in a work setting, home users are not subject to training, nor are they protected by a technical staff dedicated to keeping security software and hardware current. Thus, with over one billion people with access to the Internet, individual home computer users represent a significant point of weakness in achieving the security of the cyber infrastructure. We study the phenomenon of conscientious cybercitizens, defined as individuals who are motivated to take the necessary precautions under their direct control to secure their own computer and the Internet in a home setting. Using a multidisciplinary, phased approach, we develop a conceptual model of the conscientious cybercitizen. We present results from two studies--a survey and an experiment--conducted to understand the drivers of intentions to perform security-related behavior, and the interventions that can positively influence these drivers. In the first study, we use protection motivation theory as the underlying conceptual foundation and extend the theory by drawing upon the public goods literature and the concept of psychological ownership. Results from a survey of 594 home computer users from a wide range of demographic and socioeconomic backgrounds suggest that a home computer user's intention to perform security-related behavior is influenced by a combination of cognitive, social, and psychological components. In the second study, we draw upon the concepts of goal framing and self-view to examine how the proximal drivers of intentions to perform security-related behavior identified in the first study can be influenced by appropriate messaging. An experiment with 101 subjects is used to test the research hypotheses. Overall, the two studies shed important new light on creating more conscientious cybercitizens. Theoretical and practical implications of the findings are discussed.\n",
      "Content: Practicing Safe Computing: A Multimethod Empirical Examination of Home Computer User Security Behavioral Intentions Although firms are expending substantial resources to develop technology and processes that can help safeguard the security of their computing assets, increased attention is being focused on the role people play in maintaining a safe computing environment. Unlike employees in a work setting, home users are not subject to training, nor are they protected by a technical staff dedicated to keeping security software and hardware current. Thus, with over one billion people with access to the Internet, individual home computer users represent a significant point of weakness in achieving the security of the cyber infrastructure. We study the phenomenon of conscientious cybercitizens, defined as individuals who are motivated to take the necessary precautions under their direct control to secure their own computer and the Internet in a home setting. Using a multidisciplinary, phased approach, we develop a conceptual model of the conscientious cybercitizen. We present results from two studies--a survey and an experiment--conducted to understand the drivers of intentions to perform security-related behavior, and the interventions that can positively influence these drivers. In the first study, we use protection motivation theory as the underlying conceptual foundation and extend the theory by drawing upon the public goods literature and the concept of psychological ownership. Results from a survey of 594 home computer users from a wide range of demographic and socioeconomic backgrounds suggest that a home computer user's intention to perform security-related behavior is influenced by a combination of cognitive, social, and psychological components. In the second study, we draw upon the concepts of goal framing and self-view to examine how the proximal drivers of intentions to perform security-related behavior identified in the first study can be influenced by appropriate messaging. An experiment with 101 subjects is used to test the research hypotheses. Overall, the two studies shed important new light on creating more conscientious cybercitizens. Theoretical and practical implications of the findings are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 365\n",
      "Title: The Digitization of Healthcare: Boundary Risks, Emotion, and Consumer Willingness to Disclose Personal Health Information\n",
      "Abstract: As healthcare becomes increasingly digitized, the promise of improved care enabled by technological advances inevitably must be traded off against any unintended negative consequences. There is little else that is as consequential to an individual as his or her health. In this context, the privacy of one's personal health information has escalated as a matter of significant concern for the public. We pose the question: under what circumstances will individuals be willing to disclose identified personal health information and permit it to be digitized? Using privacy boundary theory and recent developments in the literature related to risk-as-feelings as the core conceptual foundation, we propose and test a model explicating the role played by type of information requested (general health, mental health, genetic), the purpose for which it is to be used (patient care, research, marketing), and the requesting stakeholder (doctors/hospitals, the government, pharmaceutical companies) in an individual's willingness to disclose personal health information. Furthermore, we explore the impact of emotion linked to one's health condition on willingness to disclose. Results from a nationally representative sample of over 1,000 adults underscore the complexity of the health information disclosure decision and show that emotion plays a significant role, highlighting the need for re-examining the timing of consent. Theoretically, the study extends the dominant cognitive-consequentialist approach to privacy by incorporating the role of emotion. It further refines the privacy calculus to incorporate the moderating influence of contextual factors salient in the healthcare setting. The practical implications of this study include an improved understanding of consumer concerns and potential impacts regarding the electronic storage of health information that can be used to craft policy.\n",
      "Content: The Digitization of Healthcare: Boundary Risks, Emotion, and Consumer Willingness to Disclose Personal Health Information As healthcare becomes increasingly digitized, the promise of improved care enabled by technological advances inevitably must be traded off against any unintended negative consequences. There is little else that is as consequential to an individual as his or her health. In this context, the privacy of one's personal health information has escalated as a matter of significant concern for the public. We pose the question: under what circumstances will individuals be willing to disclose identified personal health information and permit it to be digitized? Using privacy boundary theory and recent developments in the literature related to risk-as-feelings as the core conceptual foundation, we propose and test a model explicating the role played by type of information requested (general health, mental health, genetic), the purpose for which it is to be used (patient care, research, marketing), and the requesting stakeholder (doctors/hospitals, the government, pharmaceutical companies) in an individual's willingness to disclose personal health information. Furthermore, we explore the impact of emotion linked to one's health condition on willingness to disclose. Results from a nationally representative sample of over 1,000 adults underscore the complexity of the health information disclosure decision and show that emotion plays a significant role, highlighting the need for re-examining the timing of consent. Theoretically, the study extends the dominant cognitive-consequentialist approach to privacy by incorporating the role of emotion. It further refines the privacy calculus to incorporate the moderating influence of contextual factors salient in the healthcare setting. The practical implications of this study include an improved understanding of consumer concerns and potential impacts regarding the electronic storage of health information that can be used to craft policy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 366\n",
      "Title: Information Security Control Theory: Achieving a Sustainable Reconciliation Between Sharing and Protecting the Privacy of Information\n",
      "Abstract: Contemporary organizations operate in highly interconnected environments where they are frequently confronted by the challenge of balancing the protection of information resources with the need for sharing information. This tension between the expected benefits and the potential security risks inherent in the information sharing process, exists in many domains, including business, health care, law enforcement, and military—yet it is not well-understood. We propose an information security control theory to explain and manage this tension. We evaluate this theory through a longitudinal case study of the iterative development of the information security policies for a health information exchange in the western United States. Our study shows that the theory offers a good framework through which to understand the information security policy development process, and a way to reconcile the tension between information sharing and information protection. The theory has practical applicability to many business domains.\n",
      "Content: Information Security Control Theory: Achieving a Sustainable Reconciliation Between Sharing and Protecting the Privacy of Information Contemporary organizations operate in highly interconnected environments where they are frequently confronted by the challenge of balancing the protection of information resources with the need for sharing information. This tension between the expected benefits and the potential security risks inherent in the information sharing process, exists in many domains, including business, health care, law enforcement, and military—yet it is not well-understood. We propose an information security control theory to explain and manage this tension. We evaluate this theory through a longitudinal case study of the iterative development of the information security policies for a health information exchange in the western United States. Our study shows that the theory offers a good framework through which to understand the information security policy development process, and a way to reconcile the tension between information sharing and information protection. The theory has practical applicability to many business domains.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 367\n",
      "Title: Managing compliance with privacy regulations through translation guardrails: A health information exchange case study\n",
      "Abstract: Information privacy is increasingly important in our digitally connected world, particularly in healthcare, and privacy regulations are ramping up to promote appropriate privacy practices. As a digital platform that enables healthcare providers to exchange protected health information (PHI), a health information exchange (HIE) is governed by health information privacy regula­ tions. The challenge for HIEs is to operate in a way that will maximize information exchange while maintaining compliance with regulations that may constrain the sharing of PHI. Regula­ tions impose a measure of universality through compliance requirements, while being flexible to allow adaptation to the local context. However, our longitudinal case study into the privacy policies of an HIE, demonstrates that the journey of privacy ideas from their original formulation in regulations, to their ultimate enactment in an organizational setting, is accompanied by translations, such that the final implementation may vary extensively from its original form. Such variability often results in interpretations that differ from what the regulators intended. Conse­ quently, translation guardrails are necessary to protect against problematic translations of reg­ ulatory ideas which could lead to compliance issues and loss of platform participation. Our findings offer two contributions. First, we contribute to the compliance literature by explaining how guardrails can balance the use of permission and obligation schemas which are necessary to translate regulations into effective organizational policies for the success of HIEs and other in­ formation exchange platforms. Second, we contribute to extending translation theory by explaining how pragmatic reasoning schemas function as the mechanism through which trans­ lation of regulations occurs.\n",
      "Content: Managing compliance with privacy regulations through translation guardrails: A health information exchange case study Information privacy is increasingly important in our digitally connected world, particularly in healthcare, and privacy regulations are ramping up to promote appropriate privacy practices. As a digital platform that enables healthcare providers to exchange protected health information (PHI), a health information exchange (HIE) is governed by health information privacy regula­ tions. The challenge for HIEs is to operate in a way that will maximize information exchange while maintaining compliance with regulations that may constrain the sharing of PHI. Regula­ tions impose a measure of universality through compliance requirements, while being flexible to allow adaptation to the local context. However, our longitudinal case study into the privacy policies of an HIE, demonstrates that the journey of privacy ideas from their original formulation in regulations, to their ultimate enactment in an organizational setting, is accompanied by translations, such that the final implementation may vary extensively from its original form. Such variability often results in interpretations that differ from what the regulators intended. Conse­ quently, translation guardrails are necessary to protect against problematic translations of reg­ ulatory ideas which could lead to compliance issues and loss of platform participation. Our findings offer two contributions. First, we contribute to the compliance literature by explaining how guardrails can balance the use of permission and obligation schemas which are necessary to translate regulations into effective organizational policies for the success of HIEs and other in­ formation exchange platforms. Second, we contribute to extending translation theory by explaining how pragmatic reasoning schemas function as the mechanism through which trans­ lation of regulations occurs.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 368\n",
      "Title: Blockchain innovation for consent self-management in health information exchanges\n",
      "Abstract: With the increasing digitalization of health data, patients need to make informed decisions about the online sharing of their protected health information. This necessitates a robust technical infrastructure that enables patients to self-manage consent and the trusted exchange of this information across sharing entities. Unfortu­ nately, current health information exchange systems in the U.S. are limited in both these regards. While there is recent work on digital patient consent management, there is limited work that provides effective solutions for patient self-management of consent. Further, interoperability issues in the way health information exchanges are currently architected and differences in regulations across localities exacerbate the challenges of consent man­ agement. In this research, we survey potential patients' willingness to self-manage healthcare-related consent. Having established the desire for consent self-management, we propose a solution that enables the seamless sharing of patient consent across different healthcare providers and health information exchanges. Specifically, we use a rigorous design science approach to create a blockchain-based, self-managed patient consent system and we evaluate the design through an instantiated prototype. The results of our study should be useful to researchers in healthcare information management as well as to practitioners designing consent management systems. Our research contributes to design science research with an innovative, rigorously evaluated, design principles-based artifact that addresses a critical problem of sharing protected health information.\n",
      "Content: Blockchain innovation for consent self-management in health information exchanges With the increasing digitalization of health data, patients need to make informed decisions about the online sharing of their protected health information. This necessitates a robust technical infrastructure that enables patients to self-manage consent and the trusted exchange of this information across sharing entities. Unfortu­ nately, current health information exchange systems in the U.S. are limited in both these regards. While there is recent work on digital patient consent management, there is limited work that provides effective solutions for patient self-management of consent. Further, interoperability issues in the way health information exchanges are currently architected and differences in regulations across localities exacerbate the challenges of consent man­ agement. In this research, we survey potential patients' willingness to self-manage healthcare-related consent. Having established the desire for consent self-management, we propose a solution that enables the seamless sharing of patient consent across different healthcare providers and health information exchanges. Specifically, we use a rigorous design science approach to create a blockchain-based, self-managed patient consent system and we evaluate the design through an instantiated prototype. The results of our study should be useful to researchers in healthcare information management as well as to practitioners designing consent management systems. Our research contributes to design science research with an innovative, rigorously evaluated, design principles-based artifact that addresses a critical problem of sharing protected health information.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 369\n",
      "Title: Affordance potency: Explaining the actualization of technology affordances\n",
      "Abstract: Given the importance of information technology (IT) in effecting organizational change, scholars have strived for many years to theorize the ways in which IT can produce the changes intended for it. Recent arguments claim that most information systems (IS) research has taken a limited theoretical focus on the information technology (IT) artifact, which arguably should be at the core of the IS discipline (Benbasat & Zmud, 2003). This research engages directly with the IT artifact by evaluating the use of an electronic medical records system and its relation to actualization of technology affordances. We conducted a case study at a large urban acute care hospital in the Midwestern United States with registered nurses working on inpatient care units as the clinicians of interest. Through interviews with nurses and other clinical stakeholders, observation of nurse's work practices on three patient care units in the hospital, and direct examination of the medical records system, we develop theoretical insights into the role of IT in work practices. The novel concept of affordance potency is introduced as an integral theoretical construct in our model of affordances, helping to explain actualizations of IT in use. Our contribution provides a nuanced yet powerful way of understanding the nature of IT artifacts and their relationships to technology users and work practice.\n",
      "Content: Affordance potency: Explaining the actualization of technology affordances Given the importance of information technology (IT) in effecting organizational change, scholars have strived for many years to theorize the ways in which IT can produce the changes intended for it. Recent arguments claim that most information systems (IS) research has taken a limited theoretical focus on the information technology (IT) artifact, which arguably should be at the core of the IS discipline (Benbasat & Zmud, 2003). This research engages directly with the IT artifact by evaluating the use of an electronic medical records system and its relation to actualization of technology affordances. We conducted a case study at a large urban acute care hospital in the Midwestern United States with registered nurses working on inpatient care units as the clinicians of interest. Through interviews with nurses and other clinical stakeholders, observation of nurse's work practices on three patient care units in the hospital, and direct examination of the medical records system, we develop theoretical insights into the role of IT in work practices. The novel concept of affordance potency is introduced as an integral theoretical construct in our model of affordances, helping to explain actualizations of IT in use. Our contribution provides a nuanced yet powerful way of understanding the nature of IT artifacts and their relationships to technology users and work practice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 370\n",
      "Title: Managing Distributed Product Development Projects: Integration Strategies for Time-Zone and Language Barriers\n",
      "Abstract: Distributed product development projects encompass product and process development activities that span organizational and country boundaries. The increasing trend toward globalizing projects requires firms to coordinate development efforts made by team members from various functions within the firm, speaking multiple languages, and working in various time zones. We analyze qualitative data from 70 distributed product development projects that span 14 countries and involve cross-functional team members speaking 10 different languages. We find that commonly discussed integration strategies such as modular product designs and colocating team members by themselves are insufficient to coordinate project work. Rather, our field interviews suggest that firms invest in design information systems (DIS) with specific features to facilitate product design and empower their project managers to integrate the development efforts. Specifically, our interviews suggest that firms often modify the organization by “unifying” the engineering and purchasing functions into a single supply chain integrator function to increase the scope of responsibilities for these managers. We then test our hypotheses on the benefits of these strategies on project outcomes by using survey data from 55 distributed product development projects in 20 firms. Results indicate that the use of DIS is associated with higher quality and relationship performance when there are differences between focal and supplier firm personnel languages. Unifying engineering and purchasing functions into a supply chain integrator is associated with improved response time in the presence of time zone differences. We also find that a unifying strategy is associated with lower cost in the presence of language differences, but is also associated with a worsening of response time. These results provide guidance to product designers in organizations that must coordinate complex work across time zone barriers and languages. The results also provide guidance to researchers, by showing that different integration mechanisms may have differential effects across various coordination barriers and across multiple dimensions of project performance. We conclude by linking these results to integration mechanisms previously discussed in the coordination literature.\n",
      "Content: Managing Distributed Product Development Projects: Integration Strategies for Time-Zone and Language Barriers Distributed product development projects encompass product and process development activities that span organizational and country boundaries. The increasing trend toward globalizing projects requires firms to coordinate development efforts made by team members from various functions within the firm, speaking multiple languages, and working in various time zones. We analyze qualitative data from 70 distributed product development projects that span 14 countries and involve cross-functional team members speaking 10 different languages. We find that commonly discussed integration strategies such as modular product designs and colocating team members by themselves are insufficient to coordinate project work. Rather, our field interviews suggest that firms invest in design information systems (DIS) with specific features to facilitate product design and empower their project managers to integrate the development efforts. Specifically, our interviews suggest that firms often modify the organization by “unifying” the engineering and purchasing functions into a single supply chain integrator function to increase the scope of responsibilities for these managers. We then test our hypotheses on the benefits of these strategies on project outcomes by using survey data from 55 distributed product development projects in 20 firms. Results indicate that the use of DIS is associated with higher quality and relationship performance when there are differences between focal and supplier firm personnel languages. Unifying engineering and purchasing functions into a supply chain integrator is associated with improved response time in the presence of time zone differences. We also find that a unifying strategy is associated with lower cost in the presence of language differences, but is also associated with a worsening of response time. These results provide guidance to product designers in organizations that must coordinate complex work across time zone barriers and languages. The results also provide guidance to researchers, by showing that different integration mechanisms may have differential effects across various coordination barriers and across multiple dimensions of project performance. We conclude by linking these results to integration mechanisms previously discussed in the coordination literature.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 371\n",
      "Title: Microcomputer software evaluation: An econometric model\n",
      "Abstract: Microcomputer software selection is made difficult by the multiplicity of products, variation in product performance, and the uncertainties of user needs. This paper presents a methodology for the empirical evaluation of competing software packages. The process proposed identifies the most relevant performance attribute set and, through a simultaneous system of equations, the relative importance of each attribute in explaining the satisfaction of users. The methodology is illustrated using sample data derived from user evaluations of five different software types: word processing, spreadsheet, data base management systems, communications, and graphics. The applicability of the methodology and the implications of the findings are discussed.\n",
      "Content: Microcomputer software evaluation: An econometric model Microcomputer software selection is made difficult by the multiplicity of products, variation in product performance, and the uncertainties of user needs. This paper presents a methodology for the empirical evaluation of competing software packages. The process proposed identifies the most relevant performance attribute set and, through a simultaneous system of equations, the relative importance of each attribute in explaining the satisfaction of users. The methodology is illustrated using sample data derived from user evaluations of five different software types: word processing, spreadsheet, data base management systems, communications, and graphics. The applicability of the methodology and the implications of the findings are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 372\n",
      "Title: Platform Performance Investment in the Presence of Network Externalities\n",
      "Abstract: Managers of emerging platforms must decide what level of platform performance to invest in at each product development cycle in markets that exhibit two-sided network externalities. High performance is a selling point for consumers, but in many cases it requires developers to make large investments to participate. Abstracting from an example drawn from the video game industry, we build a strategic model to investigate the trade-off between investing in high platform performance versus reducing investment in order to facilitate third party content development. We carry out a full analysis of three distinct settings: monopoly, price-setting duopoly, and price-taking duopoly. We provide insights on the optimum investment in platform performance and demonstrate how conventional wisdom about product development may be misleading in the presence of strong cross-network externalities. In particular, we show that, contrary to the conventional wisdom about “winner-take-all” markets, heavily investing in the core performance of a platform does not always yield a competitive edge. We characterize the conditions under which offering a platform with lower performance but greater availability of content can be a winning strategy. Keywords: two-sided markets; network externality; product development; video game industry\n",
      "Content: Platform Performance Investment in the Presence of Network Externalities Managers of emerging platforms must decide what level of platform performance to invest in at each product development cycle in markets that exhibit two-sided network externalities. High performance is a selling point for consumers, but in many cases it requires developers to make large investments to participate. Abstracting from an example drawn from the video game industry, we build a strategic model to investigate the trade-off between investing in high platform performance versus reducing investment in order to facilitate third party content development. We carry out a full analysis of three distinct settings: monopoly, price-setting duopoly, and price-taking duopoly. We provide insights on the optimum investment in platform performance and demonstrate how conventional wisdom about product development may be misleading in the presence of strong cross-network externalities. In particular, we show that, contrary to the conventional wisdom about “winner-take-all” markets, heavily investing in the core performance of a platform does not always yield a competitive edge. We characterize the conditions under which offering a platform with lower performance but greater availability of content can be a winning strategy. Keywords: two-sided markets; network externality; product development; video game industry\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 373\n",
      "Title: “Standardizing information security – a structurational analysis”\n",
      "Abstract: Given that there are an increasing number of information security breaches, organizations are being driven to adopt best practice for coping with attacks. Information security standards are designed to embody best practice and the legitimacy of these standards is a core issue for standardizing organizations. This study uncovers how structures at play in de jure standard development affect the input and throughput legitimacy of standards. We participated as members responsible for standards on information security and our analysis revealed two structures: consensus and warfare. A major implication of the combination of these structures is that legitimacy claims based on appeals to best practice are futile because it is difficult to know which the best practice is.\n",
      "Content: “Standardizing information security – a structurational analysis” Given that there are an increasing number of information security breaches, organizations are being driven to adopt best practice for coping with attacks. Information security standards are designed to embody best practice and the legitimacy of these standards is a core issue for standardizing organizations. This study uncovers how structures at play in de jure standard development affect the input and throughput legitimacy of standards. We participated as members responsible for standards on information security and our analysis revealed two structures: consensus and warfare. A major implication of the combination of these structures is that legitimacy claims based on appeals to best practice are futile because it is difficult to know which the best practice is.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 374\n",
      "Title: Architectural knowledge in inter-organizational IT innovation\n",
      "Abstract: This paper examines the front-end process of inter-organizational IT innovation. In particular, it focuses on the nature and role of architectural knowledge. Such knowledge is important for development of architectures capable of serving the goals of heterogeneous actors and technologies. Yet, surprisingly little research has been done on how architectural knowledge may be developed through collective achievements. This paper presents a theoretical model of architectural knowledge development in inter-organizational IT innovation. Applying this model throughout an action research project within the Swedish transport industry, the paper identifies four dimensions of architectural knowledge that proved important for facilitating an industry-wide ubiquitous computing environment. The four dimensions are technology capability awareness, use context sensitivity, business model understanding, and boundary-spanning competence. We conclude the paper by outlining the theoretical and strategy implications of the model and the four dimensions of architectural knowledge.\n",
      "Content: Architectural knowledge in inter-organizational IT innovation This paper examines the front-end process of inter-organizational IT innovation. In particular, it focuses on the nature and role of architectural knowledge. Such knowledge is important for development of architectures capable of serving the goals of heterogeneous actors and technologies. Yet, surprisingly little research has been done on how architectural knowledge may be developed through collective achievements. This paper presents a theoretical model of architectural knowledge development in inter-organizational IT innovation. Applying this model throughout an action research project within the Swedish transport industry, the paper identifies four dimensions of architectural knowledge that proved important for facilitating an industry-wide ubiquitous computing environment. The four dimensions are technology capability awareness, use context sensitivity, business model understanding, and boundary-spanning competence. We conclude the paper by outlining the theoretical and strategy implications of the model and the four dimensions of architectural knowledge.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 375\n",
      "Title: Examining the state of empirical business intelligence and analytics research: A poly-theoretic approach\n",
      "Abstract: Interest in Business Intelligence and Analytics (BI&A) has led to a growing body of impactful scholarly articles. We investigate the state of BI&A research by answering what is the state of BI&A research in terms of constructs studied, article’s macrostructure, and theoretical contributions, and how do the constructs studied, macro­ structure, and theoretical contributions, influence an article’s impact? We propose a poly-theoretic framework that classifies articles from top IS journals and conferences by studied construct, macrostructure, contribution, distribution, and impact. Findings provide an understanding of how articles’ components influence the impact of BI&A research. Implications and future research areas are discussed.\n",
      "Content: Examining the state of empirical business intelligence and analytics research: A poly-theoretic approach Interest in Business Intelligence and Analytics (BI&A) has led to a growing body of impactful scholarly articles. We investigate the state of BI&A research by answering what is the state of BI&A research in terms of constructs studied, article’s macrostructure, and theoretical contributions, and how do the constructs studied, macro­ structure, and theoretical contributions, influence an article’s impact? We propose a poly-theoretic framework that classifies articles from top IS journals and conferences by studied construct, macrostructure, contribution, distribution, and impact. Findings provide an understanding of how articles’ components influence the impact of BI&A research. Implications and future research areas are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 376\n",
      "Title: Competitive Brokerage: How Information Management Capability And Collaboration Networks Act As Substitutes\n",
      "Abstract: IT-based information management and collaboration networks are both important sources of competitive information. Despite anecdotal evidence, limited research examines their contemporaneous impact on firms' ability to compete effectively. We take an information asymmetry perspective to examine the mechanisms through which the firm's information management capability and the structure of its collaboration network influence the structure of its competition network in product markets. We argue that taking a brokerage position in competition networks has a positive influence on a firm's performance. We then explain how the firm's information management capability and its position in collaboration networks each have a direct positive influence on its brokerage position in the competition network. Finally, we propose that information management capability and a central position in collaboration networks act as substitutes. We empirically test our model using a longitudinal competition network, a longitudinal collaboration network, and longitudinal secondary data about firms' information management capability, drawn from 262 firms over a 15-year period. Our findings, robust to endogeneity concerns and alternative model specifications, reveal the direct and substitution effects of information management capability and collaboration networks on competition networks. This research contributes to the information systems and strategy literatures by offering insights into how IT enables firms to design competitive strategies by facilitating analyses of competitors' information, and how the information gained by firms as they collaborate can enhance their ability to compete.\n",
      "Content: Competitive Brokerage: How Information Management Capability And Collaboration Networks Act As Substitutes IT-based information management and collaboration networks are both important sources of competitive information. Despite anecdotal evidence, limited research examines their contemporaneous impact on firms' ability to compete effectively. We take an information asymmetry perspective to examine the mechanisms through which the firm's information management capability and the structure of its collaboration network influence the structure of its competition network in product markets. We argue that taking a brokerage position in competition networks has a positive influence on a firm's performance. We then explain how the firm's information management capability and its position in collaboration networks each have a direct positive influence on its brokerage position in the competition network. Finally, we propose that information management capability and a central position in collaboration networks act as substitutes. We empirically test our model using a longitudinal competition network, a longitudinal collaboration network, and longitudinal secondary data about firms' information management capability, drawn from 262 firms over a 15-year period. Our findings, robust to endogeneity concerns and alternative model specifications, reveal the direct and substitution effects of information management capability and collaboration networks on competition networks. This research contributes to the information systems and strategy literatures by offering insights into how IT enables firms to design competitive strategies by facilitating analyses of competitors' information, and how the information gained by firms as they collaborate can enhance their ability to compete.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 377\n",
      "Title: Temporal enactment of resettled refugees' ICT-mediated information practices\n",
      "Abstract: “In this paper, we explain how resettled refugees use information and communication technology (ICT) to respond to their changed circumstances and, in doing so, enhance their well-being and effective participation in a new society. Focusing on three modes of ICT-mediated information practices (ie, orienting, instrumental, and expressive), we identify eight patterns of ICT use: learning about a new environment, keeping informed, transacting online, communicating with others, managing everyday life, sustaining support networks, maintaining transnational ties, and expressing cultural identity. Further, we draw on a temporal theory of human agency to explain how current dilemmas and contingencies, cultural identities and connections to the past, and future expectations and aspirations shape resettled refugees' enactment of these patterns of ICT-mediated information practices. We show that, as resettled refugees move between multiple and overlapping temporal-relational contexts, ICT use makes a difference to managing their bifurcated lives.”\n",
      "Content: Temporal enactment of resettled refugees' ICT-mediated information practices “In this paper, we explain how resettled refugees use information and communication technology (ICT) to respond to their changed circumstances and, in doing so, enhance their well-being and effective participation in a new society. Focusing on three modes of ICT-mediated information practices (ie, orienting, instrumental, and expressive), we identify eight patterns of ICT use: learning about a new environment, keeping informed, transacting online, communicating with others, managing everyday life, sustaining support networks, maintaining transnational ties, and expressing cultural identity. Further, we draw on a temporal theory of human agency to explain how current dilemmas and contingencies, cultural identities and connections to the past, and future expectations and aspirations shape resettled refugees' enactment of these patterns of ICT-mediated information practices. We show that, as resettled refugees move between multiple and overlapping temporal-relational contexts, ICT use makes a difference to managing their bifurcated lives.”\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 378\n",
      "Title: Digital enforcement: Rethinking the pursuit of a digitally-enabled society\n",
      "Abstract: In this article, we aim to sensitise the information systems community about the dispossession of choice that the extended reliance on Internet technology creates for individuals. The overemphasis of digital inclusion as a solution to the digital divide problem frames Internet use as desirable in a progressive society but labels non-use as problematic or a deficiency that needs to be remedied. This situation, we argue, creates a new modality of inequality that we term digital enforcement, defined as the process of dispossession that reduces choices for individuals who prefer to minimise their reliance on the Internet if given the opportunity or those who want to live their lives offline altogether. We present digital enforcement as an ethical problem and draw on the concepts of governmentality and technologies of power to explain how practices around Internet use in society result in digital enforcement. We conclude with a hopeful perspective to call for an ethical agenda to develop desirable futures.\n",
      "Content: Digital enforcement: Rethinking the pursuit of a digitally-enabled society In this article, we aim to sensitise the information systems community about the dispossession of choice that the extended reliance on Internet technology creates for individuals. The overemphasis of digital inclusion as a solution to the digital divide problem frames Internet use as desirable in a progressive society but labels non-use as problematic or a deficiency that needs to be remedied. This situation, we argue, creates a new modality of inequality that we term digital enforcement, defined as the process of dispossession that reduces choices for individuals who prefer to minimise their reliance on the Internet if given the opportunity or those who want to live their lives offline altogether. We present digital enforcement as an ethical problem and draw on the concepts of governmentality and technologies of power to explain how practices around Internet use in society result in digital enforcement. We conclude with a hopeful perspective to call for an ethical agenda to develop desirable futures.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 379\n",
      "Title: Seeing for Understanding: Unlocking the Potential of Visual Research in Information Systems\n",
      "Abstract: In this paper, we argue that information researchers should use images as a source of data. The information systems field is overwhelmingly visual in nature. Not only is the Internet crammed with images, but also almost every detail observed during fieldwork in different research settings can be captured in the form of digital images. Yet, we rarely engage with those images. Except for sporadic video recordings in analyzing human-computer interaction and, more recently, neurophysiological imaging, using images in information systems research has been sparse and non-systematic. Where images are used, the purpose of using them has been largely restricted to visually representing the context of the research setting. This approach underuses the knowledge embedded in visual material, which needs to be unpacked in a systematic fashion. We discuss the theoretical underpinnings of visual research and illustrate via a three-step framework how images in information systems research can be collected, analyzed, and presented. We conclude with four considerations for researchers that can help them develop a visual research capacity in information systems and encourage researchers to engage with the images that are now a major feature of the information systems environment.\n",
      "Content: Seeing for Understanding: Unlocking the Potential of Visual Research in Information Systems In this paper, we argue that information researchers should use images as a source of data. The information systems field is overwhelmingly visual in nature. Not only is the Internet crammed with images, but also almost every detail observed during fieldwork in different research settings can be captured in the form of digital images. Yet, we rarely engage with those images. Except for sporadic video recordings in analyzing human-computer interaction and, more recently, neurophysiological imaging, using images in information systems research has been sparse and non-systematic. Where images are used, the purpose of using them has been largely restricted to visually representing the context of the research setting. This approach underuses the knowledge embedded in visual material, which needs to be unpacked in a systematic fashion. We discuss the theoretical underpinnings of visual research and illustrate via a three-step framework how images in information systems research can be collected, analyzed, and presented. We conclude with four considerations for researchers that can help them develop a visual research capacity in information systems and encourage researchers to engage with the images that are now a major feature of the information systems environment.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 380\n",
      "Title: Formal conceptualisation as a basis for a more procedural knowledge management\n",
      "Abstract: Knowledge management at an organisational level can only be brought into practice if a corporate memory is defined. Unfortunately, at this moment there is no complete and procedural specification on how to build it. This paper presents a complete and generic knowledge representation scheme that makes it possible to conceptualise/represent the knowledge of any domain in a systematic way, guiding the definition of a corporate memory and allowing us to reach a more procedural level in knowledge management discipline. The conclusions of our study, which follows the generic and formal definition of any conceptualisation, are illustrated by a real project.\n",
      "Content: Formal conceptualisation as a basis for a more procedural knowledge management Knowledge management at an organisational level can only be brought into practice if a corporate memory is defined. Unfortunately, at this moment there is no complete and procedural specification on how to build it. This paper presents a complete and generic knowledge representation scheme that makes it possible to conceptualise/represent the knowledge of any domain in a systematic way, guiding the definition of a corporate memory and allowing us to reach a more procedural level in knowledge management discipline. The conclusions of our study, which follows the generic and formal definition of any conceptualisation, are illustrated by a real project.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 381\n",
      "Title: A decision support system for the collaborative selection of strategies in enterprise networks\n",
      "Abstract: Collaborative networks (CN) consist of autonomous and heterogeneous partners, and each defines its own objectives and formulates its own strategies, which are selected and activated to achieve these objectives. The heterogeneity that characterises network partners could lead to contradictions appearing among the strategies formulated in each CN enterprise. Consequently, the strategies formulated in one enterprise could negatively influence the achievement of the objectives defined in other enterprises of the same network. These contradictions lead to strategies misalignments, which worsens the network performance. In order to deal with these misalignments, a DSS is proposed to support the process of selecting the strategies among all those formulated, with the aim of achieving higher alignment levels. The proposed DSS considers the impacts that each strategy formulated in each enterprise has on the performance of the objectives defined by each network partner. This allows enterprises to select a set of aligned strategies. The selection of proper strategies to be activated in each enterprise strongly influences the CN's performance level, and higher levels of network adaptability, agility and competitiveness are achieved. The proposed DSS is validated under real conditions in a food industry network. The DSS is evaluated by emulating real collaborative conditions and is compared with the equivalent non-collaborative decision making perspective used for selecting strategies. The results demonstrate that the collaborative approach outperforms the performance level of the non-collaborative one and is more effective for handling the robustness and the long-term operation of the CN.\n",
      "Content: A decision support system for the collaborative selection of strategies in enterprise networks Collaborative networks (CN) consist of autonomous and heterogeneous partners, and each defines its own objectives and formulates its own strategies, which are selected and activated to achieve these objectives. The heterogeneity that characterises network partners could lead to contradictions appearing among the strategies formulated in each CN enterprise. Consequently, the strategies formulated in one enterprise could negatively influence the achievement of the objectives defined in other enterprises of the same network. These contradictions lead to strategies misalignments, which worsens the network performance. In order to deal with these misalignments, a DSS is proposed to support the process of selecting the strategies among all those formulated, with the aim of achieving higher alignment levels. The proposed DSS considers the impacts that each strategy formulated in each enterprise has on the performance of the objectives defined by each network partner. This allows enterprises to select a set of aligned strategies. The selection of proper strategies to be activated in each enterprise strongly influences the CN's performance level, and higher levels of network adaptability, agility and competitiveness are achieved. The proposed DSS is validated under real conditions in a food industry network. The DSS is evaluated by emulating real collaborative conditions and is compared with the equivalent non-collaborative decision making perspective used for selecting strategies. The results demonstrate that the collaborative approach outperforms the performance level of the non-collaborative one and is more effective for handling the robustness and the long-term operation of the CN.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 382\n",
      "Title: A Contingency Approach to Software Project Coordination\n",
      "Abstract: Before software project managers can enhance productivity and satisfaction of the software project team member, the effect of task characteristics, goal orientations, and coordination strategies on design and coding-task outcomes must be understood. A research model, which suggests that task interdependence, goal conflict, and coordination strategies significantly affect productivity and satisfaction associated with software design and coding activities, is presented. Issues such as contingency/design misfit, conflicting contingencies, and the extent of deviation to theoretically prescribed coordination mechanisms applied to contingencies are used to make predictions on productivity and process satisfaction. A 2x2x2 factorial experiment was utilized. Overall, projects characterized by low task interdependence exhibited greater productivity than projects with high task interdependence. Also, in general, organic coordination was more productive than mechanistic coordination. There was also a significant interaction between task interdependence and coordination strategy. Low goal conflict and organic coordination each lead to greater process satisfaction. Productivity results for the goal conflict manipulation was opposite to the hypothesized direction. Unconflicted contingencies addressed with consistent coordination and partially conflicted contingencies, regardless of the coordination used, exhibited significant gains in productivity. In comparison, unconflicted contingencies with inconsistent coordination and conflicted contingencies, regardless of the coordination applied, resulted in lower productivity. This suggests that there are instances where multiple contingencies, which warrant the use of different coordination strategies, can be adequately addressed with a specific coordination strategy.\n",
      "Content: A Contingency Approach to Software Project Coordination Before software project managers can enhance productivity and satisfaction of the software project team member, the effect of task characteristics, goal orientations, and coordination strategies on design and coding-task outcomes must be understood. A research model, which suggests that task interdependence, goal conflict, and coordination strategies significantly affect productivity and satisfaction associated with software design and coding activities, is presented. Issues such as contingency/design misfit, conflicting contingencies, and the extent of deviation to theoretically prescribed coordination mechanisms applied to contingencies are used to make predictions on productivity and process satisfaction. A 2x2x2 factorial experiment was utilized. Overall, projects characterized by low task interdependence exhibited greater productivity than projects with high task interdependence. Also, in general, organic coordination was more productive than mechanistic coordination. There was also a significant interaction between task interdependence and coordination strategy. Low goal conflict and organic coordination each lead to greater process satisfaction. Productivity results for the goal conflict manipulation was opposite to the hypothesized direction. Unconflicted contingencies addressed with consistent coordination and partially conflicted contingencies, regardless of the coordination used, exhibited significant gains in productivity. In comparison, unconflicted contingencies with inconsistent coordination and conflicted contingencies, regardless of the coordination applied, resulted in lower productivity. This suggests that there are instances where multiple contingencies, which warrant the use of different coordination strategies, can be adequately addressed with a specific coordination strategy.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 383\n",
      "Title: Organisational learning and core capabilities development: the role of IT\n",
      "Abstract: The resource-based view of the firm (RBVF) focuses on the firm's resources and capabilities to understand business strategy and to provide direction to strategy formulation. This paper emphasizes the learning aspects of capability development and explores how information technology (IT) can contribute to it. As a standardized resource widely available, IT can participate in the fundamental process that transforms resources into capabilities and eventually into core capabilities. In this way, IT can become — embedded in core capabilities — an active component of the firm's competitive advantages. The process by which resources end up being components of core capabilities in firms is a learning process that can be described and understood using RBVF concepts. Furthermore, the development of IT strategic applications (also called ‘strategic information systems’, or SIS) follows patterns that closely parallel the structure of that learning process. For this reason we propose an organizational learning model based on the RBVF, and use it to derive guidelines for management action aimed at improving IT effectiveness in organizations. The paper is organized as follows: the RBVF framework is summarized, including the concepts of capabilities and core capabilities and the organizational processes that lead to them. Next, an organizational learning model is presented: an interpretation of capability development that emphasizes situated learning and knowledge accumulation. The model is then used to show how IT can contribute to core capability formation in a firm: management action can mold the process to some extent, although it often unfolds ‘naturally’ embedded in an organizational context that is both determined by and determinant of learning. Finally, guidelines are discussed to come up and build strategic IT applications, based on the previous analysis. Short conclusions follow.\n",
      "Content: Organisational learning and core capabilities development: the role of IT The resource-based view of the firm (RBVF) focuses on the firm's resources and capabilities to understand business strategy and to provide direction to strategy formulation. This paper emphasizes the learning aspects of capability development and explores how information technology (IT) can contribute to it. As a standardized resource widely available, IT can participate in the fundamental process that transforms resources into capabilities and eventually into core capabilities. In this way, IT can become — embedded in core capabilities — an active component of the firm's competitive advantages. The process by which resources end up being components of core capabilities in firms is a learning process that can be described and understood using RBVF concepts. Furthermore, the development of IT strategic applications (also called ‘strategic information systems’, or SIS) follows patterns that closely parallel the structure of that learning process. For this reason we propose an organizational learning model based on the RBVF, and use it to derive guidelines for management action aimed at improving IT effectiveness in organizations. The paper is organized as follows: the RBVF framework is summarized, including the concepts of capabilities and core capabilities and the organizational processes that lead to them. Next, an organizational learning model is presented: an interpretation of capability development that emphasizes situated learning and knowledge accumulation. The model is then used to show how IT can contribute to core capability formation in a firm: management action can mold the process to some extent, although it often unfolds ‘naturally’ embedded in an organizational context that is both determined by and determinant of learning. Finally, guidelines are discussed to come up and build strategic IT applications, based on the previous analysis. Short conclusions follow.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 384\n",
      "Title: The strategic dimension of transactional information systems: some organizational implications\n",
      "Abstract: Abstract. Often information systems (IS) are classified in three groups: (a) transactional, used mainly for co-ordination and resource allocation purposes at the operational level of a company; (b) tactical, often employed to support the resource procurement activities typical of middle management; and (c) information systems for strategic decision making, designed to help in the planning and strategy design processes which are the direct responsibility of top management. In general, the amount of care and management attention that companies give to these different types of systems is proportional to their position in this hierarchy: little attention is devoted to the mundane transaction-pushing systems and exquisite care is put into developing the sophisticated decision making aid for the CEO and his/her staff. The IS/IT literature has been reporting quite commonly cases in which companies have attained or lost great competitive advantages by way of their transactional information systems [for example, Emery Worldwide, Baxter Healthcare ASAP system, and Frontier Airlines]. The aim of this paper is to identify actions that companies can take to realize potential benefits of their IS, in particular from their low-level, transactional IS. Among other actions, we will conclude that companies would be better off if they: (a) have the IS department at the right place in the organization, staffed with people knowledgeable about the basic nature of the business in which the company is engaged; (b) are sensible to what can be called ‘strategic maintenance’ of systems, (c) set up a formal procedure for IS planning to ensure coherence between IS plans and business plans, derived, in turn, from business strategy, and (d) keep abreast of the relevant technology. Several examples taken from European companies are used to illustrate these conclusions.\n",
      "Content: The strategic dimension of transactional information systems: some organizational implications Abstract. Often information systems (IS) are classified in three groups: (a) transactional, used mainly for co-ordination and resource allocation purposes at the operational level of a company; (b) tactical, often employed to support the resource procurement activities typical of middle management; and (c) information systems for strategic decision making, designed to help in the planning and strategy design processes which are the direct responsibility of top management. In general, the amount of care and management attention that companies give to these different types of systems is proportional to their position in this hierarchy: little attention is devoted to the mundane transaction-pushing systems and exquisite care is put into developing the sophisticated decision making aid for the CEO and his/her staff. The IS/IT literature has been reporting quite commonly cases in which companies have attained or lost great competitive advantages by way of their transactional information systems [for example, Emery Worldwide, Baxter Healthcare ASAP system, and Frontier Airlines]. The aim of this paper is to identify actions that companies can take to realize potential benefits of their IS, in particular from their low-level, transactional IS. Among other actions, we will conclude that companies would be better off if they: (a) have the IS department at the right place in the organization, staffed with people knowledgeable about the basic nature of the business in which the company is engaged; (b) are sensible to what can be called ‘strategic maintenance’ of systems, (c) set up a formal procedure for IS planning to ensure coherence between IS plans and business plans, derived, in turn, from business strategy, and (d) keep abreast of the relevant technology. Several examples taken from European companies are used to illustrate these conclusions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 385\n",
      "Title: Quality-informed semi-automated event log generation for process mining\n",
      "Abstract: Process mining, as with any form of data analysis, relies heavily on the quality of input data to generate accurate and reliable results. A fit-for-purpose event log nearly always requires time-consuming, manual pre-processing to extract events from source data, with data quality dependent on the analyst's domain knowledge and skills. Despite much being written about data quality in general, a generalisable framework for analysing event data quality issues when extracting logs for process mining remains unrealised. Following the DSR paradigm, we present RDB2Log, a quality-aware, semi-automated approach for extracting event logs from relational data. We validated RDB2Log's design against design objectives extracted from literature and competing artifacts, evaluated its design and performance with process mining experts, implemented a prototype with a defined set of quality metrics, and applied it in laboratory settings and in a real-world case study. The evaluation shows that RDB2Log is understandable, of relevance in current research, and supports process mining in practice.\n",
      "Content: Quality-informed semi-automated event log generation for process mining Process mining, as with any form of data analysis, relies heavily on the quality of input data to generate accurate and reliable results. A fit-for-purpose event log nearly always requires time-consuming, manual pre-processing to extract events from source data, with data quality dependent on the analyst's domain knowledge and skills. Despite much being written about data quality in general, a generalisable framework for analysing event data quality issues when extracting logs for process mining remains unrealised. Following the DSR paradigm, we present RDB2Log, a quality-aware, semi-automated approach for extracting event logs from relational data. We validated RDB2Log's design against design objectives extracted from literature and competing artifacts, evaluated its design and performance with process mining experts, implemented a prototype with a defined set of quality metrics, and applied it in laboratory settings and in a real-world case study. The evaluation shows that RDB2Log is understandable, of relevance in current research, and supports process mining in practice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 386\n",
      "Title: An empirical model of IT usage in the Malaysian public sector\n",
      "Abstract: Whilst there have been many studies to determine the factors that influence the use of information technology (IT) in organisations, few have considered how these factors change with the level of IT use. This paper presents the results of such a study involving the use of IT to support Total Quality Management (TQM). The population studied consisted of those organisations in the Malaysian public sector that had applied for the Malaysian Prime Minister's Quality Award during the period 1992–1997. Three sets of factors were investigated for their impact on the use of IT to support TQM in this setting: external, organisational, and technological factors. Overall, the organisational and technological factors had more influence on IT usage than did the external factors. However, as organisations became more experienced in their use of IT, the major contextual influences on IT usage levels changed. At low levels of IT usage the major contextual influences were organisational. At medium levels of IT usage a combination of technological and organisational factors became important, whilst at high IT usage levels, the dominant factors were technological.\n",
      "Content: An empirical model of IT usage in the Malaysian public sector Whilst there have been many studies to determine the factors that influence the use of information technology (IT) in organisations, few have considered how these factors change with the level of IT use. This paper presents the results of such a study involving the use of IT to support Total Quality Management (TQM). The population studied consisted of those organisations in the Malaysian public sector that had applied for the Malaysian Prime Minister's Quality Award during the period 1992–1997. Three sets of factors were investigated for their impact on the use of IT to support TQM in this setting: external, organisational, and technological factors. Overall, the organisational and technological factors had more influence on IT usage than did the external factors. However, as organisations became more experienced in their use of IT, the major contextual influences on IT usage levels changed. At low levels of IT usage the major contextual influences were organisational. At medium levels of IT usage a combination of technological and organisational factors became important, whilst at high IT usage levels, the dominant factors were technological.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 387\n",
      "Title: User information satisfaction, job satisfaction and computer background: An exploratory study\n",
      "Abstract: The relationships among user information satisfaction (UIS), job satisfaction and the users' computer background were examined. UIS was measured using a modified version of the short-form of UIS, while job satisfaction was measured using the short-form Minnesota satisfaction questionnaire (MSQ). We found that UIS provides a sound indication of job satisfaction. However, none of the user computer-background parameters has any significant effect on UIS and job satisfaction. Data for the study were collected from three large organizations which had similar organizational structure and comparable information systems maturity; people who used computer as part of their jobs were randomly selected to take part in the study. A study with more organizations would yield better results.\n",
      "Content: User information satisfaction, job satisfaction and computer background: An exploratory study The relationships among user information satisfaction (UIS), job satisfaction and the users' computer background were examined. UIS was measured using a modified version of the short-form of UIS, while job satisfaction was measured using the short-form Minnesota satisfaction questionnaire (MSQ). We found that UIS provides a sound indication of job satisfaction. However, none of the user computer-background parameters has any significant effect on UIS and job satisfaction. Data for the study were collected from three large organizations which had similar organizational structure and comparable information systems maturity; people who used computer as part of their jobs were randomly selected to take part in the study. A study with more organizations would yield better results.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 388\n",
      "Title: Critical success factors in implementing MRP and government assistance: A Singapore context\n",
      "Abstract: It is generally acknowledged that Manufacturing Resource Planning (MRP) could revolutionize manufacturing operations. Making use of data from a recent survey of MRP practices in Singapore conducted jointly by the National Computer Board (NCB) and the National University of Singapore (NUS), this paper provides a profile of manufacturing companies in Singapore that both have and have not implemented MRP. The Critical Success Factors involved in implementing MRP are identified and the theoretical justification behind each factor examined. Finally, the ways in which the Singapore Government can help local companies implement, operate, and maintain MRP systems are discussed. It is an expensive investment, and difficult to implement, due to its complexity. It is hoped that knowledge and understanding of these factors will assist firms in successfully implementing MRP and enable them to further improve their systems in order to maximize returns.\n",
      "Content: Critical success factors in implementing MRP and government assistance: A Singapore context It is generally acknowledged that Manufacturing Resource Planning (MRP) could revolutionize manufacturing operations. Making use of data from a recent survey of MRP practices in Singapore conducted jointly by the National Computer Board (NCB) and the National University of Singapore (NUS), this paper provides a profile of manufacturing companies in Singapore that both have and have not implemented MRP. The Critical Success Factors involved in implementing MRP are identified and the theoretical justification behind each factor examined. Finally, the ways in which the Singapore Government can help local companies implement, operate, and maintain MRP systems are discussed. It is an expensive investment, and difficult to implement, due to its complexity. It is hoped that knowledge and understanding of these factors will assist firms in successfully implementing MRP and enable them to further improve their systems in order to maximize returns.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 389\n",
      "Title: A multiple-case design methodology for studying MRP success and CSFs\n",
      "Abstract: We used a multiple-case design to study materials requirements planning (MRP) implementation outcome in 10 manufacturing companies in Singapore. Using a two-phased data collection approach (pre-interview questionnaires and personal interviews), we sought to develop a comprehensive and operationally acceptable measure of MRP success. Our measure consists of two linked components. They are a satisfaction score (a quantitative measure) and a complementary measure based on comments from the interviewees regarding the level of usage and acceptance of the system. We also extended and consolidated a seven-factor critical success factor (CSF) framework using this methodology. CSFs are important, but knowing the linkages between them is even more important, because these linkages tell us which CSFs to emphasize at various stages of the project.\n",
      "Content: A multiple-case design methodology for studying MRP success and CSFs We used a multiple-case design to study materials requirements planning (MRP) implementation outcome in 10 manufacturing companies in Singapore. Using a two-phased data collection approach (pre-interview questionnaires and personal interviews), we sought to develop a comprehensive and operationally acceptable measure of MRP success. Our measure consists of two linked components. They are a satisfaction score (a quantitative measure) and a complementary measure based on comments from the interviewees regarding the level of usage and acceptance of the system. We also extended and consolidated a seven-factor critical success factor (CSF) framework using this methodology. CSFs are important, but knowing the linkages between them is even more important, because these linkages tell us which CSFs to emphasize at various stages of the project.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 390\n",
      "Title: CSFs and sources of assistance and expertise in strategic IS planning: a Singapore perspective\n",
      "Abstract: Strategic information systems (IS) planning is not an easy task and knowing which critical areas to manage certainly enhances IS planning success. Studies of critical success factors (CSFs) usually dealt with specific systems or management technique implementation, such as manufacturing resource planning (MRP) and total quality management (TQM). There exists little empirical research on CSFs per se in strategic IS planning. This paper is an effort to enhance existing knowledge on how strategic IS planning should be effectively managed. Using data from a survey on IS planning conducted in 1996 by the National University of Singapore, we identified and rank-ordered the CSFs in strategic IS planning in the Singapore context. We also examined the sources of assistance and expertise that companies undertaking IS planning in Singapore can tap.\n",
      "Content: CSFs and sources of assistance and expertise in strategic IS planning: a Singapore perspective Strategic information systems (IS) planning is not an easy task and knowing which critical areas to manage certainly enhances IS planning success. Studies of critical success factors (CSFs) usually dealt with specific systems or management technique implementation, such as manufacturing resource planning (MRP) and total quality management (TQM). There exists little empirical research on CSFs per se in strategic IS planning. This paper is an effort to enhance existing knowledge on how strategic IS planning should be effectively managed. Using data from a survey on IS planning conducted in 1996 by the National University of Singapore, we identified and rank-ordered the CSFs in strategic IS planning in the Singapore context. We also examined the sources of assistance and expertise that companies undertaking IS planning in Singapore can tap.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 391\n",
      "Title: Management issues in data warehousing: insights from the Housing and Development Board\n",
      "Abstract: Data warehousing has emerged as one of the most powerful tools in delivering information to users. In this paper, we examine data warehousing at the Housing and Development Board (HDB), which is responsible for providing affordable, high-quality public housing to Singapore citizens. The HDB embarked on building a data warehouse because access to the diverse and large amount of data in its operational systems, was becoming increasingly cumbersome and time consuming. By building a data warehouse, the HDB aims to facilitate users' access to corporate information for planning and decision making. The experiences and lessons learned from building a data warehouse at HDB are discussed.\n",
      "Content: Management issues in data warehousing: insights from the Housing and Development Board Data warehousing has emerged as one of the most powerful tools in delivering information to users. In this paper, we examine data warehousing at the Housing and Development Board (HDB), which is responsible for providing affordable, high-quality public housing to Singapore citizens. The HDB embarked on building a data warehouse because access to the diverse and large amount of data in its operational systems, was becoming increasingly cumbersome and time consuming. By building a data warehouse, the HDB aims to facilitate users' access to corporate information for planning and decision making. The experiences and lessons learned from building a data warehouse at HDB are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 392\n",
      "Title: Work Outcomes and Job Design for Contract Versus Permanent Information: Systems Professionals on Software Development Teams\n",
      "Abstract:  Organizations have significantly increased their use of contracting in information systems (IS), hiring contractors to work with permanent professionals. Based on theories of social exchange and social comparison, we hypothesize differences in work attitudes, behaviors, and performance across \n",
      "Content: Work Outcomes and Job Design for Contract Versus Permanent Information: Systems Professionals on Software Development Teams  Organizations have significantly increased their use of contracting in information systems (IS), hiring contractors to work with permanent professionals. Based on theories of social exchange and social comparison, we hypothesize differences in work attitudes, behaviors, and performance across \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 393\n",
      "Title: Production and Transaction Economies and IS Outsourcing: A Study of the U.S. Banking Industry\n",
      "Abstract:  outsourcing decision. To empirically test these relationships, information was gathered from senior 17~ managers in 243 U.S. banks. Financial indices from the archives of the Federal Reserve Bank were a second important source of data.Results of the study show that IS outsourcing in banks was strongly influenced by production cost advantages offered by vendors. Transaction costs played a role in the outsourcing decision, but they were much smaller than production costs. Finally, financial slack was not found to be a significant explanator, although firm size was a significant control factor. The paper has important implications for research and practice. For researchers, the findings provide evidence that financial criteria can be key factors in outsourcing decisions and compare the relative effects of production and transaction costs. For practitioners, the findings suggest that managerial sourcing strategies need to weigh both costs when hiring systems integrators. AbstractThis paper studies economic determinants of IS outsourcing. It argues that a focus on comparative economic theories and models can improve our ability to explain outsourcing within the larger context of organizational strategy and environment. Specifically, the research constructs of production cost, transaction cost, and financial slack are examined simultaneously to understand what influences the 1 Robert Zmud was the accepting senior editor for this paper \n",
      "Content: Production and Transaction Economies and IS Outsourcing: A Study of the U.S. Banking Industry  outsourcing decision. To empirically test these relationships, information was gathered from senior 17~ managers in 243 U.S. banks. Financial indices from the archives of the Federal Reserve Bank were a second important source of data.Results of the study show that IS outsourcing in banks was strongly influenced by production cost advantages offered by vendors. Transaction costs played a role in the outsourcing decision, but they were much smaller than production costs. Finally, financial slack was not found to be a significant explanator, although firm size was a significant control factor. The paper has important implications for research and practice. For researchers, the findings provide evidence that financial criteria can be key factors in outsourcing decisions and compare the relative effects of production and transaction costs. For practitioners, the findings suggest that managerial sourcing strategies need to weigh both costs when hiring systems integrators. AbstractThis paper studies economic determinants of IS outsourcing. It argues that a focus on comparative economic theories and models can improve our ability to explain outsourcing within the larger context of organizational strategy and environment. Specifically, the research constructs of production cost, transaction cost, and financial slack are examined simultaneously to understand what influences the 1 Robert Zmud was the accepting senior editor for this paper \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 394\n",
      "Title: A comment on the intellectual structures of information systems development\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 395\n",
      "Title: DSS research and practice in perspective\n",
      "Abstract: The aim of this paper is to assess the state-of-the-art in the Decision Support Systems (DSS) field from both a research and a practice perspective. Three main dimensions of DSS research and practice are addressed: 1) supporting human decision-making processes, 2) integrating DSS into the organizational context, and 3) identifying new application domains. The related analysis and discussion provides a better understanding of past developments in the DSS field and insights into future evolution patterns.\n",
      "Content: DSS research and practice in perspective The aim of this paper is to assess the state-of-the-art in the Decision Support Systems (DSS) field from both a research and a practice perspective. Three main dimensions of DSS research and practice are addressed: 1) supporting human decision-making processes, 2) integrating DSS into the organizational context, and 3) identifying new application domains. The related analysis and discussion provides a better understanding of past developments in the DSS field and insights into future evolution patterns.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 396\n",
      "Title: An empirical study of EDI trading partner selection criteria in customer-supplier relationships\n",
      "Abstract: Electronic data interchange (EDI)-enabled trading partnerships are even more important now that EDI and electronic commerce-based technologies are underlying long-term strategic business partnerships. This study investigates the trading partner selection criteria used by firms in a customer-supplier dyad and their relative importance according to EDI implementation level is also established. Using the survey method implementing paired questionnaires for a dyad of customer-supplier firms, the study gathered data from 152 respondent firms. Factor analysis yielded six factors in trading partner selection: strategic commitment, trading partner flexibility, joint partnering for EDI, readiness for high-level EDI, EDI infrastructure, and communications. MANOVA and t-tests were used to test differences in the means of the responses of customer and supplier firms to the selection criteria. Overall, customer firms assigned higher means to all six factors than did the supplier firms. The gap between the two groups of firms were widest for the factors readiness for high-level EDI, trading partner flexibility, and communications.\n",
      "Content: An empirical study of EDI trading partner selection criteria in customer-supplier relationships Electronic data interchange (EDI)-enabled trading partnerships are even more important now that EDI and electronic commerce-based technologies are underlying long-term strategic business partnerships. This study investigates the trading partner selection criteria used by firms in a customer-supplier dyad and their relative importance according to EDI implementation level is also established. Using the survey method implementing paired questionnaires for a dyad of customer-supplier firms, the study gathered data from 152 respondent firms. Factor analysis yielded six factors in trading partner selection: strategic commitment, trading partner flexibility, joint partnering for EDI, readiness for high-level EDI, EDI infrastructure, and communications. MANOVA and t-tests were used to test differences in the means of the responses of customer and supplier firms to the selection criteria. Overall, customer firms assigned higher means to all six factors than did the supplier firms. The gap between the two groups of firms were widest for the factors readiness for high-level EDI, trading partner flexibility, and communications.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 397\n",
      "Title: Eight scenarios of national information superhighway development\n",
      "Abstract: Leading industrialized nations are now involved in the full implementation of national information superhighways. Their efforts will depend largely on the roles and levels of involvement of public institutions and how this is perceived and reacted to by the private sector. Through the evolution of eight scenarios of public institution involvement, this paper extrapolates the consequences that each brings about in the development of a national information superhighway. In particular, this paper examines the consequences of the development of an information superhighway for the UK within the context of each scenario and draws comparisons with developments in other countries. Contrary to popular belief, we show that there is no full solution for the shortcomings experienced in the development of the UK’s information superhighway; rather, each scenario offers solutions for a subset of the shortcomings.\n",
      "Content: Eight scenarios of national information superhighway development Leading industrialized nations are now involved in the full implementation of national information superhighways. Their efforts will depend largely on the roles and levels of involvement of public institutions and how this is perceived and reacted to by the private sector. Through the evolution of eight scenarios of public institution involvement, this paper extrapolates the consequences that each brings about in the development of a national information superhighway. In particular, this paper examines the consequences of the development of an information superhighway for the UK within the context of each scenario and draws comparisons with developments in other countries. Contrary to popular belief, we show that there is no full solution for the shortcomings experienced in the development of the UK’s information superhighway; rather, each scenario offers solutions for a subset of the shortcomings.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 398\n",
      "Title: A methodology for specific, total enterprise, role-playing, intelligent gaming-simulation environment development\n",
      "Abstract: This paper contributes a methodology for integrating intelligent tutoring into a specific, total enterprise, role-playing gaming-simulation environment. The result of the application of the methodology is a prototype intelligent gaming-simulation environment.\n",
      "Content: A methodology for specific, total enterprise, role-playing, intelligent gaming-simulation environment development This paper contributes a methodology for integrating intelligent tutoring into a specific, total enterprise, role-playing gaming-simulation environment. The result of the application of the methodology is a prototype intelligent gaming-simulation environment.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 399\n",
      "Title: ‘Though this be madness, yet there is method in't’\n",
      "Abstract: The paper is a synthesis of the growing doubt and dissatisfaction felt by the authors about the direction that the study of ‘information systems’ (IS) seems to be taking. The first author has over 20 years' experience, as an information technology/IS academic, as a consultant, and as a co-author of a large and commercially successful software package. The second author is relatively new to the subject, but is working full-time in a commercial IS environment. His views reflect the real uncertainties felt by many practitioners who put current IS theory into practice. Together the two authors confront some of the many methodical features that the discipline considers self-evidently true, but which they have come to see as bizarre.\n",
      "Content: ‘Though this be madness, yet there is method in't’ The paper is a synthesis of the growing doubt and dissatisfaction felt by the authors about the direction that the study of ‘information systems’ (IS) seems to be taking. The first author has over 20 years' experience, as an information technology/IS academic, as a consultant, and as a co-author of a large and commercially successful software package. The second author is relatively new to the subject, but is working full-time in a commercial IS environment. His views reflect the real uncertainties felt by many practitioners who put current IS theory into practice. Together the two authors confront some of the many methodical features that the discipline considers self-evidently true, but which they have come to see as bizarre.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 400\n",
      "Title: A model and a performance measurement system for collaborative supply chains\n",
      "Abstract: Modeling the constituents of a collaborative supply chain, the key parameters they influence, and the appropriate performance measures in a decision support environment enables prior understanding of the impact on the performance of a collaborative supply chain as a result of changes in the constituents and key parameters. In turn, this allows pinpointing of those areas where the actual supply chain can be improved and hence manage the chain's performance. This paper shows how the constituents, key parameters and performance indicators are modelled into the environment and through a case study illustrates how the decision support environment may be used to improve the performance of a collaborative supply chain by pinpointing areas for improvement.\n",
      "Content: A model and a performance measurement system for collaborative supply chains Modeling the constituents of a collaborative supply chain, the key parameters they influence, and the appropriate performance measures in a decision support environment enables prior understanding of the impact on the performance of a collaborative supply chain as a result of changes in the constituents and key parameters. In turn, this allows pinpointing of those areas where the actual supply chain can be improved and hence manage the chain's performance. This paper shows how the constituents, key parameters and performance indicators are modelled into the environment and through a case study illustrates how the decision support environment may be used to improve the performance of a collaborative supply chain by pinpointing areas for improvement.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 401\n",
      "Title: Taxo-Semantics: Assessing similarity between multi-word expressions for extending e-catalogs\n",
      "Abstract: Taxonomies, also named directories, are utilized in e-catalogs to classify goods in a hierarchical manner with the help of concepts. If there is a need to create new concepts when modifying the taxonomy, the semantic similarity between the provided concepts has to be assessed properly. Existing semantic similarity assessment techniques lack in a comprehensive support for e-commerce, as those are not supporting multi-word expressions, multilingualism, the import/export to relational databases, and supervised user-involvement. This paper proposes Taxo-Semantics, a decision support system that is based on the progress in taxonomy matching to match each expression against various sources of background knowledge. The similarity assessment is based on providing three different matching strategies: a lexical-based strategy named Taxo-Semantics-Label, the strategy Taxo-Semantics-Bk, which is using different sources of background knowledge, and the strategy Taxo-Semantics-User that is providing user-involvement. The proposed system includes a translating service to analyze non-English concepts with the help of the WordNet lexicon, can parse taxonomies of relational databases, supports user-involvement to match single sequences with WordNet, and is capable to analyze each sequence as (sub)-taxonomy. The three proposed matching strategies significantly outperformed existing techniques. Taxo-Semantics-Label could improve the accuracy result by more than 7% as compared to state-of-the-art lexical techniques. Taxo-Semantics-Bk could improve the accuracy compared to structure-based techniques by more than 8%. And, Taxo-Semantics-User could additionally increase the accuracy by on average 23%.\n",
      "Content: Taxo-Semantics: Assessing similarity between multi-word expressions for extending e-catalogs Taxonomies, also named directories, are utilized in e-catalogs to classify goods in a hierarchical manner with the help of concepts. If there is a need to create new concepts when modifying the taxonomy, the semantic similarity between the provided concepts has to be assessed properly. Existing semantic similarity assessment techniques lack in a comprehensive support for e-commerce, as those are not supporting multi-word expressions, multilingualism, the import/export to relational databases, and supervised user-involvement. This paper proposes Taxo-Semantics, a decision support system that is based on the progress in taxonomy matching to match each expression against various sources of background knowledge. The similarity assessment is based on providing three different matching strategies: a lexical-based strategy named Taxo-Semantics-Label, the strategy Taxo-Semantics-Bk, which is using different sources of background knowledge, and the strategy Taxo-Semantics-User that is providing user-involvement. The proposed system includes a translating service to analyze non-English concepts with the help of the WordNet lexicon, can parse taxonomies of relational databases, supports user-involvement to match single sequences with WordNet, and is capable to analyze each sequence as (sub)-taxonomy. The three proposed matching strategies significantly outperformed existing techniques. Taxo-Semantics-Label could improve the accuracy result by more than 7% as compared to state-of-the-art lexical techniques. Taxo-Semantics-Bk could improve the accuracy compared to structure-based techniques by more than 8%. And, Taxo-Semantics-User could additionally increase the accuracy by on average 23%.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 402\n",
      "Title: Information technology and voluntary quality disclosure by hospitals\n",
      "Abstract: Information asymmetry between consumers and health care providers is a well-known phenomenon in health care systems. Disclosure of health care quality information is one important mechanism through which hospitals can signal performance to potential patients and competitors, yet little is known about the organizational factors that contribute to voluntary disclosure. In this study we develop an empirical model to investigate the factors associated with choosing to participate in a voluntary quality disclosure initiative, specifically isolating the importance of information technology (IT) in facilitating disclosure. We extend the scope of prior work on the quality disclosure choice by augmenting it with an important decision variable: the operational costs of collecting and reporting quality data. We suggest that IT can facilitate disclosure by reducing these costs, thereby extending the literature on the value of IT. Empirical findings using data from a major voluntary quality disclosure program in California hospitals support our assertion related to the role of IT. Our results further highlight other hospital characteristics contributing to disclosure. We discuss implications of these findings for research and practice.\n",
      "Content: Information technology and voluntary quality disclosure by hospitals Information asymmetry between consumers and health care providers is a well-known phenomenon in health care systems. Disclosure of health care quality information is one important mechanism through which hospitals can signal performance to potential patients and competitors, yet little is known about the organizational factors that contribute to voluntary disclosure. In this study we develop an empirical model to investigate the factors associated with choosing to participate in a voluntary quality disclosure initiative, specifically isolating the importance of information technology (IT) in facilitating disclosure. We extend the scope of prior work on the quality disclosure choice by augmenting it with an important decision variable: the operational costs of collecting and reporting quality data. We suggest that IT can facilitate disclosure by reducing these costs, thereby extending the literature on the value of IT. Empirical findings using data from a major voluntary quality disclosure program in California hospitals support our assertion related to the role of IT. Our results further highlight other hospital characteristics contributing to disclosure. We discuss implications of these findings for research and practice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 403\n",
      "Title: Adoption of Electronic Health Records in the Presence of Privacy Concerns: The Elaboration Likelihood Model and Individual Persuasion\n",
      "Abstract: Within the emerging context of the digitization of health care, electronic health records (EHRs) constitute a significant technological advance in the way medical information is stored, communicated, and processed by the multiple parties involved in health care delivery. However, in spite of the anticipated value potential of this technology, there is widespread concern that consumer privacy issues may impede its diffusion. In this study, we pose the question: Can individuals be persuaded to change their attitudes and opt-in behavioral intentions toward EHRs, and allow their medical information to be digitized even in the presence of significant privacy concerns? To investigate this question, we integrate an individual's concern for information privacy (CFIP) with the elaboration likelihood model (ELM) to examine attitude change and likelihood of opting-in to an EHR system. We theorize that issue involvement and argument framing interact to influence attitude change, and that concern for information privacy further moderates the effects of these variables. We also propose that likelihood of adoption is driven by concern for information privacy and attitude. We test our predictions using an experiment with 366 subjects where we manipulate the framing of the arguments supporting EHRs. We find that an individual's CFIP interacts with argument framing and issue involvement to affect attitudes toward the use of EHRs. In addition, results suggest that attitude toward EHR use and CFIP directly influence opt-in behavioral intentions. An important finding for both theory and practice is that even when people have high concerns for privacy, their attitudes can be positively altered with appropriate message framing. These results as well as other theoretical and practical implications are discussed.\n",
      "Content: Adoption of Electronic Health Records in the Presence of Privacy Concerns: The Elaboration Likelihood Model and Individual Persuasion Within the emerging context of the digitization of health care, electronic health records (EHRs) constitute a significant technological advance in the way medical information is stored, communicated, and processed by the multiple parties involved in health care delivery. However, in spite of the anticipated value potential of this technology, there is widespread concern that consumer privacy issues may impede its diffusion. In this study, we pose the question: Can individuals be persuaded to change their attitudes and opt-in behavioral intentions toward EHRs, and allow their medical information to be digitized even in the presence of significant privacy concerns? To investigate this question, we integrate an individual's concern for information privacy (CFIP) with the elaboration likelihood model (ELM) to examine attitude change and likelihood of opting-in to an EHR system. We theorize that issue involvement and argument framing interact to influence attitude change, and that concern for information privacy further moderates the effects of these variables. We also propose that likelihood of adoption is driven by concern for information privacy and attitude. We test our predictions using an experiment with 366 subjects where we manipulate the framing of the arguments supporting EHRs. We find that an individual's CFIP interacts with argument framing and issue involvement to affect attitudes toward the use of EHRs. In addition, results suggest that attitude toward EHR use and CFIP directly influence opt-in behavioral intentions. An important finding for both theory and practice is that even when people have high concerns for privacy, their attitudes can be positively altered with appropriate message framing. These results as well as other theoretical and practical implications are discussed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 404\n",
      "Title: When Do It Security Investments Matter? Accounting for the Influence of Institutional Factors in the Context of Healthcare Data Breaches\n",
      "Abstract: In this study, we argue that institutional factors determine the extent to which hospitals are symbolic or substantive adopters of information technology (IT) specific organizational practices. We then propose that symbolic and substantive adoption will moderate the effect that IT security investments have on reducing the incidence of data security breaches over time. Using data from three different sources, we create a matched panel of over 5,000 U.S. hospitals and 938 breaches over the 2005–2013 time frame. Using a growth mixture model approach to model the heterogeneity in likelihood of breach, we use a two class solution in which hospitals that (1) belong to smaller health systems, (2) are older, (3) smaller in size, (4) for-profit, (5) nonacademic, (6) faith-based, and (7) less entrepreneurial with IT are classified as symbolic adopters. We find that symbolic adoption diminishes the effectiveness of IT security investments, resulting in an increased likelihood of breach. Contrary to our theorizing, the use of more IT security is not directly responsible for reducing breaches, but instead, institutional factors create the conditions under which IT security investments can be more effective. Implications of these findings are significant for policy and practice, the most important of which may be the discovery that firms need to consider how adoption is influenced by institutional factors and how this should be balanced with technological solutions. In particular, our results support the notion that deeper integration of security into IT-related processes and routines leads to fewer breaches, with the caveat that it takes time for these benefits to be realized.\n",
      "Content: When Do It Security Investments Matter? Accounting for the Influence of Institutional Factors in the Context of Healthcare Data Breaches In this study, we argue that institutional factors determine the extent to which hospitals are symbolic or substantive adopters of information technology (IT) specific organizational practices. We then propose that symbolic and substantive adoption will moderate the effect that IT security investments have on reducing the incidence of data security breaches over time. Using data from three different sources, we create a matched panel of over 5,000 U.S. hospitals and 938 breaches over the 2005–2013 time frame. Using a growth mixture model approach to model the heterogeneity in likelihood of breach, we use a two class solution in which hospitals that (1) belong to smaller health systems, (2) are older, (3) smaller in size, (4) for-profit, (5) nonacademic, (6) faith-based, and (7) less entrepreneurial with IT are classified as symbolic adopters. We find that symbolic adoption diminishes the effectiveness of IT security investments, resulting in an increased likelihood of breach. Contrary to our theorizing, the use of more IT security is not directly responsible for reducing breaches, but instead, institutional factors create the conditions under which IT security investments can be more effective. Implications of these findings are significant for policy and practice, the most important of which may be the discovery that firms need to consider how adoption is influenced by institutional factors and how this should be balanced with technological solutions. In particular, our results support the notion that deeper integration of security into IT-related processes and routines leads to fewer breaches, with the caveat that it takes time for these benefits to be realized.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 405\n",
      "Title: Dual Role of IT-Assisted Communication in Patient Care: A Validated Structure-Process-Outcome Framework\n",
      "Abstract: Despite the fact that about 90 percent of information transactions in hospitals are communications between patients, doctors, nurses, and other staff, little research has addressed the role that information technology (IT) plays in improving the efficiency and effectiveness of these communications-based transactions. Addressing this research gap is important considering that a substantial number of adverse hospital events stem from communication failures. Furthermore, effective communication is a major driver of patient satisfaction in hospitals. Using a structure-process-outcome (SPO) framework and guided by the strategic role of IT literature, we develop a model that includes \"structure,\" operationalized as organizational characteristics and two different categories of IT; \"process,\" two different communication-based processes; and \"outcomes,\" quantified as case-mix adjusted mortality, patient loyalty, and patient ratings. Specifically, we hypothesize that a subset of clinical IT (cardiology IT) will affect technical protocols of patient care, which in turn affects mortality, while administrative IT will affect interpersonal patient care, which relates to patient loyalty and ratings. Thus, IT can serve as a double-edged sword affecting both technical and interpersonal processes of care, but possibly independently and differentially. We test our hypotheses on 2,179 hospitals using data collected and matched from three different sources. Our findings suggest that different types of IT differentially affect hospital processes and these same processes influence performance metrics such as mortality and patient satisfaction. For example, cardiology IT has a greater effect on objective patient health status through improvements in the technical protocols of care. Surprisingly, administrative IT was shown to adversely affect interpersonal care processes. It could be true that the IT is intrusive and interferes in the doctor-patient relationship; however, a post hoc analysis suggests the possibility of curvilinear impacts. Thus, managers should recognize that over- and underinvestment in IT can potentially have negative effects on performance and these results vary by IT type. Both technical and interpersonal processes yielded significant relationships to their respective outcomes and some cross-outcome effects were found, further suggesting that the mediating role of processes is an important link between IT and value.\n",
      "Content: Dual Role of IT-Assisted Communication in Patient Care: A Validated Structure-Process-Outcome Framework Despite the fact that about 90 percent of information transactions in hospitals are communications between patients, doctors, nurses, and other staff, little research has addressed the role that information technology (IT) plays in improving the efficiency and effectiveness of these communications-based transactions. Addressing this research gap is important considering that a substantial number of adverse hospital events stem from communication failures. Furthermore, effective communication is a major driver of patient satisfaction in hospitals. Using a structure-process-outcome (SPO) framework and guided by the strategic role of IT literature, we develop a model that includes \"structure,\" operationalized as organizational characteristics and two different categories of IT; \"process,\" two different communication-based processes; and \"outcomes,\" quantified as case-mix adjusted mortality, patient loyalty, and patient ratings. Specifically, we hypothesize that a subset of clinical IT (cardiology IT) will affect technical protocols of patient care, which in turn affects mortality, while administrative IT will affect interpersonal patient care, which relates to patient loyalty and ratings. Thus, IT can serve as a double-edged sword affecting both technical and interpersonal processes of care, but possibly independently and differentially. We test our hypotheses on 2,179 hospitals using data collected and matched from three different sources. Our findings suggest that different types of IT differentially affect hospital processes and these same processes influence performance metrics such as mortality and patient satisfaction. For example, cardiology IT has a greater effect on objective patient health status through improvements in the technical protocols of care. Surprisingly, administrative IT was shown to adversely affect interpersonal care processes. It could be true that the IT is intrusive and interferes in the doctor-patient relationship; however, a post hoc analysis suggests the possibility of curvilinear impacts. Thus, managers should recognize that over- and underinvestment in IT can potentially have negative effects on performance and these results vary by IT type. Both technical and interpersonal processes yielded significant relationships to their respective outcomes and some cross-outcome effects were found, further suggesting that the mediating role of processes is an important link between IT and value.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 406\n",
      "Title: Antecedents of Information Systems Sourcing Strategies in U.s. Hospitals: A Longitudinal Study\n",
      "Abstract: The popular press has long used the terms single-sourcing and multisourcing (also known as best of breed) to describe organizations’ sourcing strategies. Whereas there is an implicit understanding of these terms, no research has quantified what distinguishes one sourcing configuration from another or what institutional factors contribute to the pursuit of one strategy over the other. We leverage institutional theory to examine how key organizational antecedents such as strategic orientation (mission), formal structure (size), and internal dynamics (patient case mix complexity) influence the rate at which organizations move toward or away from a single-sourcing configuration. Employing longitudinal modeling combined with sequence analysis techniques, we empirically evaluate IS sourcing strategies of nearly all U.S. hospitals operating continuously over a 9-year time frame from 2005 to 2013. We find that hospitals are generally trending toward a single-sourcing configuration and that formal structure and internal dynamics serve as predictors of this trend. Contrary to the predictions of institutional theory, we find that strategic orientation is not predictive of IS sourcing strategy. These results have important implications for research and practice. Notably, we are the first to quantify sourcing strategies, and, by doing so, are able to inform practitioners and academics of the key organizational characteristics that lead hospitals to move more quickly toward single-sourcing configurations.\n",
      "Content: Antecedents of Information Systems Sourcing Strategies in U.s. Hospitals: A Longitudinal Study The popular press has long used the terms single-sourcing and multisourcing (also known as best of breed) to describe organizations’ sourcing strategies. Whereas there is an implicit understanding of these terms, no research has quantified what distinguishes one sourcing configuration from another or what institutional factors contribute to the pursuit of one strategy over the other. We leverage institutional theory to examine how key organizational antecedents such as strategic orientation (mission), formal structure (size), and internal dynamics (patient case mix complexity) influence the rate at which organizations move toward or away from a single-sourcing configuration. Employing longitudinal modeling combined with sequence analysis techniques, we empirically evaluate IS sourcing strategies of nearly all U.S. hospitals operating continuously over a 9-year time frame from 2005 to 2013. We find that hospitals are generally trending toward a single-sourcing configuration and that formal structure and internal dynamics serve as predictors of this trend. Contrary to the predictions of institutional theory, we find that strategic orientation is not predictive of IS sourcing strategy. These results have important implications for research and practice. Notably, we are the first to quantify sourcing strategies, and, by doing so, are able to inform practitioners and academics of the key organizational characteristics that lead hospitals to move more quickly toward single-sourcing configurations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 407\n",
      "Title: An Odyssey into Virtual Worlds: Exploring the Impacts of Technological and Spatial Environments on Intention to Purchase Virtual Products1\n",
      "Abstract: Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus-organism-response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants' virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants' virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.\n",
      "Content: An Odyssey into Virtual Worlds: Exploring the Impacts of Technological and Spatial Environments on Intention to Purchase Virtual Products1 Although research on three-dimensional virtual environments abounds, little is known about the social and business aspects of virtual worlds. Given the emergence of large-scale social virtual worlds, such as Second Life, and the dramatic growth in sales of virtual goods, it is important to understand the dynamics that govern the purchase of virtual goods in virtual worlds. Employing the stimulus-organism-response (S-O-R) framework, we investigate how technological (interactivity and sociability) and spatial (density and stability) environments in virtual worlds influence the participants' virtual experiences (telepresence, social presence, and flow), and how experiences subsequently affect their response (intention to purchase virtual goods). The results of our survey of 354 Second Life residents indicate that interactivity, which enhances the interaction with objects, has a significant positive impact on telepresence and flow. Also, sociability, which fosters interactions with participants, is significantly associated with social presence, although no such significant impact was observed on flow. Furthermore, both density and stability are found to significantly influence participants' virtual experiences; stability helps users to develop strong social bonds, thereby increasing both social presence and flow. However, contrary to our prediction of curvilinear patterns, density is linearly associated with flow and social presence. Interestingly, the results exhibit two opposing effects of density: while it reduces the extent of flow, density increases the amount of social presence. Since social presence is found to increase flow, the net impact of density on flow depends heavily on the relative strength of the associations involving these three constructs. Finally, we find that flow mediates the impacts of technological and spatial environments on intention to purchase virtual products. We conclude the paper with a discussion of the theoretical and practical contributions of our findings.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 408\n",
      "Title: Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation\n",
      "Abstract: Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.\n",
      "Content: Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 409\n",
      "Title: Competing \"Creatively\" in Sponsored Search Markets: The Effect of Rank, Differentiation Strategy, and Competition on Performance\n",
      "Abstract: Although efficiency-enhancing features of online markets have been well studied, much less is known about firms' differentiation strategies in these competitive markets or the outcomes of such differentiation. This study examines competition among firms in online sponsored search markets—one of the fastest growing and most competitive of online markets. We develop and test a model that predicts the clickthrough rate (CTR) of a seller's listing in a sponsored search setting. Drawing on consumer search theory and competitive positioning strategies, we theorize that CTR is jointly driven by a seller's positioning strategy as reflected by the unique selling proposition (USP) in its \"ad creative,\" by its rank in a sponsored search listing, and by the nature of competition around the focal firm's listing. We use data from a field experiment conducted by a leading firm in the mortgage industry where the firm varied its rank and USP dynamically. Results suggest that sponsored search listings can act as effective customer segmentation mechanisms, consistent with a model of consumer search in directional markets. We further find that the effect on CTR of a firm's positioning strategy and its rank in a listing is strongly moderated by its ability to differentiate itself from adjacent rivals. We discuss the implications of our findings for sellers' strategies in sponsored search markets and for extending the understanding of consumer search behavior in directional markets.\n",
      "Content: Competing \"Creatively\" in Sponsored Search Markets: The Effect of Rank, Differentiation Strategy, and Competition on Performance Although efficiency-enhancing features of online markets have been well studied, much less is known about firms' differentiation strategies in these competitive markets or the outcomes of such differentiation. This study examines competition among firms in online sponsored search markets—one of the fastest growing and most competitive of online markets. We develop and test a model that predicts the clickthrough rate (CTR) of a seller's listing in a sponsored search setting. Drawing on consumer search theory and competitive positioning strategies, we theorize that CTR is jointly driven by a seller's positioning strategy as reflected by the unique selling proposition (USP) in its \"ad creative,\" by its rank in a sponsored search listing, and by the nature of competition around the focal firm's listing. We use data from a field experiment conducted by a leading firm in the mortgage industry where the firm varied its rank and USP dynamically. Results suggest that sponsored search listings can act as effective customer segmentation mechanisms, consistent with a model of consumer search in directional markets. We further find that the effect on CTR of a firm's positioning strategy and its rank in a listing is strongly moderated by its ability to differentiate itself from adjacent rivals. We discuss the implications of our findings for sellers' strategies in sponsored search markets and for extending the understanding of consumer search behavior in directional markets.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 410\n",
      "Title: Support Structures and Their Impacts on Employee Outcomes: A Longitudinal Field Study of an Enterprise System Implementation\n",
      "Abstract: Despite the impressive progress in understanding the benefits and challenges related to enterprise system (ES) implementations—such as enterprise resource planning (ERP) systems—little is known about how the support structures traditionally used by organizations to help employees cope with a new ES affect employee outcomes related to the system and their jobs. Likewise, little is known about how existing peer advice ties in the business unit influence these outcomes after an ES implementation. Understanding employee outcomes is critical because of their ramifications for long-term ES success. This paper examines the impacts of four traditional support structures (namely, training, online support, help desk support, and change management support), and peer advice ties on four key employee outcomes (namely, system satisfaction, job stress, job satisfaction, and job performance). This paper also seeks to show that it is peer advice ties that best fill the complex informational needs of employees after an ES implementation by providing the right information at the right time and in the right context. The proposed model was tested in a field study conducted in one business unit of a large telecommunications company and gathered data from 120 supplier liaisons over the course of a year. Both traditional support structures and peer advice ties were found to influence the various outcomes, even after controlling for pre-implementation levels of the dependent variables. In all cases, peer advice ties was the strongest predictor, thus underscoring the importance of this critical internal resource.\n",
      "Content: Support Structures and Their Impacts on Employee Outcomes: A Longitudinal Field Study of an Enterprise System Implementation Despite the impressive progress in understanding the benefits and challenges related to enterprise system (ES) implementations—such as enterprise resource planning (ERP) systems—little is known about how the support structures traditionally used by organizations to help employees cope with a new ES affect employee outcomes related to the system and their jobs. Likewise, little is known about how existing peer advice ties in the business unit influence these outcomes after an ES implementation. Understanding employee outcomes is critical because of their ramifications for long-term ES success. This paper examines the impacts of four traditional support structures (namely, training, online support, help desk support, and change management support), and peer advice ties on four key employee outcomes (namely, system satisfaction, job stress, job satisfaction, and job performance). This paper also seeks to show that it is peer advice ties that best fill the complex informational needs of employees after an ES implementation by providing the right information at the right time and in the right context. The proposed model was tested in a field study conducted in one business unit of a large telecommunications company and gathered data from 120 supplier liaisons over the course of a year. Both traditional support structures and peer advice ties were found to influence the various outcomes, even after controlling for pre-implementation levels of the dependent variables. In all cases, peer advice ties was the strongest predictor, thus underscoring the importance of this critical internal resource.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 411\n",
      "Title: Improving the retention of women in the IT workforce: An investigation of gender diversity interventions in the USA\n",
      "Abstract: To meet the high demand for information technology (IT) professionals, organizations must become more effective at attracting and retaining women. Ninety-seven percent of companies surveyed by Forbes in 2011 had implemented diversity and inclusion interventions. Despite these efforts, the percentage of women working in IT continues to decline, raising questions about the effectiveness of current organizational interventions aimed at increasing gender diversity. This study sought to gain a better understanding of these organizational interventions by developing a comprehensive framework based on comparative case studies of 9 organizations. The framework integrates intervention characteristics and barriers IT women experience and the coping methods they use to address barriers. This paper presents propositions based on this theoretical framework to guide further research on the effectiveness of gender diversity and inclusion interventions in IT.\n",
      "Content: Improving the retention of women in the IT workforce: An investigation of gender diversity interventions in the USA To meet the high demand for information technology (IT) professionals, organizations must become more effective at attracting and retaining women. Ninety-seven percent of companies surveyed by Forbes in 2011 had implemented diversity and inclusion interventions. Despite these efforts, the percentage of women working in IT continues to decline, raising questions about the effectiveness of current organizational interventions aimed at increasing gender diversity. This study sought to gain a better understanding of these organizational interventions by developing a comprehensive framework based on comparative case studies of 9 organizations. The framework integrates intervention characteristics and barriers IT women experience and the coping methods they use to address barriers. This paper presents propositions based on this theoretical framework to guide further research on the effectiveness of gender diversity and inclusion interventions in IT.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 412\n",
      "Title: Logics' shift and depletion of innovation: A multi-level study of agile use in a multinational telco company\n",
      "Abstract: The use of Agile practices is typically associated to a wide array of benefits for organizations. This paper extends growing research on the ‘dark’ side of Agile by investigating the depletion of innovation in a large telco company following the large-scale implementation of Agile in R&D units. Our qualitative study reveals a shift in the organizational logics underpinning new product development, from a “navigating through unchartered waters” to a “putting out fires” logic. We tracked the change in key components of logics (goals of teams, source of legitimacy of team members and support and control systems) and explained the multi-level mechanisms through which the shift occurred, i.e., changes in processes of workflow management, work allocation, and performance management. We found that the new organizational logic negatively impacted individual attitudes towards the generation of new ideas by promoting the internalization of short-termism, a perceived drain in competences and confidence, and the lack of accountability for innovation. By focusing on changes in organizational logics, our insights expand current knowledge about the relationship between Agile implementation and individual attitudes. We also explain why unexpected effects of Agile implementation may go undetected in organizations, because they derive from multi-level, diffused, changes in the organization.\n",
      "Content: Logics' shift and depletion of innovation: A multi-level study of agile use in a multinational telco company The use of Agile practices is typically associated to a wide array of benefits for organizations. This paper extends growing research on the ‘dark’ side of Agile by investigating the depletion of innovation in a large telco company following the large-scale implementation of Agile in R&D units. Our qualitative study reveals a shift in the organizational logics underpinning new product development, from a “navigating through unchartered waters” to a “putting out fires” logic. We tracked the change in key components of logics (goals of teams, source of legitimacy of team members and support and control systems) and explained the multi-level mechanisms through which the shift occurred, i.e., changes in processes of workflow management, work allocation, and performance management. We found that the new organizational logic negatively impacted individual attitudes towards the generation of new ideas by promoting the internalization of short-termism, a perceived drain in competences and confidence, and the lack of accountability for innovation. By focusing on changes in organizational logics, our insights expand current knowledge about the relationship between Agile implementation and individual attitudes. We also explain why unexpected effects of Agile implementation may go undetected in organizations, because they derive from multi-level, diffused, changes in the organization.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 413\n",
      "Title: SYNOPSE: a model-based decision support system for the evaluation of flight schedules for cargo airlines\n",
      "Abstract: The ability to evaluate flight schedules, with respect to cost, revenue and contribution to profit, is essential for cargo airlines to respond properly to changing environments in this highly competitive and consumer-oriented market. In this paper, we introduce SYNOPSE, our model-based decision support system for the evaluation of flight schedules for cargo airlines, by describing the underlying planning situation, the data model and the decision models used, and the implementation, i.e., the development process, as well as the architecture. A small example illustrates the high complexity of the analysis process and the supporting qualities of our decision-support system (DSS).\n",
      "Content: SYNOPSE: a model-based decision support system for the evaluation of flight schedules for cargo airlines The ability to evaluate flight schedules, with respect to cost, revenue and contribution to profit, is essential for cargo airlines to respond properly to changing environments in this highly competitive and consumer-oriented market. In this paper, we introduce SYNOPSE, our model-based decision support system for the evaluation of flight schedules for cargo airlines, by describing the underlying planning situation, the data model and the decision models used, and the implementation, i.e., the development process, as well as the architecture. A small example illustrates the high complexity of the analysis process and the supporting qualities of our decision-support system (DSS).\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 414\n",
      "Title: The leveraging influence of strategic alignment on IT investment: An empirical examination\n",
      "Abstract: Businesses have invested enormous sums in information technology (IT). The challenge now is to optimize these investments. We empirically examined the influence of the alignment between IS strategy and business strategy (strategic alignment) on the payoff of IT investment. Many studies have been performed on the value of IT investment and strategic alignment separately, in the past, but here we combined them by investigating the moderating affect of strategic alignment on the relationship between IT investment and firm performance for a group of manufacturing firms. The results indicated that there is a synergistic coupling between strategic alignment and IT investment with firm performance. Firms that have aligned IT and business strategies can invest in additional IT resources with some assurance that they will be leveraged substantially. One of our main contributions was in the examination of four differing perspectives of strategic alignment and their relationship with the payoff of IT investment.\n",
      "Content: The leveraging influence of strategic alignment on IT investment: An empirical examination Businesses have invested enormous sums in information technology (IT). The challenge now is to optimize these investments. We empirically examined the influence of the alignment between IS strategy and business strategy (strategic alignment) on the payoff of IT investment. Many studies have been performed on the value of IT investment and strategic alignment separately, in the past, but here we combined them by investigating the moderating affect of strategic alignment on the relationship between IT investment and firm performance for a group of manufacturing firms. The results indicated that there is a synergistic coupling between strategic alignment and IT investment with firm performance. Firms that have aligned IT and business strategies can invest in additional IT resources with some assurance that they will be leveraged substantially. One of our main contributions was in the examination of four differing perspectives of strategic alignment and their relationship with the payoff of IT investment.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 415\n",
      "Title: The property of being causal – The conduct of qualitative comparative analysis in information systems research\n",
      "Abstract: Grounded in configuration and complexity theory, qualitative comparative analysis (QCA) combines the ad­ vantages of case-based and variable-oriented methods for rendering complex information systems (IS) phe­ nomena comprehensible. Given its manifold benefits, the QCA method has attracted considerable attention in IS research, with an increasing number of studies employing it as their methodological approach. Based on a comprehensive review and synthesis of recent QCA practices from the IS field, covering 12 years of research, we outline the most prevalent research gaps and limitations concerning QCA’s methodological application prior to identifying issues for further improvement as well as highlighting future research directions.\n",
      "Content: The property of being causal – The conduct of qualitative comparative analysis in information systems research Grounded in configuration and complexity theory, qualitative comparative analysis (QCA) combines the ad­ vantages of case-based and variable-oriented methods for rendering complex information systems (IS) phe­ nomena comprehensible. Given its manifold benefits, the QCA method has attracted considerable attention in IS research, with an increasing number of studies employing it as their methodological approach. Based on a comprehensive review and synthesis of recent QCA practices from the IS field, covering 12 years of research, we outline the most prevalent research gaps and limitations concerning QCA’s methodological application prior to identifying issues for further improvement as well as highlighting future research directions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 416\n",
      "Title: The use of a knowledge-based system in conceptual data modeling\n",
      "Abstract: Based on a study of the data modeling process of novice designers, and the errors they commit, a knowledge-based system (KBS) was designed and developed. It was found that the performance of novice designers was significantly better when they utilized the KBS instead of a system with no knowledge base. Two versions of the KBS—one with a guidance interface that advised the designer on appropriate design choices and another with a restrictive interface that restricted the design choices available to the designer—were developed. The restrictive interface was rated as being significantly easier to use than the guidance interface.\n",
      "Content: The use of a knowledge-based system in conceptual data modeling Based on a study of the data modeling process of novice designers, and the errors they commit, a knowledge-based system (KBS) was designed and developed. It was found that the performance of novice designers was significantly better when they utilized the KBS instead of a system with no knowledge base. Two versions of the KBS—one with a guidance interface that advised the designer on appropriate design choices and another with a restrictive interface that restricted the design choices available to the designer—were developed. The restrictive interface was rated as being significantly easier to use than the guidance interface.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 417\n",
      "Title: Determinants of escrow service adoption in consumer-to-consumer online auction market: An experimental study\n",
      "Abstract: Risk relief services (RRSs), as complementary to online trust promoting services, are becoming versatile options for risk reduction in online consumer-to-consumer auctions. In this paper, we identify factors that affect the behavior of buyers in an online auction market who had to either adopt or not adopt online escrow services (OES). An experimental C2C auction system with embedded decision support features was used to collect data. Results show that market factors, such as fraud rate, product price, and seller's reputation are important in determining buyers' OES adoption. This study also finds that sellers' reputation has a significant effect on buyer's risk perception, which influences his OES adoption decision. Furthermore, the buyers' OES adoption decisions were found to be congruent with the implied recommendations that were based on expected utility calculations.\n",
      "Content: Determinants of escrow service adoption in consumer-to-consumer online auction market: An experimental study Risk relief services (RRSs), as complementary to online trust promoting services, are becoming versatile options for risk reduction in online consumer-to-consumer auctions. In this paper, we identify factors that affect the behavior of buyers in an online auction market who had to either adopt or not adopt online escrow services (OES). An experimental C2C auction system with embedded decision support features was used to collect data. Results show that market factors, such as fraud rate, product price, and seller's reputation are important in determining buyers' OES adoption. This study also finds that sellers' reputation has a significant effect on buyer's risk perception, which influences his OES adoption decision. Furthermore, the buyers' OES adoption decisions were found to be congruent with the implied recommendations that were based on expected utility calculations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 418\n",
      "Title: Could the use of a knowledge-based system lead to implicit learning?\n",
      "Abstract: The primary objective of a knowledge-based system (KBS) is to use stored knowledge to provide support for decision-making activities. Empirical studies identify improvements in decision processes and outcomes with the use of such knowledge-based systems. This research suggests that though a KBS is primarily developed to help users in their decision-making activities, as an unintentional consequence, it may induce them to implicitly learn more about a problem. Implicit learning occurs when a person learns unconsciously or unintentionally, without being explicitly instructed or tutored. To test these ideas, a laboratory-based experiment was conducted with a KBS that could provide support for data modeling activities. Results indicated support for implicit learning because subjects who interacted with the KBS exhibited better knowledge on data modeling concepts than those who did not interact with the KBS. Two versions of the KBS were tested, one with a restrictive interface and the other with a guidance interface, and both versions of the interface supported implicit learning. Implications for future research on the design and development of KBSs are proposed.\n",
      "Content: Could the use of a knowledge-based system lead to implicit learning? The primary objective of a knowledge-based system (KBS) is to use stored knowledge to provide support for decision-making activities. Empirical studies identify improvements in decision processes and outcomes with the use of such knowledge-based systems. This research suggests that though a KBS is primarily developed to help users in their decision-making activities, as an unintentional consequence, it may induce them to implicitly learn more about a problem. Implicit learning occurs when a person learns unconsciously or unintentionally, without being explicitly instructed or tutored. To test these ideas, a laboratory-based experiment was conducted with a KBS that could provide support for data modeling activities. Results indicated support for implicit learning because subjects who interacted with the KBS exhibited better knowledge on data modeling concepts than those who did not interact with the KBS. Two versions of the KBS were tested, one with a restrictive interface and the other with a guidance interface, and both versions of the interface supported implicit learning. Implications for future research on the design and development of KBSs are proposed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 419\n",
      "Title: Managing uncertainty in decision support models foreword to the special issue\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 420\n",
      "Title: TEA-IS: A hybrid DEA-TOPSIS approach for assessing performance and synergy in Chinese health care\n",
      "Abstract: This paper presents an assessment of the Chinese healthcare system in 31 provinces for a 10-year period in light of relevant physical and human resource variables. First, a novel TEA-IS (Trigonometric Envelopment Analysis for Ideal Solutions) model is developed to assess healthcare efficiency at the province level. Machine learning methods are also employed to predict high-low performance and the synergistic Chinese healthcare province in terms of contextual variables. The results indicate that synergy has played a pivotal role in the Chinese healthcare systems, not only by triggering higher performance levels due to the progressive adoption of best practices over the course of time, but also by being closely related to different socioeconomic and demographic variables, such as the illiteracy rate. It is possible to claim that healthcare performance has remained stable in China over the past two decades, performance and synergy at the province level are still heterogeneous.\n",
      "Content: TEA-IS: A hybrid DEA-TOPSIS approach for assessing performance and synergy in Chinese health care This paper presents an assessment of the Chinese healthcare system in 31 provinces for a 10-year period in light of relevant physical and human resource variables. First, a novel TEA-IS (Trigonometric Envelopment Analysis for Ideal Solutions) model is developed to assess healthcare efficiency at the province level. Machine learning methods are also employed to predict high-low performance and the synergistic Chinese healthcare province in terms of contextual variables. The results indicate that synergy has played a pivotal role in the Chinese healthcare systems, not only by triggering higher performance levels due to the progressive adoption of best practices over the course of time, but also by being closely related to different socioeconomic and demographic variables, such as the illiteracy rate. It is possible to claim that healthcare performance has remained stable in China over the past two decades, performance and synergy at the province level are still heterogeneous.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 421\n",
      "Title: Gamification: A key determinant of massive open online course (MOOC) success\n",
      "Abstract: Massive open online courses (MOOCs), contribute significantly to individual empowerment because they can help people learn about a wide range of topics. To realize the full potential of MOOCs, we need to understand their factors of success, here defined as the use, user satisfaction, along the individual and organizational performance resulting from the user involvement. We propose a theoretical framework to identify the determinants of successful MOOCs, and empirically measure these factors in a real MOOC context. We put forward the role of gamification and suggest that, together with information system (IS) theory, gamification proved to play a crucial role in the success of MOOCs.\n",
      "Content: Gamification: A key determinant of massive open online course (MOOC) success Massive open online courses (MOOCs), contribute significantly to individual empowerment because they can help people learn about a wide range of topics. To realize the full potential of MOOCs, we need to understand their factors of success, here defined as the use, user satisfaction, along the individual and organizational performance resulting from the user involvement. We propose a theoretical framework to identify the determinants of successful MOOCs, and empirically measure these factors in a real MOOC context. We put forward the role of gamification and suggest that, together with information system (IS) theory, gamification proved to play a crucial role in the success of MOOCs.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 422\n",
      "Title: Strategic profiles and Internet Performance: An empirical investigation into the development of a strategic Internet system\n",
      "Abstract: Organizations continue to work on defining and developing better strategic Internet systems. The use of the Internet should not, however, adversely affect their existing business processes but rather incorporate and support them. Therefore, developing a strategic Internet system should support their strategic profile while providing high performance from its use. We took a new approach to Internet development by analyzing an organization's Internet use and performance by adopting Miles and Snow's classifications of business strategy: Defenders, Analyzers, and Prospectors. A sample of 257 IT managers and professionals helped in suggesting relationships between Internet use and performance; the consequent model was then validated. Further analysis was then conducted at the dimension level to highlight the differences between strategic profiles and the appropriate Internet use. The results indicated identifiable approaches for different strategic profiles and their desired performance levels.\n",
      "Content: Strategic profiles and Internet Performance: An empirical investigation into the development of a strategic Internet system Organizations continue to work on defining and developing better strategic Internet systems. The use of the Internet should not, however, adversely affect their existing business processes but rather incorporate and support them. Therefore, developing a strategic Internet system should support their strategic profile while providing high performance from its use. We took a new approach to Internet development by analyzing an organization's Internet use and performance by adopting Miles and Snow's classifications of business strategy: Defenders, Analyzers, and Prospectors. A sample of 257 IT managers and professionals helped in suggesting relationships between Internet use and performance; the consequent model was then validated. Further analysis was then conducted at the dimension level to highlight the differences between strategic profiles and the appropriate Internet use. The results indicated identifiable approaches for different strategic profiles and their desired performance levels.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 423\n",
      "Title: Investigating Retrieval-Induced Forgetting During Information Requirements Determination\n",
      "Abstract: Successful systems development requires that appropriate and accurate information be gathered from people who use or will use the system. One critical issue in information gathering is the recall of relevant information by users and other stakeholders. Prior research has shown that users do not recall all the relevant information they have about the requirements for systems, and we suggest that this problem is exacerbated by current systems development practice, in which the same users are often interviewed multiple times by analysts. A potential theoretical explanation for recall failure in requirements determination is the psychological phenomenon known as Retrieval-Induced Forgetting. Retrieval-Induced Forgetting (RIF) theory and empirical findings show that when people are asked to recall information about a situation multiple times, they are likely to recall the same information on subsequent attempts as they recalled on the first attempt (to the exclusion of other relevant information). Although the RIF phenomenon has been investigated in several contexts, such as eyewitness testimony, there have been no studies that have examined the issue in applied contexts such as systems development, in which prior domain knowledge exists and has been learned over a period of time by users and other stakeholders. In the current study, experimental results showed the presence of RIF in both short-term and longer-term information requirements determination (IRD) contexts, thereby providing a memory-based explanation for missing requirements in IRD. Our results have strong implications for the type and sequencing of requirements elicitation techniques and demonstrate threats to both traditional and iterative development methodologies.\n",
      "Content: Investigating Retrieval-Induced Forgetting During Information Requirements Determination Successful systems development requires that appropriate and accurate information be gathered from people who use or will use the system. One critical issue in information gathering is the recall of relevant information by users and other stakeholders. Prior research has shown that users do not recall all the relevant information they have about the requirements for systems, and we suggest that this problem is exacerbated by current systems development practice, in which the same users are often interviewed multiple times by analysts. A potential theoretical explanation for recall failure in requirements determination is the psychological phenomenon known as Retrieval-Induced Forgetting. Retrieval-Induced Forgetting (RIF) theory and empirical findings show that when people are asked to recall information about a situation multiple times, they are likely to recall the same information on subsequent attempts as they recalled on the first attempt (to the exclusion of other relevant information). Although the RIF phenomenon has been investigated in several contexts, such as eyewitness testimony, there have been no studies that have examined the issue in applied contexts such as systems development, in which prior domain knowledge exists and has been learned over a period of time by users and other stakeholders. In the current study, experimental results showed the presence of RIF in both short-term and longer-term information requirements determination (IRD) contexts, thereby providing a memory-based explanation for missing requirements in IRD. Our results have strong implications for the type and sequencing of requirements elicitation techniques and demonstrate threats to both traditional and iterative development methodologies.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 424\n",
      "Title: The Impact of Analyst-Induced Misinformation on the Requirements Elicitation Process\n",
      "Abstract: Information requirements determination (IRD) is concerned with developing accurate requirements for a proposed system, primarily by eliciting information from users and other organizational stakeholders. In this paper we build and test theory concerning a significant threat to the accuracy of information requirements, termed the misinformation effect. Misinformation is distorted, false, or other erroneous or misleading information that does not reflect the true state of the world or state of mind of the person communicating the information. The misinformation effect refers to the tendency of people to recall misleading or false information introduced to them following an event instead of original material learned or observed at the time the event occurred. During user-analyst communication in the IRD process, analysts may introduce misinformation in their discussions with users. We use the misinformation effect literature to hypothesize that in such circumstance users are likely to recall misinformation introduced by analysts rather than their true beliefs and knowledge of facts. Additionally, we use literature in social psychology to hypothesize that the misinformation effect will be stronger when misinformation is introduced using a social technique rather than a nonsocial technique. We conducted an experiment to test the misinformation effect in the requirements elicitation process. Results indicated that (1) introduction of misinformation reduces the accuracy of requirements provided by users, and (2) social techniques (interviews) are more vulnerable to the misinformation effect than nonsocial techniques (surveys). Our research contributes to the information systems literature by identifying an important reason that requirements provided by users may be inaccurate, and to IRD practice by identifying important dilemmas caused by the misinformation effect as well as potential solutions. We also contribute to the psychology literature by demonstrating the existence of the misinformation effect with users' experiential factual knowledge and beliefs in a business context, and by aiding in understanding the underlying causes of the misinformation effect. We discuss implications of our findings and directions for future research to address challenges resulting from the misinformation effect.\n",
      "Content: The Impact of Analyst-Induced Misinformation on the Requirements Elicitation Process Information requirements determination (IRD) is concerned with developing accurate requirements for a proposed system, primarily by eliciting information from users and other organizational stakeholders. In this paper we build and test theory concerning a significant threat to the accuracy of information requirements, termed the misinformation effect. Misinformation is distorted, false, or other erroneous or misleading information that does not reflect the true state of the world or state of mind of the person communicating the information. The misinformation effect refers to the tendency of people to recall misleading or false information introduced to them following an event instead of original material learned or observed at the time the event occurred. During user-analyst communication in the IRD process, analysts may introduce misinformation in their discussions with users. We use the misinformation effect literature to hypothesize that in such circumstance users are likely to recall misinformation introduced by analysts rather than their true beliefs and knowledge of facts. Additionally, we use literature in social psychology to hypothesize that the misinformation effect will be stronger when misinformation is introduced using a social technique rather than a nonsocial technique. We conducted an experiment to test the misinformation effect in the requirements elicitation process. Results indicated that (1) introduction of misinformation reduces the accuracy of requirements provided by users, and (2) social techniques (interviews) are more vulnerable to the misinformation effect than nonsocial techniques (surveys). Our research contributes to the information systems literature by identifying an important reason that requirements provided by users may be inaccurate, and to IRD practice by identifying important dilemmas caused by the misinformation effect as well as potential solutions. We also contribute to the psychology literature by demonstrating the existence of the misinformation effect with users' experiential factual knowledge and beliefs in a business context, and by aiding in understanding the underlying causes of the misinformation effect. We discuss implications of our findings and directions for future research to address challenges resulting from the misinformation effect.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 425\n",
      "Title: New Information Systems Leaders: A Changing Role in a Changing World\n",
      "Abstract:  It is widely argued that the information systems (IS) leadership function has undergone fundamental changes over the past decade. To better understand the changes, this study compares the backgrounds, responsibilities, reporting relationships, and power of newly appointed IS executives (who had been in their position for two years or less) with established IS executives (who had been in their position for five years or more). The study found that approximately half of the new IS executives were external hires, whereas almost all of the established IS executives were promoted from within the company. More than two-thirds of the new IS executives had more than five years' experience managing a non-IS function within the past 15 years. Established IS executives had spent the majority of their career within the IS function. The activities receiving the most attention from new IS executives were information technology (IT) strategic planning and control, IT architecture management and stan-' Portions of this study were previously reported in: Applegate, L.M. and Elam, J.J. \"CIO and SuperCIO.\" ClO, April 1991.dards development, and human resource management. For established IS executives, the activities receiving the most attention were IT architecture management and standards development, human resource management, and operations. An increasing number of new IS executives reported directly to the CEO, and almost half were members of the senior management/ strategic policy committee. These findings have several important implications. First, the senior IS executive must be able to bring a broad business perspective to the position. Current senior IS executives who have not broadened their own knowledge, skills, and experiences in business strategy, management, and operations should immediately develop a personal career development program to gain these valuable perspectives. Second, senior IS executives should implement career development strategies within their own organizations that ensure that IS professionals have the opportunity to acquire the business management experience necessary to advance to higher IS management levels. Third, graduate and executive programs designed to prepare future IS managers and leaders must provide both a business and IT perspective throughout the curriculum. \n",
      "Content: New Information Systems Leaders: A Changing Role in a Changing World  It is widely argued that the information systems (IS) leadership function has undergone fundamental changes over the past decade. To better understand the changes, this study compares the backgrounds, responsibilities, reporting relationships, and power of newly appointed IS executives (who had been in their position for two years or less) with established IS executives (who had been in their position for five years or more). The study found that approximately half of the new IS executives were external hires, whereas almost all of the established IS executives were promoted from within the company. More than two-thirds of the new IS executives had more than five years' experience managing a non-IS function within the past 15 years. Established IS executives had spent the majority of their career within the IS function. The activities receiving the most attention from new IS executives were information technology (IT) strategic planning and control, IT architecture management and stan-' Portions of this study were previously reported in: Applegate, L.M. and Elam, J.J. \"CIO and SuperCIO.\" ClO, April 1991.dards development, and human resource management. For established IS executives, the activities receiving the most attention were IT architecture management and standards development, human resource management, and operations. An increasing number of new IS executives reported directly to the CEO, and almost half were members of the senior management/ strategic policy committee. These findings have several important implications. First, the senior IS executive must be able to bring a broad business perspective to the position. Current senior IS executives who have not broadened their own knowledge, skills, and experiences in business strategy, management, and operations should immediately develop a personal career development program to gain these valuable perspectives. Second, senior IS executives should implement career development strategies within their own organizations that ensure that IS professionals have the opportunity to acquire the business management experience necessary to advance to higher IS management levels. Third, graduate and executive programs designed to prepare future IS managers and leaders must provide both a business and IT perspective throughout the curriculum. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 426\n",
      "Title: Reusability-Based Strategy for Development of Information Systems: Implementation Experience of a Bank\n",
      "Abstract:  This paper describes the experience of a large bank in designing and implementing an information systems strategy that is based on the concept of resuability. The design and implementation was performed in two stages: (1) building prototype to investigate the feasibility and attractiveness of reusability concept for the bank; and (2) its subsequent implementation using a library of reusable entities and a programmer's workbench. The implementation experience confirmed that applying the reusability concept to all stages of the system's life cycle results in both strategic (e.g., improving programmer productivity and increasing the bank's capacity for timely response to market opportunities) and operational (e.g., reducing and controlling system development and maintenance costs) benefits. It is estimated that the library of reusable entities embedded within the programmer workbench saved the bank over $1.5 million in development costs in 1989 alone. Two of the most important lessons learned in implementing the reusabilitybased strategy are: (1) reusability comes in many flavors and should be applied to all stages of systems fife cycle; and (2) major challenges implementing the reusability-based strategy are managerial, not technical. \n",
      "Content: Reusability-Based Strategy for Development of Information Systems: Implementation Experience of a Bank  This paper describes the experience of a large bank in designing and implementing an information systems strategy that is based on the concept of resuability. The design and implementation was performed in two stages: (1) building prototype to investigate the feasibility and attractiveness of reusability concept for the bank; and (2) its subsequent implementation using a library of reusable entities and a programmer's workbench. The implementation experience confirmed that applying the reusability concept to all stages of the system's life cycle results in both strategic (e.g., improving programmer productivity and increasing the bank's capacity for timely response to market opportunities) and operational (e.g., reducing and controlling system development and maintenance costs) benefits. It is estimated that the library of reusable entities embedded within the programmer workbench saved the bank over $1.5 million in development costs in 1989 alone. Two of the most important lessons learned in implementing the reusabilitybased strategy are: (1) reusability comes in many flavors and should be applied to all stages of systems fife cycle; and (2) major challenges implementing the reusability-based strategy are managerial, not technical. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 427\n",
      "Title: High tech or high touch? Efficient channel strategies for delivering financial services\n",
      "Abstract: With the progress of information technology, financial service institutions are restructuring their delivery channels. The delivery channel applications now range from direct sales and agency systems to all-electronic, customer-accessed networks. A key issue in the design of a delivery channel is achieving a proper mix of new technology and traditional human-centred delivery approach. Moreover, the delivery channel design should be properly matched with the characteristics of the service being offered. Based on a model of economic trade-offs, a theoretical framework, called the service channel strategies (SCS) approach, is proposed for identifying, analysing and designing appropriate delivery channel applications in the financial services industry. An example from banking is discussed to highlight the scope and normative focus of the proposed theoretical framework.\n",
      "Content: High tech or high touch? Efficient channel strategies for delivering financial services With the progress of information technology, financial service institutions are restructuring their delivery channels. The delivery channel applications now range from direct sales and agency systems to all-electronic, customer-accessed networks. A key issue in the design of a delivery channel is achieving a proper mix of new technology and traditional human-centred delivery approach. Moreover, the delivery channel design should be properly matched with the characteristics of the service being offered. Based on a model of economic trade-offs, a theoretical framework, called the service channel strategies (SCS) approach, is proposed for identifying, analysing and designing appropriate delivery channel applications in the financial services industry. An example from banking is discussed to highlight the scope and normative focus of the proposed theoretical framework.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 428\n",
      "Title: Exploring contributions of public resources in social bookmarking systems\n",
      "Abstract: Our study examines whether users’ contributions of public resources to social bookmarking sites are circumstantial (a side effect of bookmarking for oneself), or motivational (intentional bookmarking for others). We develop a research model based on these two explanations and test it using survey data from users of two bookmarking sites. Our results suggest that public contributions are mainly driven by intentional bookmarking of resources for other users. In addition, we found that users deliberately bookmark resources for others when they believe that their bookmarks are valuable to other users and when they perceive that other users are contributing as well.\n",
      "Content: Exploring contributions of public resources in social bookmarking systems Our study examines whether users’ contributions of public resources to social bookmarking sites are circumstantial (a side effect of bookmarking for oneself), or motivational (intentional bookmarking for others). We develop a research model based on these two explanations and test it using survey data from users of two bookmarking sites. Our results suggest that public contributions are mainly driven by intentional bookmarking of resources for other users. In addition, we found that users deliberately bookmark resources for others when they believe that their bookmarks are valuable to other users and when they perceive that other users are contributing as well.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 429\n",
      "Title: Digital Consumer Networks and Producer-Consumer Collaboration: Innovation and Product Development in the Video Game Industry\n",
      "Abstract: This paper examines new forms of collaboration between producers and consumers that are emerging in the digital entertainment space. Taking the case of the video game industry, we show how some firms have opened a portion of their proprietary content for transformation by consumers and allowed the development of consumer-designed and consumer-implemented derivative products. By reappropriating these derivatives, video game firms are successfully outsourcing parts of their game design and development process to digital consumer networks. Applying economic analysis, we explore the potential benefits and risks associated with outsourcing to networks of consumers. We also derive the optimal combination of copyright enforcement and consumer compensation. Our results suggest that profit-maximizing producers of video games have incentive to partially open game content to their users and to remunerate the most innovative ones, under the condition that the derivatives constitute complements to, and not substitutes for, the original product. We discuss the implications on firm strategy for innovation.\n",
      "Content: Digital Consumer Networks and Producer-Consumer Collaboration: Innovation and Product Development in the Video Game Industry This paper examines new forms of collaboration between producers and consumers that are emerging in the digital entertainment space. Taking the case of the video game industry, we show how some firms have opened a portion of their proprietary content for transformation by consumers and allowed the development of consumer-designed and consumer-implemented derivative products. By reappropriating these derivatives, video game firms are successfully outsourcing parts of their game design and development process to digital consumer networks. Applying economic analysis, we explore the potential benefits and risks associated with outsourcing to networks of consumers. We also derive the optimal combination of copyright enforcement and consumer compensation. Our results suggest that profit-maximizing producers of video games have incentive to partially open game content to their users and to remunerate the most innovative ones, under the condition that the derivatives constitute complements to, and not substitutes for, the original product. We discuss the implications on firm strategy for innovation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 430\n",
      "Title: Information, Technology, and Information Worker Productivity\n",
      "Abstract: We econometrically evaluate information worker productivity at a midsize executive recruiting firm and assess whether the knowledge that workers accessed through their electronic communication networks enabled them to multitask more productively. We estimate dynamic panel data models of multitasking, knowledge networks, and productivity using several types of micro-level data: (a) direct observation of more than 125,000 email messages over a period of 10 months; (b) detailed accounting data on individuals' project output and team membership for more than 1,300 projects spanning five years; and (c) survey and interview data about the same workers' IT skills, IT use, and information sharing. We find that (1) more multitasking is associated with more project output, but diminishing marginal returns, and (2) recruiters whose network contacts have heterogeneous knowledge—an even distribution of expertise over many project types—are less productive on average but more productive when juggling diverse multitasking portfolios. These results show how multitasking affects productivity and how knowledge networks, enabled by IT, can improve worker performance. The methods developed can be replicated in other settings, opening new frontiers for research on social networks and IT value.\n",
      "Content: Information, Technology, and Information Worker Productivity We econometrically evaluate information worker productivity at a midsize executive recruiting firm and assess whether the knowledge that workers accessed through their electronic communication networks enabled them to multitask more productively. We estimate dynamic panel data models of multitasking, knowledge networks, and productivity using several types of micro-level data: (a) direct observation of more than 125,000 email messages over a period of 10 months; (b) detailed accounting data on individuals' project output and team membership for more than 1,300 projects spanning five years; and (c) survey and interview data about the same workers' IT skills, IT use, and information sharing. We find that (1) more multitasking is associated with more project output, but diminishing marginal returns, and (2) recruiters whose network contacts have heterogeneous knowledge—an even distribution of expertise over many project types—are less productive on average but more productive when juggling diverse multitasking portfolios. These results show how multitasking affects productivity and how knowledge networks, enabled by IT, can improve worker performance. The methods developed can be replicated in other settings, opening new frontiers for research on social networks and IT value.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 431\n",
      "Title: Introduction to the Special Issue—Social Media and Business Transformation: A Framework for Research\n",
      "Abstract: Social media are fundamentally changing the way we communicate, collaborate, consume, and create. They represent one of the most transformative impacts of information technology on business, both within and outside firm boundaries. This special issue was designed to stimulate innovative investigations of the relationship between social media and business transformation. In this paper we outline a broad research agenda for understanding the relationships among social media, business, and society. We place the papers comprising the special issue within this research framework and identify areas where further research is needed. We hope that the flexible framework we outline will help guide future research and develop a cumulative research tradition in this area.\n",
      "Content: Introduction to the Special Issue—Social Media and Business Transformation: A Framework for Research Social media are fundamentally changing the way we communicate, collaborate, consume, and create. They represent one of the most transformative impacts of information technology on business, both within and outside firm boundaries. This special issue was designed to stimulate innovative investigations of the relationship between social media and business transformation. In this paper we outline a broad research agenda for understanding the relationships among social media, business, and society. We place the papers comprising the special issue within this research framework and identify areas where further research is needed. We hope that the flexible framework we outline will help guide future research and develop a cumulative research tradition in this area.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 432\n",
      "Title: Simulation modeling for pandemic decision making: A case study with bi-criteria analysis on school closures\n",
      "Abstract: Pandemic influenza continues to be a national and international public health concern, and has received significant attention worldwide with the A/H1N1 influenza outbreak in 2009. Many countries, including the United States, have developed preparedness plans for an influenza pandemic. Preparedness plans are falling under renewed scrutiny as decision-makers apply new findings and seek key leverage points for more effective preparedness and response. School closure has been recommended by the World Health Organization as one of the best ways to protect children and other susceptible individuals at the early stages of the pandemic. However, school closure is a difficult mitigation policy to implement from both strategic and operational points of view. Challenges include impacts on alternative education delivery services, such as student meals and after-school oversight, as well as direct and indirect economic outfalls. To help public health decision makers address these issues, we developed an epidemiological simulation tool for pandemic influenza which enables users to make decisions during a simulated pandemic. We then designed a school closure tabletop exercise using our simulation model as a decision-support tool for evaluating the effectiveness of school closure as a community mitigation strategy for pandemic influenza. We conducted two exercises in February 2009 for the Arizona Department of Health and Human Services including high-ranking health and education administrators from across the state. The purpose of these exercises was to test the state's pandemic preparedness plans with respect to school closure timing and impact. The exercises required participants to make (hypothetical) strategic and operational decisions to mitigate the impacts of pandemic influenza at the state and local levels. Our simulation and decision analysis tool was used to assess the impact of key decisions in the exercises. This paper presents the technical details involved in the design and evaluation of this pandemic decision-support tool. Based on the decisions made in the exercises, we present a bi-criteria decision analysis framework to evaluate analytical results obtained from the simulation model. Our analyses show that sequential school closure and re-opening strategy with a specific decision rule gives the best compromised solution in terms of minimizing the total number of infections and providing minimal educational discontinuity.\n",
      "Content: Simulation modeling for pandemic decision making: A case study with bi-criteria analysis on school closures Pandemic influenza continues to be a national and international public health concern, and has received significant attention worldwide with the A/H1N1 influenza outbreak in 2009. Many countries, including the United States, have developed preparedness plans for an influenza pandemic. Preparedness plans are falling under renewed scrutiny as decision-makers apply new findings and seek key leverage points for more effective preparedness and response. School closure has been recommended by the World Health Organization as one of the best ways to protect children and other susceptible individuals at the early stages of the pandemic. However, school closure is a difficult mitigation policy to implement from both strategic and operational points of view. Challenges include impacts on alternative education delivery services, such as student meals and after-school oversight, as well as direct and indirect economic outfalls. To help public health decision makers address these issues, we developed an epidemiological simulation tool for pandemic influenza which enables users to make decisions during a simulated pandemic. We then designed a school closure tabletop exercise using our simulation model as a decision-support tool for evaluating the effectiveness of school closure as a community mitigation strategy for pandemic influenza. We conducted two exercises in February 2009 for the Arizona Department of Health and Human Services including high-ranking health and education administrators from across the state. The purpose of these exercises was to test the state's pandemic preparedness plans with respect to school closure timing and impact. The exercises required participants to make (hypothetical) strategic and operational decisions to mitigate the impacts of pandemic influenza at the state and local levels. Our simulation and decision analysis tool was used to assess the impact of key decisions in the exercises. This paper presents the technical details involved in the design and evaluation of this pandemic decision-support tool. Based on the decisions made in the exercises, we present a bi-criteria decision analysis framework to evaluate analytical results obtained from the simulation model. Our analyses show that sequential school closure and re-opening strategy with a specific decision rule gives the best compromised solution in terms of minimizing the total number of infections and providing minimal educational discontinuity.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 433\n",
      "Title: Turbulent Stability of Emergent Roles: The Dualistic Nature of Self-Organizing Knowledge Coproduction\n",
      "Abstract: Increasingly, new forms of organizing for knowledge production are built around self-organizing coproduction community models with ambiguous role definitions. Current theories struggle to explain how high-quality knowledge is developed in these settings and how participants self-organize in the absence of role definitions, traditional organizational controls, or formal coordination mechanisms. In this article, we engage the puzzle by investigating the temporal dynamics underlying emergent roles on individual and organizational levels. Comprised of a multilevel large-scale empirical study of Wikipedia stretching over a decade, our study investigates emergent roles in terms of prototypical activity patterns that organically emerge from individuals’ knowledge production actions. Employing a stratified sample of 1,000 Wikipedia articles, we tracked 200,000 distinct participants and 700,000 coproduction activities, and recorded each activity’s type. We found that participants’ role-taking behavior is turbulent across roles, with substantial flow in and out of coproduction work. Our findings at the organizational level, however, show that work is organized around a highly stable set of emergent roles, despite the absence of traditional stabilizing mechanisms such as predefined work procedures or role expectations. This dualism in emergent work is conceptualized as “turbulent stability.” We attribute the stabilizing factor to the artifact-centric production process and present evidence to illustrate the mutual adjustment of role taking according to the artifact’s needs and stage. We discuss the importance of the affordances of Wikipedia in enabling such tacit coordination. This study advances our theoretical understanding of the nature of emergent roles and self-organizing knowledge coproduction. We discuss the implications for custodians of online communities as well as for managers of firms engaging in self-organized knowledge collaboration.\n",
      "Content: Turbulent Stability of Emergent Roles: The Dualistic Nature of Self-Organizing Knowledge Coproduction Increasingly, new forms of organizing for knowledge production are built around self-organizing coproduction community models with ambiguous role definitions. Current theories struggle to explain how high-quality knowledge is developed in these settings and how participants self-organize in the absence of role definitions, traditional organizational controls, or formal coordination mechanisms. In this article, we engage the puzzle by investigating the temporal dynamics underlying emergent roles on individual and organizational levels. Comprised of a multilevel large-scale empirical study of Wikipedia stretching over a decade, our study investigates emergent roles in terms of prototypical activity patterns that organically emerge from individuals’ knowledge production actions. Employing a stratified sample of 1,000 Wikipedia articles, we tracked 200,000 distinct participants and 700,000 coproduction activities, and recorded each activity’s type. We found that participants’ role-taking behavior is turbulent across roles, with substantial flow in and out of coproduction work. Our findings at the organizational level, however, show that work is organized around a highly stable set of emergent roles, despite the absence of traditional stabilizing mechanisms such as predefined work procedures or role expectations. This dualism in emergent work is conceptualized as “turbulent stability.” We attribute the stabilizing factor to the artifact-centric production process and present evidence to illustrate the mutual adjustment of role taking according to the artifact’s needs and stage. We discuss the importance of the affordances of Wikipedia in enabling such tacit coordination. This study advances our theoretical understanding of the nature of emergent roles and self-organizing knowledge coproduction. We discuss the implications for custodians of online communities as well as for managers of firms engaging in self-organized knowledge collaboration.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 434\n",
      "Title: Corporate Wikis: The Effects of Owners' Motivation and Behavior on Group Members' Engagement\n",
      "Abstract: Originally designed as a tool to alleviate bottlenecks associated with knowledge management, the suitability of wikis for corporate settings has been questioned given the inherent tensions between wiki affordances and the realities of organizational life. Drawing on regulatory focus theory and social cognitive theory, we developed and tested a model of the motivational dynamics underlying corporate wikis. We examined leaders (owners) and users of 187 wiki-based projects within a large multinational firm. Our findings revealed two countervailing motivational forces, one oriented toward accomplishment and achievement (promotion focus) and one oriented toward safety and security (prevention focus), that not only predicted owners' participation but also the overall level of engagement within the wiki groups. Our primary contribution is in showing that, notwithstanding the potential benefits to users, wikis can trigger risk-avoidance motives that potentially impede engagement. Practically, our findings call for an alignment between organizational procedures surrounding wiki deployment and the technology's affordances.\n",
      "Content: Corporate Wikis: The Effects of Owners' Motivation and Behavior on Group Members' Engagement Originally designed as a tool to alleviate bottlenecks associated with knowledge management, the suitability of wikis for corporate settings has been questioned given the inherent tensions between wiki affordances and the realities of organizational life. Drawing on regulatory focus theory and social cognitive theory, we developed and tested a model of the motivational dynamics underlying corporate wikis. We examined leaders (owners) and users of 187 wiki-based projects within a large multinational firm. Our findings revealed two countervailing motivational forces, one oriented toward accomplishment and achievement (promotion focus) and one oriented toward safety and security (prevention focus), that not only predicted owners' participation but also the overall level of engagement within the wiki groups. Our primary contribution is in showing that, notwithstanding the potential benefits to users, wikis can trigger risk-avoidance motives that potentially impede engagement. Practically, our findings call for an alignment between organizational procedures surrounding wiki deployment and the technology's affordances.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 435\n",
      "Title: Heuristic Principles and Differential Judgments in the Assessment of Information Quality\n",
      "Abstract: Information quality (IQ) is a multidimensional construct and includes dimensions such as accuracy, completeness, objectivity, and representation that are difficult to measure. Recently, research has shown that independent assessors who rated IQ yielded high inter-rater agreement for some information quality dimensions as opposed to others. In this paper, we explore the reasons that underlie the differences in the “measurability” of IQ. Employing Gigerenzer’s “building blocks” framework, we conjecture that the feasibility of using a set of heuristic principles consistently when assessing different dimensions of IQ is a key factor driving inter-rater agreement in IQ judgments. We report on two studies. In the first study, we qualitatively explored the manner in which participants applied the heuristic principles of search rules, stopping rules, and decision rules in assessing the IQ dimensions of accuracy, completeness, objectivity, and representation. In the second study, we investigated the extent to which participants could reach an agreement in rating the quality of Wikipedia articles along these dimensions. Our findings show an alignment between the consistent application of heuristic principles and inter-rater agreement levels found on particular dimensions of IQ judgments. Specifically, on the dimensions of completeness and representation, assessors applied the heuristic principles consistently and tended to agree in their ratings, whereas, on the dimensions of accuracy and objectivity, they not apply the heuristic principles in a uniform manner and inter-rater agreement was relatively low. We discuss our findings implications for research and practice.\n",
      "Content: Heuristic Principles and Differential Judgments in the Assessment of Information Quality Information quality (IQ) is a multidimensional construct and includes dimensions such as accuracy, completeness, objectivity, and representation that are difficult to measure. Recently, research has shown that independent assessors who rated IQ yielded high inter-rater agreement for some information quality dimensions as opposed to others. In this paper, we explore the reasons that underlie the differences in the “measurability” of IQ. Employing Gigerenzer’s “building blocks” framework, we conjecture that the feasibility of using a set of heuristic principles consistently when assessing different dimensions of IQ is a key factor driving inter-rater agreement in IQ judgments. We report on two studies. In the first study, we qualitatively explored the manner in which participants applied the heuristic principles of search rules, stopping rules, and decision rules in assessing the IQ dimensions of accuracy, completeness, objectivity, and representation. In the second study, we investigated the extent to which participants could reach an agreement in rating the quality of Wikipedia articles along these dimensions. Our findings show an alignment between the consistent application of heuristic principles and inter-rater agreement levels found on particular dimensions of IQ judgments. Specifically, on the dimensions of completeness and representation, assessors applied the heuristic principles consistently and tended to agree in their ratings, whereas, on the dimensions of accuracy and objectivity, they not apply the heuristic principles in a uniform manner and inter-rater agreement was relatively low. We discuss our findings implications for research and practice.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 436\n",
      "Title: A Theory-Driven Design Framework for Social Recommender Systems\n",
      "Abstract: Social recommender systems utilize data regarding users' social relationships in filtering relevant information to users. To date, results show that incorporating social relationship data - beyond consumption profile similarity - is beneficial only in a very limited set of cases. The main conjecture of this study is that the inconclusive results are, at least to some extent, due to an under-specification of the nature of the social relations. To date, there exist no clear guidelines for using behavioral theory to guide systems design. Our primary objective is to propose a methodology for theory-driven design. We enhance Walls et al.'s (1992) IS Design Theory by introducing the notion of \"applied behavioral theory,\" as a means of better linking theory and system design. Our second objective is to apply our theory-driven design methodology to social recommender systems, with the aim of improving prediction accuracy. A behavioral study found that some social relationships (e.g., competence, benevolence) are most likely to affect a recipient's advice-taking decision. We designed, developed, and tested a recommender system based on these principles, and found that the same types of relationships yield the best recommendation accuracy. This striking correspondence highlights the importance of behavioral theory in guiding system design. We discuss implications for design science and for research on recommender systems.\n",
      "Content: A Theory-Driven Design Framework for Social Recommender Systems Social recommender systems utilize data regarding users' social relationships in filtering relevant information to users. To date, results show that incorporating social relationship data - beyond consumption profile similarity - is beneficial only in a very limited set of cases. The main conjecture of this study is that the inconclusive results are, at least to some extent, due to an under-specification of the nature of the social relations. To date, there exist no clear guidelines for using behavioral theory to guide systems design. Our primary objective is to propose a methodology for theory-driven design. We enhance Walls et al.'s (1992) IS Design Theory by introducing the notion of \"applied behavioral theory,\" as a means of better linking theory and system design. Our second objective is to apply our theory-driven design methodology to social recommender systems, with the aim of improving prediction accuracy. A behavioral study found that some social relationships (e.g., competence, benevolence) are most likely to affect a recipient's advice-taking decision. We designed, developed, and tested a recommender system based on these principles, and found that the same types of relationships yield the best recommendation accuracy. This striking correspondence highlights the importance of behavioral theory in guiding system design. We discuss implications for design science and for research on recommender systems.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 437\n",
      "Title: The Evolutionary Trajectories of Peer-Produced Artifacts: Group Composition, the Trajectories’ Exploration, and the Quality of Artifacts\n",
      "Abstract: Members of an online community peer-produce digital artifacts by negotiating different perspectives and personal knowledge bases. These negotiations are manifested in the temporal evolution of the peer-produced artifact. In this study, we conceptualize the evolution of a digital artifact as a trajectory in a feature space. Our theoretical frame suggests that, through negotiations, contributors’ actions “pull” the trajectory and shape its movement in the feature space. We hypothesize that the type of contributors that work on a focal article influences the extent to which that article’s trajectory explores alternative positions within that space, and that the trajectory’s exploration is, in turn, associated with the artifact’s quality. To test these hypotheses, we analyzed the trajectories of wiki articles drawn from two peer-production communities, Wikipedia and Wikia, tracking the evolution of 242 paired articles for over a decade during which the articles went through 536,745 revisions. We found that the contributors who are the most likely to increase the trajectory’s exploration are those that (1) return to work on the focal artifact and (2) are unregistered members in the broader online community. Further, our results show that the trajectory’s exploration has a curvilinear association with article quality, indicating that exploration contributes positively to quality, but that the effect is reversed when exploration exceeds a certain level. The insights derived from this study highlight the value of an artifact-centric approach to increasing our understanding of the dynamics underlying peer-production.\n",
      "Content: The Evolutionary Trajectories of Peer-Produced Artifacts: Group Composition, the Trajectories’ Exploration, and the Quality of Artifacts Members of an online community peer-produce digital artifacts by negotiating different perspectives and personal knowledge bases. These negotiations are manifested in the temporal evolution of the peer-produced artifact. In this study, we conceptualize the evolution of a digital artifact as a trajectory in a feature space. Our theoretical frame suggests that, through negotiations, contributors’ actions “pull” the trajectory and shape its movement in the feature space. We hypothesize that the type of contributors that work on a focal article influences the extent to which that article’s trajectory explores alternative positions within that space, and that the trajectory’s exploration is, in turn, associated with the artifact’s quality. To test these hypotheses, we analyzed the trajectories of wiki articles drawn from two peer-production communities, Wikipedia and Wikia, tracking the evolution of 242 paired articles for over a decade during which the articles went through 536,745 revisions. We found that the contributors who are the most likely to increase the trajectory’s exploration are those that (1) return to work on the focal artifact and (2) are unregistered members in the broader online community. Further, our results show that the trajectory’s exploration has a curvilinear association with article quality, indicating that exploration contributes positively to quality, but that the effect is reversed when exploration exceeds a certain level. The insights derived from this study highlight the value of an artifact-centric approach to increasing our understanding of the dynamics underlying peer-production.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 438\n",
      "Title: Information Quality in Wikipedia: The Effects of Group Composition and Task Conflict\n",
      "Abstract: The success of Wikipedia demonstrates that self-organizing production communities can produce high-quality information-based products. Research on Wikipedia has proceeded largely atheoretically, focusing on (1) the diversity in members’ knowledge bases as a determinant of Wikipedia’s content quality, (2) the task-related conflicts that occur during the collaborative authoring process, and (3) the different roles members play in Wikipedia. We develop a theoretical model that explains how these three factors interact to determine the quality of Wikipedia articles. The results from the empirical study of 96 Wikipedia articles suggest that (1) diversity should be encouraged, as the creative abrasion that is generated when cognitively diverse members engage in task-related conflict leads to higher-quality articles, (2) task conflict should be managed, as conflict —notwithstanding its contribution to creative abrasion—can negatively affect group output, and (3) groups should maintain a balance of both administrative- and content-oriented members, as both contribute to the collaborative process.\n",
      "Content: Information Quality in Wikipedia: The Effects of Group Composition and Task Conflict The success of Wikipedia demonstrates that self-organizing production communities can produce high-quality information-based products. Research on Wikipedia has proceeded largely atheoretically, focusing on (1) the diversity in members’ knowledge bases as a determinant of Wikipedia’s content quality, (2) the task-related conflicts that occur during the collaborative authoring process, and (3) the different roles members play in Wikipedia. We develop a theoretical model that explains how these three factors interact to determine the quality of Wikipedia articles. The results from the empirical study of 96 Wikipedia articles suggest that (1) diversity should be encouraged, as the creative abrasion that is generated when cognitively diverse members engage in task-related conflict leads to higher-quality articles, (2) task conflict should be managed, as conflict —notwithstanding its contribution to creative abrasion—can negatively affect group output, and (3) groups should maintain a balance of both administrative- and content-oriented members, as both contribute to the collaborative process.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 439\n",
      "Title: Enhancing Information Retrieval Through Statistical Natural Language Processing: A Study of Collocation Indexing\n",
      "Abstract: Although the management of information assets--specifically, of text documents that make up 80 percent of these assets--an provide organizations with a competitive advantage, the ability of information retrieval (IR) systems to deliver relevant information to users is severely hampered by the difficulty of disambiguating natural language. The word ambiguity problem is addressed with moderate success in restricted settings, but continues to be the main challenge for general settings, characterized by large, heterogeneous document collections. In this paper, we provide preliminary evidence for the usefulness of statistical natural language processing (NLP) techniques, and specifically of collocation indexing, for IR in general settings. We investigate the effect of three key parameters on collocation indexing performance: directionality, distance, and weighting. We build on previous work in IR to (1) advance our knowledge of key design elements for collocation indexing, (2) demonstrate gains in retrieval precision from the use of statistical NLP for general-settings IR, and, finally, (3) provide practitioners with a useful cost-benefit analysis of the methods under investigation.\n",
      "Content: Enhancing Information Retrieval Through Statistical Natural Language Processing: A Study of Collocation Indexing Although the management of information assets--specifically, of text documents that make up 80 percent of these assets--an provide organizations with a competitive advantage, the ability of information retrieval (IR) systems to deliver relevant information to users is severely hampered by the difficulty of disambiguating natural language. The word ambiguity problem is addressed with moderate success in restricted settings, but continues to be the main challenge for general settings, characterized by large, heterogeneous document collections. In this paper, we provide preliminary evidence for the usefulness of statistical natural language processing (NLP) techniques, and specifically of collocation indexing, for IR in general settings. We investigate the effect of three key parameters on collocation indexing performance: directionality, distance, and weighting. We build on previous work in IR to (1) advance our knowledge of key design elements for collocation indexing, (2) demonstrate gains in retrieval precision from the use of statistical NLP for general-settings IR, and, finally, (3) provide practitioners with a useful cost-benefit analysis of the methods under investigation.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 440\n",
      "Title: The importance of participant interaction in online environments\n",
      "Abstract: An emerging body of research suggests that participant interaction is one of the strongest predictors of success in online environments. However, studies about the effects of participant interaction in a large sample of multiple online environments are rather limited. Using hierarchical modeling techniques, we examine a sample of 40 online MBA courses to determine whether learner–instructor, learner–learner, or learner–system interaction is most significantly related to online course outcomes. Our findings suggest that while collaborative environments were associated with higher levels of learner–learner and learner–system interaction, only learner–instructor and learner–system interaction were significantly associated with increased perceived learning.\n",
      "Content: The importance of participant interaction in online environments An emerging body of research suggests that participant interaction is one of the strongest predictors of success in online environments. However, studies about the effects of participant interaction in a large sample of multiple online environments are rather limited. Using hierarchical modeling techniques, we examine a sample of 40 online MBA courses to determine whether learner–instructor, learner–learner, or learner–system interaction is most significantly related to online course outcomes. Our findings suggest that while collaborative environments were associated with higher levels of learner–learner and learner–system interaction, only learner–instructor and learner–system interaction were significantly associated with increased perceived learning.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 441\n",
      "Title: The Role of Signaling Identity in the Adoption of Personal Technologies\n",
      "Abstract: We explore symbolic determinants of technology acceptance to complement more functional frameworks and better predict decisions to adopt information appliances. Previous research has investigated such variables as \"need for uniqueness\" and \"status gains\" to capture relevant aspects of technology acceptance. However, the more we move toward personal and ubiquitous technologies, the more we need to broaden and deepen our understanding of the symbolic aspects of adoption. This study reinterprets the symbolic dimension of adoption by broadening its scope to include the self-concept. Results support a prominent role for self-identity in predicting intentions to adopt mobile TVs. Self-identity is shown to complement the effects of \"need for uniqueness\" and \"status gains\" in this regard.\n",
      "Content: The Role of Signaling Identity in the Adoption of Personal Technologies We explore symbolic determinants of technology acceptance to complement more functional frameworks and better predict decisions to adopt information appliances. Previous research has investigated such variables as \"need for uniqueness\" and \"status gains\" to capture relevant aspects of technology acceptance. However, the more we move toward personal and ubiquitous technologies, the more we need to broaden and deepen our understanding of the symbolic aspects of adoption. This study reinterprets the symbolic dimension of adoption by broadening its scope to include the self-concept. Results support a prominent role for self-identity in predicting intentions to adopt mobile TVs. Self-identity is shown to complement the effects of \"need for uniqueness\" and \"status gains\" in this regard.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 442\n",
      "Title: An analytic approach to assessing organizational citizenship behavior\n",
      "Abstract: This study examines the organizational citizenship behavior (OCB) of employees by designing and developing an analytic network process (ANP) methodology. The viability of the proposed methodology is demonstrated via the sales representatives of Beko, a brand name controlled by Koç Group. We first develop a conceptual framework based on qualitative research methods – in-depth interviews and focus group sessions. We employ the principles of ANP methodology to examine and discover the inter-relationships among the OCBs. This process results in a descriptive model that encapsulates the findings from both qualitative and analytics methods. Necessity, altruism, departmental, compliance, and independence are the underlying dimensions of OCBs found to be the most influential/important. The key novelty of this study resides in designing and developing a prescriptive analytics (i.e. ANP) methodology to evaluate the OCBs, which is rare in the area of organizational behavior (a managerial field of study that have been dominated by traditional statistical methods), and thus serves as a useful contribution/augmentation to the business/managerial research methods, and also extends the reach/coverage of analytics-based decision support systems research and practice into a new direction.\n",
      "Content: An analytic approach to assessing organizational citizenship behavior This study examines the organizational citizenship behavior (OCB) of employees by designing and developing an analytic network process (ANP) methodology. The viability of the proposed methodology is demonstrated via the sales representatives of Beko, a brand name controlled by Koç Group. We first develop a conceptual framework based on qualitative research methods – in-depth interviews and focus group sessions. We employ the principles of ANP methodology to examine and discover the inter-relationships among the OCBs. This process results in a descriptive model that encapsulates the findings from both qualitative and analytics methods. Necessity, altruism, departmental, compliance, and independence are the underlying dimensions of OCBs found to be the most influential/important. The key novelty of this study resides in designing and developing a prescriptive analytics (i.e. ANP) methodology to evaluate the OCBs, which is rare in the area of organizational behavior (a managerial field of study that have been dominated by traditional statistical methods), and thus serves as a useful contribution/augmentation to the business/managerial research methods, and also extends the reach/coverage of analytics-based decision support systems research and practice into a new direction.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 443\n",
      "Title: A cost-oriented approach for the design of IT architectures\n",
      "Abstract: Multiple combinations of hardware and network components can be selected to design an information technology (IT) infrastructure that satisfies organizational requirements. The professional criterion to deal with these degrees of freedom is cost minimization. However, a scientific approach has been rarely applied to cost minimization and a rigorous verification of professional design guidelines is still lacking. The methodological contribution of this paper is the representation of complex infrastructural design issues as a single cost-minimization problem. The approach to cost-minimization is empirically verified with a database of costs that has also been built as part of this research. The paper shows how an overall cost-minimization approach can provide significant cost reductions and indicates that infrastructural design rules previously identified by the professional literature can lead to sub-optimal solutions.\n",
      "Content: A cost-oriented approach for the design of IT architectures Multiple combinations of hardware and network components can be selected to design an information technology (IT) infrastructure that satisfies organizational requirements. The professional criterion to deal with these degrees of freedom is cost minimization. However, a scientific approach has been rarely applied to cost minimization and a rigorous verification of professional design guidelines is still lacking. The methodological contribution of this paper is the representation of complex infrastructural design issues as a single cost-minimization problem. The approach to cost-minimization is empirically verified with a database of costs that has also been built as part of this research. The paper shows how an overall cost-minimization approach can provide significant cost reductions and indicates that infrastructural design rules previously identified by the professional literature can lead to sub-optimal solutions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 444\n",
      "Title: Knowledge entrepreneurship: institutionalising wiki-based knowledge-management processes in competitive and hierarchical organisations\n",
      "Abstract: Social media in general and wikis in particular offer unique opportunities for knowledge management. Despite widely publicised successes in public settings, wikis in businesses evince mixed results; enterprises struggle to apply wikis to institutionalise knowledge-management practices. We investigate the inherent tensions underlying knowledge-sharing in competitive and hierarchical organisations. Our application of the multi-level organisational learning framework demonstrates that, although wikis facilitate some important learning stages, other critical challenges remain. A unique blend of project leadership can facilitate the institutionalisation of wiki-based knowledge-management processes. To observe the leadership archetype, we use a longitudinal case study of wiki use within a division of NBC Universal. On the basis of our observations, we propose a new archetype of project leadership called Knowledge Entrepreneurship that integrates managerial skills, technology affordances, and critical factors in knowledge-management processes.\n",
      "Content: Knowledge entrepreneurship: institutionalising wiki-based knowledge-management processes in competitive and hierarchical organisations Social media in general and wikis in particular offer unique opportunities for knowledge management. Despite widely publicised successes in public settings, wikis in businesses evince mixed results; enterprises struggle to apply wikis to institutionalise knowledge-management practices. We investigate the inherent tensions underlying knowledge-sharing in competitive and hierarchical organisations. Our application of the multi-level organisational learning framework demonstrates that, although wikis facilitate some important learning stages, other critical challenges remain. A unique blend of project leadership can facilitate the institutionalisation of wiki-based knowledge-management processes. To observe the leadership archetype, we use a longitudinal case study of wiki use within a division of NBC Universal. On the basis of our observations, we propose a new archetype of project leadership called Knowledge Entrepreneurship that integrates managerial skills, technology affordances, and critical factors in knowledge-management processes.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 445\n",
      "Title: Preface\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 446\n",
      "Title: Computer monitoring: benefits and pitfalls facing management\n",
      "Abstract: The Information Age has enabled businesses to improve their efficiency through the use of advanced technology. The increase in the use of computers in the workplace has led to the ease of electronic monitoring of employees. Many feel that monitoring is important for the survival of their business. However, some employees regard this action as negatively impacting their work habits and privacy. This article examines the benefits and pitfalls of computer monitoring and recommends specific steps that need to be taken to monitor employees safely and ethically.\n",
      "Content: Computer monitoring: benefits and pitfalls facing management The Information Age has enabled businesses to improve their efficiency through the use of advanced technology. The increase in the use of computers in the workplace has led to the ease of electronic monitoring of employees. Many feel that monitoring is important for the survival of their business. However, some employees regard this action as negatively impacting their work habits and privacy. This article examines the benefits and pitfalls of computer monitoring and recommends specific steps that need to be taken to monitor employees safely and ethically.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 447\n",
      "Title: Key organizational factors in data warehouse architecture selection\n",
      "Abstract: Even though data warehousing has been in existence for over a decade, companies are still uncertain about a critical decision — which data warehouse architecture to implement? Based on the existing literature, theory, and interviews with experts, a research model was created that identifies the various contextual factors that affect the selection decision. The results from the field survey and multinomial logistic regression suggest that various combinations of organizational factors influence data warehouse architecture selection. The strategic view of the data warehouse prior to implementation emerged as a key determinant. The research suggests an overall model for predicting the data warehouse architecture selection decision.\n",
      "Content: Key organizational factors in data warehouse architecture selection Even though data warehousing has been in existence for over a decade, companies are still uncertain about a critical decision — which data warehouse architecture to implement? Based on the existing literature, theory, and interviews with experts, a research model was created that identifies the various contextual factors that affect the selection decision. The results from the field survey and multinomial logistic regression suggest that various combinations of organizational factors influence data warehouse architecture selection. The strategic view of the data warehouse prior to implementation emerged as a key determinant. The research suggests an overall model for predicting the data warehouse architecture selection decision.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 448\n",
      "Title: A dynamic simulation approach to support the evaluation of cyber risks and security investments in SMEs\n",
      "Abstract: The growing amount of cyberspace threats highlights the need to evaluate cybersecurity risks and to plan for effective investments. One internationally recognized document for cybersecurity risk management is the framework for Improving Critical Infrastructure Cybersecurity by the US National Institute of Standards and Technology (NIST). It provides guidelines, best practices and standards for cybersecurity risk management. Nevertheless, as other self-assessment frameworks, it produces a static view of an organization’s cyber posture and does not capture the dynamics of organizational changes and cyberattacks. Moreover, the current situation sees small and medium enterprises (SMEs) in a critical position since they need to manage their cybersecurity while usually not being skilled or equipped enough to internalize this process. Therefore, there is a need for a practical and easily applicable model able to identify a cybersecurity risk profile and its dynamics. This study proposes a system dynamics methodology and tool (SMECRA - SME Cyber Risk Assessment) for supporting cybersecurity investment decisions for SMEs through the evaluation of cyber risk and previous investments. SMECRA addresses dynamic organizational complexity and can be used to assess cyber risks and related dy­ namics over time. Three case studies demonstrate its capability to assess a SME’s cybersecurity status and to evaluate investments impacts on an organization’s risk profile, raising cybersecurity awareness. This study is important for SMEs wishing to manage their own cybersecurity risk and for insurance companies in their eco­ nomic evaluation of residual risks that SMEs wish to externalize.\n",
      "Content: A dynamic simulation approach to support the evaluation of cyber risks and security investments in SMEs The growing amount of cyberspace threats highlights the need to evaluate cybersecurity risks and to plan for effective investments. One internationally recognized document for cybersecurity risk management is the framework for Improving Critical Infrastructure Cybersecurity by the US National Institute of Standards and Technology (NIST). It provides guidelines, best practices and standards for cybersecurity risk management. Nevertheless, as other self-assessment frameworks, it produces a static view of an organization’s cyber posture and does not capture the dynamics of organizational changes and cyberattacks. Moreover, the current situation sees small and medium enterprises (SMEs) in a critical position since they need to manage their cybersecurity while usually not being skilled or equipped enough to internalize this process. Therefore, there is a need for a practical and easily applicable model able to identify a cybersecurity risk profile and its dynamics. This study proposes a system dynamics methodology and tool (SMECRA - SME Cyber Risk Assessment) for supporting cybersecurity investment decisions for SMEs through the evaluation of cyber risk and previous investments. SMECRA addresses dynamic organizational complexity and can be used to assess cyber risks and related dy­ namics over time. Three case studies demonstrate its capability to assess a SME’s cybersecurity status and to evaluate investments impacts on an organization’s risk profile, raising cybersecurity awareness. This study is important for SMEs wishing to manage their own cybersecurity risk and for insurance companies in their eco­ nomic evaluation of residual risks that SMEs wish to externalize.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 449\n",
      "Title: Information Technology Assimilation in Firms: The Influence of Senior Leadership and IT Infrastructures\n",
      "Abstract: IT assimilation is regarded as an important outcome in the efforts of firms to leverage the potential of information technologies in their business activities and strategies. Despite significant investments in information technology, considerable diversity exists in how well firms have been able to assimilate IT and leverage the business value of IT. This research draws upon the emerging knowledge-based and resource-based views of the firm to examine the influence of three factors on IT assimilation: (i) quality of senior leadership, (ii) sophistication of IT infrastructures, and (iii) organizational size. Drawing upon a large-scale sample survey where responses were obtained from CIOs and senior business executives who were members of the firms' top management teams, the study examines a variety of mostly normative prescriptions. The findings provide robust evidence about the impacts of CIOs' business and IT knowledge on IT assimilation. Further, we find that CIOs' membership in top management teams and their informal interactions with TMT members enhance their knowledge, particularly their business knowledge. We find that the intensity of the relationship between CIO's interactions with the top management team and their level of IT and business knowledge is much stronger in firms that articulate a transformational IT vision. The sophistication of IT infrastructures was also found to significantly impact IT assimilation. Surprisingly, the IT knowledge of senior business executives was not found to be a significant influence on IT assimilation. The implications of these findings for evolving a deeper understanding of the dynamics underlying IT assimilation are presented.\n",
      "Content: Information Technology Assimilation in Firms: The Influence of Senior Leadership and IT Infrastructures IT assimilation is regarded as an important outcome in the efforts of firms to leverage the potential of information technologies in their business activities and strategies. Despite significant investments in information technology, considerable diversity exists in how well firms have been able to assimilate IT and leverage the business value of IT. This research draws upon the emerging knowledge-based and resource-based views of the firm to examine the influence of three factors on IT assimilation: (i) quality of senior leadership, (ii) sophistication of IT infrastructures, and (iii) organizational size. Drawing upon a large-scale sample survey where responses were obtained from CIOs and senior business executives who were members of the firms' top management teams, the study examines a variety of mostly normative prescriptions. The findings provide robust evidence about the impacts of CIOs' business and IT knowledge on IT assimilation. Further, we find that CIOs' membership in top management teams and their informal interactions with TMT members enhance their knowledge, particularly their business knowledge. We find that the intensity of the relationship between CIO's interactions with the top management team and their level of IT and business knowledge is much stronger in firms that articulate a transformational IT vision. The sophistication of IT infrastructures was also found to significantly impact IT assimilation. Surprisingly, the IT knowledge of senior business executives was not found to be a significant influence on IT assimilation. The implications of these findings for evolving a deeper understanding of the dynamics underlying IT assimilation are presented.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 450\n",
      "Title: CD-ROM: an effective use of technology?\n",
      "Abstract: Abstract. The correct use of information systems implies, in part, the effective delivery and use of information. In situations where, as has frequently been the case in recent years, CD-ROMs form a part of an information system, there is very little that users can do to regulate or control the quality of the information being supplied. Poor information cannot be effectively used. This paper discusses the effective use of the medium and questions whether all publishers are paying sufficient attention to quality.\n",
      "Content: CD-ROM: an effective use of technology? Abstract. The correct use of information systems implies, in part, the effective delivery and use of information. In situations where, as has frequently been the case in recent years, CD-ROMs form a part of an information system, there is very little that users can do to regulate or control the quality of the information being supplied. Poor information cannot be effectively used. This paper discusses the effective use of the medium and questions whether all publishers are paying sufficient attention to quality.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 451\n",
      "Title: Exhaustion from Information System Career Experience: Implications for Turn-Away Intention\n",
      "Abstract: While the U.S. economy is recovering slowly, reports tell us that the supply of information systems (IS) professionals is declining and demand is once again on the rise. With organizations challenged in their efforts to hire additional staff, IS professionals are being asked to do even more, often leading to burnout, turnover, and turnaway intentions. Building on Ahuja et al.’s (2007) work on turnover intentions and using the job demands–resources model of burnout as an organizing framework for the antecedents to exhaustion from IS career experience (EISCE), this illustrative research note draws attention to exhaustion in IS professionals that spans an individual’s professional career. Findings indicate that IS professionals’ perceived workload (demand) was associated with higher levels of EISCE, whereas fairness and perceived control of career (resources) were associated with lower levels of EISCE. The influence of EISCE on affective commitment to the IS profession (ACISP) was found to be negative and, ultimately, ACISP fully mediated the effect of EISCE on the intention to turn away from an IS career. The results suggest the importance of studying IS professionals' perceptions regarding the demands and resources associated with working in the IS field when testing exhaustion across IS career experience.\n",
      "Content: Exhaustion from Information System Career Experience: Implications for Turn-Away Intention While the U.S. economy is recovering slowly, reports tell us that the supply of information systems (IS) professionals is declining and demand is once again on the rise. With organizations challenged in their efforts to hire additional staff, IS professionals are being asked to do even more, often leading to burnout, turnover, and turnaway intentions. Building on Ahuja et al.’s (2007) work on turnover intentions and using the job demands–resources model of burnout as an organizing framework for the antecedents to exhaustion from IS career experience (EISCE), this illustrative research note draws attention to exhaustion in IS professionals that spans an individual’s professional career. Findings indicate that IS professionals’ perceived workload (demand) was associated with higher levels of EISCE, whereas fairness and perceived control of career (resources) were associated with lower levels of EISCE. The influence of EISCE on affective commitment to the IS profession (ACISP) was found to be negative and, ultimately, ACISP fully mediated the effect of EISCE on the intention to turn away from an IS career. The results suggest the importance of studying IS professionals' perceptions regarding the demands and resources associated with working in the IS field when testing exhaustion across IS career experience.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 452\n",
      "Title: Exhaustion from Information System Career Experience: Implications for Turn-Away Intention\n",
      "Abstract: While the U.S. economy is recovering slowly, reports tell us that the supply of information systems (IS) professionals is declining and demand is once again on the rise. With organizations challenged in their efforts to hire additional staff, IS professionals are being asked to do even more, often leading to burnout, turnover, and turn-away intentions. Building on Ahuja et al.'s (2007) work on turnover intentions and using the job demands- resources model of burnout as an organizing framework for the antecedents to exhaustion from IS career experience (EISCE), this illustrative research note draws attention to exhaustion in IS professionals that spans an individual's professional career. Findings indicate that IS professionals' perceived workload (demand) was associated with higher levels of EISCE, whereas fairness and perceived control of career (resources) were associated with lower levels of EISCE. The influence of EISCE on affective commitment to the IS profession (ACISP) was found to be negative and, ultimately, ACISP fully mediated the effect of EISCE on the intention to turn away from an IS career. The results suggest the importance of studying IS professionals' perceptions regarding the demands and resources associated with working in the IS field when testing exhaustion across IS career experience.\n",
      "Content: Exhaustion from Information System Career Experience: Implications for Turn-Away Intention While the U.S. economy is recovering slowly, reports tell us that the supply of information systems (IS) professionals is declining and demand is once again on the rise. With organizations challenged in their efforts to hire additional staff, IS professionals are being asked to do even more, often leading to burnout, turnover, and turn-away intentions. Building on Ahuja et al.'s (2007) work on turnover intentions and using the job demands- resources model of burnout as an organizing framework for the antecedents to exhaustion from IS career experience (EISCE), this illustrative research note draws attention to exhaustion in IS professionals that spans an individual's professional career. Findings indicate that IS professionals' perceived workload (demand) was associated with higher levels of EISCE, whereas fairness and perceived control of career (resources) were associated with lower levels of EISCE. The influence of EISCE on affective commitment to the IS profession (ACISP) was found to be negative and, ultimately, ACISP fully mediated the effect of EISCE on the intention to turn away from an IS career. The results suggest the importance of studying IS professionals' perceptions regarding the demands and resources associated with working in the IS field when testing exhaustion across IS career experience.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 453\n",
      "Title: Understanding Mindshift Learning: The Transition to Object-Oriented Development\n",
      "Abstract: Information systems professionals increasingly face changes in their work environment. Some of these changes are incremental, but many require fundamental shifts in mindset (referred to as a mindshift). Within the domain of software development, previous research has determined that veteran developers experience difficulty making the transition to new forms of development. Although prior research has brought awareness to the problems caused by a mindshift and has provided some insight, it has not answered the question of why software developers have difficulty making the transition. This study begins to answer that question by positing and examining the mindshift learning theory (MLT). The MLT suggests that the degree of perceived novelty of the fundamental concepts that characterize the new mindset will impact learning. Specifically, concepts may be perceived as novel (i.e., not familiar to the learner), changed (i.e., similar to a known concept, but a different meaning in the new context), or carryover (i.e., known concept with a similar meaning in the new context). As an exemplar mindshift learning situation, this study explores the phenomenon in the context of software developers transitioning from traditional to object-oriented (OO) software development. Findings indicate that software developers had higher knowledge scores on the OO concepts they perceived as novel or carryover compared to those they perceived as changed. Thus, developers experienced detrimental interference from their existing traditional software development knowledge structure when trying to learn OO software development. The findings have implications for organizations and individuals as an understanding  of mindshifts could mean an easier transition through decreased frustration and a more effective learning process.\n",
      "Content: Understanding Mindshift Learning: The Transition to Object-Oriented Development Information systems professionals increasingly face changes in their work environment. Some of these changes are incremental, but many require fundamental shifts in mindset (referred to as a mindshift). Within the domain of software development, previous research has determined that veteran developers experience difficulty making the transition to new forms of development. Although prior research has brought awareness to the problems caused by a mindshift and has provided some insight, it has not answered the question of why software developers have difficulty making the transition. This study begins to answer that question by positing and examining the mindshift learning theory (MLT). The MLT suggests that the degree of perceived novelty of the fundamental concepts that characterize the new mindset will impact learning. Specifically, concepts may be perceived as novel (i.e., not familiar to the learner), changed (i.e., similar to a known concept, but a different meaning in the new context), or carryover (i.e., known concept with a similar meaning in the new context). As an exemplar mindshift learning situation, this study explores the phenomenon in the context of software developers transitioning from traditional to object-oriented (OO) software development. Findings indicate that software developers had higher knowledge scores on the OO concepts they perceived as novel or carryover compared to those they perceived as changed. Thus, developers experienced detrimental interference from their existing traditional software development knowledge structure when trying to learn OO software development. The findings have implications for organizations and individuals as an understanding  of mindshifts could mean an easier transition through decreased frustration and a more effective learning process.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 454\n",
      "Title: Advancement, voluntary turnover and women in IT: A cognitive study of work–family conflict\n",
      "Abstract: We used quality of work life theory and the causal mapping method to evoke the concepts and linkages of women's cognitions about work–family conflict in order to better understand the issues contributing to advancement barriers and voluntary turnover of women in IT. The major concepts (Managing Family Responsibilities, Work Stress, Work Schedule Flexibility, and Job Qualities) were found to not only impact each other but also were key factors influencing women's advancement opportunities and voluntary turnover. Organizations may use these insights to mitigate voluntary turnover and increase workforce diversity by addressing female IT professionals’ concerns regarding work–family conflict issues.\n",
      "Content: Advancement, voluntary turnover and women in IT: A cognitive study of work–family conflict We used quality of work life theory and the causal mapping method to evoke the concepts and linkages of women's cognitions about work–family conflict in order to better understand the issues contributing to advancement barriers and voluntary turnover of women in IT. The major concepts (Managing Family Responsibilities, Work Stress, Work Schedule Flexibility, and Job Qualities) were found to not only impact each other but also were key factors influencing women's advancement opportunities and voluntary turnover. Organizations may use these insights to mitigate voluntary turnover and increase workforce diversity by addressing female IT professionals’ concerns regarding work–family conflict issues.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 455\n",
      "Title: The advancement and persistence of women in the information technology profession: An extension of Ahuja's gendered theory of IT career stages\n",
      "Abstract: In 2002, Manju Ahuja articulated the challenges women face in the information technology (IT) profession with the goal of developing a theoretical model of factors influencing career choice, career advancement, and career persistence for women in the IT profession. While Ahuja's work has been regularly cited in the IT workforce literature (citation count was around 120 using ISI Web of Science and around 425 using Google Scholar as of September 30, 2017), women continue to leave the IT profession at a disturbing rate. Using Ahuja's theoretical model as the foundation, this study asked women working in IT what workplace challenges they face. The findings from this study validate many of Ahuja's propositions and suggest an extended theoretical model that could be used to further explore the challenges women face at various career stages in the IT field. In addition, the extended theoretical model might be used to galvanize the discussion around developing a more inclusive IT work environment.\n",
      "Content: The advancement and persistence of women in the information technology profession: An extension of Ahuja's gendered theory of IT career stages In 2002, Manju Ahuja articulated the challenges women face in the information technology (IT) profession with the goal of developing a theoretical model of factors influencing career choice, career advancement, and career persistence for women in the IT profession. While Ahuja's work has been regularly cited in the IT workforce literature (citation count was around 120 using ISI Web of Science and around 425 using Google Scholar as of September 30, 2017), women continue to leave the IT profession at a disturbing rate. Using Ahuja's theoretical model as the foundation, this study asked women working in IT what workplace challenges they face. The findings from this study validate many of Ahuja's propositions and suggest an extended theoretical model that could be used to further explore the challenges women face at various career stages in the IT field. In addition, the extended theoretical model might be used to galvanize the discussion around developing a more inclusive IT work environment.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 456\n",
      "Title: Firms that choose outsourcing: A profile\n",
      "Abstract: A nationwide survey of senior Information Systems (IS) managers in U.S. organizations reveals several structural and managerial characteristics of organizations that outsource one or more IS activities. The characteristics include organizational position of the IS manager, CEO involvement in IS (e.g., presence on an IS steering committee and personal use of computers), and IS performance. Outsourcing activities examined are hardware (e.g., network, PC, workstation, minicomputer, and mainframe maintenance and support); software (e.g., contract programming and software support/ training) and comprehensive management activities (e.g., facility management and systems integration). Apparently CEOs who are heavily involved in a steering committee are the least likely to outsource. CEOs that actively use computers are more likely to outsource specific hardware and software activities, whereas CEOs who do not personally use a computer are more likely to outsource comprehensive management activities. In addition, the distance between the CEO and the IS manager is a factor: further distance makes it more likely that IS functions are outsourced. Industry leaders are among the smallest proportion of outsourcing firms, whereas close followers are the largest.\n",
      "Content: Firms that choose outsourcing: A profile A nationwide survey of senior Information Systems (IS) managers in U.S. organizations reveals several structural and managerial characteristics of organizations that outsource one or more IS activities. The characteristics include organizational position of the IS manager, CEO involvement in IS (e.g., presence on an IS steering committee and personal use of computers), and IS performance. Outsourcing activities examined are hardware (e.g., network, PC, workstation, minicomputer, and mainframe maintenance and support); software (e.g., contract programming and software support/ training) and comprehensive management activities (e.g., facility management and systems integration). Apparently CEOs who are heavily involved in a steering committee are the least likely to outsource. CEOs that actively use computers are more likely to outsource specific hardware and software activities, whereas CEOs who do not personally use a computer are more likely to outsource comprehensive management activities. In addition, the distance between the CEO and the IS manager is a factor: further distance makes it more likely that IS functions are outsourced. Industry leaders are among the smallest proportion of outsourcing firms, whereas close followers are the largest.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 457\n",
      "Title: On the phenomenology of technology: the “Janus-faces” of mobile phones\n",
      "Abstract: This paper argues that technologies perform in Janus faced ways; that is, in ways that are ironic, perverse and paradoxical, and it is argued that these qualities are important to apprehend if we are to more fully understand the role of technology in organizations and in our daily lives. The argument opens with an account of Janus as a metaphorical evocation of irony and paradox, and general examples of Janus faced technologies are given. Prominent philosophies of technology and theoretical approaches to technology are discussed in terms of their capacity to account for generalized examples of irony and paradox. Of these, it is argued that the most satisfactory account is provided by (a) Heidegger’s suggestion that our world is enframed by technology, taken together with (b) a logic of sociotechnical systems based in relational and hybrid ontologies. This sketch of the philosophical landscape occupied by Janus is followed by a interpretation of the specific case of mobile phones, which provides concrete and hopefully vivid examples of the Janus faced performance of technology. The conclusion reached is that the Janus faced metaphor and its philosophical context provides the researcher with the analytic advantages of foregrounding uncertainty, avoiding an essentialist or determinist role for technology, and allowing for the possibility of the presence of tension and contradiction in accounts of sociotechnical outcomes.\n",
      "Content: On the phenomenology of technology: the “Janus-faces” of mobile phones This paper argues that technologies perform in Janus faced ways; that is, in ways that are ironic, perverse and paradoxical, and it is argued that these qualities are important to apprehend if we are to more fully understand the role of technology in organizations and in our daily lives. The argument opens with an account of Janus as a metaphorical evocation of irony and paradox, and general examples of Janus faced technologies are given. Prominent philosophies of technology and theoretical approaches to technology are discussed in terms of their capacity to account for generalized examples of irony and paradox. Of these, it is argued that the most satisfactory account is provided by (a) Heidegger’s suggestion that our world is enframed by technology, taken together with (b) a logic of sociotechnical systems based in relational and hybrid ontologies. This sketch of the philosophical landscape occupied by Janus is followed by a interpretation of the specific case of mobile phones, which provides concrete and hopefully vivid examples of the Janus faced performance of technology. The conclusion reached is that the Janus faced metaphor and its philosophical context provides the researcher with the analytic advantages of foregrounding uncertainty, avoiding an essentialist or determinist role for technology, and allowing for the possibility of the presence of tension and contradiction in accounts of sociotechnical outcomes.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 458\n",
      "Title: Competing pressures of risk and absorptive capacity potential on commitment and information sharing in global supply chains\n",
      "Abstract: Organizations’ competitiveness and success are no longer dependent solely on their own performance, but rather are dependent on the competitiveness of the supply chains in which they participate. Increasingly, these supply chains are globally distributed introducing the possibility of greater benefits, as well as greater risk. This study examines the countervailing impact of a global supply chain partner's business-to-business e-commerce business risk and absorptive capacity on an organization's willingness to commit to and share information with that supply chain partner. We survey 207 organizations on their perceptions of specific offshore outsourcing and supply chain partners across dimensions of risk, absorptive capacity, commitment, and information sharing. The results support the theorized relationships indicating that a supply chain partner's increased levels of perceived risk has a strong negative effect on an organization's commitment and information sharing; conjointly, increases in a supply chain partner's absorptive capacity has a strong positive effect on commitment and information sharing. For both risk and absorptive capacity, commitment partially mediates the relationship with information sharing. Testing for systemic effects from geographical/cultural location on the relationship factors provides no evidence of a regional effect on measured items.\n",
      "Content: Competing pressures of risk and absorptive capacity potential on commitment and information sharing in global supply chains Organizations’ competitiveness and success are no longer dependent solely on their own performance, but rather are dependent on the competitiveness of the supply chains in which they participate. Increasingly, these supply chains are globally distributed introducing the possibility of greater benefits, as well as greater risk. This study examines the countervailing impact of a global supply chain partner's business-to-business e-commerce business risk and absorptive capacity on an organization's willingness to commit to and share information with that supply chain partner. We survey 207 organizations on their perceptions of specific offshore outsourcing and supply chain partners across dimensions of risk, absorptive capacity, commitment, and information sharing. The results support the theorized relationships indicating that a supply chain partner's increased levels of perceived risk has a strong negative effect on an organization's commitment and information sharing; conjointly, increases in a supply chain partner's absorptive capacity has a strong positive effect on commitment and information sharing. For both risk and absorptive capacity, commitment partially mediates the relationship with information sharing. Testing for systemic effects from geographical/cultural location on the relationship factors provides no evidence of a regional effect on measured items.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 459\n",
      "Title: The Differential Use and Effect of Knowledge-Based System Explanations in Novice and Expert Judgment Decisions\n",
      "Abstract: Explanation facilities are considered essential in facilitating user interaction with knowledge-based systems (KBS). Research on explanation provision and the impact on KBS users has shown that the domain expertise affects the type of explanations selected by the user and the basis for seeking such explanations. The prior literature has been limited, however, by the use of simulated KBS that generally provide only feedback explanations (i.e., ex post to the recommendation of the KBS being presented to the user). The purpose of this study is to examine the way users with varying levels of expertise use alternative types of KBS explanations and the impact of that use on decision making. A total of 64 partner/ manager-level and 82 senior/staff-level insolvency professionals participated in an experiment involving the use of a fully functioning KBS to complete a complex judgment task. In addition to feedback explanations, the KBS also provided feedforward explanations (i.e., general explanations during user input about the relationships between information cues in the KBS) and included definition type explanations (i.e., declarative-level knowledge). The results show that users were more likely to adhere to recommendations of the KBS when an explanation facility was available. Choice patterns in using explanations indicated that novices used feedforward explanations more than experts did, while experts were more likely than novices to use feedback explanations. Novices also used more declarative knowledge and initial problem solving type explanations, while experts used more procedural knowledge explanations. Finally, use of feedback explanations led to greater adherence to the KBS recommendation by experts--a condition that was even more prevalent as the use of feedback explanations increased. The results have several implications for the design and use of KBS in a professional decision-making environment.\n",
      "Content: The Differential Use and Effect of Knowledge-Based System Explanations in Novice and Expert Judgment Decisions Explanation facilities are considered essential in facilitating user interaction with knowledge-based systems (KBS). Research on explanation provision and the impact on KBS users has shown that the domain expertise affects the type of explanations selected by the user and the basis for seeking such explanations. The prior literature has been limited, however, by the use of simulated KBS that generally provide only feedback explanations (i.e., ex post to the recommendation of the KBS being presented to the user). The purpose of this study is to examine the way users with varying levels of expertise use alternative types of KBS explanations and the impact of that use on decision making. A total of 64 partner/ manager-level and 82 senior/staff-level insolvency professionals participated in an experiment involving the use of a fully functioning KBS to complete a complex judgment task. In addition to feedback explanations, the KBS also provided feedforward explanations (i.e., general explanations during user input about the relationships between information cues in the KBS) and included definition type explanations (i.e., declarative-level knowledge). The results show that users were more likely to adhere to recommendations of the KBS when an explanation facility was available. Choice patterns in using explanations indicated that novices used feedforward explanations more than experts did, while experts were more likely than novices to use feedback explanations. Novices also used more declarative knowledge and initial problem solving type explanations, while experts used more procedural knowledge explanations. Finally, use of feedback explanations led to greater adherence to the KBS recommendation by experts--a condition that was even more prevalent as the use of feedback explanations increased. The results have several implications for the design and use of KBS in a professional decision-making environment.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 460\n",
      "Title: Behavioral economics for decision support systems researchers\n",
      "Abstract: Theories of decision-making, both prescriptive and descriptive, have long been important to decision support systems (DSS). Currently, the field of behavioral economics (BE) provides the dominant descriptive approach for understanding human decision-making. An indication of the field's standing is that three Nobel Prizes have been awarded to behavioral economics. Contemporary BE has two major theory foundations – the dual process theory of decision-making cognition and a set of judgment heuristics and cognitive biases. These foundations have been combined to create important theories like prospect theory and action strategies like nudging. Previous research has found that DSS has been slow to adopt recent advances in BE, even to the extent that some projects continue to use older theories like the phase model of decision making. This paper aims to make DSS researchers aware of contemporary BE, its nature, and its differences with early BE. We believe that behavioral economics is a useful and productive foundation for DSS research and that the use of BE in DSS should be significantly expanded.\n",
      "Content: Behavioral economics for decision support systems researchers Theories of decision-making, both prescriptive and descriptive, have long been important to decision support systems (DSS). Currently, the field of behavioral economics (BE) provides the dominant descriptive approach for understanding human decision-making. An indication of the field's standing is that three Nobel Prizes have been awarded to behavioral economics. Contemporary BE has two major theory foundations – the dual process theory of decision-making cognition and a set of judgment heuristics and cognitive biases. These foundations have been combined to create important theories like prospect theory and action strategies like nudging. Previous research has found that DSS has been slow to adopt recent advances in BE, even to the extent that some projects continue to use older theories like the phase model of decision making. This paper aims to make DSS researchers aware of contemporary BE, its nature, and its differences with early BE. We believe that behavioral economics is a useful and productive foundation for DSS research and that the use of BE in DSS should be significantly expanded.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 461\n",
      "Title: Behavioral economics in information systems research: Critical analysis and research strategies\n",
      "Abstract: Theories of decision-making have long been important foundations for information systems research and much of the information system is concerned with information processing for decision-making. The discipline of behavioral economics provides the dominant contemporary approach for understanding human decision-making. Therefore, it is logical that information systems research that involves decision-making should consider behavioral economics as a foundation or reference theory. Surprisingly, and despite calls for greater use of behavioral economics in information systems research, it seems that information systems has been slow to adopt contemporary behavioral economics as reference theory. This article reports a critical analysis of behavioral economics in all fields of information systems based on an intensive investigation of quality information systems research using bibliometric content analysis. The analysis shows that information systems researchers have a general understanding of behavioral economics, but their use of the theories has an ad hoc feel where only a narrow range of behavioral economic concepts and theories tend to form the foundation of information systems research. The factors constraining the adoption of behavioral economic theories in information systems are discussed and strategies for the use of this influential foundation theory are proposed. Guidance is provided on how behavioral economics could be used in various aspects of information systems. The article concludes with the view that behavioral economic reference theory has the potential to transform significant areas of information systems research.\n",
      "Content: Behavioral economics in information systems research: Critical analysis and research strategies Theories of decision-making have long been important foundations for information systems research and much of the information system is concerned with information processing for decision-making. The discipline of behavioral economics provides the dominant contemporary approach for understanding human decision-making. Therefore, it is logical that information systems research that involves decision-making should consider behavioral economics as a foundation or reference theory. Surprisingly, and despite calls for greater use of behavioral economics in information systems research, it seems that information systems has been slow to adopt contemporary behavioral economics as reference theory. This article reports a critical analysis of behavioral economics in all fields of information systems based on an intensive investigation of quality information systems research using bibliometric content analysis. The analysis shows that information systems researchers have a general understanding of behavioral economics, but their use of the theories has an ad hoc feel where only a narrow range of behavioral economic concepts and theories tend to form the foundation of information systems research. The factors constraining the adoption of behavioral economic theories in information systems are discussed and strategies for the use of this influential foundation theory are proposed. Guidance is provided on how behavioral economics could be used in various aspects of information systems. The article concludes with the view that behavioral economic reference theory has the potential to transform significant areas of information systems research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 462\n",
      "Title: Executive information systems development in an emerging economy\n",
      "Abstract: This paper addresses executive information systems (EIS) development in an emerging economy. In particular, it examines EIS development in Thailand, a nation that is more representative of the majority of emerging economies in the South East Asian region than the four Asian Tigers (Singapore, Hong Kong, Taiwan and South Korea). Case studies of the development of four systems in large Thai organizations are presented. The analysis of the cases and their comparison to a benchmark study gives rise to the concept of EIS cultural fit, a concept that adds to our understanding of the reasons for the success and failure of EIS projects in emerging economies. The cases also raise questions about using outsourcing as a development strategy for EIS in emerging economies.\n",
      "Content: Executive information systems development in an emerging economy This paper addresses executive information systems (EIS) development in an emerging economy. In particular, it examines EIS development in Thailand, a nation that is more representative of the majority of emerging economies in the South East Asian region than the four Asian Tigers (Singapore, Hong Kong, Taiwan and South Korea). Case studies of the development of four systems in large Thai organizations are presented. The analysis of the cases and their comparison to a benchmark study gives rise to the concept of EIS cultural fit, a concept that adds to our understanding of the reasons for the success and failure of EIS projects in emerging economies. The cases also raise questions about using outsourcing as a development strategy for EIS in emerging economies.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 463\n",
      "Title: Patterns of business intelligence systems use in organizations\n",
      "Abstract: Business intelligence (BI) is often used as the umbrella term for large-scale decision support systems (DSS) in organizations. BI is currently the largest area of IT investment in organizations and has been rated as the top technology priority by CIOs worldwide for many years. The most important use patterns in decision support are concerned with the type of decision to be supported and the type of manager that makes the decision. The seminal Gorry and Scott Morton MIS/DSS framework remains the most popular framework to describe these use patterns. It is widely believed that DSS theory like this framework can be transferred to BI. This paper investigates BI systems use patterns using the Gorry and Scott Morton framework and contemporary decision-making theory from behavioral economics. The paper presents secondary case study research that analyzes eight BI systems and 86 decisions supported by these systems. Based on the results of the case studies a framework to describe BI use patterns is developed. The framework provides both a theoretical and empirically based foundation for the development of high quality BI theory. It also provides a guide for developing organizational strategy for BI provision. The framework shows that enterprise and smaller functional BI systems exist together in an organization to support different decisions and different decision makers. The framework shows that personal DSS theory cannot be applied to BI systems without specific empirical support.\n",
      "Content: Patterns of business intelligence systems use in organizations Business intelligence (BI) is often used as the umbrella term for large-scale decision support systems (DSS) in organizations. BI is currently the largest area of IT investment in organizations and has been rated as the top technology priority by CIOs worldwide for many years. The most important use patterns in decision support are concerned with the type of decision to be supported and the type of manager that makes the decision. The seminal Gorry and Scott Morton MIS/DSS framework remains the most popular framework to describe these use patterns. It is widely believed that DSS theory like this framework can be transferred to BI. This paper investigates BI systems use patterns using the Gorry and Scott Morton framework and contemporary decision-making theory from behavioral economics. The paper presents secondary case study research that analyzes eight BI systems and 86 decisions supported by these systems. Based on the results of the case studies a framework to describe BI use patterns is developed. The framework provides both a theoretical and empirically based foundation for the development of high quality BI theory. It also provides a guide for developing organizational strategy for BI provision. The framework shows that enterprise and smaller functional BI systems exist together in an organization to support different decisions and different decision makers. The framework shows that personal DSS theory cannot be applied to BI systems without specific empirical support.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 464\n",
      "Title: A note on an experimental study of DSS and forecasting exponential growth\n",
      "Abstract: Many managers need to make forecasts of variables that are growing rapidly. Business variables that increase or decrease exponentially are common in turbulent and complex markets, and misjudging the exponential nature of these variables in an important decision could have major adverse consequences for an organization. This paper reports on an experiment that investigated the use of DSS in an exponential decision task. It found that the use of a simple DSS significantly improved decision performance. This study forms the start of a series of investigations into DSS and complexity.\n",
      "Content: A note on an experimental study of DSS and forecasting exponential growth Many managers need to make forecasts of variables that are growing rapidly. Business variables that increase or decrease exponentially are common in turbulent and complex markets, and misjudging the exponential nature of these variables in an important decision could have major adverse consequences for an organization. This paper reports on an experiment that investigated the use of DSS in an exponential decision task. It found that the use of a simple DSS significantly improved decision performance. This study forms the start of a series of investigations into DSS and complexity.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 465\n",
      "Title: A critical analysis of decision support systems research\n",
      "Abstract: This paper critically analyses the nature and state of decision support systems (DSS) research. To provide context for the analysis, a history of DSS is presented which focuses on the evolution of a number of sub-groupings of research and practice: personal DSS, group support systems, negotiation support systems, intelligent DSS, knowledge management-based DSS, executive information systems/business intelligence, and data warehousing. To understand the state of DSS research an empirical investigation of published DSS research is presented. This investigation is based on the detailed analysis of 1,020 DSS articles published in 14 major journals from 1990 to 2003. The analysis found that DSS publication has been falling steadily since its peak in 1994 and the current publication rate is at early 1990s levels. Other findings include that personal DSS and group support systems dominate research activity and data warehousing is the least published type of DSS. The journal DSS is the major publishing outlet; US 'Other' journals dominate DSS publishing and there is very low exposure of DSS in European journals. Around two-thirds of DSS research is empirical, a much higher proportion than general IS research. DSS empirical research is overwhelming positivist, and is more dominated by positivism than IS research in general. Design science is a major DSS research category. The decision support focus of the sample shows a well-balanced mix of development, technology, process, and outcome studies. Almost half of DSS papers did not use judgement and decision-making reference research in the design and analysis of their projects and most cited reference works are relatively old. A major omission in DSS scholarship is the poor identification of the clients and users of the various DSS applications that are the focus of investigation. The analysis of the professional or practical contribution of DSS research shows a field that is facing a crisis of relevance. Using the history and empirical study as a foundation, a number of strategies for improving DSS research are suggested.\n",
      "Content: A critical analysis of decision support systems research This paper critically analyses the nature and state of decision support systems (DSS) research. To provide context for the analysis, a history of DSS is presented which focuses on the evolution of a number of sub-groupings of research and practice: personal DSS, group support systems, negotiation support systems, intelligent DSS, knowledge management-based DSS, executive information systems/business intelligence, and data warehousing. To understand the state of DSS research an empirical investigation of published DSS research is presented. This investigation is based on the detailed analysis of 1,020 DSS articles published in 14 major journals from 1990 to 2003. The analysis found that DSS publication has been falling steadily since its peak in 1994 and the current publication rate is at early 1990s levels. Other findings include that personal DSS and group support systems dominate research activity and data warehousing is the least published type of DSS. The journal DSS is the major publishing outlet; US 'Other' journals dominate DSS publishing and there is very low exposure of DSS in European journals. Around two-thirds of DSS research is empirical, a much higher proportion than general IS research. DSS empirical research is overwhelming positivist, and is more dominated by positivism than IS research in general. Design science is a major DSS research category. The decision support focus of the sample shows a well-balanced mix of development, technology, process, and outcome studies. Almost half of DSS papers did not use judgement and decision-making reference research in the design and analysis of their projects and most cited reference works are relatively old. A major omission in DSS scholarship is the poor identification of the clients and users of the various DSS applications that are the focus of investigation. The analysis of the professional or practical contribution of DSS research shows a field that is facing a crisis of relevance. Using the history and empirical study as a foundation, a number of strategies for improving DSS research are suggested.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 466\n",
      "Title: Eight key issues for the decision support systems discipline\n",
      "Abstract: This paper integrates a number of strands of a long-term project that is critically analysing the academic field of decision support systems (DSS). The project is based on the content analysis of 1093 DSS articles published in 14 major journals from 1990 to 2004. An examination of the findings of each part of the project yields eight key issues that the DSS field should address for it to continue to play an important part in information systems scholarship. These eight issues are: the relevance of DSS research, DSS research methods and paradigms, the judgement and decision-making theoretical foundations of DSS research, the role of the IT artifact in DSS research, the funding of DSS research, inertia and conservatism of DSS research agendas, DSS exposure in general “A” journals, and discipline coherence. The discussion of each issue is based on the data derived from the article content analysis. A number of suggestions are made for the improvement of DSS research. These relate to case study research, design science, professional relevance, industry funding, theoretical foundations, data warehousing, and business intelligence. The suggestions should help DSS researchers construct high quality research agendas that are relevant and rigorous.\n",
      "Content: Eight key issues for the decision support systems discipline This paper integrates a number of strands of a long-term project that is critically analysing the academic field of decision support systems (DSS). The project is based on the content analysis of 1093 DSS articles published in 14 major journals from 1990 to 2004. An examination of the findings of each part of the project yields eight key issues that the DSS field should address for it to continue to play an important part in information systems scholarship. These eight issues are: the relevance of DSS research, DSS research methods and paradigms, the judgement and decision-making theoretical foundations of DSS research, the role of the IT artifact in DSS research, the funding of DSS research, inertia and conservatism of DSS research agendas, DSS exposure in general “A” journals, and discipline coherence. The discussion of each issue is based on the data derived from the article content analysis. A number of suggestions are made for the improvement of DSS research. These relate to case study research, design science, professional relevance, industry funding, theoretical foundations, data warehousing, and business intelligence. The suggestions should help DSS researchers construct high quality research agendas that are relevant and rigorous.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 467\n",
      "Title: Design Science in Decision Support Systems Research: An Assessment using the Hevner, March, Park, and Ram Guidelines\n",
      "Abstract: Design science has been an important strategy in decision support systems (DSS) research since the field's inception in the early 1970s. Recent reviews of DSS research have indicated a need to improve its quality and relevance. DSS design-science research has an important role in this improvement because design-science research can engage industry and the profession in intellectually important projects. The Hevner, March, Park, and Ram's (HMPR) guidelines for the conduct and assessment of information systems design-science research, published in MIS Quarterly in 2004, provides a vehicle for assessing DSS design-science research. This paper presents research that used bibliometric content analysis to apply the HMPR guidelines to a representative sample of 362 DSS design-science research papers in 14 journals. The analysis highlights major issues in DSS research that need attention: research design, evaluation, relevance, strategic focus, and theorizing.\n",
      "Content: Design Science in Decision Support Systems Research: An Assessment using the Hevner, March, Park, and Ram Guidelines Design science has been an important strategy in decision support systems (DSS) research since the field's inception in the early 1970s. Recent reviews of DSS research have indicated a need to improve its quality and relevance. DSS design-science research has an important role in this improvement because design-science research can engage industry and the profession in intellectually important projects. The Hevner, March, Park, and Ram's (HMPR) guidelines for the conduct and assessment of information systems design-science research, published in MIS Quarterly in 2004, provides a vehicle for assessing DSS design-science research. This paper presents research that used bibliometric content analysis to apply the HMPR guidelines to a representative sample of 362 DSS design-science research papers in 14 journals. The analysis highlights major issues in DSS research that need attention: research design, evaluation, relevance, strategic focus, and theorizing.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 468\n",
      "Title: A critical analysis of decision support systems research revisited: the rise of design science\n",
      "Abstract: In 2005 the Journal of Information Technology article ‘A critical analysis of decision support systems research’ analyzed 1020 decision support systems (DSS) articles from 1990 to 2003. Since 2003 business intelligence (BI) and business analytics have gained popularity in practice. In theory and research the period since 2003 has seen a change in the decision-making theory orthodoxy and the codification and acceptance of design science. To investigate the changes in the DSS field, a number of expectations were derived from previous literature analyses. These expectations were assessed using bibliometric content analysis. The article sample to 2010 now includes 1466 articles from 16 journals. The analysis of the expectations yields mixed results for the DSS field. On the negative side, there has been an overall decline in DSS publishing, the relevance of DSS research published in journals to IT professionals has declined, and the rigor of DSS research designs has not improved. On the positive side, there has been improvement in relevance to managers, grant funding of DSS research has increased, there has been a positive shift in judgment and decision-making foundations, BI publishing has increased, and group support systems publishing has reduced to a more balanced level. An important result from the analysis of the last 7 years of DSS research is the significant increase in DSS design-science research (DSR) to almost half of published articles. It is clear from the analysis that DSS is undergoing a transition from a field based on statistical hypothesis testing and conceptual studies to one where DSR is the most popular method.\n",
      "Content: A critical analysis of decision support systems research revisited: the rise of design science In 2005 the Journal of Information Technology article ‘A critical analysis of decision support systems research’ analyzed 1020 decision support systems (DSS) articles from 1990 to 2003. Since 2003 business intelligence (BI) and business analytics have gained popularity in practice. In theory and research the period since 2003 has seen a change in the decision-making theory orthodoxy and the codification and acceptance of design science. To investigate the changes in the DSS field, a number of expectations were derived from previous literature analyses. These expectations were assessed using bibliometric content analysis. The article sample to 2010 now includes 1466 articles from 16 journals. The analysis of the expectations yields mixed results for the DSS field. On the negative side, there has been an overall decline in DSS publishing, the relevance of DSS research published in journals to IT professionals has declined, and the rigor of DSS research designs has not improved. On the positive side, there has been improvement in relevance to managers, grant funding of DSS research has increased, there has been a positive shift in judgment and decision-making foundations, BI publishing has increased, and group support systems publishing has reduced to a more balanced level. An important result from the analysis of the last 7 years of DSS research is the significant increase in DSS design-science research (DSR) to almost half of published articles. It is clear from the analysis that DSS is undergoing a transition from a field based on statistical hypothesis testing and conceptual studies to one where DSR is the most popular method.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 469\n",
      "Title: Decision support systems evolution: framework, case study and research agenda\n",
      "Abstract: Evolutionary development has been central to the theory and practice of decision support systems (DSS) since the inception of the field. Terms such as 'adaptive' and 'evolutionary' capture the organic nature of the development of a decision support system. However, the terms are rarely defined and their meaning varies widely in the research literature. The aim of this paper is to contribute to decision support systems theory by investigating and clearly specifying the nature of the evolutionary process of a DSS. Using insights from other disciplines and prior DSS research, a framework for understanding DSS evolution is developed based on the aetiology, lineage, and tempo of evolution. The descriptive validity of the framework is demonstrated by applying it to published DSS studies and to an intensive case study of DSS development. The framework and the case study findings are used to define a research agenda that is important for evolutionary DSS development.\n",
      "Content: Decision support systems evolution: framework, case study and research agenda Evolutionary development has been central to the theory and practice of decision support systems (DSS) since the inception of the field. Terms such as 'adaptive' and 'evolutionary' capture the organic nature of the development of a decision support system. However, the terms are rarely defined and their meaning varies widely in the research literature. The aim of this paper is to contribute to decision support systems theory by investigating and clearly specifying the nature of the evolutionary process of a DSS. Using insights from other disciplines and prior DSS research, a framework for understanding DSS evolution is developed based on the aetiology, lineage, and tempo of evolution. The descriptive validity of the framework is demonstrated by applying it to published DSS studies and to an intensive case study of DSS development. The framework and the case study findings are used to define a research agenda that is important for evolutionary DSS development.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 470\n",
      "Title: Cognitive biases and decision support systems development: a design science approach\n",
      "Abstract: This paper presents design science research that aims to improve decision support systems (DSS) development in organizations. Evolutionary development has been central to DSS theory and practice for decades, but a significant problem for DSS analysts remains how to conceptualize the improvement of a decision task during evolutionary DSS development. The objective of a DSS project is to improve the decision process and outcome for a manager making an important decision. The DSS analyst needs to have a clear idea of the nature of the target decision task and a clear strategy of how to support the decision process. Existing psychological research was examined for help with the conceptualization problem, and the theory of cognitive bias is proposed as a candidate for this assistance. A taxonomy of 37 cognitive biases that codifies a complex area of psychological research is developed. The core of the project involves the construction of a design artefact – an evolutionary DSS development methodology that uses cognitive bias theory as a focusing construct, especially in its analysis cycles. The methodology is the major contribution of the project. The feasibility and effectiveness of the development methodology are evaluated in a participatory case study of a strategic DSS project where a managing director is supported in a decision about whether to close a division of a company.\n",
      "Content: Cognitive biases and decision support systems development: a design science approach This paper presents design science research that aims to improve decision support systems (DSS) development in organizations. Evolutionary development has been central to DSS theory and practice for decades, but a significant problem for DSS analysts remains how to conceptualize the improvement of a decision task during evolutionary DSS development. The objective of a DSS project is to improve the decision process and outcome for a manager making an important decision. The DSS analyst needs to have a clear idea of the nature of the target decision task and a clear strategy of how to support the decision process. Existing psychological research was examined for help with the conceptualization problem, and the theory of cognitive bias is proposed as a candidate for this assistance. A taxonomy of 37 cognitive biases that codifies a complex area of psychological research is developed. The core of the project involves the construction of a design artefact – an evolutionary DSS development methodology that uses cognitive bias theory as a focusing construct, especially in its analysis cycles. The methodology is the major contribution of the project. The feasibility and effectiveness of the development methodology are evaluated in a participatory case study of a strategic DSS project where a managing director is supported in a decision about whether to close a division of a company.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 471\n",
      "Title: New ways of working (NWW): Workplace transformation in the digital age\n",
      "Abstract: In the introductory paper of this special issue on new ways of working (NWW) the editors first reflect on the meaning of the ‘new’, finding inspiration in Hannes Meyer's essay “The New World” (1926). The ‘new’ is always relative, of course, closely associated with technological innovation, in our case digitalization, and integrates spatiotemporal, technological and socio-cultural di­ mensions of life and organizing. This SI seeks to offer a reflection on and contribution to deeper understanding of ongoing flexibilization, virtualization and mediation of work practices. The authors go on to contextualize and discuss the contributions of the papers included in this special issue, focussing on significant technological, spatiotemporal, organizational and individual de­ velopments associated with new ways of working. Finally, they reflect on the possible relevance of the recent Covid-19 pandemic for the future of work, arguing that this pandemic accelerated NWW in many ways and – given the many paradoxical NWW dynamics and developments – that there could very well be unexpected and adverse consequences, including a turn away from formal ways of working.\n",
      "Content: New ways of working (NWW): Workplace transformation in the digital age In the introductory paper of this special issue on new ways of working (NWW) the editors first reflect on the meaning of the ‘new’, finding inspiration in Hannes Meyer's essay “The New World” (1926). The ‘new’ is always relative, of course, closely associated with technological innovation, in our case digitalization, and integrates spatiotemporal, technological and socio-cultural di­ mensions of life and organizing. This SI seeks to offer a reflection on and contribution to deeper understanding of ongoing flexibilization, virtualization and mediation of work practices. The authors go on to contextualize and discuss the contributions of the papers included in this special issue, focussing on significant technological, spatiotemporal, organizational and individual de­ velopments associated with new ways of working. Finally, they reflect on the possible relevance of the recent Covid-19 pandemic for the future of work, arguing that this pandemic accelerated NWW in many ways and – given the many paradoxical NWW dynamics and developments – that there could very well be unexpected and adverse consequences, including a turn away from formal ways of working.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 472\n",
      "Title: Dynamic collaboration: A personal reflection\n",
      "Abstract: This paper explores the nature of, and possibilities arising from, dynamic collaboration, where large numbers of people can collaborate on an evolving set of initiatives, without prior knowledge of each other. It references early examples of dynamic collaboration including Topcoder, Innocentive, Zopa, and Wikipedia. It then speculates about the future of dynamic collaboration.\n",
      "Content: Dynamic collaboration: A personal reflection This paper explores the nature of, and possibilities arising from, dynamic collaboration, where large numbers of people can collaborate on an evolving set of initiatives, without prior knowledge of each other. It references early examples of dynamic collaboration including Topcoder, Innocentive, Zopa, and Wikipedia. It then speculates about the future of dynamic collaboration.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 473\n",
      "Title: Just Right Outsourcing: Understanding and Managing Risk\n",
      "Abstract: The risks associated with outsourcing have been the principal limitation on the growth of business process outsourcing, especially cross-border outsourcing. In addition to technological improvements in risk management, it is possible to reduce the risk of opportunistic behavior faced by the buyer by redesigning work flows and dividing work among multiple vendors, increasing the range of tasks that are now appropriate candidates for outsourcing. We provide a taxonomy of risks associated with the outsourcing of business processes. We focus on strategic risks and identify the components of this risk and the means by which it can be mitigated.\n",
      "Content: Just Right Outsourcing: Understanding and Managing Risk The risks associated with outsourcing have been the principal limitation on the growth of business process outsourcing, especially cross-border outsourcing. In addition to technological improvements in risk management, it is possible to reduce the risk of opportunistic behavior faced by the buyer by redesigning work flows and dividing work among multiple vendors, increasing the range of tasks that are now appropriate candidates for outsourcing. We provide a taxonomy of risks associated with the outsourcing of business processes. We focus on strategic risks and identify the components of this risk and the means by which it can be mitigated.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 474\n",
      "Title: Achieving the Optimal Balance Between Investment in Quality and Investment in Self-Promotion for Information Products\n",
      "Abstract: When producers of goods (or services) are confronted by a situation in which their offerings no longer perfectly match consumer preferences, they must determine the extent to which the advertised features of the product reflect the product's actual attributes. We find that the two important determinants of sellers' advertising strategy are the Repeg Cost Ratio, and the Repeat Sales Coefficient. The interplay of these two factors gives rise to four possible strategic scenarios. In the ambiguous fourth scenario, we show that sellers' strategy for information production goods will differ considerably from information consumption goods based on product complexity and cost of product return (borne by the buyer). Finally, we demonstrate that markets are often characterized by self-reinforcing limits on the extent of opportunistic advertising by sellers.\n",
      "Content: Achieving the Optimal Balance Between Investment in Quality and Investment in Self-Promotion for Information Products When producers of goods (or services) are confronted by a situation in which their offerings no longer perfectly match consumer preferences, they must determine the extent to which the advertised features of the product reflect the product's actual attributes. We find that the two important determinants of sellers' advertising strategy are the Repeg Cost Ratio, and the Repeat Sales Coefficient. The interplay of these two factors gives rise to four possible strategic scenarios. In the ambiguous fourth scenario, we show that sellers' strategy for information production goods will differ considerably from information consumption goods based on product complexity and cost of product return (borne by the buyer). Finally, we demonstrate that markets are often characterized by self-reinforcing limits on the extent of opportunistic advertising by sellers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 475\n",
      "Title: The Impact of Automation of Systems on Medical Errors: Evidence from Field Research\n",
      "Abstract: We use panel data from multiple wards from two hospitals spanning a three-year period to investigate the impact of automation of the core error prevention functions in hospitals on medical error rates. Although there are studies based on anecdotal evidence and self-reported data on how automation impacts medical errors, no systematic studies exist that are based on actual error rates from hospitals. Further, there is no systematic evidence on how incremental automation over time and across multiple wards impacts the rate of medical errors. The primary objective of our study is to fill this gap in the literature by empirically examining how the automation of core error prevention functions affects two types of medical errors. We draw on the medical informatics literature and principal-agency theory and use a unique panel data set of actual documented medical errors from two major hospitals to analyze the interplay between automation and medical errors.We hypothesize that the automation of the sensing function (recording and observing agent actions) will have the greatest impact on reducing error rates. We show that there are significant complementarities between quality management training imparted to hospital staff and the automation of control systems in reducing interpretative medical errors. We also offer insights to practitioners and theoreticians alike on how the automation of error prevention functions can be combined with training in quality management to yield better outcomes. Our results suggest an optimal implementation path for the automation of error prevention functions in hospitals.\n",
      "Content: The Impact of Automation of Systems on Medical Errors: Evidence from Field Research We use panel data from multiple wards from two hospitals spanning a three-year period to investigate the impact of automation of the core error prevention functions in hospitals on medical error rates. Although there are studies based on anecdotal evidence and self-reported data on how automation impacts medical errors, no systematic studies exist that are based on actual error rates from hospitals. Further, there is no systematic evidence on how incremental automation over time and across multiple wards impacts the rate of medical errors. The primary objective of our study is to fill this gap in the literature by empirically examining how the automation of core error prevention functions affects two types of medical errors. We draw on the medical informatics literature and principal-agency theory and use a unique panel data set of actual documented medical errors from two major hospitals to analyze the interplay between automation and medical errors.We hypothesize that the automation of the sensing function (recording and observing agent actions) will have the greatest impact on reducing error rates. We show that there are significant complementarities between quality management training imparted to hospital staff and the automation of control systems in reducing interpretative medical errors. We also offer insights to practitioners and theoreticians alike on how the automation of error prevention functions can be combined with training in quality management to yield better outcomes. Our results suggest an optimal implementation path for the automation of error prevention functions in hospitals.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 476\n",
      "Title: Disaggregating the Differential Impact of Healthcare IT in Complex Care Delivery:  Insights from Field Research in Chronic Care\n",
      "Abstract:  This study focuses on the impact of digitizing medical information on the efficiency and perceived quality of chronic care delivery at the individual physician level. This study extends the theory of task technology fit to activity systems consisting of highly interdependent tasks. We find that the outcomes of efficiency and quality gains are driven by the structure of interdependencies between tasks that physicians perform. While structured information plays a key role in enabling both decision-making and task execution, we find that physician-created semistructured information is also an important predictor of both efficiency and quality gains. We show that the structure of activity systems (task interdependencies) has a strong moderating influence on the factors that drive efficiency and quality gains. We find that digitization enables physicians to preprocess patients' records prior to their visit which in turn drives gains in both the efficiency and the perceived quality of care delivered. \n",
      "Content: Disaggregating the Differential Impact of Healthcare IT in Complex Care Delivery:  Insights from Field Research in Chronic Care  This study focuses on the impact of digitizing medical information on the efficiency and perceived quality of chronic care delivery at the individual physician level. This study extends the theory of task technology fit to activity systems consisting of highly interdependent tasks. We find that the outcomes of efficiency and quality gains are driven by the structure of interdependencies between tasks that physicians perform. While structured information plays a key role in enabling both decision-making and task execution, we find that physician-created semistructured information is also an important predictor of both efficiency and quality gains. We show that the structure of activity systems (task interdependencies) has a strong moderating influence on the factors that drive efficiency and quality gains. We find that digitization enables physicians to preprocess patients' records prior to their visit which in turn drives gains in both the efficiency and the perceived quality of care delivered. \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 477\n",
      "Title: Intelligent agents in electronic markets for information goods: customization, preference revelation and pricing\n",
      "Abstract: Electronic commerce has enabled the use of intelligent agent technologies that can evaluate buyers, customize products, and price in real-time. Our model of an electronic market with customizable products analyzes the pricing, profitability and welfare implications of agent-based technologies that price dynamically based on product preference information revealed by consumers. We find that in making the trade-off between better prices and better customization, consumers invariably choose less-than-ideal products. Furthermore, this trade-off has a higher impact on buyers on the higher end of the market and causes a transfer of consumer surplus towards buyers with a lower willingness to pay. As buyers adjust their product choices in response to better demand agent technologies, seller revenues decrease since the gains from better buyer information are dominated by the lowering of the total value created from the transactions. We study the strategic and welfare implications of these findings, and discuss managerial and technology development guidelines.\n",
      "Content: Intelligent agents in electronic markets for information goods: customization, preference revelation and pricing Electronic commerce has enabled the use of intelligent agent technologies that can evaluate buyers, customize products, and price in real-time. Our model of an electronic market with customizable products analyzes the pricing, profitability and welfare implications of agent-based technologies that price dynamically based on product preference information revealed by consumers. We find that in making the trade-off between better prices and better customization, consumers invariably choose less-than-ideal products. Furthermore, this trade-off has a higher impact on buyers on the higher end of the market and causes a transfer of consumer surplus towards buyers with a lower willingness to pay. As buyers adjust their product choices in response to better demand agent technologies, seller revenues decrease since the gains from better buyer information are dominated by the lowering of the total value created from the transactions. We study the strategic and welfare implications of these findings, and discuss managerial and technology development guidelines.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 478\n",
      "Title: Risk and the future of AI: Algorithmic bias, data colonialism, and marginalization\n",
      "Abstract: No abstract available\n",
      "Content: nan\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 479\n",
      "Title: Proximity and Information Technology Outsourcing: How Local Are IT Services Markets?\n",
      "Abstract: We examine the question of which services are tradable within a concrete setting: the outsourcing of information technology (IT) services across a broad cross-section of establishments in the United States. If markets for IT services are local, then we should expect increases in local supply would increase the likelihood of outsourcing by lowering the cost of outsourcing. If markets are not local, then local supply will not affect outsourcing demand. We analyze the outsourcing decisions of a large sample of 99,775 establishments in 2002 and 2004, for two types of IT services—programming and design and hosting. Programming and design projects require communication of detailed user requirements whereas hosting requires less coordination between client and service provider than programming and design. Our empirical results bear out this intuition: the probability of outsourcing programming and design is increasing in the local supply of outsourcing, and this sensitivity to local supply conditions has been increasing over time. This suggests there is some nontradable or \"local\" component to programming and design services that cannot be easily removed. In contrast, the decision to outsource hosting is sensitive to local supply only for firms for which network uptime and security concerns are particularly acute.\n",
      "Content: Proximity and Information Technology Outsourcing: How Local Are IT Services Markets? We examine the question of which services are tradable within a concrete setting: the outsourcing of information technology (IT) services across a broad cross-section of establishments in the United States. If markets for IT services are local, then we should expect increases in local supply would increase the likelihood of outsourcing by lowering the cost of outsourcing. If markets are not local, then local supply will not affect outsourcing demand. We analyze the outsourcing decisions of a large sample of 99,775 establishments in 2002 and 2004, for two types of IT services—programming and design and hosting. Programming and design projects require communication of detailed user requirements whereas hosting requires less coordination between client and service provider than programming and design. Our empirical results bear out this intuition: the probability of outsourcing programming and design is increasing in the local supply of outsourcing, and this sensitivity to local supply conditions has been increasing over time. This suggests there is some nontradable or \"local\" component to programming and design services that cannot be easily removed. In contrast, the decision to outsource hosting is sensitive to local supply only for firms for which network uptime and security concerns are particularly acute.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 480\n",
      "Title: An Empirical Analysis of Software Vendors' Patch Release Behavior: Impact of Vulnerability Disclosure\n",
      "Abstract: A key aspect of better and more secure software is timely patch release by software vendors for the vulnerabilities in their products. Software vulnerability disclosure, which refers to the publication of vulnerability information, has generated intense debate. An important consideration in this debate is the behavior of software vendors. How quickly do vendors patch vulnerabilities and how does disclosure affect patch release time? This paper compiles a unique data set from the Computer Emergency Response Team/Coordination Center (CERT) and SecurityFocus to answer this question. Our results suggest that disclosure accelerates patch release. The instantaneous probability of releasing the patch rises by nearly two and a half times because of disclosure. Open source vendors release patches more quickly than closed source vendors. Vendors are more responsive to more severe vulnerabilities. We also find that vendors respond more slowly to vulnerabilities not disclosed by CERT. We verify our results by using another publicly available data set and find that results are consistent. We also show how our estimates can aid policy makers in their decision making.\n",
      "Content: An Empirical Analysis of Software Vendors' Patch Release Behavior: Impact of Vulnerability Disclosure A key aspect of better and more secure software is timely patch release by software vendors for the vulnerabilities in their products. Software vulnerability disclosure, which refers to the publication of vulnerability information, has generated intense debate. An important consideration in this debate is the behavior of software vendors. How quickly do vendors patch vulnerabilities and how does disclosure affect patch release time? This paper compiles a unique data set from the Computer Emergency Response Team/Coordination Center (CERT) and SecurityFocus to answer this question. Our results suggest that disclosure accelerates patch release. The instantaneous probability of releasing the patch rises by nearly two and a half times because of disclosure. Open source vendors release patches more quickly than closed source vendors. Vendors are more responsive to more severe vulnerabilities. We also find that vendors respond more slowly to vulnerabilities not disclosed by CERT. We verify our results by using another publicly available data set and find that results are consistent. We also show how our estimates can aid policy makers in their decision making.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 481\n",
      "Title: Resource allocation for demand surge mitigation during disaster response\n",
      "Abstract: Large-scale public health emergencies can result in an overwhelming demand for healthcare resources. Regional aid in the form of central stockpiles and resource redistribution can help mitigate the resulting demand surge. This paper discusses a resource allocation approach for optimizing regional aid during public health emergencies. We find that, optimal response involves delaying the distribution of resources from the central stockpile as much as possible. Also, smaller counties stand to benefit the most from mutual aid. And finally, policy level decisions that alter the objectives of pandemic relief efforts can significantly impact the allocations to affected regions.\n",
      "Content: Resource allocation for demand surge mitigation during disaster response Large-scale public health emergencies can result in an overwhelming demand for healthcare resources. Regional aid in the form of central stockpiles and resource redistribution can help mitigate the resulting demand surge. This paper discusses a resource allocation approach for optimizing regional aid during public health emergencies. We find that, optimal response involves delaying the distribution of resources from the central stockpile as much as possible. Also, smaller counties stand to benefit the most from mutual aid. And finally, policy level decisions that alter the objectives of pandemic relief efforts can significantly impact the allocations to affected regions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 482\n",
      "Title: Static R&D project portfolio selection in public organizations\n",
      "Abstract: The problem of static research and development (R&D) project portfolio selection arises when a public organization opens a call for proposals and then builds a portfolio choosing which ones to fund in terms of impact measures including social objectives, emphasis areas, geographical influence, and other non-monetary factors. The funds are classified into types of expenses, depending on the goals and preferences of the organization: equipment purchases, travel expenses, scholarships, publication fees, etc. We propose a mathematical model framework in which each project proposal comprised tasks with a specific type of expense and the assigned funding may be a fraction of the requested amount, accounting for possible inter- and intra-proposal dependencies. We present computational experiments that show our models to be efficiently resolvable.\n",
      "Content: Static R&D project portfolio selection in public organizations The problem of static research and development (R&D) project portfolio selection arises when a public organization opens a call for proposals and then builds a portfolio choosing which ones to fund in terms of impact measures including social objectives, emphasis areas, geographical influence, and other non-monetary factors. The funds are classified into types of expenses, depending on the goals and preferences of the organization: equipment purchases, travel expenses, scholarships, publication fees, etc. We propose a mathematical model framework in which each project proposal comprised tasks with a specific type of expense and the assigned funding may be a fraction of the requested amount, accounting for possible inter- and intra-proposal dependencies. We present computational experiments that show our models to be efficiently resolvable.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 483\n",
      "Title: Application of KM measures to the impact of a specialized groupware system on corporate productivity and operations\n",
      "Abstract: Here we describe an experiment to show the improvement in productivity resulting from use of a specialized groupware system, using quantitative data and observations of the knowledge management (KM) processes, styles, and critical success factors. Data describing the work process before and after the deployment of the system was applied to measure the impact on performance, operations, and knowledge sharing behavior. The attitude of the organization toward knowledge sharing and the deployed groupware system was then studied along several dimensions that represent KM styles, by assessing the KM orientation and motivation of the organization. Lessons learned were presented and used for directing the attention of management to the importance of supporting collaborative and KM technologies for corporate strategic competitiveness.\n",
      "Content: Application of KM measures to the impact of a specialized groupware system on corporate productivity and operations Here we describe an experiment to show the improvement in productivity resulting from use of a specialized groupware system, using quantitative data and observations of the knowledge management (KM) processes, styles, and critical success factors. Data describing the work process before and after the deployment of the system was applied to measure the impact on performance, operations, and knowledge sharing behavior. The attitude of the organization toward knowledge sharing and the deployed groupware system was then studied along several dimensions that represent KM styles, by assessing the KM orientation and motivation of the organization. Lessons learned were presented and used for directing the attention of management to the importance of supporting collaborative and KM technologies for corporate strategic competitiveness.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 484\n",
      "Title: An empirical investigation of judgment feedback and computerized decision support in a prediction task\n",
      "Abstract: This study examines the effects on judgment accuracy of cognitive and outcome feedback provided using a computerized decision support tool. Five feedback conditions were examined in a two-stage experiment utilizing 294 participants: an outcome feedback condition, two cognitive feedback conditions (judgment policy feedback and model predictions feedback), and two joint feedback conditions (judgment policy plus outcome feedback, and model predictions plus outcome feedback). In the first stage, decision makers specified the judgment policies (i.e. cue weights and function forms) that they believed they would use in making their earnings predictions. They were then asked to forecast earnings per share for several companies based on average earnings for the last three years, current year gross margin percentage, quick ratio and eamings yield. Using appropriately modified end-user software, feedback was then provided to all participants, except those receiving outcome feedback only. Judgment policy feedback consisted of informing decision makers of the cue weights and function forms underlying their actual predictions, while model predictions feedback consisted of earnings predictions generated from the decision makers' stated judgment policies. In the second stage, decision makers revised or retained their original judgment policies and then made another set of earnings predictions. Outcome feedback, consisting of information about the actual earnings attained by the companies, was then provided to participants in the outcome feedback and joint feedback conditions. This process was then repeated for a new set of companies to determine how the various forms of feedback influenced judgment accuracy. Results indicated that providing decision makers with either type of cognitive feedback, relative to providing outcome feedback, contributed to improvements in judgment accuracy. There were no significant differences between the judgment accuracy of the cognitive feedback conditions and of the respective joint feedback conditions, indicating that adding outcome feedback did not enhance judgment accuracy. Results also suggested that model predictions feedback may be more effective than judgment policy feedback, which in turn is superior to outcome feedback. All cognitive feedback conditions, relative to outcome feedback only, also demonstrated convergence between stated model predictions and actual predictions. These results are discussed in terms of implications for the design of decision support systems for individual judgment tasks.\n",
      "Content: An empirical investigation of judgment feedback and computerized decision support in a prediction task This study examines the effects on judgment accuracy of cognitive and outcome feedback provided using a computerized decision support tool. Five feedback conditions were examined in a two-stage experiment utilizing 294 participants: an outcome feedback condition, two cognitive feedback conditions (judgment policy feedback and model predictions feedback), and two joint feedback conditions (judgment policy plus outcome feedback, and model predictions plus outcome feedback). In the first stage, decision makers specified the judgment policies (i.e. cue weights and function forms) that they believed they would use in making their earnings predictions. They were then asked to forecast earnings per share for several companies based on average earnings for the last three years, current year gross margin percentage, quick ratio and eamings yield. Using appropriately modified end-user software, feedback was then provided to all participants, except those receiving outcome feedback only. Judgment policy feedback consisted of informing decision makers of the cue weights and function forms underlying their actual predictions, while model predictions feedback consisted of earnings predictions generated from the decision makers' stated judgment policies. In the second stage, decision makers revised or retained their original judgment policies and then made another set of earnings predictions. Outcome feedback, consisting of information about the actual earnings attained by the companies, was then provided to participants in the outcome feedback and joint feedback conditions. This process was then repeated for a new set of companies to determine how the various forms of feedback influenced judgment accuracy. Results indicated that providing decision makers with either type of cognitive feedback, relative to providing outcome feedback, contributed to improvements in judgment accuracy. There were no significant differences between the judgment accuracy of the cognitive feedback conditions and of the respective joint feedback conditions, indicating that adding outcome feedback did not enhance judgment accuracy. Results also suggested that model predictions feedback may be more effective than judgment policy feedback, which in turn is superior to outcome feedback. All cognitive feedback conditions, relative to outcome feedback only, also demonstrated convergence between stated model predictions and actual predictions. These results are discussed in terms of implications for the design of decision support systems for individual judgment tasks.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 485\n",
      "Title: An economic analysis of electronic secondary markets: installed base, technology, durability and firm profitability\n",
      "Abstract: The Internet has spawned a number of partially structured electronic secondary markets, which enable the trading of secondary goods between consumers. Many of these, such as Usenet groups, or WWW sites for niche products, tend to be self-administering; however, there has been significant recent growth in the number of more general web-based markets of this kind. These electronic secondary markets, while facilitating reliable and liquid trade of used goods, could also have an impact on the desirability of new products, as well as products that are complementary/compatible to those traded. We present an economic framework for analyzing how these markets affect the demand for a primary product. We examine when it is optimal for a firm to operate a market of this kind, and when its presence is socially optimal. Surprisingly, we find that in a number of cases, the presence of these markets has a primary positive effect on the profitability of a new good; this leads us to conjecture that there will soon be a number of such trading forums operated by manufacturers of primary goods. We also find that in a majority of cases, it is feasible for a third-party intermediary to profitably operate such a market. Key parameters that affect the desirability of the market are the existing installed customer base, the cost of information technology, the durability of the products in question, their rate of technological obsolescence and the nature of customer preferences.\n",
      "Content: An economic analysis of electronic secondary markets: installed base, technology, durability and firm profitability The Internet has spawned a number of partially structured electronic secondary markets, which enable the trading of secondary goods between consumers. Many of these, such as Usenet groups, or WWW sites for niche products, tend to be self-administering; however, there has been significant recent growth in the number of more general web-based markets of this kind. These electronic secondary markets, while facilitating reliable and liquid trade of used goods, could also have an impact on the desirability of new products, as well as products that are complementary/compatible to those traded. We present an economic framework for analyzing how these markets affect the demand for a primary product. We examine when it is optimal for a firm to operate a market of this kind, and when its presence is socially optimal. Surprisingly, we find that in a number of cases, the presence of these markets has a primary positive effect on the profitability of a new good; this leads us to conjecture that there will soon be a number of such trading forums operated by manufacturers of primary goods. We also find that in a majority of cases, it is feasible for a third-party intermediary to profitably operate such a market. Key parameters that affect the desirability of the market are the existing installed customer base, the cost of information technology, the durability of the products in question, their rate of technological obsolescence and the nature of customer preferences.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 486\n",
      "Title: Information systems use as strategy practice: A multi-dimensional view of strategic information system implementation and use\n",
      "Abstract: Information systems (IS) are strategic in so far as they are used to realize strategic intent. Yet, while much has been said about aligning IS functionality with the strategic intent and how to organizationally implement strategically aligned systems, less is known of how to successfully implement strategic change associated with system use – a truly critical challenge within strategic IS implementation. Drawing on a strategy-as-practice perspective we address this gap by developing a multi-dimensional view of IS strategy, conceptualizing three key challenges in the IS strategy process, to explain how and why a paper mill, despite successfully implementing a strategic production management system, failed to produce intended strategic change. We call this outcome strategy blindness: organizational incapability to realize the strategic intent of implemented, available system capabilities. Using a longitudinal case study we investigate how cognitive rigidity of key actors and fixed, interrelated practices shaped the implementation of the new production system. We also identify core components and dynamics that constitute a richer multi-dimensional view of the IS strategy implementation (alignment) process. In particular, we identify three salient factors that contribute to strategy blindness – mistranslation of intent, flexibility of the IT artifact and cognitive entrenchment – and discuss how they affect strategic implementation processes. We conclude by discussing implications of our findings for IS strategy theory and practice, especially the contribution of strategy-as-practice to this stream of research.\n",
      "Content: Information systems use as strategy practice: A multi-dimensional view of strategic information system implementation and use Information systems (IS) are strategic in so far as they are used to realize strategic intent. Yet, while much has been said about aligning IS functionality with the strategic intent and how to organizationally implement strategically aligned systems, less is known of how to successfully implement strategic change associated with system use – a truly critical challenge within strategic IS implementation. Drawing on a strategy-as-practice perspective we address this gap by developing a multi-dimensional view of IS strategy, conceptualizing three key challenges in the IS strategy process, to explain how and why a paper mill, despite successfully implementing a strategic production management system, failed to produce intended strategic change. We call this outcome strategy blindness: organizational incapability to realize the strategic intent of implemented, available system capabilities. Using a longitudinal case study we investigate how cognitive rigidity of key actors and fixed, interrelated practices shaped the implementation of the new production system. We also identify core components and dynamics that constitute a richer multi-dimensional view of the IS strategy implementation (alignment) process. In particular, we identify three salient factors that contribute to strategy blindness – mistranslation of intent, flexibility of the IT artifact and cognitive entrenchment – and discuss how they affect strategic implementation processes. We conclude by discussing implications of our findings for IS strategy theory and practice, especially the contribution of strategy-as-practice to this stream of research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 487\n",
      "Title: Generating innovation potential: How digital entrepreneurs conceal, sequence, anchor, and propagate new technology\n",
      "Abstract: Organizations that fail to innovate become disrupted by those that do. Digital technology makes corporate entrepreneurship increasingly potent and prolific but simultaneously blurs the link between entrepreneurial processes and innovation outcomes. Our understanding of how corporate entrepreneurship with digital technology unfolds in organizations is thus limited. We develop a framework that captures four tactics that digital entrepreneurs may use to generate innovation potential. Specifically, we report how these tactics helped employees at a Norwegian hospital to develop and scale an application for time planning and resource analytics. Our study shows that managing digital entrepreneurship strategically requires organizations to harness the multiplicity in information systems (IS) use that increasingly malleable digital technology affords.\n",
      "Content: Generating innovation potential: How digital entrepreneurs conceal, sequence, anchor, and propagate new technology Organizations that fail to innovate become disrupted by those that do. Digital technology makes corporate entrepreneurship increasingly potent and prolific but simultaneously blurs the link between entrepreneurial processes and innovation outcomes. Our understanding of how corporate entrepreneurship with digital technology unfolds in organizations is thus limited. We develop a framework that captures four tactics that digital entrepreneurs may use to generate innovation potential. Specifically, we report how these tactics helped employees at a Norwegian hospital to develop and scale an application for time planning and resource analytics. Our study shows that managing digital entrepreneurship strategically requires organizations to harness the multiplicity in information systems (IS) use that increasingly malleable digital technology affords.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 488\n",
      "Title: Lessons from enterprise systems competency centers in adopting digital transformation initiatives: An assemblage approach\n",
      "Abstract: Firms are increasingly adopting digital transformation as a strategic priority. However, the path to successful transformation remains uncertain for many organizations. This paper examines the establishment and evolution of competency centers in two case study organizations, historically used in enterprise systems, in addressing the complexity and challenges of digital transformation. The interactions within these competency centers are analyzed through assemblage theory to understand the emergent relations between heterogeneous parts (technology, people, and orga­ nization) and the dynamic processes of new configurations. The insights from this research show the critical role of the competency center in any enterprise system’s success and how it could continue playing a central role in future digital transformation initiatives. By providing a new lens to examine these issues, the assemblage theory provided a new theoretical perspective to the IS field and a new alternative empirical setting to the organizational literature.\n",
      "Content: Lessons from enterprise systems competency centers in adopting digital transformation initiatives: An assemblage approach Firms are increasingly adopting digital transformation as a strategic priority. However, the path to successful transformation remains uncertain for many organizations. This paper examines the establishment and evolution of competency centers in two case study organizations, historically used in enterprise systems, in addressing the complexity and challenges of digital transformation. The interactions within these competency centers are analyzed through assemblage theory to understand the emergent relations between heterogeneous parts (technology, people, and orga­ nization) and the dynamic processes of new configurations. The insights from this research show the critical role of the competency center in any enterprise system’s success and how it could continue playing a central role in future digital transformation initiatives. By providing a new lens to examine these issues, the assemblage theory provided a new theoretical perspective to the IS field and a new alternative empirical setting to the organizational literature.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 489\n",
      "Title: The ambiguous proposal evaluation problem\n",
      "Abstract: A complex decision-making challenge senior managers commonly face is the selection of the winning bid from multiple project proposals. Project selection decisions become more complex when providers deliberately choose to introduce ambiguity to their project proposals rather than address the client's predetermined set of desired specifications. In particular, providers suggest in their proposals a range of values for some product specifications. Providers introduce ambiguity for various reasons, such as future technological advances and strategic misrepresentation. Further, such disruptive behaviour is tolerated by clients for reasons such as ambition, lack of knowledge, uncertain needs, complexity and a lack of competition. This paper defines the ambiguous proposal evaluation problem and develops a solution which enables such proposals to be compared and ranked. The solution is developed through the utilisation of fuzzy logic in combination with a multi-criteria decision-making method, and is illustrated on a procurement project. The contribution of this paper is firstly to define a practical problem in the literature and secondly to develop a solution which enables ranking ambiguous options.\n",
      "Content: The ambiguous proposal evaluation problem A complex decision-making challenge senior managers commonly face is the selection of the winning bid from multiple project proposals. Project selection decisions become more complex when providers deliberately choose to introduce ambiguity to their project proposals rather than address the client's predetermined set of desired specifications. In particular, providers suggest in their proposals a range of values for some product specifications. Providers introduce ambiguity for various reasons, such as future technological advances and strategic misrepresentation. Further, such disruptive behaviour is tolerated by clients for reasons such as ambition, lack of knowledge, uncertain needs, complexity and a lack of competition. This paper defines the ambiguous proposal evaluation problem and develops a solution which enables such proposals to be compared and ranked. The solution is developed through the utilisation of fuzzy logic in combination with a multi-criteria decision-making method, and is illustrated on a procurement project. The contribution of this paper is firstly to define a practical problem in the literature and secondly to develop a solution which enables ranking ambiguous options.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 490\n",
      "Title: Transforming society by transforming technology: the science and politics of participatory design\n",
      "Abstract: This article attempts to shed historical light on some of the social, political, and ethical issues that have arisen from two disparate perspectives on technology which have both come to integrate an explicit consideration of social factors into systems design. It presents two distinct historical traditions which have contributed to the current field of participatory design methodologies—Joint Application Design (JAD®), and the British “socio-technical systems” and Scandinavian “collective resources” approaches—and which in practice integrated the end-users in different ways consequent upon their differing perspectives on workers, professional relationships to technology, and stated goals. One interest in examining the independent development of methodologies from these two perspectives is that, despite their differences, the approaches ultimately converged on a set of shared concerns and very similar practices. The paper also examines the relation of these traditions to transformations in the theorization of business organization and trends of corporate restructuring which helped to secure a place for variants of related methodologies in major US and multinational corporations. It concludes with an examination of some broader issues in the relationship between technology and society and the prospects for the critical study of technology. I argue that participatory design and its related methodologies are best understood as a model for involving users, designers and the technology itself in a process of technological development. Rather than seeing participatory design as merely the insertion of public dialog within technological design practices, as several observers have done, we should see it as a model for the critical practice of developing technological designs.\n",
      "Content: Transforming society by transforming technology: the science and politics of participatory design This article attempts to shed historical light on some of the social, political, and ethical issues that have arisen from two disparate perspectives on technology which have both come to integrate an explicit consideration of social factors into systems design. It presents two distinct historical traditions which have contributed to the current field of participatory design methodologies—Joint Application Design (JAD®), and the British “socio-technical systems” and Scandinavian “collective resources” approaches—and which in practice integrated the end-users in different ways consequent upon their differing perspectives on workers, professional relationships to technology, and stated goals. One interest in examining the independent development of methodologies from these two perspectives is that, despite their differences, the approaches ultimately converged on a set of shared concerns and very similar practices. The paper also examines the relation of these traditions to transformations in the theorization of business organization and trends of corporate restructuring which helped to secure a place for variants of related methodologies in major US and multinational corporations. It concludes with an examination of some broader issues in the relationship between technology and society and the prospects for the critical study of technology. I argue that participatory design and its related methodologies are best understood as a model for involving users, designers and the technology itself in a process of technological development. Rather than seeing participatory design as merely the insertion of public dialog within technological design practices, as several observers have done, we should see it as a model for the critical practice of developing technological designs.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 491\n",
      "Title: Constructing continuity across the organisational culture boundary in a highly virtual work environment\n",
      "Abstract: While remote work allows organisations to offer their employees flexibility and harness global talent and markets for business growth, inability to rely on physical interactions between employees imposes challenges specific to operations in highly virtual work environments. Among these characteristic issues are challenges associated with organisational socialisation and organisational culture. Accordingly, an action design research project was carried out for building a socialisation substitute (an information artefact in the form of a digital organisational culture handbook) to support synthesis of symbolic and pragmatic components of organisational culture at case company Smartly.io, a highly virtual organisation experiencing rapid growth. The paper contributes to the literature on socialisation and organisational culture by demonstrating one approach to designing a surrogate for socialisation that acts as a conduit between the symbolic aspects of organisational culture (such as values) and the pragmatic ones (such as toolkits). The work contributes to organisational discontinuity theory also, via theory-generating descriptive analysis of the process of building continuity across the organisational culture boundary through creation of an information artefact. The resulting artefact was found to deliver practical utility to the case company and encapsulate generalisable design principles for this building process.\n",
      "Content: Constructing continuity across the organisational culture boundary in a highly virtual work environment While remote work allows organisations to offer their employees flexibility and harness global talent and markets for business growth, inability to rely on physical interactions between employees imposes challenges specific to operations in highly virtual work environments. Among these characteristic issues are challenges associated with organisational socialisation and organisational culture. Accordingly, an action design research project was carried out for building a socialisation substitute (an information artefact in the form of a digital organisational culture handbook) to support synthesis of symbolic and pragmatic components of organisational culture at case company Smartly.io, a highly virtual organisation experiencing rapid growth. The paper contributes to the literature on socialisation and organisational culture by demonstrating one approach to designing a surrogate for socialisation that acts as a conduit between the symbolic aspects of organisational culture (such as values) and the pragmatic ones (such as toolkits). The work contributes to organisational discontinuity theory also, via theory-generating descriptive analysis of the process of building continuity across the organisational culture boundary through creation of an information artefact. The resulting artefact was found to deliver practical utility to the case company and encapsulate generalisable design principles for this building process.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 492\n",
      "Title: Sociotechnical Envelopment of Artificial Intelligence:  An Approach to Organizational Deployment  of Inscrutable Artificial Intelligence Systems\n",
      "Abstract:  The paper presents an approach for implementing inscrutable (i.e., nonexplainable) artificial intelligence (AI) such as neural networks in an accountable and safe manner in organizational settings. Drawing on an exploratory case study and the recently proposed concept of envelopment, it describes a case of an organization successfully \"enveloping\" its AI solutions to balance the performance benefits of flexible AI models with the risks that inscrutable models can entail. The authors present several envelopment methods-establishing clear boundaries within which the AI is to interact with its surroundings, choosing and curating the training data well, and appropriately managing input and output sources-alongside their influence on the choice of AI models within the organization. This work makes two key contributions: It introduces the concept of sociotechnical envelopment by demonstrating the ways in which an organization's successful AI envelopment depends on the interaction of social and technical factors, thus extending the literature's focus beyond mere technical issues. Secondly, the empirical examples illustrate how operationalizing a sociotechnical envelopment enables an organization to manage the trade-off between low explainability and high performance presented by inscrutable models. These contributions pave the way for more responsible, accountable AI implementations in organizations, whereby humans can gain better control of even inscrutable machine-learning models.Hind Benbya was the accepting senior editor. This research article was submitted on February 29, 2020 and underwent three revisions. About the AuthorsAleksandre Asatiani is an assistant professor in information systems at the Department of Applied Information Technology, at the University of Gothenburg. He is also an affiliated researcher with the Swedish Center for Digital Innovation (SCDI). His research focuses on artificial intelligence, robotic process automation, virtual organizations, and IS sourcing. His work has previously appeared in leading IS journals such as Information Systems Journal, Journal of Information Technology, and MIS Quarterly Executive.Pekka Malo is a tenured associate professor of statistics at Aalto University School of Business. His research has been published in leading journals in operations research, information science, and artificial intelligence. Pekka is considered as one of the pioneers in the development of evolutionary optimization algorithms for solving challenging bilevel programming problems. His research interests include business analytics, computational statistics, machine learning, optimization and evolutionary computation, and their applications to marketing, finance, and healthcare.Per Rådberg Nagbøl is a PhD fellow at the IT University of Copenhagen doing a collaborative PhD with the Danish Business Authority within the field of information systems. He uses action design research to design systems and procedures for quality assurance and evaluation of machine learning, focusing on accurate, transparent, and responsible use in the public sector from a risk management perspective.Esko Penttinen is a professor of practice in information systems at Aalto University School of Business in Helsinki.He holds a PhD in information systems science and an MSc in Economics from Helsinki School of Economics. Esko leads the Real-Time Economy Competence Center and is the co-founder and chairman of XBRL Finland. He studies the interplay between humans and machines, organizational implementation of artificial intelligence, and governance issues related to outsourcing and virtual organizing. His main practical expertise lies in the assimilation and economic implications of interorganizational information systems, focusing on application areas such as electronic financial systems, government reporting, and electronic invoicing. Esko's research has appeared in leading IS outlets such as \n",
      "Content: Sociotechnical Envelopment of Artificial Intelligence:  An Approach to Organizational Deployment  of Inscrutable Artificial Intelligence Systems  The paper presents an approach for implementing inscrutable (i.e., nonexplainable) artificial intelligence (AI) such as neural networks in an accountable and safe manner in organizational settings. Drawing on an exploratory case study and the recently proposed concept of envelopment, it describes a case of an organization successfully \"enveloping\" its AI solutions to balance the performance benefits of flexible AI models with the risks that inscrutable models can entail. The authors present several envelopment methods-establishing clear boundaries within which the AI is to interact with its surroundings, choosing and curating the training data well, and appropriately managing input and output sources-alongside their influence on the choice of AI models within the organization. This work makes two key contributions: It introduces the concept of sociotechnical envelopment by demonstrating the ways in which an organization's successful AI envelopment depends on the interaction of social and technical factors, thus extending the literature's focus beyond mere technical issues. Secondly, the empirical examples illustrate how operationalizing a sociotechnical envelopment enables an organization to manage the trade-off between low explainability and high performance presented by inscrutable models. These contributions pave the way for more responsible, accountable AI implementations in organizations, whereby humans can gain better control of even inscrutable machine-learning models.Hind Benbya was the accepting senior editor. This research article was submitted on February 29, 2020 and underwent three revisions. About the AuthorsAleksandre Asatiani is an assistant professor in information systems at the Department of Applied Information Technology, at the University of Gothenburg. He is also an affiliated researcher with the Swedish Center for Digital Innovation (SCDI). His research focuses on artificial intelligence, robotic process automation, virtual organizations, and IS sourcing. His work has previously appeared in leading IS journals such as Information Systems Journal, Journal of Information Technology, and MIS Quarterly Executive.Pekka Malo is a tenured associate professor of statistics at Aalto University School of Business. His research has been published in leading journals in operations research, information science, and artificial intelligence. Pekka is considered as one of the pioneers in the development of evolutionary optimization algorithms for solving challenging bilevel programming problems. His research interests include business analytics, computational statistics, machine learning, optimization and evolutionary computation, and their applications to marketing, finance, and healthcare.Per Rådberg Nagbøl is a PhD fellow at the IT University of Copenhagen doing a collaborative PhD with the Danish Business Authority within the field of information systems. He uses action design research to design systems and procedures for quality assurance and evaluation of machine learning, focusing on accurate, transparent, and responsible use in the public sector from a risk management perspective.Esko Penttinen is a professor of practice in information systems at Aalto University School of Business in Helsinki.He holds a PhD in information systems science and an MSc in Economics from Helsinki School of Economics. Esko leads the Real-Time Economy Competence Center and is the co-founder and chairman of XBRL Finland. He studies the interplay between humans and machines, organizational implementation of artificial intelligence, and governance issues related to outsourcing and virtual organizing. His main practical expertise lies in the assimilation and economic implications of interorganizational information systems, focusing on application areas such as electronic financial systems, government reporting, and electronic invoicing. Esko's research has appeared in leading IS outlets such as \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 493\n",
      "Title: Information systems for sustainable remote workplaces\n",
      "Abstract: This review discusses the challenges associated with the sustainability of remote workplaces, which have become more prevalent due to the growing trend of work digitalization and the pandemic-induced push to remote work. These challenges are highlighted in literature across various disciplines, including information systems, but these discourses have remained isolated from each other. In this review, we consolidated and synthesized research on remote work from the perspective of individual workers by reviewing 187 articles published between 1999 and 2020 in recognized academic journals from fields including information systems, organizational studies, economics, human resources, sociology, and psychology. We identified five key themes that concern opportunities and challenges to sustainable remote workplaces: (1) key character­ istics, (2) work-life boundaries; (3) health and well-being; (4) social interaction, and (5) lead­ ership. Building on our findings we created a framework that recognizes two interrelated categories of factors influencing remote workplace sustainability – rigid base characteristics and contextual remote workplace variables – that together shape the trajectory of remote workplace sustainability in the long term. The framework also identifies the potential role of information systems in modulating the impact of the base characteristics to build continuities that encourage more sustainable remote workplaces. The paper concludes by offering a research agenda for in­ formation systems for sustainable remote workplaces based on the three IS theoretical frames: inclusion, dignity, and boundary objects.\n",
      "Content: Information systems for sustainable remote workplaces This review discusses the challenges associated with the sustainability of remote workplaces, which have become more prevalent due to the growing trend of work digitalization and the pandemic-induced push to remote work. These challenges are highlighted in literature across various disciplines, including information systems, but these discourses have remained isolated from each other. In this review, we consolidated and synthesized research on remote work from the perspective of individual workers by reviewing 187 articles published between 1999 and 2020 in recognized academic journals from fields including information systems, organizational studies, economics, human resources, sociology, and psychology. We identified five key themes that concern opportunities and challenges to sustainable remote workplaces: (1) key character­ istics, (2) work-life boundaries; (3) health and well-being; (4) social interaction, and (5) lead­ ership. Building on our findings we created a framework that recognizes two interrelated categories of factors influencing remote workplace sustainability – rigid base characteristics and contextual remote workplace variables – that together shape the trajectory of remote workplace sustainability in the long term. The framework also identifies the potential role of information systems in modulating the impact of the base characteristics to build continuities that encourage more sustainable remote workplaces. The paper concludes by offering a research agenda for in­ formation systems for sustainable remote workplaces based on the three IS theoretical frames: inclusion, dignity, and boundary objects.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 494\n",
      "Title: Uncovering the nature of the relationship between outsourcing motivations and the degree of outsourcing: An empirical study on Finnish small and medium-sized enterprises\n",
      "Abstract: Prior literature has identified several outsourcing motivations, such as cost reduction and access to expertise, and deciphered the influence of these variables on outsourcing decisions. In another stream of outsourcing studies, researchers have gauged the degree of outsourcing, unearthing how companies may choose to outsource a set or processes instead of the whole business function. In this article, we draw on both of these streams of outsourcing research to study the relationship between outsourcing motivations and the degree of outsourcing within a particular business function. We probe the effect of nine motivation items on outsourcing decision through an empirical study using survey data gathered from 337 small and medium-sized enterprises. We find that cost reduction, a focus on core competence and business/process improvements are all associated with a higher degree of outsourcing, but interestingly, access to expertise is negatively associated with the degree of outsourcing. This finding suggests that companies that outsource mainly to acquire external expertise outsource only a limited number of processes within a specific business function. Our main theoretical contribution lies in uncovering the dynamic nature of outsourcing motivations, meaning that as companies outsource a larger degree of their business processes, some motivation items become more accentuated and others fade in importance.\n",
      "Content: Uncovering the nature of the relationship between outsourcing motivations and the degree of outsourcing: An empirical study on Finnish small and medium-sized enterprises Prior literature has identified several outsourcing motivations, such as cost reduction and access to expertise, and deciphered the influence of these variables on outsourcing decisions. In another stream of outsourcing studies, researchers have gauged the degree of outsourcing, unearthing how companies may choose to outsource a set or processes instead of the whole business function. In this article, we draw on both of these streams of outsourcing research to study the relationship between outsourcing motivations and the degree of outsourcing within a particular business function. We probe the effect of nine motivation items on outsourcing decision through an empirical study using survey data gathered from 337 small and medium-sized enterprises. We find that cost reduction, a focus on core competence and business/process improvements are all associated with a higher degree of outsourcing, but interestingly, access to expertise is negatively associated with the degree of outsourcing. This finding suggests that companies that outsource mainly to acquire external expertise outsource only a limited number of processes within a specific business function. Our main theoretical contribution lies in uncovering the dynamic nature of outsourcing motivations, meaning that as companies outsource a larger degree of their business processes, some motivation items become more accentuated and others fade in importance.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 495\n",
      "Title: Constructing continuities in virtual work environments: A multiple case study of two firms with differing degrees of virtuality\n",
      "Abstract: In this paper, we study how continuities are constructed in virtual work environments by comparing two firms with differing degrees of virtuality. Using Organizational Discontinuity Theory and drawing on a qualitative study of two accounting firms operating in Finland, we observe virtual work discontinuities in the two firms and identify constructed continuities. We find that in constructing continuities, virtual organizations need to balance rigid and flexible approaches regarding governance structure, the role of technology, communication management, and workflow management. Our main contributions are an empirical application of Organizational Discontinuity Theory to the comparison of virtual work environments and a set of propositions regarding how firms approach continuity construction in different virtuality contexts.\n",
      "Content: Constructing continuities in virtual work environments: A multiple case study of two firms with differing degrees of virtuality In this paper, we study how continuities are constructed in virtual work environments by comparing two firms with differing degrees of virtuality. Using Organizational Discontinuity Theory and drawing on a qualitative study of two accounting firms operating in Finland, we observe virtual work discontinuities in the two firms and identify constructed continuities. We find that in constructing continuities, virtual organizations need to balance rigid and flexible approaches regarding governance structure, the role of technology, communication management, and workflow management. Our main contributions are an empirical application of Organizational Discontinuity Theory to the comparison of virtual work environments and a set of propositions regarding how firms approach continuity construction in different virtuality contexts.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 496\n",
      "Title: Pricing Models for Online Advertising: CPM vs. CPC\n",
      "Abstract: Online advertising has transformed the advertising industry with its measurability and accountability. Online software and services supported by online advertising is becoming a reality as evidenced by the success of Google and its initiatives. Therefore, the choice of a pricing model for advertising becomes a critical issue for these firms. We present a formal model of pricing models in online advertising using the principal–agent framework to study the two most popular pricing models: input-based cost per thousand impressions (CPM) and performance-based cost per click-through (CPC). We identify four important factors that affect the preference of CPM to the CPC model, and vice versa. In particular, we highlight the interplay between uncertainty in the decision environment, value of advertising, cost of mistargeting advertisements, and alignment of incentives. These factors shed light on the preferred online-advertising pricing model for publishers and advertisers under different market conditions.\n",
      "Content: Pricing Models for Online Advertising: CPM vs. CPC Online advertising has transformed the advertising industry with its measurability and accountability. Online software and services supported by online advertising is becoming a reality as evidenced by the success of Google and its initiatives. Therefore, the choice of a pricing model for advertising becomes a critical issue for these firms. We present a formal model of pricing models in online advertising using the principal–agent framework to study the two most popular pricing models: input-based cost per thousand impressions (CPM) and performance-based cost per click-through (CPC). We identify four important factors that affect the preference of CPM to the CPC model, and vice versa. In particular, we highlight the interplay between uncertainty in the decision environment, value of advertising, cost of mistargeting advertisements, and alignment of incentives. These factors shed light on the preferred online-advertising pricing model for publishers and advertisers under different market conditions.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 497\n",
      "Title: Ad-Blockers: A Blessing or a Curse?\n",
      "Abstract: Users who have an ad-blocker installed present a genuine predicament for a website (also known as the publisher): On the one hand, these users do not generate revenue for the website; on the other hand, denying them access can shrink the user base and adversely affect the popularity of the website, ultimately reducing traffic over the long run. This has led some websites to require that ad-block users “white-list” them for obtaining access to an “ad-light” experience. We model the decision problem for a website facing two user segments: regular users and ad-block users. The first-level decision or gating strategy is whether to allow ad-free access to ad-block users or require them to white-list the website for gaining access. When ad-block users are allowed ad-free access, the second-level decision is the level of advertising (or ad-intensity) for regular users. When ad-block users are required to white-list, the second-level decisions are the ad-intensities for regular users and ad-block users. The net utility of a user from visiting the website depends on the intrinsic value of the website’s content, the value obtained due to network effects driven by the amount of traffic/popularity of the website, and the cost incurred due to the presence of ads. We derive an optimal gating and ad-intensity strategy for the website and also solve an identical model for a world without ad-block software. We show that the website can increase its revenue by discriminating between regular and ad-block users via the ad-intensities shown to them. More interestingly, we find that the discriminatory power bestowed on the website by ad-blockers can increase the social surplus and, in particular, increase the surplus of both user segments, namely, regular users and ad-block users, when the utility from their outside option is below a threshold. Thus, the advent of ad-blockers can lead to a win-win for both the website and its users. Finally, we propose a superior selective-gating strategy in which only a fraction of ad-block users are gated. We establish the robustness of our conclusions under several enhancements to our base setting: (a) heterogeneous profitabilities from regular users and ad-block users, (b) endogenous adoption of ad-blockers, (c) the presence of a subscription option, and (d) negative externality due to increased traffic. Our analysis ends with recommendations for three stakeholders in this problem, namely, publishers, web-browser developers, and policy makers.\n",
      "Content: Ad-Blockers: A Blessing or a Curse? Users who have an ad-blocker installed present a genuine predicament for a website (also known as the publisher): On the one hand, these users do not generate revenue for the website; on the other hand, denying them access can shrink the user base and adversely affect the popularity of the website, ultimately reducing traffic over the long run. This has led some websites to require that ad-block users “white-list” them for obtaining access to an “ad-light” experience. We model the decision problem for a website facing two user segments: regular users and ad-block users. The first-level decision or gating strategy is whether to allow ad-free access to ad-block users or require them to white-list the website for gaining access. When ad-block users are allowed ad-free access, the second-level decision is the level of advertising (or ad-intensity) for regular users. When ad-block users are required to white-list, the second-level decisions are the ad-intensities for regular users and ad-block users. The net utility of a user from visiting the website depends on the intrinsic value of the website’s content, the value obtained due to network effects driven by the amount of traffic/popularity of the website, and the cost incurred due to the presence of ads. We derive an optimal gating and ad-intensity strategy for the website and also solve an identical model for a world without ad-block software. We show that the website can increase its revenue by discriminating between regular and ad-block users via the ad-intensities shown to them. More interestingly, we find that the discriminatory power bestowed on the website by ad-blockers can increase the social surplus and, in particular, increase the surplus of both user segments, namely, regular users and ad-block users, when the utility from their outside option is below a threshold. Thus, the advent of ad-blockers can lead to a win-win for both the website and its users. Finally, we propose a superior selective-gating strategy in which only a fraction of ad-block users are gated. We establish the robustness of our conclusions under several enhancements to our base setting: (a) heterogeneous profitabilities from regular users and ad-block users, (b) endogenous adoption of ad-blockers, (c) the presence of a subscription option, and (d) negative externality due to increased traffic. Our analysis ends with recommendations for three stakeholders in this problem, namely, publishers, web-browser developers, and policy makers.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 498\n",
      "Title: Assessing the benefits from e-business transformation through effective enterprise management\n",
      "Abstract: This paper reports on research carried out in 1999–2001 on the use of e-business applications in enterprise resource planning (ERP)-based organisations. Multiple structured interviews were used to collect data on 11 established organisations from a diverse range of industries. The findings are analysed according to the level of sophistication of e-business models and their transformational impact on the organisation. Early adopters of e-business show a trend towards cost reductions and administrative efficiencies from e-procurement and self-service applications used by customers and employees. More mature users focus on strategic advantage and generate this through an evolutionary model of organisational change. Two complex case studies of e-business integration with global suppliers and their corporate customers are analysed to identify specific stages of benefits accrual through the e-business transformation process. Collectively, the set of case studies is used to demonstrate the increased benefits derived from an e-business architecture based on a network of ERP-enabled organisations.\n",
      "Content: Assessing the benefits from e-business transformation through effective enterprise management This paper reports on research carried out in 1999–2001 on the use of e-business applications in enterprise resource planning (ERP)-based organisations. Multiple structured interviews were used to collect data on 11 established organisations from a diverse range of industries. The findings are analysed according to the level of sophistication of e-business models and their transformational impact on the organisation. Early adopters of e-business show a trend towards cost reductions and administrative efficiencies from e-procurement and self-service applications used by customers and employees. More mature users focus on strategic advantage and generate this through an evolutionary model of organisational change. Two complex case studies of e-business integration with global suppliers and their corporate customers are analysed to identify specific stages of benefits accrual through the e-business transformation process. Collectively, the set of case studies is used to demonstrate the increased benefits derived from an e-business architecture based on a network of ERP-enabled organisations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 499\n",
      "Title: Trust and technologies: Implications for organizational work practices\n",
      "Abstract: In this paper, we empirically investigate the concept of trust across organizational work practices by examining three groups: within the team, between teams and when interacting with technology. This study adopts Repertory Grid methodology as an interview based technique to elicit important constructs of trust to engineering teams working in two organizations within the energy distribution industry. Thirteen key constructs of trust were identified using content analysis. Drawing on the understanding gained, this paper discusses the implications for theories on trust within teams working with technology across organizations and provides a grounded perspective that could be used as a basis for further research.\n",
      "Content: Trust and technologies: Implications for organizational work practices In this paper, we empirically investigate the concept of trust across organizational work practices by examining three groups: within the team, between teams and when interacting with technology. This study adopts Repertory Grid methodology as an interview based technique to elicit important constructs of trust to engineering teams working in two organizations within the energy distribution industry. Thirteen key constructs of trust were identified using content analysis. Drawing on the understanding gained, this paper discusses the implications for theories on trust within teams working with technology across organizations and provides a grounded perspective that could be used as a basis for further research.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Document ID: 500\n",
      "Title: The ethical and social implications of personalization technologies for e-learning\n",
      "Abstract: Personalization in information systems can be considered beneficial but also ethically and socially harmful. Like many other technologies, the uptake of personalization has been rapid, with inadequate consideration given to its effects. Personalization in e-learning systems also has potential for both harmful and beneficial outcomes, but less is known about its effects. The ethical and social hazards include privacy compromise, lack of control, reduced individual capability, and the commodification of education. Personalization is appearing in many systems already; thus, these hazards may already be occurring. Solutions, more research and community discussion of the issues are needed.\n",
      "Content: The ethical and social implications of personalization technologies for e-learning Personalization in information systems can be considered beneficial but also ethically and socially harmful. Like many other technologies, the uptake of personalization has been rapid, with inadequate consideration given to its effects. Personalization in e-learning systems also has potential for both harmful and beneficial outcomes, but less is known about its effects. The ethical and social hazards include privacy compromise, lack of control, reduced individual capability, and the commodification of education. Personalization is appearing in many systems already; thus, these hazards may already be occurring. Solutions, more research and community discussion of the issues are needed.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(f\"Document ID: {doc.metadata['id']}\")\n",
    "    print(f\"Title: {doc.metadata['title']}\")\n",
    "    print(f\"Abstract: {doc.metadata['abstract']}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up our embedding model. We specify that the model we are using is sentence-transformers/paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining it all togeather and storing everything into vector database. Here we are setting up what gets embedded(documents), which model does the embedding(embedding_model) and where are we storing the vector database(persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=documents, \n",
    "                                 embedding=embedding_model,\n",
    "                                 persist_directory=persist_directory,\n",
    "                                 collection_name=\"title_abstract_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to created vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding_model,\n",
    "                  collection_name=\"title_abstract_chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting it up as an retriever, so the source of our information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining how many reults should the query take. This step is a test if we can just connect to the created vector database and use it as a retriever. In this part no LLM model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vectordb(query, top_k=1):\n",
    "    results = retriever.get_relevant_documents(query, k=top_k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"AD blockers\"\n",
    "results = query_vectordb(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Ad-Blockers: A Blessing or a Curse? Users who have an ad-blocker installed present a genuine predicament for a website (also known as the publisher): On the one hand, these users do not generate revenue for the website; on the other hand, denying them access can shrink the user base and adversely affect the popularity of the website, ultimately reducing traffic over the long run. This has led some websites to require that ad-block users “white-list” them for obtaining access to an “ad-light” experience. We model the decision problem for a website facing two user segments: regular users and ad-block users. The first-level decision or gating strategy is whether to allow ad-free access to ad-block users or require them to white-list the website for gaining access. When ad-block users are allowed ad-free access, the second-level decision is the level of advertising (or ad-intensity) for regular users. When ad-block users are required to white-list, the second-level decisions are the ad-intensities for regular users and ad-block users. The net utility of a user from visiting the website depends on the intrinsic value of the website’s content, the value obtained due to network effects driven by the amount of traffic/popularity of the website, and the cost incurred due to the presence of ads. We derive an optimal gating and ad-intensity strategy for the website and also solve an identical model for a world without ad-block software. We show that the website can increase its revenue by discriminating between regular and ad-block users via the ad-intensities shown to them. More interestingly, we find that the discriminatory power bestowed on the website by ad-blockers can increase the social surplus and, in particular, increase the surplus of both user segments, namely, regular users and ad-block users, when the utility from their outside option is below a threshold. Thus, the advent of ad-blockers can lead to a win-win for both the website and its users. Finally, we propose a superior selective-gating strategy in which only a fraction of ad-block users are gated. We establish the robustness of our conclusions under several enhancements to our base setting: (a) heterogeneous profitabilities from regular users and ad-block users, (b) endogenous adoption of ad-blockers, (c) the presence of a subscription option, and (d) negative externality due to increased traffic. Our analysis ends with recommendations for three stakeholders in this problem, namely, publishers, web-browser developers, and policy makers.', metadata={'abstract': 'Users who have an ad-blocker installed present a genuine predicament for a website (also known as the publisher): On the one hand, these users do not generate revenue for the website; on the other hand, denying them access can shrink the user base and adversely affect the popularity of the website, ultimately reducing traffic over the long run. This has led some websites to require that ad-block users “white-list” them for obtaining access to an “ad-light” experience. We model the decision problem for a website facing two user segments: regular users and ad-block users. The first-level decision or gating strategy is whether to allow ad-free access to ad-block users or require them to white-list the website for gaining access. When ad-block users are allowed ad-free access, the second-level decision is the level of advertising (or ad-intensity) for regular users. When ad-block users are required to white-list, the second-level decisions are the ad-intensities for regular users and ad-block users. The net utility of a user from visiting the website depends on the intrinsic value of the website’s content, the value obtained due to network effects driven by the amount of traffic/popularity of the website, and the cost incurred due to the presence of ads. We derive an optimal gating and ad-intensity strategy for the website and also solve an identical model for a world without ad-block software. We show that the website can increase its revenue by discriminating between regular and ad-block users via the ad-intensities shown to them. More interestingly, we find that the discriminatory power bestowed on the website by ad-blockers can increase the social surplus and, in particular, increase the surplus of both user segments, namely, regular users and ad-block users, when the utility from their outside option is below a threshold. Thus, the advent of ad-blockers can lead to a win-win for both the website and its users. Finally, we propose a superior selective-gating strategy in which only a fraction of ad-block users are gated. We establish the robustness of our conclusions under several enhancements to our base setting: (a) heterogeneous profitabilities from regular users and ad-block users, (b) endogenous adoption of ad-blockers, (c) the presence of a subscription option, and (d) negative externality due to increased traffic. Our analysis ends with recommendations for three stakeholders in this problem, namely, publishers, web-browser developers, and policy makers.', 'id': 497, 'title': 'Ad-Blockers: A Blessing or a Curse?'}), Document(page_content='Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.', metadata={'abstract': 'Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.', 'id': 408, 'title': 'Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation'}), Document(page_content='The Impact of Competing Ads on Click Performance in Sponsored Search Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.', metadata={'abstract': 'Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.', 'id': 104, 'title': 'The Impact of Competing Ads on Click Performance in Sponsored Search'}), Document(page_content='Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools’ warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the “what” but rather on operationalizing the “why” into salient detection cues.', metadata={'abstract': 'Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools’ warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the “what” but rather on operationalizing the “why” into salient detection cues.', 'id': 20, 'title': 'Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information'})]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
