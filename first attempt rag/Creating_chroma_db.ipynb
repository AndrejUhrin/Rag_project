{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to Duckdb and selecting onyl 3 columns with query also limiting the number of rows for 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = '../duck_db/isrecon_AIS11.duckdb'\n",
    "\n",
    "with duckdb.connect(database=db_path, read_only=True) as conn:\n",
    "    query = 'SELECT article_id, title, abstract FROM papers LIMIT 500'\n",
    "    df = conn.execute(query).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Examining interdependence between product user...</td>\n",
       "      <td>Firm-sponsored online user communities have be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Computer support for strategic organizational ...</td>\n",
       "      <td>While information systems continue to be promo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Essence: facilitating software innovation</td>\n",
       "      <td>This paper suggests ways to facilitate creativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The dark side of data ecosystems: A longitudin...</td>\n",
       "      <td>Data are often vividly depicted as strategic a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Symbolic Action Research in Information System...</td>\n",
       "      <td>An essay is presented as an introduction to th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  \\\n",
       "0           1  Examining interdependence between product user...   \n",
       "1           2  Computer support for strategic organizational ...   \n",
       "2           3          Essence: facilitating software innovation   \n",
       "3           4  The dark side of data ecosystems: A longitudin...   \n",
       "4           5  Symbolic Action Research in Information System...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Firm-sponsored online user communities have be...  \n",
       "1  While information systems continue to be promo...  \n",
       "2  This paper suggests ways to facilitate creativ...  \n",
       "3  Data are often vividly depicted as strategic a...  \n",
       "4  An essay is presented as an introduction to th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate columns together into a list. We do this beacuse in the vector database we cannot store the vectores in column, meaning it does not have tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = (df['title'] + ' ' + df['abstract']).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a persist direcotry here will be the vector database stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = '../first attempt rag/chroma_db'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null values, they cause errors when we are creating vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None value found for abstract in row with article_id=113 and title=Editors' Preface\n",
      "None value found for abstract in row with article_id=124 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=125 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=127 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=128 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=129 and title=Editorial Notes\n",
      "None value found for abstract in row with article_id=247 and title=Letting living intelligence put the artificial version in its place\n",
      "None value found for abstract in row with article_id=249 and title=An introduction to qualitative research\n",
      "None value found for abstract in row with article_id=250 and title=Software process improvement: Concepts and practices\n",
      "None value found for abstract in row with article_id=251 and title=Handbook of Action Research Participative Inquiry and Practice\n",
      "None value found for abstract in row with article_id=252 and title=Managing Industrial Knowledge; Creation, Transfer and Utilization\n",
      "None value found for abstract in row with article_id=269 and title=Some systems implications of EU Data Protection Directive\n",
      "None value found for abstract in row with article_id=299 and title=Oracle8i Data Warehousing\n",
      "None value found for abstract in row with article_id=347 and title=An economic view of information systems\n",
      "None value found for abstract in row with article_id=394 and title=A comment on the intellectual structures of information systems development\n",
      "None value found for abstract in row with article_id=419 and title=Managing uncertainty in decision support models foreword to the special issue\n",
      "None value found for abstract in row with article_id=445 and title=Preface\n",
      "None value found for abstract in row with article_id=478 and title=Risk and the future of AI: Algorithmic bias, data colonialism, and marginalization\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.iterrows():\n",
    "    article_id = row['article_id']\n",
    "    title = row['title']\n",
    "    abstract = row['abstract']\n",
    "    \n",
    "    if article_id is None:\n",
    "        print(f\"None value found for article_id in row with title={title} and abstract={abstract}\")\n",
    "    if title is None:\n",
    "        print(f\"None value found for title in row with article_id={article_id} and abstract={abstract}\")\n",
    "    if abstract is None:\n",
    "        print(f\"None value found for abstract in row with article_id={article_id} and title={title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_id'].fillna('Unknown article_id', inplace=True)\n",
    "df['title'].fillna('No title available', inplace=True)\n",
    "df['abstract'].fillna('No abstract available', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(THIS PART I HAVE TO CHECK NOT SURE IF IT IS WORKING PROPERLY)\n",
    "Creating document object. Page content is the concatenated text and we add the metadata for improved similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    Document(page_content=text, metadata={'id': row['article_id'], 'title': row['title'], 'abstract': row['abstract']})\n",
    "    for text, (_, row) in zip(texts, df.iterrows())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up our embedding model. We specify that the model we are using is sentence-transformers/paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining it all togeather and storing everything into vector database. Here we are setting up what gets embedded(documents), which model does the embedding(embedding_model) and where are we storing the vector database(persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=documents, \n",
    "                                 embedding=embedding_model,\n",
    "                                 persist_directory=persist_directory,\n",
    "                                 collection_name=\"title_abstract_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to created vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding_model,\n",
    "                  collection_name=\"title_abstract_chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting it up as an retriever, so the source of our information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining how many reults should the query take. This step is a test if we can just connect to the created vector database and use it as a retriever. In this part no LLM model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_vectordb(query, top_k=1):\n",
    "    results = retriever.get_relevant_documents(query, k=top_k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "query = \"AD blockers\"\n",
    "results = query_vectordb(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Ad-Blockers: A Blessing or a Curse? Users who have an ad-blocker installed present a genuine predicament for a website (also known as the publisher): On the one hand, these users do not generate revenue for the website; on the other hand, denying them access can shrink the user base and adversely affect the popularity of the website, ultimately reducing traffic over the long run. This has led some websites to require that ad-block users “white-list” them for obtaining access to an “ad-light” experience. We model the decision problem for a website facing two user segments: regular users and ad-block users. The first-level decision or gating strategy is whether to allow ad-free access to ad-block users or require them to white-list the website for gaining access. When ad-block users are allowed ad-free access, the second-level decision is the level of advertising (or ad-intensity) for regular users. When ad-block users are required to white-list, the second-level decisions are the ad-intensities for regular users and ad-block users. The net utility of a user from visiting the website depends on the intrinsic value of the website’s content, the value obtained due to network effects driven by the amount of traffic/popularity of the website, and the cost incurred due to the presence of ads. We derive an optimal gating and ad-intensity strategy for the website and also solve an identical model for a world without ad-block software. We show that the website can increase its revenue by discriminating between regular and ad-block users via the ad-intensities shown to them. More interestingly, we find that the discriminatory power bestowed on the website by ad-blockers can increase the social surplus and, in particular, increase the surplus of both user segments, namely, regular users and ad-block users, when the utility from their outside option is below a threshold. Thus, the advent of ad-blockers can lead to a win-win for both the website and its users. Finally, we propose a superior selective-gating strategy in which only a fraction of ad-block users are gated. We establish the robustness of our conclusions under several enhancements to our base setting: (a) heterogeneous profitabilities from regular users and ad-block users, (b) endogenous adoption of ad-blockers, (c) the presence of a subscription option, and (d) negative externality due to increased traffic. Our analysis ends with recommendations for three stakeholders in this problem, namely, publishers, web-browser developers, and policy makers.', metadata={'abstract': 'Users who have an ad-blocker installed present a genuine predicament for a website (also known as the publisher): On the one hand, these users do not generate revenue for the website; on the other hand, denying them access can shrink the user base and adversely affect the popularity of the website, ultimately reducing traffic over the long run. This has led some websites to require that ad-block users “white-list” them for obtaining access to an “ad-light” experience. We model the decision problem for a website facing two user segments: regular users and ad-block users. The first-level decision or gating strategy is whether to allow ad-free access to ad-block users or require them to white-list the website for gaining access. When ad-block users are allowed ad-free access, the second-level decision is the level of advertising (or ad-intensity) for regular users. When ad-block users are required to white-list, the second-level decisions are the ad-intensities for regular users and ad-block users. The net utility of a user from visiting the website depends on the intrinsic value of the website’s content, the value obtained due to network effects driven by the amount of traffic/popularity of the website, and the cost incurred due to the presence of ads. We derive an optimal gating and ad-intensity strategy for the website and also solve an identical model for a world without ad-block software. We show that the website can increase its revenue by discriminating between regular and ad-block users via the ad-intensities shown to them. More interestingly, we find that the discriminatory power bestowed on the website by ad-blockers can increase the social surplus and, in particular, increase the surplus of both user segments, namely, regular users and ad-block users, when the utility from their outside option is below a threshold. Thus, the advent of ad-blockers can lead to a win-win for both the website and its users. Finally, we propose a superior selective-gating strategy in which only a fraction of ad-block users are gated. We establish the robustness of our conclusions under several enhancements to our base setting: (a) heterogeneous profitabilities from regular users and ad-block users, (b) endogenous adoption of ad-blockers, (c) the presence of a subscription option, and (d) negative externality due to increased traffic. Our analysis ends with recommendations for three stakeholders in this problem, namely, publishers, web-browser developers, and policy makers.', 'id': 497, 'title': 'Ad-Blockers: A Blessing or a Curse?'}), Document(page_content='Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.', metadata={'abstract': 'Online sponsored search advertising has emerged as the dominant online advertising format largely because of their pay-for-performance nature, wherein advertising expenditures are closely tied to outcomes. While the pay-for-performance format substantially reduces the wastage incurred by advertisers compared to traditional pay-per-exposure advertising formats, the reduction of such wastage also carries the risk of reducing the signaling properties of advertising. Lacking a separating equilibrium, low-quality firms in these markets may be able to mimic the advertising strategies of high-quality firms. This study examines this issue in the context of online sponsored search markets. Using data gathered from sponsored search auctions for keywords in a market without intervention by the intermediary, we find evidence of adverse selection for products/services characterized by high uncertainty. On the other hand, there is no evidence of adverse selection for similar products in a regulated sponsored search market, suggesting that intervention by the search intermediary can have a significant impact on market outcomes and consumer welfare.', 'id': 408, 'title': 'Quality Uncertainty and the Performance of Online Sponsored Search Markets: An Empirical Investigation'}), Document(page_content='The Impact of Competing Ads on Click Performance in Sponsored Search Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.', metadata={'abstract': 'Our research examines the impact of competing ads on click performance of an ad in sponsored search. We use a unique data set of 1,267 advertiser keyword pairs with differing ad quality related to 360 keywords from a search engine to evaluate the click performance. We find that competing high-quality ads, appearing above the focal ad, have a lower negative effect on the click performance as compared to competing low-quality ads. We also find that this effect of competing ads varies with the ad position and the type of keyword. In general, the negative effect of competing high-quality ads decreases at low positions as compared to high positions. Furthermore, this decrease in the negative effect of competing high-quality ads is more substantial for specific keywords. Our results reveal consumer behavior in evaluating different quality ads in sponsored search. More specifically, our results suggest that consumers use the presence of high-quality competing ads as a signal of higher quality of the focal ad. Our findings can help advertisers better evaluate their relative performance for different positions for various types of keywords. This can also help evaluate the efficacy of the auction design mechanism.', 'id': 104, 'title': 'The Impact of Competing Ads on Click Performance in Sponsored Search'}), Document(page_content='Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools’ warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the “what” but rather on operationalizing the “why” into salient detection cues.', metadata={'abstract': 'Phishing websites continue to successfully exploit user vulnerabilities in household and enterprise settings. Existing anti-phishing tools lack the accuracy and generalizability needed to protect Internet users and organizations from the myriad of attacks encountered daily. Consequently, users often disregard these tools’ warnings. In this study, using a design science approach, we propose a novel method for detecting phishing websites. By adopting a genre theoretic perspective, the proposed genre tree kernel method utilizes fraud cues that are associated with differences in purpose between legitimate and phishing websites, manifested through genre composition and design structure, resulting in enhanced anti-phishing capabilities. To evaluate the genre tree kernel method, a series of experiments were conducted on a testbed encompassing thousands of legitimate and phishing websites. The results revealed that the proposed method provided significantly better detection capabilities than state-of-the-art anti-phishing methods. An additional experiment demonstrated the effectiveness of the genre tree kernel technique in user settings; users utilizing the method were able to better identify and avoid phishing websites, and were consequently less likely to transact with them. Given the extensive monetary and social ramifications associated with phishing, the results have important implications for future anti-phishing strategies. More broadly, the results underscore the importance of considering intention/purpose as a critical dimension for automated credibility assessment: focusing not only on the “what” but rather on operationalizing the “why” into salient detection cues.', 'id': 20, 'title': 'Enhancing Predictive Analytics for Anti-Phishing by Exploiting Website Genre Information'})]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
